[2024-12-17T07:43:22.642+0530] {processor.py:186} INFO - Started process (PID=32550) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T07:43:22.643+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-17T07:43:22.645+0530] {logging_mixin.py:190} INFO - [2024-12-17T07:43:22.645+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T07:43:22.751+0530] {processor.py:925} INFO - DAG(s) 'movie_pipeline' retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T07:43:22.872+0530] {logging_mixin.py:190} INFO - [2024-12-17T07:43:22.872+0530] {dag.py:3239} INFO - Sync 1 DAGs
[2024-12-17T07:43:22.902+0530] {logging_mixin.py:190} INFO - [2024-12-17T07:43:22.902+0530] {dag.py:4180} INFO - Setting next_dagrun for movie_pipeline to 2024-12-17 01:30:00+00:00, run_after=2024-12-18 01:30:00+00:00
[2024-12-17T07:43:22.925+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 0.290 seconds
[2024-12-17T07:46:39.663+0530] {processor.py:186} INFO - Started process (PID=32885) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T07:46:39.666+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-17T07:46:39.667+0530] {logging_mixin.py:190} INFO - [2024-12-17T07:46:39.667+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T07:46:39.710+0530] {processor.py:925} INFO - DAG(s) 'movie_pipeline' retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T07:46:39.842+0530] {logging_mixin.py:190} INFO - [2024-12-17T07:46:39.842+0530] {dag.py:3239} INFO - Sync 1 DAGs
[2024-12-17T07:46:39.862+0530] {logging_mixin.py:190} INFO - [2024-12-17T07:46:39.862+0530] {dag.py:4180} INFO - Setting next_dagrun for movie_pipeline to 2024-12-17 01:30:00+00:00, run_after=2024-12-18 01:30:00+00:00
[2024-12-17T07:46:39.884+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 0.224 seconds
[2024-12-17T07:49:15.029+0530] {processor.py:186} INFO - Started process (PID=33171) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T07:49:15.031+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-17T07:49:15.033+0530] {logging_mixin.py:190} INFO - [2024-12-17T07:49:15.033+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T07:49:15.144+0530] {logging_mixin.py:190} INFO - [2024-12-17T07:49:15.134+0530] {dagbag.py:387} ERROR - Failed to import: /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
Traceback (most recent call last):
  File "/Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/.venv/lib/python3.12/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py", line 16, in <module>
    t1= BashOperator(
        ^^^^^^^^^^^^^
  File "/Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/.venv/lib/python3.12/site-packages/airflow/models/baseoperator.py", line 501, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/.venv/lib/python3.12/site-packages/airflow/operators/bash.py", line 166, in __init__
    super().__init__(**kwargs)
  File "/Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/.venv/lib/python3.12/site-packages/airflow/models/baseoperator.py", line 501, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/.venv/lib/python3.12/site-packages/airflow/models/baseoperator.py", line 960, in __init__
    validate_key(task_id)
  File "/Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/.venv/lib/python3.12/site-packages/airflow/utils/helpers.py", line 57, in validate_key
    raise AirflowException(
airflow.exceptions.AirflowException: The key 'logical date render' has to be made of alphanumeric characters, dashes, dots and underscores exclusively
[2024-12-17T07:49:15.146+0530] {processor.py:927} WARNING - No viable dags retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T07:49:15.268+0530] {logging_mixin.py:190} INFO - [2024-12-17T07:49:15.267+0530] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-12-17T07:49:15.295+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 0.270 seconds
[2024-12-17T07:49:49.867+0530] {processor.py:186} INFO - Started process (PID=33229) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T07:49:49.870+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-17T07:49:49.871+0530] {logging_mixin.py:190} INFO - [2024-12-17T07:49:49.871+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T07:49:49.979+0530] {logging_mixin.py:190} INFO - [2024-12-17T07:49:49.968+0530] {dagbag.py:387} ERROR - Failed to import: /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
Traceback (most recent call last):
  File "/Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/.venv/lib/python3.12/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py", line 16, in <module>
    t1= BashOperator(
        ^^^^^^^^^^^^^
  File "/Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/.venv/lib/python3.12/site-packages/airflow/models/baseoperator.py", line 501, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/.venv/lib/python3.12/site-packages/airflow/operators/bash.py", line 166, in __init__
    super().__init__(**kwargs)
  File "/Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/.venv/lib/python3.12/site-packages/airflow/models/baseoperator.py", line 501, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/.venv/lib/python3.12/site-packages/airflow/models/baseoperator.py", line 960, in __init__
    validate_key(task_id)
  File "/Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/.venv/lib/python3.12/site-packages/airflow/utils/helpers.py", line 57, in validate_key
    raise AirflowException(
airflow.exceptions.AirflowException: The key 'logical date render' has to be made of alphanumeric characters, dashes, dots and underscores exclusively
[2024-12-17T07:49:49.980+0530] {processor.py:927} WARNING - No viable dags retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T07:49:50.128+0530] {logging_mixin.py:190} INFO - [2024-12-17T07:49:50.128+0530] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-12-17T07:49:50.160+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 0.299 seconds
[2024-12-17T07:51:43.457+0530] {processor.py:186} INFO - Started process (PID=33440) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T07:51:43.459+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-17T07:51:43.461+0530] {logging_mixin.py:190} INFO - [2024-12-17T07:51:43.460+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T07:51:43.505+0530] {processor.py:925} INFO - DAG(s) 'movie_pipeline' retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T07:51:44.044+0530] {logging_mixin.py:190} INFO - [2024-12-17T07:51:44.044+0530] {dag.py:3239} INFO - Sync 1 DAGs
[2024-12-17T07:51:44.179+0530] {logging_mixin.py:190} INFO - [2024-12-17T07:51:44.178+0530] {dag.py:4180} INFO - Setting next_dagrun for movie_pipeline to 2024-12-17 01:30:00+00:00, run_after=2024-12-18 01:30:00+00:00
[2024-12-17T07:51:44.236+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 0.783 seconds
[2024-12-17T07:53:23.408+0530] {processor.py:186} INFO - Started process (PID=33584) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T07:53:23.410+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-17T07:53:23.411+0530] {logging_mixin.py:190} INFO - [2024-12-17T07:53:23.411+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T07:53:23.458+0530] {processor.py:925} INFO - DAG(s) 'movie_pipeline' retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T07:53:23.769+0530] {logging_mixin.py:190} INFO - [2024-12-17T07:53:23.769+0530] {dag.py:3239} INFO - Sync 1 DAGs
[2024-12-17T07:53:23.791+0530] {logging_mixin.py:190} INFO - [2024-12-17T07:53:23.791+0530] {dag.py:4180} INFO - Setting next_dagrun for movie_pipeline to 2024-12-17 01:30:00+00:00, run_after=2024-12-18 01:30:00+00:00
[2024-12-17T07:53:23.815+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 0.411 seconds
[2024-12-17T07:55:59.025+0530] {processor.py:186} INFO - Started process (PID=33802) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T07:55:59.027+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-17T07:55:59.030+0530] {logging_mixin.py:190} INFO - [2024-12-17T07:55:59.029+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T07:55:59.087+0530] {processor.py:925} INFO - DAG(s) 'movie_pipeline' retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T07:55:59.657+0530] {logging_mixin.py:190} INFO - [2024-12-17T07:55:59.657+0530] {dag.py:3239} INFO - Sync 1 DAGs
[2024-12-17T07:55:59.685+0530] {logging_mixin.py:190} INFO - [2024-12-17T07:55:59.685+0530] {dag.py:4180} INFO - Setting next_dagrun for movie_pipeline to 2024-12-17 01:30:00+00:00, run_after=2024-12-18 01:30:00+00:00
[2024-12-17T07:55:59.711+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 0.691 seconds
[2024-12-17T07:57:06.435+0530] {processor.py:186} INFO - Started process (PID=33911) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T07:57:06.437+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-17T07:57:06.439+0530] {logging_mixin.py:190} INFO - [2024-12-17T07:57:06.439+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T07:57:06.482+0530] {processor.py:925} INFO - DAG(s) 'movie_pipeline' retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T07:57:06.949+0530] {logging_mixin.py:190} INFO - [2024-12-17T07:57:06.948+0530] {dag.py:3239} INFO - Sync 1 DAGs
[2024-12-17T07:57:06.975+0530] {logging_mixin.py:190} INFO - [2024-12-17T07:57:06.975+0530] {dag.py:4180} INFO - Setting next_dagrun for movie_pipeline to 2024-12-17 01:30:00+00:00, run_after=2024-12-18 01:30:00+00:00
[2024-12-17T07:57:07.010+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 0.579 seconds
[2024-12-17T08:00:30.291+0530] {processor.py:186} INFO - Started process (PID=34316) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T08:00:30.294+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-17T08:00:30.298+0530] {logging_mixin.py:190} INFO - [2024-12-17T08:00:30.296+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T08:00:30.350+0530] {processor.py:925} INFO - DAG(s) 'movie_pipeline' retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T08:00:30.944+0530] {logging_mixin.py:190} INFO - [2024-12-17T08:00:30.943+0530] {dag.py:3239} INFO - Sync 1 DAGs
[2024-12-17T08:00:30.998+0530] {logging_mixin.py:190} INFO - [2024-12-17T08:00:30.997+0530] {dag.py:4180} INFO - Setting next_dagrun for movie_pipeline to 2024-12-17 01:30:00+00:00, run_after=2024-12-18 01:30:00+00:00
[2024-12-17T08:00:31.137+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 0.850 seconds
[2024-12-17T08:03:32.357+0530] {processor.py:186} INFO - Started process (PID=34601) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T08:03:32.358+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-17T08:03:32.359+0530] {logging_mixin.py:190} INFO - [2024-12-17T08:03:32.359+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T08:03:32.401+0530] {processor.py:925} INFO - DAG(s) 'movie_pipeline' retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T08:03:32.693+0530] {logging_mixin.py:190} INFO - [2024-12-17T08:03:32.693+0530] {dag.py:3239} INFO - Sync 1 DAGs
[2024-12-17T08:03:32.713+0530] {logging_mixin.py:190} INFO - [2024-12-17T08:03:32.713+0530] {dag.py:4180} INFO - Setting next_dagrun for movie_pipeline to 2024-12-17 01:30:00+00:00, run_after=2024-12-18 01:30:00+00:00
[2024-12-17T08:03:32.737+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 0.384 seconds
[2024-12-17T08:06:47.203+0530] {processor.py:186} INFO - Started process (PID=34959) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T08:06:47.205+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-17T08:06:47.207+0530] {logging_mixin.py:190} INFO - [2024-12-17T08:06:47.206+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T08:06:47.252+0530] {processor.py:925} INFO - DAG(s) 'movie_pipeline' retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T08:06:47.537+0530] {logging_mixin.py:190} INFO - [2024-12-17T08:06:47.536+0530] {dag.py:3239} INFO - Sync 1 DAGs
[2024-12-17T08:06:47.560+0530] {logging_mixin.py:190} INFO - [2024-12-17T08:06:47.560+0530] {dag.py:4180} INFO - Setting next_dagrun for movie_pipeline to 2024-12-17 01:30:00+00:00, run_after=2024-12-18 01:30:00+00:00
[2024-12-17T08:06:47.583+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 0.384 seconds
[2024-12-17T08:10:06.697+0530] {processor.py:186} INFO - Started process (PID=35349) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T08:10:06.699+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-17T08:10:06.701+0530] {logging_mixin.py:190} INFO - [2024-12-17T08:10:06.701+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T08:10:06.743+0530] {processor.py:925} INFO - DAG(s) 'movie_pipeline' retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T08:10:07.246+0530] {logging_mixin.py:190} INFO - [2024-12-17T08:10:07.246+0530] {dag.py:3239} INFO - Sync 1 DAGs
[2024-12-17T08:10:07.266+0530] {logging_mixin.py:190} INFO - [2024-12-17T08:10:07.266+0530] {dag.py:4180} INFO - Setting next_dagrun for movie_pipeline to 2024-12-17 01:30:00+00:00, run_after=2024-12-18 01:30:00+00:00
[2024-12-17T08:10:07.289+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 0.596 seconds
[2024-12-17T08:12:52.941+0530] {processor.py:186} INFO - Started process (PID=35684) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T08:12:52.942+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-17T08:12:52.943+0530] {logging_mixin.py:190} INFO - [2024-12-17T08:12:52.943+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T08:12:52.985+0530] {processor.py:925} INFO - DAG(s) 'movie_pipeline' retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T08:12:53.115+0530] {logging_mixin.py:190} INFO - [2024-12-17T08:12:53.115+0530] {dag.py:3239} INFO - Sync 1 DAGs
[2024-12-17T08:12:53.134+0530] {logging_mixin.py:190} INFO - [2024-12-17T08:12:53.134+0530] {dag.py:4180} INFO - Setting next_dagrun for movie_pipeline to 2024-12-17 01:30:00+00:00, run_after=2024-12-18 01:30:00+00:00
[2024-12-17T08:12:53.158+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 0.221 seconds
[2024-12-17T08:15:54.743+0530] {processor.py:186} INFO - Started process (PID=36046) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T08:15:54.746+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-17T08:15:54.747+0530] {logging_mixin.py:190} INFO - [2024-12-17T08:15:54.747+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T08:15:54.753+0530] {logging_mixin.py:190} INFO - [2024-12-17T08:15:54.750+0530] {dagbag.py:387} ERROR - Failed to import: /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
Traceback (most recent call last):
  File "/Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/.venv/lib/python3.12/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 991, in exec_module
  File "<frozen importlib._bootstrap_external>", line 1129, in get_code
  File "<frozen importlib._bootstrap_external>", line 1059, in source_to_code
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py", line 17
    
    ^
IndentationError: unexpected unindent
[2024-12-17T08:15:54.754+0530] {processor.py:927} WARNING - No viable dags retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T08:15:54.891+0530] {logging_mixin.py:190} INFO - [2024-12-17T08:15:54.890+0530] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-12-17T08:15:54.923+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 0.184 seconds
[2024-12-17T08:18:50.881+0530] {processor.py:186} INFO - Started process (PID=36391) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T08:18:50.884+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-17T08:18:50.886+0530] {logging_mixin.py:190} INFO - [2024-12-17T08:18:50.885+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T08:18:50.894+0530] {logging_mixin.py:190} INFO - [2024-12-17T08:18:50.891+0530] {dagbag.py:387} ERROR - Failed to import: /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
Traceback (most recent call last):
  File "/Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/.venv/lib/python3.12/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 991, in exec_module
  File "<frozen importlib._bootstrap_external>", line 1129, in get_code
  File "<frozen importlib._bootstrap_external>", line 1059, in source_to_code
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py", line 17
    
    ^
IndentationError: unexpected unindent
[2024-12-17T08:18:50.895+0530] {processor.py:927} WARNING - No viable dags retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T08:18:51.030+0530] {logging_mixin.py:190} INFO - [2024-12-17T08:18:51.030+0530] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-12-17T08:18:51.060+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 0.182 seconds
[2024-12-17T08:21:46.067+0530] {processor.py:186} INFO - Started process (PID=36743) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T08:21:46.069+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-17T08:21:46.070+0530] {logging_mixin.py:190} INFO - [2024-12-17T08:21:46.070+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T08:21:46.076+0530] {logging_mixin.py:190} INFO - [2024-12-17T08:21:46.073+0530] {dagbag.py:387} ERROR - Failed to import: /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
Traceback (most recent call last):
  File "/Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/.venv/lib/python3.12/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 991, in exec_module
  File "<frozen importlib._bootstrap_external>", line 1129, in get_code
  File "<frozen importlib._bootstrap_external>", line 1059, in source_to_code
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py", line 17
    
    ^
IndentationError: unexpected unindent
[2024-12-17T08:21:46.077+0530] {processor.py:927} WARNING - No viable dags retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T08:21:46.213+0530] {logging_mixin.py:190} INFO - [2024-12-17T08:21:46.213+0530] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-12-17T08:21:46.240+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 0.177 seconds
[2024-12-17T08:24:37.034+0530] {processor.py:186} INFO - Started process (PID=37096) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T08:24:37.037+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-17T08:24:37.039+0530] {logging_mixin.py:190} INFO - [2024-12-17T08:24:37.038+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T08:24:37.045+0530] {logging_mixin.py:190} INFO - [2024-12-17T08:24:37.042+0530] {dagbag.py:387} ERROR - Failed to import: /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
Traceback (most recent call last):
  File "/Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/.venv/lib/python3.12/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 991, in exec_module
  File "<frozen importlib._bootstrap_external>", line 1129, in get_code
  File "<frozen importlib._bootstrap_external>", line 1059, in source_to_code
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py", line 17
    
    ^
IndentationError: unexpected unindent
[2024-12-17T08:24:37.046+0530] {processor.py:927} WARNING - No viable dags retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T08:24:37.188+0530] {logging_mixin.py:190} INFO - [2024-12-17T08:24:37.187+0530] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-12-17T08:24:37.216+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 0.188 seconds
[2024-12-17T08:27:26.773+0530] {processor.py:186} INFO - Started process (PID=37444) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T08:27:26.775+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-17T08:27:26.777+0530] {logging_mixin.py:190} INFO - [2024-12-17T08:27:26.777+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T08:27:26.784+0530] {logging_mixin.py:190} INFO - [2024-12-17T08:27:26.781+0530] {dagbag.py:387} ERROR - Failed to import: /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
Traceback (most recent call last):
  File "/Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/.venv/lib/python3.12/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 991, in exec_module
  File "<frozen importlib._bootstrap_external>", line 1129, in get_code
  File "<frozen importlib._bootstrap_external>", line 1059, in source_to_code
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py", line 17
    
    ^
IndentationError: unexpected unindent
[2024-12-17T08:27:26.785+0530] {processor.py:927} WARNING - No viable dags retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T08:27:26.928+0530] {logging_mixin.py:190} INFO - [2024-12-17T08:27:26.927+0530] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-12-17T08:27:26.955+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 0.186 seconds
[2024-12-17T08:30:17.571+0530] {processor.py:186} INFO - Started process (PID=37777) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T08:30:17.573+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-17T08:30:17.575+0530] {logging_mixin.py:190} INFO - [2024-12-17T08:30:17.574+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T08:30:17.581+0530] {logging_mixin.py:190} INFO - [2024-12-17T08:30:17.578+0530] {dagbag.py:387} ERROR - Failed to import: /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
Traceback (most recent call last):
  File "/Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/.venv/lib/python3.12/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 991, in exec_module
  File "<frozen importlib._bootstrap_external>", line 1129, in get_code
  File "<frozen importlib._bootstrap_external>", line 1059, in source_to_code
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py", line 17
    
    ^
IndentationError: unexpected unindent
[2024-12-17T08:30:17.582+0530] {processor.py:927} WARNING - No viable dags retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T08:30:17.715+0530] {logging_mixin.py:190} INFO - [2024-12-17T08:30:17.715+0530] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-12-17T08:30:17.742+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 0.175 seconds
[2024-12-17T08:33:08.887+0530] {processor.py:186} INFO - Started process (PID=38117) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T08:33:08.889+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-17T08:33:08.891+0530] {logging_mixin.py:190} INFO - [2024-12-17T08:33:08.891+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T08:33:08.897+0530] {logging_mixin.py:190} INFO - [2024-12-17T08:33:08.894+0530] {dagbag.py:387} ERROR - Failed to import: /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
Traceback (most recent call last):
  File "/Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/.venv/lib/python3.12/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 991, in exec_module
  File "<frozen importlib._bootstrap_external>", line 1129, in get_code
  File "<frozen importlib._bootstrap_external>", line 1059, in source_to_code
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py", line 17
    
    ^
IndentationError: unexpected unindent
[2024-12-17T08:33:08.897+0530] {processor.py:927} WARNING - No viable dags retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T08:33:09.043+0530] {logging_mixin.py:190} INFO - [2024-12-17T08:33:09.043+0530] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-12-17T08:33:09.077+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 0.193 seconds
[2024-12-17T08:36:01.537+0530] {processor.py:186} INFO - Started process (PID=38461) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T08:36:01.539+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-17T08:36:01.541+0530] {logging_mixin.py:190} INFO - [2024-12-17T08:36:01.541+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T08:36:01.547+0530] {logging_mixin.py:190} INFO - [2024-12-17T08:36:01.544+0530] {dagbag.py:387} ERROR - Failed to import: /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
Traceback (most recent call last):
  File "/Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/.venv/lib/python3.12/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 991, in exec_module
  File "<frozen importlib._bootstrap_external>", line 1129, in get_code
  File "<frozen importlib._bootstrap_external>", line 1059, in source_to_code
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py", line 17
    
    ^
IndentationError: unexpected unindent
[2024-12-17T08:36:01.548+0530] {processor.py:927} WARNING - No viable dags retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T08:36:01.683+0530] {logging_mixin.py:190} INFO - [2024-12-17T08:36:01.682+0530] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-12-17T08:36:01.709+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 0.176 seconds
[2024-12-17T08:39:00.124+0530] {processor.py:186} INFO - Started process (PID=38746) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T08:39:00.127+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-17T08:39:00.128+0530] {logging_mixin.py:190} INFO - [2024-12-17T08:39:00.128+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T08:39:00.135+0530] {logging_mixin.py:190} INFO - [2024-12-17T08:39:00.131+0530] {dagbag.py:387} ERROR - Failed to import: /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
Traceback (most recent call last):
  File "/Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/.venv/lib/python3.12/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 991, in exec_module
  File "<frozen importlib._bootstrap_external>", line 1129, in get_code
  File "<frozen importlib._bootstrap_external>", line 1059, in source_to_code
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py", line 17
    
    ^
IndentationError: unexpected unindent
[2024-12-17T08:39:00.135+0530] {processor.py:927} WARNING - No viable dags retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T08:39:00.278+0530] {logging_mixin.py:190} INFO - [2024-12-17T08:39:00.278+0530] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-12-17T08:39:00.307+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 0.187 seconds
[2024-12-17T08:41:52.133+0530] {processor.py:186} INFO - Started process (PID=38966) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T08:41:52.136+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-17T08:41:52.138+0530] {logging_mixin.py:190} INFO - [2024-12-17T08:41:52.137+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T08:41:52.142+0530] {logging_mixin.py:190} INFO - [2024-12-17T08:41:52.141+0530] {dagbag.py:387} ERROR - Failed to import: /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
Traceback (most recent call last):
  File "/Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/.venv/lib/python3.12/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 991, in exec_module
  File "<frozen importlib._bootstrap_external>", line 1129, in get_code
  File "<frozen importlib._bootstrap_external>", line 1059, in source_to_code
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py", line 17
    
    ^
IndentationError: unexpected unindent
[2024-12-17T08:41:52.143+0530] {processor.py:927} WARNING - No viable dags retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T08:41:52.293+0530] {logging_mixin.py:190} INFO - [2024-12-17T08:41:52.293+0530] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-12-17T08:41:52.322+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 0.193 seconds
[2024-12-17T08:44:36.974+0530] {processor.py:186} INFO - Started process (PID=39175) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T08:44:36.975+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-17T08:44:36.977+0530] {logging_mixin.py:190} INFO - [2024-12-17T08:44:36.976+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T08:44:36.980+0530] {logging_mixin.py:190} INFO - [2024-12-17T08:44:36.978+0530] {dagbag.py:387} ERROR - Failed to import: /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
Traceback (most recent call last):
  File "/Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/.venv/lib/python3.12/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 991, in exec_module
  File "<frozen importlib._bootstrap_external>", line 1129, in get_code
  File "<frozen importlib._bootstrap_external>", line 1059, in source_to_code
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py", line 17
    
    ^
IndentationError: unexpected unindent
[2024-12-17T08:44:36.980+0530] {processor.py:927} WARNING - No viable dags retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T08:44:37.138+0530] {logging_mixin.py:190} INFO - [2024-12-17T08:44:37.137+0530] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-12-17T08:44:37.162+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 0.194 seconds
[2024-12-17T08:47:28.809+0530] {processor.py:186} INFO - Started process (PID=39371) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T08:47:28.811+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-17T08:47:28.814+0530] {logging_mixin.py:190} INFO - [2024-12-17T08:47:28.813+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T08:47:28.819+0530] {logging_mixin.py:190} INFO - [2024-12-17T08:47:28.817+0530] {dagbag.py:387} ERROR - Failed to import: /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
Traceback (most recent call last):
  File "/Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/.venv/lib/python3.12/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 991, in exec_module
  File "<frozen importlib._bootstrap_external>", line 1129, in get_code
  File "<frozen importlib._bootstrap_external>", line 1059, in source_to_code
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py", line 17
    
    ^
IndentationError: unexpected unindent
[2024-12-17T08:47:28.819+0530] {processor.py:927} WARNING - No viable dags retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T08:47:28.969+0530] {logging_mixin.py:190} INFO - [2024-12-17T08:47:28.968+0530] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-12-17T08:47:28.996+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 0.191 seconds
[2024-12-17T08:50:24.479+0530] {processor.py:186} INFO - Started process (PID=39609) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T08:50:24.482+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-17T08:50:24.483+0530] {logging_mixin.py:190} INFO - [2024-12-17T08:50:24.483+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T08:50:24.488+0530] {logging_mixin.py:190} INFO - [2024-12-17T08:50:24.486+0530] {dagbag.py:387} ERROR - Failed to import: /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
Traceback (most recent call last):
  File "/Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/.venv/lib/python3.12/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 991, in exec_module
  File "<frozen importlib._bootstrap_external>", line 1129, in get_code
  File "<frozen importlib._bootstrap_external>", line 1059, in source_to_code
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py", line 17
    
    ^
IndentationError: unexpected unindent
[2024-12-17T08:50:24.489+0530] {processor.py:927} WARNING - No viable dags retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T08:50:24.613+0530] {logging_mixin.py:190} INFO - [2024-12-17T08:50:24.613+0530] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-12-17T08:50:24.635+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 0.160 seconds
[2024-12-17T08:53:08.124+0530] {processor.py:186} INFO - Started process (PID=39818) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T08:53:08.127+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-17T08:53:08.128+0530] {logging_mixin.py:190} INFO - [2024-12-17T08:53:08.128+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T08:53:08.133+0530] {logging_mixin.py:190} INFO - [2024-12-17T08:53:08.132+0530] {dagbag.py:387} ERROR - Failed to import: /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
Traceback (most recent call last):
  File "/Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/.venv/lib/python3.12/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 991, in exec_module
  File "<frozen importlib._bootstrap_external>", line 1129, in get_code
  File "<frozen importlib._bootstrap_external>", line 1059, in source_to_code
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py", line 17
    
    ^
IndentationError: unexpected unindent
[2024-12-17T08:53:08.134+0530] {processor.py:927} WARNING - No viable dags retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T08:53:08.271+0530] {logging_mixin.py:190} INFO - [2024-12-17T08:53:08.271+0530] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-12-17T08:53:08.303+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 0.182 seconds
[2024-12-17T08:55:55.878+0530] {processor.py:186} INFO - Started process (PID=40023) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T08:55:55.880+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-17T08:55:55.882+0530] {logging_mixin.py:190} INFO - [2024-12-17T08:55:55.882+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T08:55:55.885+0530] {logging_mixin.py:190} INFO - [2024-12-17T08:55:55.884+0530] {dagbag.py:387} ERROR - Failed to import: /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
Traceback (most recent call last):
  File "/Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/.venv/lib/python3.12/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 991, in exec_module
  File "<frozen importlib._bootstrap_external>", line 1129, in get_code
  File "<frozen importlib._bootstrap_external>", line 1059, in source_to_code
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py", line 17
    
    ^
IndentationError: unexpected unindent
[2024-12-17T08:55:55.886+0530] {processor.py:927} WARNING - No viable dags retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T08:55:56.080+0530] {logging_mixin.py:190} INFO - [2024-12-17T08:55:56.079+0530] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-12-17T08:55:56.105+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 0.231 seconds
[2024-12-17T08:59:01.153+0530] {processor.py:186} INFO - Started process (PID=40381) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T08:59:01.156+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-17T08:59:01.158+0530] {logging_mixin.py:190} INFO - [2024-12-17T08:59:01.158+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T08:59:01.206+0530] {processor.py:925} INFO - DAG(s) 'movie_pipeline' retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T08:59:01.508+0530] {logging_mixin.py:190} INFO - [2024-12-17T08:59:01.508+0530] {dag.py:3239} INFO - Sync 1 DAGs
[2024-12-17T08:59:01.534+0530] {logging_mixin.py:190} INFO - [2024-12-17T08:59:01.533+0530] {dag.py:4180} INFO - Setting next_dagrun for movie_pipeline to 2024-12-17 01:30:00+00:00, run_after=2024-12-18 01:30:00+00:00
[2024-12-17T08:59:01.564+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 0.415 seconds
[2024-12-17T09:00:05.970+0530] {processor.py:186} INFO - Started process (PID=40469) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T09:00:05.972+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-17T09:00:05.974+0530] {logging_mixin.py:190} INFO - [2024-12-17T09:00:05.973+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T09:00:06.017+0530] {processor.py:925} INFO - DAG(s) 'movie_pipeline' retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T09:00:06.531+0530] {logging_mixin.py:190} INFO - [2024-12-17T09:00:06.531+0530] {dag.py:3239} INFO - Sync 1 DAGs
[2024-12-17T09:00:06.552+0530] {logging_mixin.py:190} INFO - [2024-12-17T09:00:06.552+0530] {dag.py:4180} INFO - Setting next_dagrun for movie_pipeline to 2024-12-17 01:30:00+00:00, run_after=2024-12-18 01:30:00+00:00
[2024-12-17T09:00:06.575+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 0.610 seconds
[2024-12-17T09:02:23.997+0530] {processor.py:186} INFO - Started process (PID=40691) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T09:02:23.999+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-17T09:02:24.001+0530] {logging_mixin.py:190} INFO - [2024-12-17T09:02:24.001+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T09:02:24.007+0530] {logging_mixin.py:190} INFO - [2024-12-17T09:02:24.006+0530] {dagbag.py:387} ERROR - Failed to import: /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
Traceback (most recent call last):
  File "/Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/.venv/lib/python3.12/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 991, in exec_module
  File "<frozen importlib._bootstrap_external>", line 1129, in get_code
  File "<frozen importlib._bootstrap_external>", line 1059, in source_to_code
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py", line 19
    
    ^
IndentationError: expected an indented block after 'with' statement on line 9
[2024-12-17T09:02:24.008+0530] {processor.py:927} WARNING - No viable dags retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T09:02:24.142+0530] {logging_mixin.py:190} INFO - [2024-12-17T09:02:24.142+0530] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-12-17T09:02:24.170+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 0.176 seconds
[2024-12-17T09:05:23.343+0530] {processor.py:186} INFO - Started process (PID=41050) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T09:05:23.345+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-17T09:05:23.346+0530] {logging_mixin.py:190} INFO - [2024-12-17T09:05:23.346+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T09:05:23.365+0530] {logging_mixin.py:190} INFO - [2024-12-17T09:05:23.353+0530] {dagbag.py:387} ERROR - Failed to import: /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
Traceback (most recent call last):
  File "/Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/.venv/lib/python3.12/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 991, in exec_module
  File "<frozen importlib._bootstrap_external>", line 1129, in get_code
  File "<frozen importlib._bootstrap_external>", line 1059, in source_to_code
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py", line 17
    
    ^
IndentationError: expected an indented block after 'with' statement on line 10
[2024-12-17T09:05:23.367+0530] {processor.py:927} WARNING - No viable dags retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T09:05:23.519+0530] {logging_mixin.py:190} INFO - [2024-12-17T09:05:23.518+0530] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-12-17T09:05:23.547+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 0.208 seconds
[2024-12-17T09:08:15.943+0530] {processor.py:186} INFO - Started process (PID=41375) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T09:08:15.946+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-17T09:08:15.948+0530] {logging_mixin.py:190} INFO - [2024-12-17T09:08:15.948+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T09:08:15.956+0530] {logging_mixin.py:190} INFO - [2024-12-17T09:08:15.953+0530] {dagbag.py:387} ERROR - Failed to import: /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
Traceback (most recent call last):
  File "/Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/.venv/lib/python3.12/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 991, in exec_module
  File "<frozen importlib._bootstrap_external>", line 1129, in get_code
  File "<frozen importlib._bootstrap_external>", line 1059, in source_to_code
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py", line 18
    
    ^
IndentationError: expected an indented block after 'with' statement on line 10
[2024-12-17T09:08:15.957+0530] {processor.py:927} WARNING - No viable dags retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T09:08:16.096+0530] {logging_mixin.py:190} INFO - [2024-12-17T09:08:16.096+0530] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-12-17T09:08:16.126+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 0.187 seconds
[2024-12-17T09:11:12.012+0530] {processor.py:186} INFO - Started process (PID=41585) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T09:11:12.013+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-17T09:11:12.014+0530] {logging_mixin.py:190} INFO - [2024-12-17T09:11:12.014+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T09:11:12.020+0530] {logging_mixin.py:190} INFO - [2024-12-17T09:11:12.016+0530] {dagbag.py:387} ERROR - Failed to import: /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
Traceback (most recent call last):
  File "/Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/.venv/lib/python3.12/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 991, in exec_module
  File "<frozen importlib._bootstrap_external>", line 1129, in get_code
  File "<frozen importlib._bootstrap_external>", line 1059, in source_to_code
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py", line 18
    
    ^
IndentationError: expected an indented block after 'with' statement on line 10
[2024-12-17T09:11:12.020+0530] {processor.py:927} WARNING - No viable dags retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T09:11:12.152+0530] {logging_mixin.py:190} INFO - [2024-12-17T09:11:12.151+0530] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-12-17T09:11:12.176+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 0.168 seconds
[2024-12-17T09:13:58.009+0530] {processor.py:186} INFO - Started process (PID=41780) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T09:13:58.011+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-17T09:13:58.012+0530] {logging_mixin.py:190} INFO - [2024-12-17T09:13:58.011+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T09:13:58.014+0530] {logging_mixin.py:190} INFO - [2024-12-17T09:13:58.013+0530] {dagbag.py:387} ERROR - Failed to import: /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
Traceback (most recent call last):
  File "/Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/.venv/lib/python3.12/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 991, in exec_module
  File "<frozen importlib._bootstrap_external>", line 1129, in get_code
  File "<frozen importlib._bootstrap_external>", line 1059, in source_to_code
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py", line 18
    
    ^
IndentationError: expected an indented block after 'with' statement on line 10
[2024-12-17T09:13:58.015+0530] {processor.py:927} WARNING - No viable dags retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T09:13:58.147+0530] {logging_mixin.py:190} INFO - [2024-12-17T09:13:58.146+0530] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-12-17T09:13:58.178+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 0.172 seconds
[2024-12-17T09:17:09.370+0530] {processor.py:186} INFO - Started process (PID=42078) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T09:17:09.373+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-17T09:17:09.376+0530] {logging_mixin.py:190} INFO - [2024-12-17T09:17:09.376+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T09:17:09.386+0530] {logging_mixin.py:190} INFO - [2024-12-17T09:17:09.383+0530] {dagbag.py:387} ERROR - Failed to import: /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
Traceback (most recent call last):
  File "/Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/.venv/lib/python3.12/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py", line 1, in <module>
    from extract import *
ModuleNotFoundError: No module named 'extract'
[2024-12-17T09:17:09.387+0530] {processor.py:927} WARNING - No viable dags retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T09:17:09.555+0530] {logging_mixin.py:190} INFO - [2024-12-17T09:17:09.554+0530] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-12-17T09:17:09.589+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 0.227 seconds
[2024-12-17T09:20:18.413+0530] {processor.py:186} INFO - Started process (PID=42359) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T09:20:18.416+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-17T09:20:18.420+0530] {logging_mixin.py:190} INFO - [2024-12-17T09:20:18.419+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T09:20:18.423+0530] {logging_mixin.py:190} INFO - [2024-12-17T09:20:18.422+0530] {dagbag.py:387} ERROR - Failed to import: /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
Traceback (most recent call last):
  File "/Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/.venv/lib/python3.12/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py", line 1, in <module>
    from extract import *
ModuleNotFoundError: No module named 'extract'
[2024-12-17T09:20:18.424+0530] {processor.py:927} WARNING - No viable dags retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T09:20:18.713+0530] {logging_mixin.py:190} INFO - [2024-12-17T09:20:18.713+0530] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-12-17T09:20:18.749+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 0.340 seconds
[2024-12-17T09:23:06.922+0530] {processor.py:186} INFO - Started process (PID=42560) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T09:23:06.923+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-17T09:23:06.924+0530] {logging_mixin.py:190} INFO - [2024-12-17T09:23:06.924+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T09:23:06.927+0530] {logging_mixin.py:190} INFO - [2024-12-17T09:23:06.926+0530] {dagbag.py:387} ERROR - Failed to import: /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
Traceback (most recent call last):
  File "/Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/.venv/lib/python3.12/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py", line 1, in <module>
    from extract import *
ModuleNotFoundError: No module named 'extract'
[2024-12-17T09:23:06.927+0530] {processor.py:927} WARNING - No viable dags retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T09:23:07.048+0530] {logging_mixin.py:190} INFO - [2024-12-17T09:23:07.048+0530] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-12-17T09:23:07.073+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 0.155 seconds
[2024-12-17T09:26:10.010+0530] {processor.py:186} INFO - Started process (PID=42770) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T09:26:10.012+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-17T09:26:10.014+0530] {logging_mixin.py:190} INFO - [2024-12-17T09:26:10.014+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T09:26:10.019+0530] {logging_mixin.py:190} INFO - [2024-12-17T09:26:10.018+0530] {dagbag.py:387} ERROR - Failed to import: /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
Traceback (most recent call last):
  File "/Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/.venv/lib/python3.12/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py", line 1, in <module>
    from extract import *
ModuleNotFoundError: No module named 'extract'
[2024-12-17T09:26:10.020+0530] {processor.py:927} WARNING - No viable dags retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T09:26:10.192+0530] {logging_mixin.py:190} INFO - [2024-12-17T09:26:10.192+0530] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-12-17T09:26:10.224+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 0.221 seconds
[2024-12-17T09:27:40.545+0530] {processor.py:186} INFO - Started process (PID=42950) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T09:27:40.547+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-17T09:27:40.549+0530] {logging_mixin.py:190} INFO - [2024-12-17T09:27:40.549+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T09:27:40.556+0530] {logging_mixin.py:190} INFO - [2024-12-17T09:27:40.552+0530] {dagbag.py:387} ERROR - Failed to import: /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
Traceback (most recent call last):
  File "/Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/.venv/lib/python3.12/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py", line 1, in <module>
    from extract import *
ModuleNotFoundError: No module named 'extract'
[2024-12-17T09:27:40.557+0530] {processor.py:927} WARNING - No viable dags retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T09:27:40.829+0530] {logging_mixin.py:190} INFO - [2024-12-17T09:27:40.829+0530] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-12-17T09:27:40.887+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 0.346 seconds
[2024-12-17T09:29:22.510+0530] {processor.py:186} INFO - Started process (PID=43157) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T09:29:22.511+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-17T09:29:22.512+0530] {logging_mixin.py:190} INFO - [2024-12-17T09:29:22.512+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T09:29:22.515+0530] {logging_mixin.py:190} INFO - [2024-12-17T09:29:22.514+0530] {dagbag.py:387} ERROR - Failed to import: /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
Traceback (most recent call last):
  File "/Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/.venv/lib/python3.12/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py", line 1, in <module>
    from extract import *
ModuleNotFoundError: No module named 'extract'
[2024-12-17T09:29:22.515+0530] {processor.py:927} WARNING - No viable dags retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T09:29:22.642+0530] {logging_mixin.py:190} INFO - [2024-12-17T09:29:22.641+0530] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-12-17T09:29:22.664+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 0.158 seconds
[2024-12-17T09:32:14.087+0530] {processor.py:186} INFO - Started process (PID=43489) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T09:32:14.089+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-17T09:32:14.090+0530] {logging_mixin.py:190} INFO - [2024-12-17T09:32:14.089+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T09:32:14.092+0530] {logging_mixin.py:190} INFO - [2024-12-17T09:32:14.091+0530] {dagbag.py:387} ERROR - Failed to import: /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
Traceback (most recent call last):
  File "/Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/.venv/lib/python3.12/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py", line 1, in <module>
    from extract import *
ModuleNotFoundError: No module named 'extract'
[2024-12-17T09:32:14.093+0530] {processor.py:927} WARNING - No viable dags retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T09:32:14.209+0530] {logging_mixin.py:190} INFO - [2024-12-17T09:32:14.209+0530] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-12-17T09:32:14.232+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 0.148 seconds
[2024-12-17T09:35:01.149+0530] {processor.py:186} INFO - Started process (PID=43823) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T09:35:01.151+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-17T09:35:01.152+0530] {logging_mixin.py:190} INFO - [2024-12-17T09:35:01.152+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T09:35:01.155+0530] {logging_mixin.py:190} INFO - [2024-12-17T09:35:01.154+0530] {dagbag.py:387} ERROR - Failed to import: /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
Traceback (most recent call last):
  File "/Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/.venv/lib/python3.12/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py", line 1, in <module>
    from extract import *
ModuleNotFoundError: No module named 'extract'
[2024-12-17T09:35:01.155+0530] {processor.py:927} WARNING - No viable dags retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T09:35:01.272+0530] {logging_mixin.py:190} INFO - [2024-12-17T09:35:01.272+0530] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-12-17T09:35:01.296+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 0.150 seconds
[2024-12-17T09:43:01.147+0530] {processor.py:186} INFO - Started process (PID=44145) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T09:43:01.150+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-17T09:43:01.151+0530] {logging_mixin.py:190} INFO - [2024-12-17T09:43:01.151+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T09:43:01.156+0530] {logging_mixin.py:190} INFO - [2024-12-17T09:43:01.154+0530] {dagbag.py:387} ERROR - Failed to import: /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
Traceback (most recent call last):
  File "/Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/.venv/lib/python3.12/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py", line 1, in <module>
    from extract import *
ModuleNotFoundError: No module named 'extract'
[2024-12-17T09:43:01.156+0530] {processor.py:927} WARNING - No viable dags retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T09:43:01.291+0530] {logging_mixin.py:190} INFO - [2024-12-17T09:43:01.291+0530] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-12-17T09:43:01.321+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 0.177 seconds
[2024-12-17T09:46:03.329+0530] {processor.py:186} INFO - Started process (PID=44526) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T09:46:03.334+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-17T09:46:03.336+0530] {logging_mixin.py:190} INFO - [2024-12-17T09:46:03.336+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T09:46:03.341+0530] {logging_mixin.py:190} INFO - [2024-12-17T09:46:03.339+0530] {dagbag.py:387} ERROR - Failed to import: /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
Traceback (most recent call last):
  File "/Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/.venv/lib/python3.12/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py", line 1, in <module>
    from extract import *
ModuleNotFoundError: No module named 'extract'
[2024-12-17T09:46:03.342+0530] {processor.py:927} WARNING - No viable dags retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T09:46:03.489+0530] {logging_mixin.py:190} INFO - [2024-12-17T09:46:03.489+0530] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-12-17T09:46:03.518+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 0.193 seconds
[2024-12-17T09:49:04.511+0530] {processor.py:186} INFO - Started process (PID=44868) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T09:49:04.514+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-17T09:49:04.515+0530] {logging_mixin.py:190} INFO - [2024-12-17T09:49:04.515+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T09:49:04.520+0530] {logging_mixin.py:190} INFO - [2024-12-17T09:49:04.517+0530] {dagbag.py:387} ERROR - Failed to import: /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
Traceback (most recent call last):
  File "/Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/.venv/lib/python3.12/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py", line 1, in <module>
    from extract import *
ModuleNotFoundError: No module named 'extract'
[2024-12-17T09:49:04.520+0530] {processor.py:927} WARNING - No viable dags retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T09:49:04.663+0530] {logging_mixin.py:190} INFO - [2024-12-17T09:49:04.663+0530] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-12-17T09:49:04.691+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 0.184 seconds
[2024-12-17T09:52:01.548+0530] {processor.py:186} INFO - Started process (PID=45218) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T09:52:01.551+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-17T09:52:01.552+0530] {logging_mixin.py:190} INFO - [2024-12-17T09:52:01.552+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T09:52:01.555+0530] {logging_mixin.py:190} INFO - [2024-12-17T09:52:01.554+0530] {dagbag.py:387} ERROR - Failed to import: /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
Traceback (most recent call last):
  File "/Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/.venv/lib/python3.12/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py", line 1, in <module>
    from extract import *
ModuleNotFoundError: No module named 'extract'
[2024-12-17T09:52:01.556+0530] {processor.py:927} WARNING - No viable dags retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T09:52:01.683+0530] {logging_mixin.py:190} INFO - [2024-12-17T09:52:01.683+0530] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-12-17T09:52:01.707+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 0.162 seconds
[2024-12-17T09:54:49.959+0530] {processor.py:186} INFO - Started process (PID=45559) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T09:54:49.961+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-17T09:54:49.963+0530] {logging_mixin.py:190} INFO - [2024-12-17T09:54:49.963+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T09:54:49.966+0530] {logging_mixin.py:190} INFO - [2024-12-17T09:54:49.964+0530] {dagbag.py:387} ERROR - Failed to import: /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
Traceback (most recent call last):
  File "/Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/.venv/lib/python3.12/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py", line 1, in <module>
    from extract import *
ModuleNotFoundError: No module named 'extract'
[2024-12-17T09:54:49.967+0530] {processor.py:927} WARNING - No viable dags retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T09:54:50.123+0530] {logging_mixin.py:190} INFO - [2024-12-17T09:54:50.122+0530] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-12-17T09:54:50.149+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 0.195 seconds
[2024-12-17T09:57:38.567+0530] {processor.py:186} INFO - Started process (PID=45893) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T09:57:38.569+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-17T09:57:38.570+0530] {logging_mixin.py:190} INFO - [2024-12-17T09:57:38.570+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T09:57:38.572+0530] {logging_mixin.py:190} INFO - [2024-12-17T09:57:38.571+0530] {dagbag.py:387} ERROR - Failed to import: /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
Traceback (most recent call last):
  File "/Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/.venv/lib/python3.12/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py", line 1, in <module>
    from extract import *
ModuleNotFoundError: No module named 'extract'
[2024-12-17T09:57:38.573+0530] {processor.py:927} WARNING - No viable dags retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T09:57:38.718+0530] {logging_mixin.py:190} INFO - [2024-12-17T09:57:38.718+0530] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-12-17T09:57:38.757+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 0.193 seconds
[2024-12-17T10:00:28.502+0530] {processor.py:186} INFO - Started process (PID=46231) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T10:00:28.503+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-17T10:00:28.504+0530] {logging_mixin.py:190} INFO - [2024-12-17T10:00:28.504+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T10:00:28.507+0530] {logging_mixin.py:190} INFO - [2024-12-17T10:00:28.506+0530] {dagbag.py:387} ERROR - Failed to import: /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
Traceback (most recent call last):
  File "/Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/.venv/lib/python3.12/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py", line 1, in <module>
    from extract import *
ModuleNotFoundError: No module named 'extract'
[2024-12-17T10:00:28.507+0530] {processor.py:927} WARNING - No viable dags retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T10:00:28.636+0530] {logging_mixin.py:190} INFO - [2024-12-17T10:00:28.636+0530] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-12-17T10:00:28.658+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 0.160 seconds
[2024-12-17T10:03:18.245+0530] {processor.py:186} INFO - Started process (PID=46562) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T10:03:18.246+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-17T10:03:18.247+0530] {logging_mixin.py:190} INFO - [2024-12-17T10:03:18.247+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T10:03:18.250+0530] {logging_mixin.py:190} INFO - [2024-12-17T10:03:18.248+0530] {dagbag.py:387} ERROR - Failed to import: /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
Traceback (most recent call last):
  File "/Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/.venv/lib/python3.12/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py", line 1, in <module>
    from extract import *
ModuleNotFoundError: No module named 'extract'
[2024-12-17T10:03:18.250+0530] {processor.py:927} WARNING - No viable dags retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T10:03:18.377+0530] {logging_mixin.py:190} INFO - [2024-12-17T10:03:18.377+0530] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-12-17T10:03:18.401+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 0.160 seconds
[2024-12-17T10:06:07.453+0530] {processor.py:186} INFO - Started process (PID=46893) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T10:06:07.454+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-17T10:06:07.455+0530] {logging_mixin.py:190} INFO - [2024-12-17T10:06:07.455+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T10:06:07.458+0530] {logging_mixin.py:190} INFO - [2024-12-17T10:06:07.457+0530] {dagbag.py:387} ERROR - Failed to import: /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
Traceback (most recent call last):
  File "/Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/.venv/lib/python3.12/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py", line 1, in <module>
    from extract import *
ModuleNotFoundError: No module named 'extract'
[2024-12-17T10:06:07.458+0530] {processor.py:927} WARNING - No viable dags retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T10:06:07.577+0530] {logging_mixin.py:190} INFO - [2024-12-17T10:06:07.577+0530] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-12-17T10:06:07.600+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 0.151 seconds
[2024-12-17T10:08:56.660+0530] {processor.py:186} INFO - Started process (PID=47221) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T10:08:56.661+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-17T10:08:56.662+0530] {logging_mixin.py:190} INFO - [2024-12-17T10:08:56.662+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T10:08:56.665+0530] {logging_mixin.py:190} INFO - [2024-12-17T10:08:56.663+0530] {dagbag.py:387} ERROR - Failed to import: /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
Traceback (most recent call last):
  File "/Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/.venv/lib/python3.12/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py", line 1, in <module>
    from extract import *
ModuleNotFoundError: No module named 'extract'
[2024-12-17T10:08:56.665+0530] {processor.py:927} WARNING - No viable dags retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T10:08:56.791+0530] {logging_mixin.py:190} INFO - [2024-12-17T10:08:56.791+0530] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-12-17T10:08:56.816+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 0.160 seconds
[2024-12-17T10:11:46.143+0530] {processor.py:186} INFO - Started process (PID=47543) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T10:11:46.144+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-17T10:11:46.146+0530] {logging_mixin.py:190} INFO - [2024-12-17T10:11:46.146+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T10:11:46.150+0530] {logging_mixin.py:190} INFO - [2024-12-17T10:11:46.148+0530] {dagbag.py:387} ERROR - Failed to import: /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
Traceback (most recent call last):
  File "/Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/.venv/lib/python3.12/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py", line 1, in <module>
    from extract import *
ModuleNotFoundError: No module named 'extract'
[2024-12-17T10:11:46.151+0530] {processor.py:927} WARNING - No viable dags retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T10:11:46.273+0530] {logging_mixin.py:190} INFO - [2024-12-17T10:11:46.273+0530] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-12-17T10:11:46.296+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 0.158 seconds
[2024-12-17T10:14:35.796+0530] {processor.py:186} INFO - Started process (PID=47883) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T10:14:35.797+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-17T10:14:35.799+0530] {logging_mixin.py:190} INFO - [2024-12-17T10:14:35.798+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T10:14:35.801+0530] {logging_mixin.py:190} INFO - [2024-12-17T10:14:35.800+0530] {dagbag.py:387} ERROR - Failed to import: /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
Traceback (most recent call last):
  File "/Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/.venv/lib/python3.12/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py", line 1, in <module>
    from extract import *
ModuleNotFoundError: No module named 'extract'
[2024-12-17T10:14:35.802+0530] {processor.py:927} WARNING - No viable dags retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T10:14:35.918+0530] {logging_mixin.py:190} INFO - [2024-12-17T10:14:35.918+0530] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-12-17T10:14:35.941+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 0.148 seconds
[2024-12-17T10:17:25.565+0530] {processor.py:186} INFO - Started process (PID=48206) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T10:17:25.566+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-17T10:17:25.567+0530] {logging_mixin.py:190} INFO - [2024-12-17T10:17:25.567+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T10:17:25.570+0530] {logging_mixin.py:190} INFO - [2024-12-17T10:17:25.569+0530] {dagbag.py:387} ERROR - Failed to import: /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
Traceback (most recent call last):
  File "/Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/.venv/lib/python3.12/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py", line 1, in <module>
    from extract import *
ModuleNotFoundError: No module named 'extract'
[2024-12-17T10:17:25.571+0530] {processor.py:927} WARNING - No viable dags retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T10:17:25.688+0530] {logging_mixin.py:190} INFO - [2024-12-17T10:17:25.688+0530] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-12-17T10:17:25.710+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 0.150 seconds
[2024-12-17T10:20:15.604+0530] {processor.py:186} INFO - Started process (PID=48547) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T10:20:15.606+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-17T10:20:15.607+0530] {logging_mixin.py:190} INFO - [2024-12-17T10:20:15.606+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T10:20:15.609+0530] {logging_mixin.py:190} INFO - [2024-12-17T10:20:15.608+0530] {dagbag.py:387} ERROR - Failed to import: /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
Traceback (most recent call last):
  File "/Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/.venv/lib/python3.12/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py", line 1, in <module>
    from extract import *
ModuleNotFoundError: No module named 'extract'
[2024-12-17T10:20:15.610+0530] {processor.py:927} WARNING - No viable dags retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T10:20:15.736+0530] {logging_mixin.py:190} INFO - [2024-12-17T10:20:15.736+0530] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-12-17T10:20:15.762+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 0.162 seconds
[2024-12-17T10:23:05.501+0530] {processor.py:186} INFO - Started process (PID=48871) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T10:23:05.502+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-17T10:23:05.503+0530] {logging_mixin.py:190} INFO - [2024-12-17T10:23:05.503+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T10:23:05.506+0530] {logging_mixin.py:190} INFO - [2024-12-17T10:23:05.505+0530] {dagbag.py:387} ERROR - Failed to import: /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
Traceback (most recent call last):
  File "/Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/.venv/lib/python3.12/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py", line 1, in <module>
    from extract import *
ModuleNotFoundError: No module named 'extract'
[2024-12-17T10:23:05.507+0530] {processor.py:927} WARNING - No viable dags retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T10:23:05.623+0530] {logging_mixin.py:190} INFO - [2024-12-17T10:23:05.622+0530] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-12-17T10:23:05.645+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 0.149 seconds
[2024-12-17T10:25:55.144+0530] {processor.py:186} INFO - Started process (PID=49207) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T10:25:55.145+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-17T10:25:55.147+0530] {logging_mixin.py:190} INFO - [2024-12-17T10:25:55.146+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T10:25:55.149+0530] {logging_mixin.py:190} INFO - [2024-12-17T10:25:55.148+0530] {dagbag.py:387} ERROR - Failed to import: /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
Traceback (most recent call last):
  File "/Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/.venv/lib/python3.12/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py", line 1, in <module>
    from extract import *
ModuleNotFoundError: No module named 'extract'
[2024-12-17T10:25:55.150+0530] {processor.py:927} WARNING - No viable dags retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T10:25:55.281+0530] {logging_mixin.py:190} INFO - [2024-12-17T10:25:55.281+0530] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-12-17T10:25:55.306+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 0.166 seconds
[2024-12-17T10:29:03.406+0530] {processor.py:186} INFO - Started process (PID=49564) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T10:29:03.408+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-17T10:29:03.410+0530] {logging_mixin.py:190} INFO - [2024-12-17T10:29:03.409+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T10:29:03.494+0530] {processor.py:925} INFO - DAG(s) 'movie_pipeline' retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T10:29:04.145+0530] {logging_mixin.py:190} INFO - [2024-12-17T10:29:04.144+0530] {dag.py:3239} INFO - Sync 1 DAGs
[2024-12-17T10:29:04.168+0530] {logging_mixin.py:190} INFO - [2024-12-17T10:29:04.168+0530] {dag.py:4180} INFO - Setting next_dagrun for movie_pipeline to 2024-12-17 01:30:00+00:00, run_after=2024-12-18 01:30:00+00:00
[2024-12-17T10:29:04.194+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 0.792 seconds
[2024-12-17T10:32:05.245+0530] {processor.py:186} INFO - Started process (PID=49855) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T10:32:05.249+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-17T10:32:05.251+0530] {logging_mixin.py:190} INFO - [2024-12-17T10:32:05.250+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T10:32:05.297+0530] {processor.py:925} INFO - DAG(s) 'movie_pipeline' retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T10:32:05.841+0530] {logging_mixin.py:190} INFO - [2024-12-17T10:32:05.840+0530] {dag.py:3239} INFO - Sync 1 DAGs
[2024-12-17T10:32:05.866+0530] {logging_mixin.py:190} INFO - [2024-12-17T10:32:05.866+0530] {dag.py:4180} INFO - Setting next_dagrun for movie_pipeline to 2024-12-17 01:30:00+00:00, run_after=2024-12-18 01:30:00+00:00
[2024-12-17T10:32:05.895+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 0.654 seconds
[2024-12-17T10:35:38.181+0530] {processor.py:186} INFO - Started process (PID=50131) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T10:35:38.183+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-17T10:35:38.186+0530] {logging_mixin.py:190} INFO - [2024-12-17T10:35:38.185+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T10:35:38.228+0530] {processor.py:925} INFO - DAG(s) 'movie_pipeline' retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T10:35:38.347+0530] {logging_mixin.py:190} INFO - [2024-12-17T10:35:38.346+0530] {dag.py:3239} INFO - Sync 1 DAGs
[2024-12-17T10:35:38.367+0530] {logging_mixin.py:190} INFO - [2024-12-17T10:35:38.366+0530] {dag.py:4180} INFO - Setting next_dagrun for movie_pipeline to 2024-12-17 01:30:00+00:00, run_after=2024-12-18 01:30:00+00:00
[2024-12-17T10:35:38.390+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 0.212 seconds
[2024-12-17T10:38:38.739+0530] {processor.py:186} INFO - Started process (PID=50445) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T10:38:38.741+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-17T10:38:38.742+0530] {logging_mixin.py:190} INFO - [2024-12-17T10:38:38.742+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T10:38:38.908+0530] {logging_mixin.py:190} INFO - [2024-12-17T10:38:38.908+0530] {dag.py:4435} INFO - dagrun id: movie_pipeline
[2024-12-17T10:38:38.923+0530] {logging_mixin.py:190} INFO - [2024-12-17T10:38:38.922+0530] {dag.py:4451} INFO - created dagrun <DagRun movie_pipeline @ 2024-12-17 05:08:38.782554+00:00: manual__2024-12-17T05:08:38.782554+00:00, state:running, queued_at: None. externally triggered: False>
[2024-12-17T10:38:38.948+0530] {logging_mixin.py:190} INFO - [2024-12-17T10:38:38.948+0530] {dag.py:4396} INFO - [DAG TEST] starting task_id=scrape_collection_page map_index=-1
[2024-12-17T10:38:38.949+0530] {logging_mixin.py:190} INFO - [2024-12-17T10:38:38.948+0530] {dag.py:4399} INFO - [DAG TEST] running task <TaskInstance: movie_pipeline.scrape_collection_page manual__2024-12-17T05:08:38.782554+00:00 [scheduled]>
[2024-12-17T10:38:44.040+0530] {logging_mixin.py:190} INFO - [2024-12-17T10:38:44.039+0530] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-12-17T10:38:44.099+0530] {logging_mixin.py:190} INFO - [2024-12-17 10:38:44,099] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T05:08:38.782554+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T05:08:38.782554+00:00'
[2024-12-17T10:38:44.100+0530] {logging_mixin.py:190} INFO - [2024-12-17T10:38:44.099+0530] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T05:08:38.782554+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T05:08:38.782554+00:00'
[2024-12-17T10:38:44.103+0530] {logging_mixin.py:190} INFO - Task instance is in running state
[2024-12-17T10:38:44.103+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: queued
[2024-12-17T10:38:44.103+0530] {logging_mixin.py:190} INFO - Current task name:scrape_collection_page state:scheduled start_date:None
[2024-12-17T10:38:44.103+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline and current dag run status:running
[2024-12-17T10:38:44.104+0530] {logging_mixin.py:190} INFO - [2024-12-17T10:38:44.104+0530] {taskinstance.py:731} INFO - ::endgroup::
[2024-12-17T10:38:44.106+0530] {logging_mixin.py:190} INFO - scraping data for date: date
[2024-12-17T10:38:44.106+0530] {logging_mixin.py:190} INFO - [2024-12-17 10:38:44,106] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T10:38:44.107+0530] {logging_mixin.py:190} INFO - [2024-12-17T10:38:44.106+0530] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T10:38:44.112+0530] {logging_mixin.py:190} INFO - [2024-12-17T10:38:44.112+0530] {taskinstance.py:340} INFO - ::group::Post task execution logs
[2024-12-17T10:38:44.113+0530] {logging_mixin.py:190} INFO - [2024-12-17T10:38:44.113+0530] {taskinstance.py:352} INFO - Marking task as SUCCESS. dag_id=movie_pipeline, task_id=scrape_collection_page, run_id=manual__2024-12-17T05:08:38.782554+00:00, execution_date=20241217T050838, start_date=, end_date=20241217T050844
[2024-12-17T10:38:44.122+0530] {logging_mixin.py:190} INFO - Task instance in success state
[2024-12-17T10:38:44.122+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: running
[2024-12-17T10:38:44.122+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline queued_at:None
[2024-12-17T10:38:44.123+0530] {logging_mixin.py:190} INFO - Task hostname:Jayeshs-MacBook-Air.local operator:PythonOperator
[2024-12-17T10:38:44.123+0530] {logging_mixin.py:190} INFO - [2024-12-17T10:38:44.123+0530] {dag.py:4410} INFO - [DAG TEST] end task task_id=scrape_collection_page map_index=-1
[2024-12-17T10:38:44.128+0530] {logging_mixin.py:190} INFO - [2024-12-17T10:38:44.128+0530] {dagrun.py:854} INFO - Marking run <DagRun movie_pipeline @ 2024-12-17 05:08:38.782554+00:00: manual__2024-12-17T05:08:38.782554+00:00, state:running, queued_at: None. externally triggered: False> successful
[2024-12-17T10:38:44.129+0530] {logging_mixin.py:190} INFO - Dag run in success state
[2024-12-17T10:38:44.129+0530] {logging_mixin.py:190} INFO - Dag run start:2024-12-17 05:08:38.782554+00:00 end:2024-12-17 05:08:44.129217+00:00
[2024-12-17T10:38:44.130+0530] {logging_mixin.py:190} INFO - [2024-12-17T10:38:44.129+0530] {dagrun.py:905} INFO - DagRun Finished: dag_id=movie_pipeline, execution_date=2024-12-17 05:08:38.782554+00:00, run_id=manual__2024-12-17T05:08:38.782554+00:00, run_start_date=2024-12-17 05:08:38.782554+00:00, run_end_date=2024-12-17 05:08:44.129217+00:00, run_duration=5.346663, state=success, external_trigger=False, run_type=manual, data_interval_start=2024-12-16 05:08:38.782554+00:00, data_interval_end=2024-12-17 05:08:38.782554+00:00, dag_hash=None
[2024-12-17T10:38:44.138+0530] {processor.py:925} INFO - DAG(s) 'movie_pipeline' retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T10:38:44.150+0530] {logging_mixin.py:190} INFO - [2024-12-17T10:38:44.150+0530] {dag.py:3239} INFO - Sync 1 DAGs
[2024-12-17T10:38:44.167+0530] {logging_mixin.py:190} INFO - [2024-12-17T10:38:44.167+0530] {dag.py:4180} INFO - Setting next_dagrun for movie_pipeline to 2024-12-17 01:30:00+00:00, run_after=2024-12-18 01:30:00+00:00
[2024-12-17T10:38:44.188+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 5.453 seconds
[2024-12-17T10:41:46.517+0530] {processor.py:186} INFO - Started process (PID=50796) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T10:41:46.520+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-17T10:41:46.522+0530] {logging_mixin.py:190} INFO - [2024-12-17T10:41:46.522+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T10:41:46.690+0530] {logging_mixin.py:190} INFO - [2024-12-17T10:41:46.689+0530] {dag.py:4435} INFO - dagrun id: movie_pipeline
[2024-12-17T10:41:46.703+0530] {logging_mixin.py:190} INFO - [2024-12-17T10:41:46.703+0530] {dag.py:4451} INFO - created dagrun <DagRun movie_pipeline @ 2024-12-17 05:11:46.563149+00:00: manual__2024-12-17T05:11:46.563149+00:00, state:running, queued_at: None. externally triggered: False>
[2024-12-17T10:41:46.729+0530] {logging_mixin.py:190} INFO - [2024-12-17T10:41:46.728+0530] {dag.py:4396} INFO - [DAG TEST] starting task_id=scrape_collection_page map_index=-1
[2024-12-17T10:41:46.730+0530] {logging_mixin.py:190} INFO - [2024-12-17T10:41:46.729+0530] {dag.py:4399} INFO - [DAG TEST] running task <TaskInstance: movie_pipeline.scrape_collection_page manual__2024-12-17T05:11:46.563149+00:00 [scheduled]>
[2024-12-17T10:41:51.828+0530] {logging_mixin.py:190} INFO - [2024-12-17T10:41:51.828+0530] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-12-17T10:41:51.905+0530] {logging_mixin.py:190} INFO - [2024-12-17 10:41:51,905] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T05:11:46.563149+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T05:11:46.563149+00:00'
[2024-12-17T10:41:51.906+0530] {logging_mixin.py:190} INFO - [2024-12-17T10:41:51.905+0530] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T05:11:46.563149+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T05:11:46.563149+00:00'
[2024-12-17T10:41:51.909+0530] {logging_mixin.py:190} INFO - Task instance is in running state
[2024-12-17T10:41:51.910+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: queued
[2024-12-17T10:41:51.910+0530] {logging_mixin.py:190} INFO - Current task name:scrape_collection_page state:scheduled start_date:None
[2024-12-17T10:41:51.910+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline and current dag run status:running
[2024-12-17T10:41:51.912+0530] {logging_mixin.py:190} INFO - [2024-12-17T10:41:51.910+0530] {taskinstance.py:731} INFO - ::endgroup::
[2024-12-17T10:41:51.917+0530] {logging_mixin.py:190} INFO - date
[2024-12-17T10:41:51.918+0530] {logging_mixin.py:190} INFO - [2024-12-17 10:41:51,917] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T10:41:51.918+0530] {logging_mixin.py:190} INFO - [2024-12-17T10:41:51.917+0530] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T10:41:51.923+0530] {logging_mixin.py:190} INFO - [2024-12-17T10:41:51.923+0530] {taskinstance.py:340} INFO - ::group::Post task execution logs
[2024-12-17T10:41:51.924+0530] {logging_mixin.py:190} INFO - [2024-12-17T10:41:51.924+0530] {taskinstance.py:352} INFO - Marking task as SUCCESS. dag_id=movie_pipeline, task_id=scrape_collection_page, run_id=manual__2024-12-17T05:11:46.563149+00:00, execution_date=20241217T051146, start_date=, end_date=20241217T051151
[2024-12-17T10:41:51.936+0530] {logging_mixin.py:190} INFO - Task instance in success state
[2024-12-17T10:41:51.936+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: running
[2024-12-17T10:41:51.937+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline queued_at:None
[2024-12-17T10:41:51.937+0530] {logging_mixin.py:190} INFO - Task hostname:Jayeshs-MacBook-Air.local operator:PythonOperator
[2024-12-17T10:41:51.937+0530] {logging_mixin.py:190} INFO - [2024-12-17T10:41:51.937+0530] {dag.py:4410} INFO - [DAG TEST] end task task_id=scrape_collection_page map_index=-1
[2024-12-17T10:41:51.942+0530] {logging_mixin.py:190} INFO - [2024-12-17T10:41:51.941+0530] {dagrun.py:854} INFO - Marking run <DagRun movie_pipeline @ 2024-12-17 05:11:46.563149+00:00: manual__2024-12-17T05:11:46.563149+00:00, state:running, queued_at: None. externally triggered: False> successful
[2024-12-17T10:41:51.942+0530] {logging_mixin.py:190} INFO - Dag run in success state
[2024-12-17T10:41:51.942+0530] {logging_mixin.py:190} INFO - Dag run start:2024-12-17 05:11:46.563149+00:00 end:2024-12-17 05:11:51.942573+00:00
[2024-12-17T10:41:51.943+0530] {logging_mixin.py:190} INFO - [2024-12-17T10:41:51.943+0530] {dagrun.py:905} INFO - DagRun Finished: dag_id=movie_pipeline, execution_date=2024-12-17 05:11:46.563149+00:00, run_id=manual__2024-12-17T05:11:46.563149+00:00, run_start_date=2024-12-17 05:11:46.563149+00:00, run_end_date=2024-12-17 05:11:51.942573+00:00, run_duration=5.379424, state=success, external_trigger=False, run_type=manual, data_interval_start=2024-12-16 05:11:46.563149+00:00, data_interval_end=2024-12-17 05:11:46.563149+00:00, dag_hash=None
[2024-12-17T10:41:51.954+0530] {processor.py:925} INFO - DAG(s) 'movie_pipeline' retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T10:41:51.967+0530] {logging_mixin.py:190} INFO - [2024-12-17T10:41:51.966+0530] {dag.py:3239} INFO - Sync 1 DAGs
[2024-12-17T10:41:51.984+0530] {logging_mixin.py:190} INFO - [2024-12-17T10:41:51.983+0530] {dag.py:4180} INFO - Setting next_dagrun for movie_pipeline to 2024-12-17 01:30:00+00:00, run_after=2024-12-18 01:30:00+00:00
[2024-12-17T10:41:52.003+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 5.489 seconds
[2024-12-17T10:45:09.186+0530] {processor.py:186} INFO - Started process (PID=51156) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T10:45:09.188+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-17T10:45:09.190+0530] {logging_mixin.py:190} INFO - [2024-12-17T10:45:09.190+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T10:45:09.521+0530] {logging_mixin.py:190} INFO - [2024-12-17T10:45:09.521+0530] {dag.py:4435} INFO - dagrun id: movie_pipeline
[2024-12-17T10:45:09.555+0530] {logging_mixin.py:190} INFO - [2024-12-17T10:45:09.555+0530] {dag.py:4451} INFO - created dagrun <DagRun movie_pipeline @ 2024-12-17 05:15:09.261968+00:00: manual__2024-12-17T05:15:09.261968+00:00, state:running, queued_at: None. externally triggered: False>
[2024-12-17T10:45:09.607+0530] {logging_mixin.py:190} INFO - [2024-12-17T10:45:09.607+0530] {dag.py:4396} INFO - [DAG TEST] starting task_id=scrape_collection_page map_index=-1
[2024-12-17T10:45:09.610+0530] {logging_mixin.py:190} INFO - [2024-12-17T10:45:09.608+0530] {dag.py:4399} INFO - [DAG TEST] running task <TaskInstance: movie_pipeline.scrape_collection_page manual__2024-12-17T05:15:09.261968+00:00 [scheduled]>
[2024-12-17T10:45:14.856+0530] {logging_mixin.py:190} INFO - [2024-12-17T10:45:14.853+0530] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-12-17T10:45:14.992+0530] {logging_mixin.py:190} INFO - [2024-12-17 10:45:14,991] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T05:15:09.261968+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T05:15:09.261968+00:00'
[2024-12-17T10:45:14.992+0530] {logging_mixin.py:190} INFO - [2024-12-17T10:45:14.991+0530] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T05:15:09.261968+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T05:15:09.261968+00:00'
[2024-12-17T10:45:15.006+0530] {logging_mixin.py:190} INFO - Task instance is in running state
[2024-12-17T10:45:15.006+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: queued
[2024-12-17T10:45:15.007+0530] {logging_mixin.py:190} INFO - Current task name:scrape_collection_page state:scheduled start_date:None
[2024-12-17T10:45:15.007+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline and current dag run status:running
[2024-12-17T10:45:15.007+0530] {logging_mixin.py:190} INFO - [2024-12-17T10:45:15.007+0530] {taskinstance.py:731} INFO - ::endgroup::
[2024-12-17T10:45:15.014+0530] {logging_mixin.py:190} INFO - 2024-12-17
[2024-12-17T10:45:15.015+0530] {logging_mixin.py:190} INFO - [2024-12-17 10:45:15,014] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T10:45:15.016+0530] {logging_mixin.py:190} INFO - [2024-12-17T10:45:15.014+0530] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T10:45:15.024+0530] {logging_mixin.py:190} INFO - [2024-12-17T10:45:15.024+0530] {taskinstance.py:340} INFO - ::group::Post task execution logs
[2024-12-17T10:45:15.025+0530] {logging_mixin.py:190} INFO - [2024-12-17T10:45:15.024+0530] {taskinstance.py:352} INFO - Marking task as SUCCESS. dag_id=movie_pipeline, task_id=scrape_collection_page, run_id=manual__2024-12-17T05:15:09.261968+00:00, execution_date=20241217T051509, start_date=, end_date=20241217T051515
[2024-12-17T10:45:15.042+0530] {logging_mixin.py:190} INFO - Task instance in success state
[2024-12-17T10:45:15.043+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: running
[2024-12-17T10:45:15.047+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline queued_at:None
[2024-12-17T10:45:15.047+0530] {logging_mixin.py:190} INFO - Task hostname:Jayeshs-MacBook-Air.local operator:PythonOperator
[2024-12-17T10:45:15.063+0530] {logging_mixin.py:190} INFO - [2024-12-17T10:45:15.058+0530] {dag.py:4410} INFO - [DAG TEST] end task task_id=scrape_collection_page map_index=-1
[2024-12-17T10:45:15.071+0530] {logging_mixin.py:190} INFO - [2024-12-17T10:45:15.071+0530] {dagrun.py:854} INFO - Marking run <DagRun movie_pipeline @ 2024-12-17 05:15:09.261968+00:00: manual__2024-12-17T05:15:09.261968+00:00, state:running, queued_at: None. externally triggered: False> successful
[2024-12-17T10:45:15.071+0530] {logging_mixin.py:190} INFO - Dag run in success state
[2024-12-17T10:45:15.072+0530] {logging_mixin.py:190} INFO - Dag run start:2024-12-17 05:15:09.261968+00:00 end:2024-12-17 05:15:15.071806+00:00
[2024-12-17T10:45:15.072+0530] {logging_mixin.py:190} INFO - [2024-12-17T10:45:15.072+0530] {dagrun.py:905} INFO - DagRun Finished: dag_id=movie_pipeline, execution_date=2024-12-17 05:15:09.261968+00:00, run_id=manual__2024-12-17T05:15:09.261968+00:00, run_start_date=2024-12-17 05:15:09.261968+00:00, run_end_date=2024-12-17 05:15:15.071806+00:00, run_duration=5.809838, state=success, external_trigger=False, run_type=manual, data_interval_start=2024-12-16 05:15:09.261968+00:00, data_interval_end=2024-12-17 05:15:09.261968+00:00, dag_hash=None
[2024-12-17T10:45:15.132+0530] {processor.py:925} INFO - DAG(s) 'movie_pipeline' retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T10:45:15.823+0530] {logging_mixin.py:190} INFO - [2024-12-17T10:45:15.823+0530] {dag.py:3239} INFO - Sync 1 DAGs
[2024-12-17T10:45:15.861+0530] {logging_mixin.py:190} INFO - [2024-12-17T10:45:15.861+0530] {dag.py:4180} INFO - Setting next_dagrun for movie_pipeline to 2024-12-17 01:30:00+00:00, run_after=2024-12-18 01:30:00+00:00
[2024-12-17T10:45:15.923+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 6.743 seconds
[2024-12-17T10:48:23.729+0530] {processor.py:186} INFO - Started process (PID=51416) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T10:48:23.732+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-17T10:48:23.736+0530] {logging_mixin.py:190} INFO - [2024-12-17T10:48:23.734+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T10:48:24.010+0530] {logging_mixin.py:190} INFO - [2024-12-17T10:48:24.010+0530] {dag.py:4435} INFO - dagrun id: movie_pipeline
[2024-12-17T10:48:24.031+0530] {logging_mixin.py:190} INFO - [2024-12-17T10:48:24.030+0530] {dag.py:4451} INFO - created dagrun <DagRun movie_pipeline @ 2024-12-17 05:18:23.846902+00:00: manual__2024-12-17T05:18:23.846902+00:00, state:running, queued_at: None. externally triggered: False>
[2024-12-17T10:48:24.073+0530] {logging_mixin.py:190} INFO - [2024-12-17T10:48:24.072+0530] {dag.py:4396} INFO - [DAG TEST] starting task_id=scrape_collection_page map_index=-1
[2024-12-17T10:48:24.074+0530] {logging_mixin.py:190} INFO - [2024-12-17T10:48:24.073+0530] {dag.py:4399} INFO - [DAG TEST] running task <TaskInstance: movie_pipeline.scrape_collection_page manual__2024-12-17T05:18:23.846902+00:00 [scheduled]>
[2024-12-17T10:48:29.197+0530] {logging_mixin.py:190} INFO - [2024-12-17T10:48:29.197+0530] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-12-17T10:48:29.273+0530] {logging_mixin.py:190} INFO - [2024-12-17 10:48:29,273] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T05:18:23.846902+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T05:18:23.846902+00:00'
[2024-12-17T10:48:29.273+0530] {logging_mixin.py:190} INFO - [2024-12-17T10:48:29.273+0530] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T05:18:23.846902+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T05:18:23.846902+00:00'
[2024-12-17T10:48:29.287+0530] {logging_mixin.py:190} INFO - Task instance is in running state
[2024-12-17T10:48:29.287+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: queued
[2024-12-17T10:48:29.287+0530] {logging_mixin.py:190} INFO - Current task name:scrape_collection_page state:scheduled start_date:None
[2024-12-17T10:48:29.288+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline and current dag run status:running
[2024-12-17T10:48:29.288+0530] {logging_mixin.py:190} INFO - [2024-12-17T10:48:29.288+0530] {taskinstance.py:731} INFO - ::endgroup::
[2024-12-17T10:48:29.292+0530] {logging_mixin.py:190} INFO - 2024-12-17
[2024-12-17T10:48:29.293+0530] {logging_mixin.py:190} INFO - [2024-12-17 10:48:29,292] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T10:48:29.293+0530] {logging_mixin.py:190} INFO - [2024-12-17T10:48:29.292+0530] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T10:48:29.298+0530] {logging_mixin.py:190} INFO - [2024-12-17T10:48:29.298+0530] {taskinstance.py:340} INFO - ::group::Post task execution logs
[2024-12-17T10:48:29.299+0530] {logging_mixin.py:190} INFO - [2024-12-17T10:48:29.299+0530] {taskinstance.py:352} INFO - Marking task as SUCCESS. dag_id=movie_pipeline, task_id=scrape_collection_page, run_id=manual__2024-12-17T05:18:23.846902+00:00, execution_date=20241217T051823, start_date=, end_date=20241217T051829
[2024-12-17T10:48:29.365+0530] {logging_mixin.py:190} INFO - Task instance in success state
[2024-12-17T10:48:29.366+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: running
[2024-12-17T10:48:29.368+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline queued_at:None
[2024-12-17T10:48:29.368+0530] {logging_mixin.py:190} INFO - Task hostname:Jayeshs-MacBook-Air.local operator:PythonOperator
[2024-12-17T10:48:29.370+0530] {logging_mixin.py:190} INFO - [2024-12-17T10:48:29.369+0530] {dag.py:4410} INFO - [DAG TEST] end task task_id=scrape_collection_page map_index=-1
[2024-12-17T10:48:29.382+0530] {logging_mixin.py:190} INFO - [2024-12-17T10:48:29.381+0530] {dagrun.py:854} INFO - Marking run <DagRun movie_pipeline @ 2024-12-17 05:18:23.846902+00:00: manual__2024-12-17T05:18:23.846902+00:00, state:running, queued_at: None. externally triggered: False> successful
[2024-12-17T10:48:29.382+0530] {logging_mixin.py:190} INFO - Dag run in success state
[2024-12-17T10:48:29.383+0530] {logging_mixin.py:190} INFO - Dag run start:2024-12-17 05:18:23.846902+00:00 end:2024-12-17 05:18:29.382602+00:00
[2024-12-17T10:48:29.385+0530] {logging_mixin.py:190} INFO - [2024-12-17T10:48:29.383+0530] {dagrun.py:905} INFO - DagRun Finished: dag_id=movie_pipeline, execution_date=2024-12-17 05:18:23.846902+00:00, run_id=manual__2024-12-17T05:18:23.846902+00:00, run_start_date=2024-12-17 05:18:23.846902+00:00, run_end_date=2024-12-17 05:18:29.382602+00:00, run_duration=5.5357, state=success, external_trigger=False, run_type=manual, data_interval_start=2024-12-16 05:18:23.846902+00:00, data_interval_end=2024-12-17 05:18:23.846902+00:00, dag_hash=None
[2024-12-17T10:48:29.405+0530] {processor.py:925} INFO - DAG(s) 'movie_pipeline' retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T10:48:29.433+0530] {logging_mixin.py:190} INFO - [2024-12-17T10:48:29.433+0530] {dag.py:3239} INFO - Sync 1 DAGs
[2024-12-17T10:48:29.462+0530] {logging_mixin.py:190} INFO - [2024-12-17T10:48:29.461+0530] {dag.py:4180} INFO - Setting next_dagrun for movie_pipeline to 2024-12-17 01:30:00+00:00, run_after=2024-12-18 01:30:00+00:00
[2024-12-17T10:48:29.481+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 5.756 seconds
[2024-12-17T10:51:29.788+0530] {processor.py:186} INFO - Started process (PID=51619) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T10:51:29.790+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-17T10:51:29.792+0530] {logging_mixin.py:190} INFO - [2024-12-17T10:51:29.791+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T10:51:29.974+0530] {logging_mixin.py:190} INFO - [2024-12-17T10:51:29.974+0530] {dag.py:4435} INFO - dagrun id: movie_pipeline
[2024-12-17T10:51:29.989+0530] {logging_mixin.py:190} INFO - [2024-12-17T10:51:29.989+0530] {dag.py:4451} INFO - created dagrun <DagRun movie_pipeline @ 2024-12-17 05:21:29.835632+00:00: manual__2024-12-17T05:21:29.835632+00:00, state:running, queued_at: None. externally triggered: False>
[2024-12-17T10:51:30.016+0530] {logging_mixin.py:190} INFO - [2024-12-17T10:51:30.015+0530] {dag.py:4396} INFO - [DAG TEST] starting task_id=scrape_collection_page map_index=-1
[2024-12-17T10:51:30.017+0530] {logging_mixin.py:190} INFO - [2024-12-17T10:51:30.016+0530] {dag.py:4399} INFO - [DAG TEST] running task <TaskInstance: movie_pipeline.scrape_collection_page manual__2024-12-17T05:21:29.835632+00:00 [scheduled]>
[2024-12-17T10:51:35.119+0530] {logging_mixin.py:190} INFO - [2024-12-17T10:51:35.118+0530] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-12-17T10:51:35.190+0530] {logging_mixin.py:190} INFO - [2024-12-17 10:51:35,190] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T05:21:29.835632+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T05:21:29.835632+00:00'
[2024-12-17T10:51:35.191+0530] {logging_mixin.py:190} INFO - [2024-12-17T10:51:35.190+0530] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T05:21:29.835632+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T05:21:29.835632+00:00'
[2024-12-17T10:51:35.194+0530] {logging_mixin.py:190} INFO - Task instance is in running state
[2024-12-17T10:51:35.194+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: queued
[2024-12-17T10:51:35.195+0530] {logging_mixin.py:190} INFO - Current task name:scrape_collection_page state:scheduled start_date:None
[2024-12-17T10:51:35.195+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline and current dag run status:running
[2024-12-17T10:51:35.196+0530] {logging_mixin.py:190} INFO - [2024-12-17T10:51:35.195+0530] {taskinstance.py:731} INFO - ::endgroup::
[2024-12-17T10:51:35.199+0530] {logging_mixin.py:190} INFO - 2024-12-17
[2024-12-17T10:51:35.200+0530] {logging_mixin.py:190} INFO - [2024-12-17 10:51:35,199] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T10:51:35.200+0530] {logging_mixin.py:190} INFO - [2024-12-17T10:51:35.199+0530] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T10:51:35.206+0530] {logging_mixin.py:190} INFO - [2024-12-17T10:51:35.206+0530] {taskinstance.py:340} INFO - ::group::Post task execution logs
[2024-12-17T10:51:35.207+0530] {logging_mixin.py:190} INFO - [2024-12-17T10:51:35.207+0530] {taskinstance.py:352} INFO - Marking task as SUCCESS. dag_id=movie_pipeline, task_id=scrape_collection_page, run_id=manual__2024-12-17T05:21:29.835632+00:00, execution_date=20241217T052129, start_date=, end_date=20241217T052135
[2024-12-17T10:51:35.218+0530] {logging_mixin.py:190} INFO - Task instance in success state
[2024-12-17T10:51:35.218+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: running
[2024-12-17T10:51:35.218+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline queued_at:None
[2024-12-17T10:51:35.218+0530] {logging_mixin.py:190} INFO - Task hostname:Jayeshs-MacBook-Air.local operator:PythonOperator
[2024-12-17T10:51:35.219+0530] {logging_mixin.py:190} INFO - [2024-12-17T10:51:35.219+0530] {dag.py:4410} INFO - [DAG TEST] end task task_id=scrape_collection_page map_index=-1
[2024-12-17T10:51:35.223+0530] {logging_mixin.py:190} INFO - [2024-12-17T10:51:35.223+0530] {dagrun.py:854} INFO - Marking run <DagRun movie_pipeline @ 2024-12-17 05:21:29.835632+00:00: manual__2024-12-17T05:21:29.835632+00:00, state:running, queued_at: None. externally triggered: False> successful
[2024-12-17T10:51:35.224+0530] {logging_mixin.py:190} INFO - Dag run in success state
[2024-12-17T10:51:35.224+0530] {logging_mixin.py:190} INFO - Dag run start:2024-12-17 05:21:29.835632+00:00 end:2024-12-17 05:21:35.223931+00:00
[2024-12-17T10:51:35.224+0530] {logging_mixin.py:190} INFO - [2024-12-17T10:51:35.224+0530] {dagrun.py:905} INFO - DagRun Finished: dag_id=movie_pipeline, execution_date=2024-12-17 05:21:29.835632+00:00, run_id=manual__2024-12-17T05:21:29.835632+00:00, run_start_date=2024-12-17 05:21:29.835632+00:00, run_end_date=2024-12-17 05:21:35.223931+00:00, run_duration=5.388299, state=success, external_trigger=False, run_type=manual, data_interval_start=2024-12-16 05:21:29.835632+00:00, data_interval_end=2024-12-17 05:21:29.835632+00:00, dag_hash=None
[2024-12-17T10:51:35.235+0530] {processor.py:925} INFO - DAG(s) 'movie_pipeline' retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T10:51:35.247+0530] {logging_mixin.py:190} INFO - [2024-12-17T10:51:35.247+0530] {dag.py:3239} INFO - Sync 1 DAGs
[2024-12-17T10:51:35.271+0530] {logging_mixin.py:190} INFO - [2024-12-17T10:51:35.271+0530] {dag.py:4180} INFO - Setting next_dagrun for movie_pipeline to 2024-12-17 01:30:00+00:00, run_after=2024-12-18 01:30:00+00:00
[2024-12-17T10:51:35.293+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 5.508 seconds
[2024-12-17T10:54:41.580+0530] {processor.py:186} INFO - Started process (PID=51856) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T10:54:41.582+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-17T10:54:41.584+0530] {logging_mixin.py:190} INFO - [2024-12-17T10:54:41.584+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T10:54:41.792+0530] {logging_mixin.py:190} INFO - [2024-12-17T10:54:41.792+0530] {dag.py:4435} INFO - dagrun id: movie_pipeline
[2024-12-17T10:54:41.808+0530] {logging_mixin.py:190} INFO - [2024-12-17T10:54:41.807+0530] {dag.py:4451} INFO - created dagrun <DagRun movie_pipeline @ 2024-12-17 05:24:41.629594+00:00: manual__2024-12-17T05:24:41.629594+00:00, state:running, queued_at: None. externally triggered: False>
[2024-12-17T10:54:41.845+0530] {logging_mixin.py:190} INFO - [2024-12-17T10:54:41.845+0530] {dag.py:4396} INFO - [DAG TEST] starting task_id=scrape_collection_page map_index=-1
[2024-12-17T10:54:41.846+0530] {logging_mixin.py:190} INFO - [2024-12-17T10:54:41.845+0530] {dag.py:4399} INFO - [DAG TEST] running task <TaskInstance: movie_pipeline.scrape_collection_page manual__2024-12-17T05:24:41.629594+00:00 [scheduled]>
[2024-12-17T10:54:46.959+0530] {logging_mixin.py:190} INFO - [2024-12-17T10:54:46.959+0530] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-12-17T10:54:47.038+0530] {logging_mixin.py:190} INFO - [2024-12-17 10:54:47,038] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T05:24:41.629594+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T05:24:41.629594+00:00'
[2024-12-17T10:54:47.039+0530] {logging_mixin.py:190} INFO - [2024-12-17T10:54:47.038+0530] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T05:24:41.629594+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T05:24:41.629594+00:00'
[2024-12-17T10:54:47.052+0530] {logging_mixin.py:190} INFO - Task instance is in running state
[2024-12-17T10:54:47.053+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: queued
[2024-12-17T10:54:47.053+0530] {logging_mixin.py:190} INFO - Current task name:scrape_collection_page state:scheduled start_date:None
[2024-12-17T10:54:47.054+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline and current dag run status:running
[2024-12-17T10:54:47.055+0530] {logging_mixin.py:190} INFO - [2024-12-17T10:54:47.054+0530] {taskinstance.py:731} INFO - ::endgroup::
[2024-12-17T10:54:47.057+0530] {logging_mixin.py:190} INFO - 2024-12-17
[2024-12-17T10:54:47.057+0530] {logging_mixin.py:190} INFO - [2024-12-17 10:54:47,057] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T10:54:47.058+0530] {logging_mixin.py:190} INFO - [2024-12-17T10:54:47.057+0530] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T10:54:47.064+0530] {logging_mixin.py:190} INFO - [2024-12-17T10:54:47.063+0530] {taskinstance.py:340} INFO - ::group::Post task execution logs
[2024-12-17T10:54:47.065+0530] {logging_mixin.py:190} INFO - [2024-12-17T10:54:47.065+0530] {taskinstance.py:352} INFO - Marking task as SUCCESS. dag_id=movie_pipeline, task_id=scrape_collection_page, run_id=manual__2024-12-17T05:24:41.629594+00:00, execution_date=20241217T052441, start_date=, end_date=20241217T052447
[2024-12-17T10:54:47.075+0530] {logging_mixin.py:190} INFO - Task instance in success state
[2024-12-17T10:54:47.076+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: running
[2024-12-17T10:54:47.076+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline queued_at:None
[2024-12-17T10:54:47.076+0530] {logging_mixin.py:190} INFO - Task hostname:Jayeshs-MacBook-Air.local operator:PythonOperator
[2024-12-17T10:54:47.076+0530] {logging_mixin.py:190} INFO - [2024-12-17T10:54:47.076+0530] {dag.py:4410} INFO - [DAG TEST] end task task_id=scrape_collection_page map_index=-1
[2024-12-17T10:54:47.081+0530] {logging_mixin.py:190} INFO - [2024-12-17T10:54:47.081+0530] {dagrun.py:854} INFO - Marking run <DagRun movie_pipeline @ 2024-12-17 05:24:41.629594+00:00: manual__2024-12-17T05:24:41.629594+00:00, state:running, queued_at: None. externally triggered: False> successful
[2024-12-17T10:54:47.082+0530] {logging_mixin.py:190} INFO - Dag run in success state
[2024-12-17T10:54:47.082+0530] {logging_mixin.py:190} INFO - Dag run start:2024-12-17 05:24:41.629594+00:00 end:2024-12-17 05:24:47.082089+00:00
[2024-12-17T10:54:47.083+0530] {logging_mixin.py:190} INFO - [2024-12-17T10:54:47.082+0530] {dagrun.py:905} INFO - DagRun Finished: dag_id=movie_pipeline, execution_date=2024-12-17 05:24:41.629594+00:00, run_id=manual__2024-12-17T05:24:41.629594+00:00, run_start_date=2024-12-17 05:24:41.629594+00:00, run_end_date=2024-12-17 05:24:47.082089+00:00, run_duration=5.452495, state=success, external_trigger=False, run_type=manual, data_interval_start=2024-12-16 05:24:41.629594+00:00, data_interval_end=2024-12-17 05:24:41.629594+00:00, dag_hash=None
[2024-12-17T10:54:47.094+0530] {processor.py:925} INFO - DAG(s) 'movie_pipeline' retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T10:54:47.108+0530] {logging_mixin.py:190} INFO - [2024-12-17T10:54:47.107+0530] {dag.py:3239} INFO - Sync 1 DAGs
[2024-12-17T10:54:47.126+0530] {logging_mixin.py:190} INFO - [2024-12-17T10:54:47.125+0530] {dag.py:4180} INFO - Setting next_dagrun for movie_pipeline to 2024-12-17 01:30:00+00:00, run_after=2024-12-18 01:30:00+00:00
[2024-12-17T10:54:47.144+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 5.568 seconds
[2024-12-17T10:57:51.572+0530] {processor.py:186} INFO - Started process (PID=52074) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T10:57:51.574+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-17T10:57:51.575+0530] {logging_mixin.py:190} INFO - [2024-12-17T10:57:51.575+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T10:57:51.766+0530] {logging_mixin.py:190} INFO - [2024-12-17T10:57:51.765+0530] {dag.py:4435} INFO - dagrun id: movie_pipeline
[2024-12-17T10:57:51.784+0530] {logging_mixin.py:190} INFO - [2024-12-17T10:57:51.783+0530] {dag.py:4451} INFO - created dagrun <DagRun movie_pipeline @ 2024-12-17 05:27:51.621659+00:00: manual__2024-12-17T05:27:51.621659+00:00, state:running, queued_at: None. externally triggered: False>
[2024-12-17T10:57:51.821+0530] {logging_mixin.py:190} INFO - [2024-12-17T10:57:51.820+0530] {dag.py:4396} INFO - [DAG TEST] starting task_id=scrape_collection_page map_index=-1
[2024-12-17T10:57:51.822+0530] {logging_mixin.py:190} INFO - [2024-12-17T10:57:51.821+0530] {dag.py:4399} INFO - [DAG TEST] running task <TaskInstance: movie_pipeline.scrape_collection_page manual__2024-12-17T05:27:51.621659+00:00 [scheduled]>
[2024-12-17T10:57:56.940+0530] {logging_mixin.py:190} INFO - [2024-12-17T10:57:56.940+0530] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-12-17T10:57:57.005+0530] {logging_mixin.py:190} INFO - [2024-12-17 10:57:57,005] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T05:27:51.621659+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T05:27:51.621659+00:00'
[2024-12-17T10:57:57.006+0530] {logging_mixin.py:190} INFO - [2024-12-17T10:57:57.005+0530] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T05:27:51.621659+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T05:27:51.621659+00:00'
[2024-12-17T10:57:57.018+0530] {logging_mixin.py:190} INFO - Task instance is in running state
[2024-12-17T10:57:57.019+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: queued
[2024-12-17T10:57:57.019+0530] {logging_mixin.py:190} INFO - Current task name:scrape_collection_page state:scheduled start_date:None
[2024-12-17T10:57:57.019+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline and current dag run status:running
[2024-12-17T10:57:57.020+0530] {logging_mixin.py:190} INFO - [2024-12-17T10:57:57.020+0530] {taskinstance.py:731} INFO - ::endgroup::
[2024-12-17T10:57:57.022+0530] {logging_mixin.py:190} INFO - 2024-12-17
[2024-12-17T10:57:57.023+0530] {logging_mixin.py:190} INFO - [2024-12-17 10:57:57,023] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T10:57:57.023+0530] {logging_mixin.py:190} INFO - [2024-12-17T10:57:57.023+0530] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T10:57:57.029+0530] {logging_mixin.py:190} INFO - [2024-12-17T10:57:57.028+0530] {taskinstance.py:340} INFO - ::group::Post task execution logs
[2024-12-17T10:57:57.029+0530] {logging_mixin.py:190} INFO - [2024-12-17T10:57:57.029+0530] {taskinstance.py:352} INFO - Marking task as SUCCESS. dag_id=movie_pipeline, task_id=scrape_collection_page, run_id=manual__2024-12-17T05:27:51.621659+00:00, execution_date=20241217T052751, start_date=, end_date=20241217T052757
[2024-12-17T10:57:57.040+0530] {logging_mixin.py:190} INFO - Task instance in success state
[2024-12-17T10:57:57.040+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: running
[2024-12-17T10:57:57.040+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline queued_at:None
[2024-12-17T10:57:57.041+0530] {logging_mixin.py:190} INFO - Task hostname:Jayeshs-MacBook-Air.local operator:PythonOperator
[2024-12-17T10:57:57.041+0530] {logging_mixin.py:190} INFO - [2024-12-17T10:57:57.041+0530] {dag.py:4410} INFO - [DAG TEST] end task task_id=scrape_collection_page map_index=-1
[2024-12-17T10:57:57.045+0530] {logging_mixin.py:190} INFO - [2024-12-17T10:57:57.045+0530] {dagrun.py:854} INFO - Marking run <DagRun movie_pipeline @ 2024-12-17 05:27:51.621659+00:00: manual__2024-12-17T05:27:51.621659+00:00, state:running, queued_at: None. externally triggered: False> successful
[2024-12-17T10:57:57.045+0530] {logging_mixin.py:190} INFO - Dag run in success state
[2024-12-17T10:57:57.046+0530] {logging_mixin.py:190} INFO - Dag run start:2024-12-17 05:27:51.621659+00:00 end:2024-12-17 05:27:57.045904+00:00
[2024-12-17T10:57:57.046+0530] {logging_mixin.py:190} INFO - [2024-12-17T10:57:57.046+0530] {dagrun.py:905} INFO - DagRun Finished: dag_id=movie_pipeline, execution_date=2024-12-17 05:27:51.621659+00:00, run_id=manual__2024-12-17T05:27:51.621659+00:00, run_start_date=2024-12-17 05:27:51.621659+00:00, run_end_date=2024-12-17 05:27:57.045904+00:00, run_duration=5.424245, state=success, external_trigger=False, run_type=manual, data_interval_start=2024-12-16 05:27:51.621659+00:00, data_interval_end=2024-12-17 05:27:51.621659+00:00, dag_hash=None
[2024-12-17T10:57:57.055+0530] {processor.py:925} INFO - DAG(s) 'movie_pipeline' retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T10:57:57.067+0530] {logging_mixin.py:190} INFO - [2024-12-17T10:57:57.067+0530] {dag.py:3239} INFO - Sync 1 DAGs
[2024-12-17T10:57:57.084+0530] {logging_mixin.py:190} INFO - [2024-12-17T10:57:57.084+0530] {dag.py:4180} INFO - Setting next_dagrun for movie_pipeline to 2024-12-17 01:30:00+00:00, run_after=2024-12-18 01:30:00+00:00
[2024-12-17T10:57:57.107+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 5.539 seconds
[2024-12-17T11:01:01.625+0530] {processor.py:186} INFO - Started process (PID=52294) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T11:01:01.628+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-17T11:01:01.629+0530] {logging_mixin.py:190} INFO - [2024-12-17T11:01:01.629+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T11:01:01.816+0530] {logging_mixin.py:190} INFO - [2024-12-17T11:01:01.815+0530] {dag.py:4435} INFO - dagrun id: movie_pipeline
[2024-12-17T11:01:01.831+0530] {logging_mixin.py:190} INFO - [2024-12-17T11:01:01.830+0530] {dag.py:4451} INFO - created dagrun <DagRun movie_pipeline @ 2024-12-17 05:31:01.674554+00:00: manual__2024-12-17T05:31:01.674554+00:00, state:running, queued_at: None. externally triggered: False>
[2024-12-17T11:01:01.864+0530] {logging_mixin.py:190} INFO - [2024-12-17T11:01:01.864+0530] {dag.py:4396} INFO - [DAG TEST] starting task_id=scrape_collection_page map_index=-1
[2024-12-17T11:01:01.865+0530] {logging_mixin.py:190} INFO - [2024-12-17T11:01:01.865+0530] {dag.py:4399} INFO - [DAG TEST] running task <TaskInstance: movie_pipeline.scrape_collection_page manual__2024-12-17T05:31:01.674554+00:00 [scheduled]>
[2024-12-17T11:01:06.970+0530] {logging_mixin.py:190} INFO - [2024-12-17T11:01:06.970+0530] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-12-17T11:01:07.056+0530] {logging_mixin.py:190} INFO - [2024-12-17 11:01:07,055] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T05:31:01.674554+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T05:31:01.674554+00:00'
[2024-12-17T11:01:07.056+0530] {logging_mixin.py:190} INFO - [2024-12-17T11:01:07.055+0530] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T05:31:01.674554+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T05:31:01.674554+00:00'
[2024-12-17T11:01:07.069+0530] {logging_mixin.py:190} INFO - Task instance is in running state
[2024-12-17T11:01:07.070+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: queued
[2024-12-17T11:01:07.070+0530] {logging_mixin.py:190} INFO - Current task name:scrape_collection_page state:scheduled start_date:None
[2024-12-17T11:01:07.070+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline and current dag run status:running
[2024-12-17T11:01:07.070+0530] {logging_mixin.py:190} INFO - [2024-12-17T11:01:07.070+0530] {taskinstance.py:731} INFO - ::endgroup::
[2024-12-17T11:01:07.072+0530] {logging_mixin.py:190} INFO - 2024-12-17
[2024-12-17T11:01:07.073+0530] {logging_mixin.py:190} INFO - [2024-12-17 11:01:07,073] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T11:01:07.073+0530] {logging_mixin.py:190} INFO - [2024-12-17T11:01:07.073+0530] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T11:01:07.078+0530] {logging_mixin.py:190} INFO - [2024-12-17T11:01:07.078+0530] {taskinstance.py:340} INFO - ::group::Post task execution logs
[2024-12-17T11:01:07.079+0530] {logging_mixin.py:190} INFO - [2024-12-17T11:01:07.078+0530] {taskinstance.py:352} INFO - Marking task as SUCCESS. dag_id=movie_pipeline, task_id=scrape_collection_page, run_id=manual__2024-12-17T05:31:01.674554+00:00, execution_date=20241217T053101, start_date=, end_date=20241217T053107
[2024-12-17T11:01:07.089+0530] {logging_mixin.py:190} INFO - Task instance in success state
[2024-12-17T11:01:07.090+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: running
[2024-12-17T11:01:07.090+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline queued_at:None
[2024-12-17T11:01:07.090+0530] {logging_mixin.py:190} INFO - Task hostname:Jayeshs-MacBook-Air.local operator:PythonOperator
[2024-12-17T11:01:07.091+0530] {logging_mixin.py:190} INFO - [2024-12-17T11:01:07.090+0530] {dag.py:4410} INFO - [DAG TEST] end task task_id=scrape_collection_page map_index=-1
[2024-12-17T11:01:07.096+0530] {logging_mixin.py:190} INFO - [2024-12-17T11:01:07.096+0530] {dagrun.py:854} INFO - Marking run <DagRun movie_pipeline @ 2024-12-17 05:31:01.674554+00:00: manual__2024-12-17T05:31:01.674554+00:00, state:running, queued_at: None. externally triggered: False> successful
[2024-12-17T11:01:07.096+0530] {logging_mixin.py:190} INFO - Dag run in success state
[2024-12-17T11:01:07.097+0530] {logging_mixin.py:190} INFO - Dag run start:2024-12-17 05:31:01.674554+00:00 end:2024-12-17 05:31:07.096667+00:00
[2024-12-17T11:01:07.097+0530] {logging_mixin.py:190} INFO - [2024-12-17T11:01:07.097+0530] {dagrun.py:905} INFO - DagRun Finished: dag_id=movie_pipeline, execution_date=2024-12-17 05:31:01.674554+00:00, run_id=manual__2024-12-17T05:31:01.674554+00:00, run_start_date=2024-12-17 05:31:01.674554+00:00, run_end_date=2024-12-17 05:31:07.096667+00:00, run_duration=5.422113, state=success, external_trigger=False, run_type=manual, data_interval_start=2024-12-16 05:31:01.674554+00:00, data_interval_end=2024-12-17 05:31:01.674554+00:00, dag_hash=None
[2024-12-17T11:01:07.109+0530] {processor.py:925} INFO - DAG(s) 'movie_pipeline' retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T11:01:07.121+0530] {logging_mixin.py:190} INFO - [2024-12-17T11:01:07.121+0530] {dag.py:3239} INFO - Sync 1 DAGs
[2024-12-17T11:01:07.139+0530] {logging_mixin.py:190} INFO - [2024-12-17T11:01:07.139+0530] {dag.py:4180} INFO - Setting next_dagrun for movie_pipeline to 2024-12-17 01:30:00+00:00, run_after=2024-12-18 01:30:00+00:00
[2024-12-17T11:01:07.159+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 5.538 seconds
[2024-12-17T11:04:20.326+0530] {processor.py:186} INFO - Started process (PID=52547) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T11:04:20.330+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-17T11:04:20.334+0530] {logging_mixin.py:190} INFO - [2024-12-17T11:04:20.333+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T11:04:20.662+0530] {logging_mixin.py:190} INFO - [2024-12-17T11:04:20.662+0530] {dag.py:4435} INFO - dagrun id: movie_pipeline
[2024-12-17T11:04:20.678+0530] {logging_mixin.py:190} INFO - [2024-12-17T11:04:20.678+0530] {dag.py:4451} INFO - created dagrun <DagRun movie_pipeline @ 2024-12-17 05:34:20.480767+00:00: manual__2024-12-17T05:34:20.480767+00:00, state:running, queued_at: None. externally triggered: False>
[2024-12-17T11:04:20.709+0530] {logging_mixin.py:190} INFO - [2024-12-17T11:04:20.709+0530] {dag.py:4396} INFO - [DAG TEST] starting task_id=scrape_collection_page map_index=-1
[2024-12-17T11:04:20.710+0530] {logging_mixin.py:190} INFO - [2024-12-17T11:04:20.710+0530] {dag.py:4399} INFO - [DAG TEST] running task <TaskInstance: movie_pipeline.scrape_collection_page manual__2024-12-17T05:34:20.480767+00:00 [scheduled]>
[2024-12-17T11:04:25.808+0530] {logging_mixin.py:190} INFO - [2024-12-17T11:04:25.808+0530] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-12-17T11:04:25.881+0530] {logging_mixin.py:190} INFO - [2024-12-17 11:04:25,880] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T05:34:20.480767+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T05:34:20.480767+00:00'
[2024-12-17T11:04:25.881+0530] {logging_mixin.py:190} INFO - [2024-12-17T11:04:25.880+0530] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T05:34:20.480767+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T05:34:20.480767+00:00'
[2024-12-17T11:04:25.896+0530] {logging_mixin.py:190} INFO - Task instance is in running state
[2024-12-17T11:04:25.896+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: queued
[2024-12-17T11:04:25.896+0530] {logging_mixin.py:190} INFO - Current task name:scrape_collection_page state:scheduled start_date:None
[2024-12-17T11:04:25.896+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline and current dag run status:running
[2024-12-17T11:04:25.897+0530] {logging_mixin.py:190} INFO - [2024-12-17T11:04:25.897+0530] {taskinstance.py:731} INFO - ::endgroup::
[2024-12-17T11:04:25.900+0530] {logging_mixin.py:190} INFO - 2024-12-17
[2024-12-17T11:04:25.901+0530] {logging_mixin.py:190} INFO - [2024-12-17 11:04:25,901] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T11:04:25.901+0530] {logging_mixin.py:190} INFO - [2024-12-17T11:04:25.901+0530] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T11:04:25.907+0530] {logging_mixin.py:190} INFO - [2024-12-17T11:04:25.906+0530] {taskinstance.py:340} INFO - ::group::Post task execution logs
[2024-12-17T11:04:25.907+0530] {logging_mixin.py:190} INFO - [2024-12-17T11:04:25.907+0530] {taskinstance.py:352} INFO - Marking task as SUCCESS. dag_id=movie_pipeline, task_id=scrape_collection_page, run_id=manual__2024-12-17T05:34:20.480767+00:00, execution_date=20241217T053420, start_date=, end_date=20241217T053425
[2024-12-17T11:04:25.916+0530] {logging_mixin.py:190} INFO - Task instance in success state
[2024-12-17T11:04:25.916+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: running
[2024-12-17T11:04:25.917+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline queued_at:None
[2024-12-17T11:04:25.917+0530] {logging_mixin.py:190} INFO - Task hostname:Jayeshs-MacBook-Air.local operator:PythonOperator
[2024-12-17T11:04:25.917+0530] {logging_mixin.py:190} INFO - [2024-12-17T11:04:25.917+0530] {dag.py:4410} INFO - [DAG TEST] end task task_id=scrape_collection_page map_index=-1
[2024-12-17T11:04:25.921+0530] {logging_mixin.py:190} INFO - [2024-12-17T11:04:25.921+0530] {dagrun.py:854} INFO - Marking run <DagRun movie_pipeline @ 2024-12-17 05:34:20.480767+00:00: manual__2024-12-17T05:34:20.480767+00:00, state:running, queued_at: None. externally triggered: False> successful
[2024-12-17T11:04:25.921+0530] {logging_mixin.py:190} INFO - Dag run in success state
[2024-12-17T11:04:25.921+0530] {logging_mixin.py:190} INFO - Dag run start:2024-12-17 05:34:20.480767+00:00 end:2024-12-17 05:34:25.921646+00:00
[2024-12-17T11:04:25.922+0530] {logging_mixin.py:190} INFO - [2024-12-17T11:04:25.922+0530] {dagrun.py:905} INFO - DagRun Finished: dag_id=movie_pipeline, execution_date=2024-12-17 05:34:20.480767+00:00, run_id=manual__2024-12-17T05:34:20.480767+00:00, run_start_date=2024-12-17 05:34:20.480767+00:00, run_end_date=2024-12-17 05:34:25.921646+00:00, run_duration=5.440879, state=success, external_trigger=False, run_type=manual, data_interval_start=2024-12-16 05:34:20.480767+00:00, data_interval_end=2024-12-17 05:34:20.480767+00:00, dag_hash=None
[2024-12-17T11:04:25.932+0530] {processor.py:925} INFO - DAG(s) 'movie_pipeline' retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T11:04:25.944+0530] {logging_mixin.py:190} INFO - [2024-12-17T11:04:25.943+0530] {dag.py:3239} INFO - Sync 1 DAGs
[2024-12-17T11:04:25.963+0530] {logging_mixin.py:190} INFO - [2024-12-17T11:04:25.963+0530] {dag.py:4180} INFO - Setting next_dagrun for movie_pipeline to 2024-12-17 01:30:00+00:00, run_after=2024-12-18 01:30:00+00:00
[2024-12-17T11:04:25.982+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 5.663 seconds
[2024-12-17T11:07:22.616+0530] {processor.py:186} INFO - Started process (PID=52758) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T11:07:22.617+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-17T11:07:22.618+0530] {logging_mixin.py:190} INFO - [2024-12-17T11:07:22.618+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T11:07:22.800+0530] {logging_mixin.py:190} INFO - [2024-12-17T11:07:22.799+0530] {dag.py:4435} INFO - dagrun id: movie_pipeline
[2024-12-17T11:07:22.815+0530] {logging_mixin.py:190} INFO - [2024-12-17T11:07:22.815+0530] {dag.py:4451} INFO - created dagrun <DagRun movie_pipeline @ 2024-12-17 05:37:22.661925+00:00: manual__2024-12-17T05:37:22.661925+00:00, state:running, queued_at: None. externally triggered: False>
[2024-12-17T11:07:22.844+0530] {logging_mixin.py:190} INFO - [2024-12-17T11:07:22.844+0530] {dag.py:4396} INFO - [DAG TEST] starting task_id=scrape_collection_page map_index=-1
[2024-12-17T11:07:22.844+0530] {logging_mixin.py:190} INFO - [2024-12-17T11:07:22.844+0530] {dag.py:4399} INFO - [DAG TEST] running task <TaskInstance: movie_pipeline.scrape_collection_page manual__2024-12-17T05:37:22.661925+00:00 [scheduled]>
[2024-12-17T11:07:27.945+0530] {logging_mixin.py:190} INFO - [2024-12-17T11:07:27.944+0530] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-12-17T11:07:28.009+0530] {logging_mixin.py:190} INFO - [2024-12-17 11:07:28,009] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T05:37:22.661925+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T05:37:22.661925+00:00'
[2024-12-17T11:07:28.009+0530] {logging_mixin.py:190} INFO - [2024-12-17T11:07:28.009+0530] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T05:37:22.661925+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T05:37:22.661925+00:00'
[2024-12-17T11:07:28.012+0530] {logging_mixin.py:190} INFO - Task instance is in running state
[2024-12-17T11:07:28.012+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: queued
[2024-12-17T11:07:28.013+0530] {logging_mixin.py:190} INFO - Current task name:scrape_collection_page state:scheduled start_date:None
[2024-12-17T11:07:28.013+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline and current dag run status:running
[2024-12-17T11:07:28.013+0530] {logging_mixin.py:190} INFO - [2024-12-17T11:07:28.013+0530] {taskinstance.py:731} INFO - ::endgroup::
[2024-12-17T11:07:28.015+0530] {logging_mixin.py:190} INFO - 2024-12-17
[2024-12-17T11:07:28.015+0530] {logging_mixin.py:190} INFO - [2024-12-17 11:07:28,015] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T11:07:28.016+0530] {logging_mixin.py:190} INFO - [2024-12-17T11:07:28.015+0530] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T11:07:28.021+0530] {logging_mixin.py:190} INFO - [2024-12-17T11:07:28.020+0530] {taskinstance.py:340} INFO - ::group::Post task execution logs
[2024-12-17T11:07:28.022+0530] {logging_mixin.py:190} INFO - [2024-12-17T11:07:28.021+0530] {taskinstance.py:352} INFO - Marking task as SUCCESS. dag_id=movie_pipeline, task_id=scrape_collection_page, run_id=manual__2024-12-17T05:37:22.661925+00:00, execution_date=20241217T053722, start_date=, end_date=20241217T053728
[2024-12-17T11:07:28.031+0530] {logging_mixin.py:190} INFO - Task instance in success state
[2024-12-17T11:07:28.031+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: running
[2024-12-17T11:07:28.032+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline queued_at:None
[2024-12-17T11:07:28.032+0530] {logging_mixin.py:190} INFO - Task hostname:Jayeshs-MacBook-Air.local operator:PythonOperator
[2024-12-17T11:07:28.033+0530] {logging_mixin.py:190} INFO - [2024-12-17T11:07:28.032+0530] {dag.py:4410} INFO - [DAG TEST] end task task_id=scrape_collection_page map_index=-1
[2024-12-17T11:07:28.036+0530] {logging_mixin.py:190} INFO - [2024-12-17T11:07:28.036+0530] {dagrun.py:854} INFO - Marking run <DagRun movie_pipeline @ 2024-12-17 05:37:22.661925+00:00: manual__2024-12-17T05:37:22.661925+00:00, state:running, queued_at: None. externally triggered: False> successful
[2024-12-17T11:07:28.037+0530] {logging_mixin.py:190} INFO - Dag run in success state
[2024-12-17T11:07:28.037+0530] {logging_mixin.py:190} INFO - Dag run start:2024-12-17 05:37:22.661925+00:00 end:2024-12-17 05:37:28.037115+00:00
[2024-12-17T11:07:28.038+0530] {logging_mixin.py:190} INFO - [2024-12-17T11:07:28.038+0530] {dagrun.py:905} INFO - DagRun Finished: dag_id=movie_pipeline, execution_date=2024-12-17 05:37:22.661925+00:00, run_id=manual__2024-12-17T05:37:22.661925+00:00, run_start_date=2024-12-17 05:37:22.661925+00:00, run_end_date=2024-12-17 05:37:28.037115+00:00, run_duration=5.37519, state=success, external_trigger=False, run_type=manual, data_interval_start=2024-12-16 05:37:22.661925+00:00, data_interval_end=2024-12-17 05:37:22.661925+00:00, dag_hash=None
[2024-12-17T11:07:28.047+0530] {processor.py:925} INFO - DAG(s) 'movie_pipeline' retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T11:07:28.060+0530] {logging_mixin.py:190} INFO - [2024-12-17T11:07:28.059+0530] {dag.py:3239} INFO - Sync 1 DAGs
[2024-12-17T11:07:28.077+0530] {logging_mixin.py:190} INFO - [2024-12-17T11:07:28.077+0530] {dag.py:4180} INFO - Setting next_dagrun for movie_pipeline to 2024-12-17 01:30:00+00:00, run_after=2024-12-18 01:30:00+00:00
[2024-12-17T11:07:28.096+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 5.483 seconds
[2024-12-17T11:10:24.835+0530] {processor.py:186} INFO - Started process (PID=52973) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T11:10:24.837+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-17T11:10:24.838+0530] {logging_mixin.py:190} INFO - [2024-12-17T11:10:24.838+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T11:10:25.079+0530] {logging_mixin.py:190} INFO - [2024-12-17T11:10:25.078+0530] {dag.py:4435} INFO - dagrun id: movie_pipeline
[2024-12-17T11:10:25.093+0530] {logging_mixin.py:190} INFO - [2024-12-17T11:10:25.092+0530] {dag.py:4451} INFO - created dagrun <DagRun movie_pipeline @ 2024-12-17 05:40:24.891595+00:00: manual__2024-12-17T05:40:24.891595+00:00, state:running, queued_at: None. externally triggered: False>
[2024-12-17T11:10:25.118+0530] {logging_mixin.py:190} INFO - [2024-12-17T11:10:25.117+0530] {dag.py:4396} INFO - [DAG TEST] starting task_id=scrape_collection_page map_index=-1
[2024-12-17T11:10:25.118+0530] {logging_mixin.py:190} INFO - [2024-12-17T11:10:25.118+0530] {dag.py:4399} INFO - [DAG TEST] running task <TaskInstance: movie_pipeline.scrape_collection_page manual__2024-12-17T05:40:24.891595+00:00 [scheduled]>
[2024-12-17T11:10:30.219+0530] {logging_mixin.py:190} INFO - [2024-12-17T11:10:30.219+0530] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-12-17T11:10:30.287+0530] {logging_mixin.py:190} INFO - [2024-12-17 11:10:30,287] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T05:40:24.891595+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T05:40:24.891595+00:00'
[2024-12-17T11:10:30.288+0530] {logging_mixin.py:190} INFO - [2024-12-17T11:10:30.287+0530] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T05:40:24.891595+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T05:40:24.891595+00:00'
[2024-12-17T11:10:30.291+0530] {logging_mixin.py:190} INFO - Task instance is in running state
[2024-12-17T11:10:30.291+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: queued
[2024-12-17T11:10:30.292+0530] {logging_mixin.py:190} INFO - Current task name:scrape_collection_page state:scheduled start_date:None
[2024-12-17T11:10:30.292+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline and current dag run status:running
[2024-12-17T11:10:30.293+0530] {logging_mixin.py:190} INFO - [2024-12-17T11:10:30.292+0530] {taskinstance.py:731} INFO - ::endgroup::
[2024-12-17T11:10:30.296+0530] {logging_mixin.py:190} INFO - 2024-12-17
[2024-12-17T11:10:30.297+0530] {logging_mixin.py:190} INFO - [2024-12-17 11:10:30,296] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T11:10:30.297+0530] {logging_mixin.py:190} INFO - [2024-12-17T11:10:30.296+0530] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T11:10:30.302+0530] {logging_mixin.py:190} INFO - [2024-12-17T11:10:30.302+0530] {taskinstance.py:340} INFO - ::group::Post task execution logs
[2024-12-17T11:10:30.303+0530] {logging_mixin.py:190} INFO - [2024-12-17T11:10:30.302+0530] {taskinstance.py:352} INFO - Marking task as SUCCESS. dag_id=movie_pipeline, task_id=scrape_collection_page, run_id=manual__2024-12-17T05:40:24.891595+00:00, execution_date=20241217T054024, start_date=, end_date=20241217T054030
[2024-12-17T11:10:30.316+0530] {logging_mixin.py:190} INFO - Task instance in success state
[2024-12-17T11:10:30.317+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: running
[2024-12-17T11:10:30.317+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline queued_at:None
[2024-12-17T11:10:30.317+0530] {logging_mixin.py:190} INFO - Task hostname:Jayeshs-MacBook-Air.local operator:PythonOperator
[2024-12-17T11:10:30.317+0530] {logging_mixin.py:190} INFO - [2024-12-17T11:10:30.317+0530] {dag.py:4410} INFO - [DAG TEST] end task task_id=scrape_collection_page map_index=-1
[2024-12-17T11:10:30.322+0530] {logging_mixin.py:190} INFO - [2024-12-17T11:10:30.322+0530] {dagrun.py:854} INFO - Marking run <DagRun movie_pipeline @ 2024-12-17 05:40:24.891595+00:00: manual__2024-12-17T05:40:24.891595+00:00, state:running, queued_at: None. externally triggered: False> successful
[2024-12-17T11:10:30.323+0530] {logging_mixin.py:190} INFO - Dag run in success state
[2024-12-17T11:10:30.323+0530] {logging_mixin.py:190} INFO - Dag run start:2024-12-17 05:40:24.891595+00:00 end:2024-12-17 05:40:30.323126+00:00
[2024-12-17T11:10:30.323+0530] {logging_mixin.py:190} INFO - [2024-12-17T11:10:30.323+0530] {dagrun.py:905} INFO - DagRun Finished: dag_id=movie_pipeline, execution_date=2024-12-17 05:40:24.891595+00:00, run_id=manual__2024-12-17T05:40:24.891595+00:00, run_start_date=2024-12-17 05:40:24.891595+00:00, run_end_date=2024-12-17 05:40:30.323126+00:00, run_duration=5.431531, state=success, external_trigger=False, run_type=manual, data_interval_start=2024-12-16 05:40:24.891595+00:00, data_interval_end=2024-12-17 05:40:24.891595+00:00, dag_hash=None
[2024-12-17T11:10:30.369+0530] {processor.py:925} INFO - DAG(s) 'movie_pipeline' retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T11:10:30.394+0530] {logging_mixin.py:190} INFO - [2024-12-17T11:10:30.391+0530] {dag.py:3239} INFO - Sync 1 DAGs
[2024-12-17T11:10:30.419+0530] {logging_mixin.py:190} INFO - [2024-12-17T11:10:30.419+0530] {dag.py:4180} INFO - Setting next_dagrun for movie_pipeline to 2024-12-17 01:30:00+00:00, run_after=2024-12-18 01:30:00+00:00
[2024-12-17T11:10:30.465+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 5.636 seconds
[2024-12-17T11:13:27.918+0530] {processor.py:186} INFO - Started process (PID=53176) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T11:13:27.919+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-17T11:13:27.920+0530] {logging_mixin.py:190} INFO - [2024-12-17T11:13:27.920+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T11:13:28.098+0530] {logging_mixin.py:190} INFO - [2024-12-17T11:13:28.098+0530] {dag.py:4435} INFO - dagrun id: movie_pipeline
[2024-12-17T11:13:28.114+0530] {logging_mixin.py:190} INFO - [2024-12-17T11:13:28.114+0530] {dag.py:4451} INFO - created dagrun <DagRun movie_pipeline @ 2024-12-17 05:43:27.960974+00:00: manual__2024-12-17T05:43:27.960974+00:00, state:running, queued_at: None. externally triggered: False>
[2024-12-17T11:13:28.143+0530] {logging_mixin.py:190} INFO - [2024-12-17T11:13:28.143+0530] {dag.py:4396} INFO - [DAG TEST] starting task_id=scrape_collection_page map_index=-1
[2024-12-17T11:13:28.144+0530] {logging_mixin.py:190} INFO - [2024-12-17T11:13:28.143+0530] {dag.py:4399} INFO - [DAG TEST] running task <TaskInstance: movie_pipeline.scrape_collection_page manual__2024-12-17T05:43:27.960974+00:00 [scheduled]>
[2024-12-17T11:13:33.243+0530] {logging_mixin.py:190} INFO - [2024-12-17T11:13:33.242+0530] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-12-17T11:13:33.325+0530] {logging_mixin.py:190} INFO - [2024-12-17 11:13:33,324] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T05:43:27.960974+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T05:43:27.960974+00:00'
[2024-12-17T11:13:33.325+0530] {logging_mixin.py:190} INFO - [2024-12-17T11:13:33.324+0530] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T05:43:27.960974+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T05:43:27.960974+00:00'
[2024-12-17T11:13:33.331+0530] {logging_mixin.py:190} INFO - Task instance is in running state
[2024-12-17T11:13:33.332+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: queued
[2024-12-17T11:13:33.332+0530] {logging_mixin.py:190} INFO - Current task name:scrape_collection_page state:scheduled start_date:None
[2024-12-17T11:13:33.332+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline and current dag run status:running
[2024-12-17T11:13:33.333+0530] {logging_mixin.py:190} INFO - [2024-12-17T11:13:33.333+0530] {taskinstance.py:731} INFO - ::endgroup::
[2024-12-17T11:13:33.335+0530] {logging_mixin.py:190} INFO - 2024-12-17
[2024-12-17T11:13:33.336+0530] {logging_mixin.py:190} INFO - [2024-12-17 11:13:33,336] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T11:13:33.337+0530] {logging_mixin.py:190} INFO - [2024-12-17T11:13:33.336+0530] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T11:13:33.341+0530] {logging_mixin.py:190} INFO - [2024-12-17T11:13:33.341+0530] {taskinstance.py:340} INFO - ::group::Post task execution logs
[2024-12-17T11:13:33.342+0530] {logging_mixin.py:190} INFO - [2024-12-17T11:13:33.342+0530] {taskinstance.py:352} INFO - Marking task as SUCCESS. dag_id=movie_pipeline, task_id=scrape_collection_page, run_id=manual__2024-12-17T05:43:27.960974+00:00, execution_date=20241217T054327, start_date=, end_date=20241217T054333
[2024-12-17T11:13:33.352+0530] {logging_mixin.py:190} INFO - Task instance in success state
[2024-12-17T11:13:33.352+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: running
[2024-12-17T11:13:33.352+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline queued_at:None
[2024-12-17T11:13:33.352+0530] {logging_mixin.py:190} INFO - Task hostname:Jayeshs-MacBook-Air.local operator:PythonOperator
[2024-12-17T11:13:33.353+0530] {logging_mixin.py:190} INFO - [2024-12-17T11:13:33.352+0530] {dag.py:4410} INFO - [DAG TEST] end task task_id=scrape_collection_page map_index=-1
[2024-12-17T11:13:33.356+0530] {logging_mixin.py:190} INFO - [2024-12-17T11:13:33.356+0530] {dagrun.py:854} INFO - Marking run <DagRun movie_pipeline @ 2024-12-17 05:43:27.960974+00:00: manual__2024-12-17T05:43:27.960974+00:00, state:running, queued_at: None. externally triggered: False> successful
[2024-12-17T11:13:33.357+0530] {logging_mixin.py:190} INFO - Dag run in success state
[2024-12-17T11:13:33.357+0530] {logging_mixin.py:190} INFO - Dag run start:2024-12-17 05:43:27.960974+00:00 end:2024-12-17 05:43:33.357205+00:00
[2024-12-17T11:13:33.358+0530] {logging_mixin.py:190} INFO - [2024-12-17T11:13:33.357+0530] {dagrun.py:905} INFO - DagRun Finished: dag_id=movie_pipeline, execution_date=2024-12-17 05:43:27.960974+00:00, run_id=manual__2024-12-17T05:43:27.960974+00:00, run_start_date=2024-12-17 05:43:27.960974+00:00, run_end_date=2024-12-17 05:43:33.357205+00:00, run_duration=5.396231, state=success, external_trigger=False, run_type=manual, data_interval_start=2024-12-16 05:43:27.960974+00:00, data_interval_end=2024-12-17 05:43:27.960974+00:00, dag_hash=None
[2024-12-17T11:13:33.367+0530] {processor.py:925} INFO - DAG(s) 'movie_pipeline' retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T11:13:33.381+0530] {logging_mixin.py:190} INFO - [2024-12-17T11:13:33.381+0530] {dag.py:3239} INFO - Sync 1 DAGs
[2024-12-17T11:13:33.400+0530] {logging_mixin.py:190} INFO - [2024-12-17T11:13:33.399+0530] {dag.py:4180} INFO - Setting next_dagrun for movie_pipeline to 2024-12-17 01:30:00+00:00, run_after=2024-12-18 01:30:00+00:00
[2024-12-17T11:13:33.421+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 5.507 seconds
[2024-12-17T11:16:31.974+0530] {processor.py:186} INFO - Started process (PID=53389) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T11:16:31.975+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-17T11:16:31.976+0530] {logging_mixin.py:190} INFO - [2024-12-17T11:16:31.976+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T11:16:32.178+0530] {logging_mixin.py:190} INFO - [2024-12-17T11:16:32.178+0530] {dag.py:4435} INFO - dagrun id: movie_pipeline
[2024-12-17T11:16:32.192+0530] {logging_mixin.py:190} INFO - [2024-12-17T11:16:32.191+0530] {dag.py:4451} INFO - created dagrun <DagRun movie_pipeline @ 2024-12-17 05:46:32.023202+00:00: manual__2024-12-17T05:46:32.023202+00:00, state:running, queued_at: None. externally triggered: False>
[2024-12-17T11:16:32.219+0530] {logging_mixin.py:190} INFO - [2024-12-17T11:16:32.219+0530] {dag.py:4396} INFO - [DAG TEST] starting task_id=scrape_collection_page map_index=-1
[2024-12-17T11:16:32.220+0530] {logging_mixin.py:190} INFO - [2024-12-17T11:16:32.219+0530] {dag.py:4399} INFO - [DAG TEST] running task <TaskInstance: movie_pipeline.scrape_collection_page manual__2024-12-17T05:46:32.023202+00:00 [scheduled]>
[2024-12-17T11:16:37.321+0530] {logging_mixin.py:190} INFO - [2024-12-17T11:16:37.321+0530] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-12-17T11:16:37.387+0530] {logging_mixin.py:190} INFO - [2024-12-17 11:16:37,387] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T05:46:32.023202+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T05:46:32.023202+00:00'
[2024-12-17T11:16:37.391+0530] {logging_mixin.py:190} INFO - [2024-12-17T11:16:37.387+0530] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T05:46:32.023202+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T05:46:32.023202+00:00'
[2024-12-17T11:16:37.397+0530] {logging_mixin.py:190} INFO - Task instance is in running state
[2024-12-17T11:16:37.397+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: queued
[2024-12-17T11:16:37.397+0530] {logging_mixin.py:190} INFO - Current task name:scrape_collection_page state:scheduled start_date:None
[2024-12-17T11:16:37.397+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline and current dag run status:running
[2024-12-17T11:16:37.398+0530] {logging_mixin.py:190} INFO - [2024-12-17T11:16:37.398+0530] {taskinstance.py:731} INFO - ::endgroup::
[2024-12-17T11:16:37.399+0530] {logging_mixin.py:190} INFO - 2024-12-17
[2024-12-17T11:16:37.400+0530] {logging_mixin.py:190} INFO - [2024-12-17 11:16:37,400] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T11:16:37.401+0530] {logging_mixin.py:190} INFO - [2024-12-17T11:16:37.400+0530] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T11:16:37.412+0530] {logging_mixin.py:190} INFO - [2024-12-17T11:16:37.412+0530] {taskinstance.py:340} INFO - ::group::Post task execution logs
[2024-12-17T11:16:37.413+0530] {logging_mixin.py:190} INFO - [2024-12-17T11:16:37.413+0530] {taskinstance.py:352} INFO - Marking task as SUCCESS. dag_id=movie_pipeline, task_id=scrape_collection_page, run_id=manual__2024-12-17T05:46:32.023202+00:00, execution_date=20241217T054632, start_date=, end_date=20241217T054637
[2024-12-17T11:16:37.427+0530] {logging_mixin.py:190} INFO - Task instance in success state
[2024-12-17T11:16:37.427+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: running
[2024-12-17T11:16:37.428+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline queued_at:None
[2024-12-17T11:16:37.428+0530] {logging_mixin.py:190} INFO - Task hostname:Jayeshs-MacBook-Air.local operator:PythonOperator
[2024-12-17T11:16:37.429+0530] {logging_mixin.py:190} INFO - [2024-12-17T11:16:37.429+0530] {dag.py:4410} INFO - [DAG TEST] end task task_id=scrape_collection_page map_index=-1
[2024-12-17T11:16:37.433+0530] {logging_mixin.py:190} INFO - [2024-12-17T11:16:37.433+0530] {dagrun.py:854} INFO - Marking run <DagRun movie_pipeline @ 2024-12-17 05:46:32.023202+00:00: manual__2024-12-17T05:46:32.023202+00:00, state:running, queued_at: None. externally triggered: False> successful
[2024-12-17T11:16:37.434+0530] {logging_mixin.py:190} INFO - Dag run in success state
[2024-12-17T11:16:37.435+0530] {logging_mixin.py:190} INFO - Dag run start:2024-12-17 05:46:32.023202+00:00 end:2024-12-17 05:46:37.434638+00:00
[2024-12-17T11:16:37.435+0530] {logging_mixin.py:190} INFO - [2024-12-17T11:16:37.435+0530] {dagrun.py:905} INFO - DagRun Finished: dag_id=movie_pipeline, execution_date=2024-12-17 05:46:32.023202+00:00, run_id=manual__2024-12-17T05:46:32.023202+00:00, run_start_date=2024-12-17 05:46:32.023202+00:00, run_end_date=2024-12-17 05:46:37.434638+00:00, run_duration=5.411436, state=success, external_trigger=False, run_type=manual, data_interval_start=2024-12-16 05:46:32.023202+00:00, data_interval_end=2024-12-17 05:46:32.023202+00:00, dag_hash=None
[2024-12-17T11:16:37.446+0530] {processor.py:925} INFO - DAG(s) 'movie_pipeline' retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T11:16:37.465+0530] {logging_mixin.py:190} INFO - [2024-12-17T11:16:37.465+0530] {dag.py:3239} INFO - Sync 1 DAGs
[2024-12-17T11:16:37.485+0530] {logging_mixin.py:190} INFO - [2024-12-17T11:16:37.484+0530] {dag.py:4180} INFO - Setting next_dagrun for movie_pipeline to 2024-12-17 01:30:00+00:00, run_after=2024-12-18 01:30:00+00:00
[2024-12-17T11:16:37.504+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 5.533 seconds
[2024-12-17T11:19:38.397+0530] {processor.py:186} INFO - Started process (PID=53593) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T11:19:38.398+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-17T11:19:38.399+0530] {logging_mixin.py:190} INFO - [2024-12-17T11:19:38.399+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T11:19:38.582+0530] {logging_mixin.py:190} INFO - [2024-12-17T11:19:38.582+0530] {dag.py:4435} INFO - dagrun id: movie_pipeline
[2024-12-17T11:19:38.597+0530] {logging_mixin.py:190} INFO - [2024-12-17T11:19:38.596+0530] {dag.py:4451} INFO - created dagrun <DagRun movie_pipeline @ 2024-12-17 05:49:38.440069+00:00: manual__2024-12-17T05:49:38.440069+00:00, state:running, queued_at: None. externally triggered: False>
[2024-12-17T11:19:38.626+0530] {logging_mixin.py:190} INFO - [2024-12-17T11:19:38.626+0530] {dag.py:4396} INFO - [DAG TEST] starting task_id=scrape_collection_page map_index=-1
[2024-12-17T11:19:38.627+0530] {logging_mixin.py:190} INFO - [2024-12-17T11:19:38.627+0530] {dag.py:4399} INFO - [DAG TEST] running task <TaskInstance: movie_pipeline.scrape_collection_page manual__2024-12-17T05:49:38.440069+00:00 [scheduled]>
[2024-12-17T11:19:43.729+0530] {logging_mixin.py:190} INFO - [2024-12-17T11:19:43.729+0530] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-12-17T11:19:43.793+0530] {logging_mixin.py:190} INFO - [2024-12-17 11:19:43,793] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T05:49:38.440069+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T05:49:38.440069+00:00'
[2024-12-17T11:19:43.793+0530] {logging_mixin.py:190} INFO - [2024-12-17T11:19:43.793+0530] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T05:49:38.440069+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T05:49:38.440069+00:00'
[2024-12-17T11:19:43.797+0530] {logging_mixin.py:190} INFO - Task instance is in running state
[2024-12-17T11:19:43.797+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: queued
[2024-12-17T11:19:43.798+0530] {logging_mixin.py:190} INFO - Current task name:scrape_collection_page state:scheduled start_date:None
[2024-12-17T11:19:43.798+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline and current dag run status:running
[2024-12-17T11:19:43.798+0530] {logging_mixin.py:190} INFO - [2024-12-17T11:19:43.798+0530] {taskinstance.py:731} INFO - ::endgroup::
[2024-12-17T11:19:43.800+0530] {logging_mixin.py:190} INFO - 2024-12-17
[2024-12-17T11:19:43.800+0530] {logging_mixin.py:190} INFO - [2024-12-17 11:19:43,800] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T11:19:43.801+0530] {logging_mixin.py:190} INFO - [2024-12-17T11:19:43.800+0530] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T11:19:43.807+0530] {logging_mixin.py:190} INFO - [2024-12-17T11:19:43.806+0530] {taskinstance.py:340} INFO - ::group::Post task execution logs
[2024-12-17T11:19:43.807+0530] {logging_mixin.py:190} INFO - [2024-12-17T11:19:43.807+0530] {taskinstance.py:352} INFO - Marking task as SUCCESS. dag_id=movie_pipeline, task_id=scrape_collection_page, run_id=manual__2024-12-17T05:49:38.440069+00:00, execution_date=20241217T054938, start_date=, end_date=20241217T054943
[2024-12-17T11:19:43.817+0530] {logging_mixin.py:190} INFO - Task instance in success state
[2024-12-17T11:19:43.817+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: running
[2024-12-17T11:19:43.817+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline queued_at:None
[2024-12-17T11:19:43.817+0530] {logging_mixin.py:190} INFO - Task hostname:Jayeshs-MacBook-Air.local operator:PythonOperator
[2024-12-17T11:19:43.818+0530] {logging_mixin.py:190} INFO - [2024-12-17T11:19:43.818+0530] {dag.py:4410} INFO - [DAG TEST] end task task_id=scrape_collection_page map_index=-1
[2024-12-17T11:19:43.822+0530] {logging_mixin.py:190} INFO - [2024-12-17T11:19:43.821+0530] {dagrun.py:854} INFO - Marking run <DagRun movie_pipeline @ 2024-12-17 05:49:38.440069+00:00: manual__2024-12-17T05:49:38.440069+00:00, state:running, queued_at: None. externally triggered: False> successful
[2024-12-17T11:19:43.822+0530] {logging_mixin.py:190} INFO - Dag run in success state
[2024-12-17T11:19:43.822+0530] {logging_mixin.py:190} INFO - Dag run start:2024-12-17 05:49:38.440069+00:00 end:2024-12-17 05:49:43.822248+00:00
[2024-12-17T11:19:43.823+0530] {logging_mixin.py:190} INFO - [2024-12-17T11:19:43.823+0530] {dagrun.py:905} INFO - DagRun Finished: dag_id=movie_pipeline, execution_date=2024-12-17 05:49:38.440069+00:00, run_id=manual__2024-12-17T05:49:38.440069+00:00, run_start_date=2024-12-17 05:49:38.440069+00:00, run_end_date=2024-12-17 05:49:43.822248+00:00, run_duration=5.382179, state=success, external_trigger=False, run_type=manual, data_interval_start=2024-12-16 05:49:38.440069+00:00, data_interval_end=2024-12-17 05:49:38.440069+00:00, dag_hash=None
[2024-12-17T11:19:43.832+0530] {processor.py:925} INFO - DAG(s) 'movie_pipeline' retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T11:19:43.846+0530] {logging_mixin.py:190} INFO - [2024-12-17T11:19:43.845+0530] {dag.py:3239} INFO - Sync 1 DAGs
[2024-12-17T11:19:43.865+0530] {logging_mixin.py:190} INFO - [2024-12-17T11:19:43.865+0530] {dag.py:4180} INFO - Setting next_dagrun for movie_pipeline to 2024-12-17 01:30:00+00:00, run_after=2024-12-18 01:30:00+00:00
[2024-12-17T11:19:43.885+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 5.492 seconds
[2024-12-17T11:22:46.089+0530] {processor.py:186} INFO - Started process (PID=53810) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T11:22:46.090+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-17T11:22:46.091+0530] {logging_mixin.py:190} INFO - [2024-12-17T11:22:46.091+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T11:22:46.269+0530] {logging_mixin.py:190} INFO - [2024-12-17T11:22:46.269+0530] {dag.py:4435} INFO - dagrun id: movie_pipeline
[2024-12-17T11:22:46.283+0530] {logging_mixin.py:190} INFO - [2024-12-17T11:22:46.283+0530] {dag.py:4451} INFO - created dagrun <DagRun movie_pipeline @ 2024-12-17 05:52:46.131873+00:00: manual__2024-12-17T05:52:46.131873+00:00, state:running, queued_at: None. externally triggered: False>
[2024-12-17T11:22:46.311+0530] {logging_mixin.py:190} INFO - [2024-12-17T11:22:46.311+0530] {dag.py:4396} INFO - [DAG TEST] starting task_id=scrape_collection_page map_index=-1
[2024-12-17T11:22:46.312+0530] {logging_mixin.py:190} INFO - [2024-12-17T11:22:46.312+0530] {dag.py:4399} INFO - [DAG TEST] running task <TaskInstance: movie_pipeline.scrape_collection_page manual__2024-12-17T05:52:46.131873+00:00 [scheduled]>
[2024-12-17T11:22:51.410+0530] {logging_mixin.py:190} INFO - [2024-12-17T11:22:51.410+0530] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-12-17T11:22:51.474+0530] {logging_mixin.py:190} INFO - [2024-12-17 11:22:51,473] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T05:52:46.131873+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T05:52:46.131873+00:00'
[2024-12-17T11:22:51.474+0530] {logging_mixin.py:190} INFO - [2024-12-17T11:22:51.473+0530] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T05:52:46.131873+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T05:52:46.131873+00:00'
[2024-12-17T11:22:51.477+0530] {logging_mixin.py:190} INFO - Task instance is in running state
[2024-12-17T11:22:51.477+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: queued
[2024-12-17T11:22:51.477+0530] {logging_mixin.py:190} INFO - Current task name:scrape_collection_page state:scheduled start_date:None
[2024-12-17T11:22:51.478+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline and current dag run status:running
[2024-12-17T11:22:51.478+0530] {logging_mixin.py:190} INFO - [2024-12-17T11:22:51.478+0530] {taskinstance.py:731} INFO - ::endgroup::
[2024-12-17T11:22:51.480+0530] {logging_mixin.py:190} INFO - 2024-12-17
[2024-12-17T11:22:51.481+0530] {logging_mixin.py:190} INFO - [2024-12-17 11:22:51,481] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T11:22:51.481+0530] {logging_mixin.py:190} INFO - [2024-12-17T11:22:51.481+0530] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T11:22:51.487+0530] {logging_mixin.py:190} INFO - [2024-12-17T11:22:51.487+0530] {taskinstance.py:340} INFO - ::group::Post task execution logs
[2024-12-17T11:22:51.487+0530] {logging_mixin.py:190} INFO - [2024-12-17T11:22:51.487+0530] {taskinstance.py:352} INFO - Marking task as SUCCESS. dag_id=movie_pipeline, task_id=scrape_collection_page, run_id=manual__2024-12-17T05:52:46.131873+00:00, execution_date=20241217T055246, start_date=, end_date=20241217T055251
[2024-12-17T11:22:51.496+0530] {logging_mixin.py:190} INFO - Task instance in success state
[2024-12-17T11:22:51.497+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: running
[2024-12-17T11:22:51.497+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline queued_at:None
[2024-12-17T11:22:51.499+0530] {logging_mixin.py:190} INFO - Task hostname:Jayeshs-MacBook-Air.local operator:PythonOperator
[2024-12-17T11:22:51.499+0530] {logging_mixin.py:190} INFO - [2024-12-17T11:22:51.499+0530] {dag.py:4410} INFO - [DAG TEST] end task task_id=scrape_collection_page map_index=-1
[2024-12-17T11:22:51.503+0530] {logging_mixin.py:190} INFO - [2024-12-17T11:22:51.503+0530] {dagrun.py:854} INFO - Marking run <DagRun movie_pipeline @ 2024-12-17 05:52:46.131873+00:00: manual__2024-12-17T05:52:46.131873+00:00, state:running, queued_at: None. externally triggered: False> successful
[2024-12-17T11:22:51.504+0530] {logging_mixin.py:190} INFO - Dag run in success state
[2024-12-17T11:22:51.504+0530] {logging_mixin.py:190} INFO - Dag run start:2024-12-17 05:52:46.131873+00:00 end:2024-12-17 05:52:51.504028+00:00
[2024-12-17T11:22:51.504+0530] {logging_mixin.py:190} INFO - [2024-12-17T11:22:51.504+0530] {dagrun.py:905} INFO - DagRun Finished: dag_id=movie_pipeline, execution_date=2024-12-17 05:52:46.131873+00:00, run_id=manual__2024-12-17T05:52:46.131873+00:00, run_start_date=2024-12-17 05:52:46.131873+00:00, run_end_date=2024-12-17 05:52:51.504028+00:00, run_duration=5.372155, state=success, external_trigger=False, run_type=manual, data_interval_start=2024-12-16 05:52:46.131873+00:00, data_interval_end=2024-12-17 05:52:46.131873+00:00, dag_hash=None
[2024-12-17T11:22:51.514+0530] {processor.py:925} INFO - DAG(s) 'movie_pipeline' retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T11:22:51.526+0530] {logging_mixin.py:190} INFO - [2024-12-17T11:22:51.526+0530] {dag.py:3239} INFO - Sync 1 DAGs
[2024-12-17T11:22:51.546+0530] {logging_mixin.py:190} INFO - [2024-12-17T11:22:51.546+0530] {dag.py:4180} INFO - Setting next_dagrun for movie_pipeline to 2024-12-17 01:30:00+00:00, run_after=2024-12-18 01:30:00+00:00
[2024-12-17T11:22:51.568+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 5.483 seconds
[2024-12-17T11:25:50.754+0530] {processor.py:186} INFO - Started process (PID=54020) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T11:25:50.756+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-17T11:25:50.757+0530] {logging_mixin.py:190} INFO - [2024-12-17T11:25:50.757+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T11:25:50.944+0530] {logging_mixin.py:190} INFO - [2024-12-17T11:25:50.944+0530] {dag.py:4435} INFO - dagrun id: movie_pipeline
[2024-12-17T11:25:50.961+0530] {logging_mixin.py:190} INFO - [2024-12-17T11:25:50.960+0530] {dag.py:4451} INFO - created dagrun <DagRun movie_pipeline @ 2024-12-17 05:55:50.798572+00:00: manual__2024-12-17T05:55:50.798572+00:00, state:running, queued_at: None. externally triggered: False>
[2024-12-17T11:25:50.987+0530] {logging_mixin.py:190} INFO - [2024-12-17T11:25:50.987+0530] {dag.py:4396} INFO - [DAG TEST] starting task_id=scrape_collection_page map_index=-1
[2024-12-17T11:25:50.988+0530] {logging_mixin.py:190} INFO - [2024-12-17T11:25:50.987+0530] {dag.py:4399} INFO - [DAG TEST] running task <TaskInstance: movie_pipeline.scrape_collection_page manual__2024-12-17T05:55:50.798572+00:00 [scheduled]>
[2024-12-17T11:25:56.094+0530] {logging_mixin.py:190} INFO - [2024-12-17T11:25:56.093+0530] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-12-17T11:25:56.161+0530] {logging_mixin.py:190} INFO - [2024-12-17 11:25:56,161] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T05:55:50.798572+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T05:55:50.798572+00:00'
[2024-12-17T11:25:56.161+0530] {logging_mixin.py:190} INFO - [2024-12-17T11:25:56.161+0530] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T05:55:50.798572+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T05:55:50.798572+00:00'
[2024-12-17T11:25:56.164+0530] {logging_mixin.py:190} INFO - Task instance is in running state
[2024-12-17T11:25:56.165+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: queued
[2024-12-17T11:25:56.165+0530] {logging_mixin.py:190} INFO - Current task name:scrape_collection_page state:scheduled start_date:None
[2024-12-17T11:25:56.165+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline and current dag run status:running
[2024-12-17T11:25:56.166+0530] {logging_mixin.py:190} INFO - [2024-12-17T11:25:56.165+0530] {taskinstance.py:731} INFO - ::endgroup::
[2024-12-17T11:25:56.167+0530] {logging_mixin.py:190} INFO - 2024-12-17
[2024-12-17T11:25:56.168+0530] {logging_mixin.py:190} INFO - [2024-12-17 11:25:56,167] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T11:25:56.168+0530] {logging_mixin.py:190} INFO - [2024-12-17T11:25:56.167+0530] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T11:25:56.175+0530] {logging_mixin.py:190} INFO - [2024-12-17T11:25:56.175+0530] {taskinstance.py:340} INFO - ::group::Post task execution logs
[2024-12-17T11:25:56.176+0530] {logging_mixin.py:190} INFO - [2024-12-17T11:25:56.176+0530] {taskinstance.py:352} INFO - Marking task as SUCCESS. dag_id=movie_pipeline, task_id=scrape_collection_page, run_id=manual__2024-12-17T05:55:50.798572+00:00, execution_date=20241217T055550, start_date=, end_date=20241217T055556
[2024-12-17T11:25:56.185+0530] {logging_mixin.py:190} INFO - Task instance in success state
[2024-12-17T11:25:56.185+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: running
[2024-12-17T11:25:56.186+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline queued_at:None
[2024-12-17T11:25:56.186+0530] {logging_mixin.py:190} INFO - Task hostname:Jayeshs-MacBook-Air.local operator:PythonOperator
[2024-12-17T11:25:56.186+0530] {logging_mixin.py:190} INFO - [2024-12-17T11:25:56.186+0530] {dag.py:4410} INFO - [DAG TEST] end task task_id=scrape_collection_page map_index=-1
[2024-12-17T11:25:56.191+0530] {logging_mixin.py:190} INFO - [2024-12-17T11:25:56.191+0530] {dagrun.py:854} INFO - Marking run <DagRun movie_pipeline @ 2024-12-17 05:55:50.798572+00:00: manual__2024-12-17T05:55:50.798572+00:00, state:running, queued_at: None. externally triggered: False> successful
[2024-12-17T11:25:56.191+0530] {logging_mixin.py:190} INFO - Dag run in success state
[2024-12-17T11:25:56.192+0530] {logging_mixin.py:190} INFO - Dag run start:2024-12-17 05:55:50.798572+00:00 end:2024-12-17 05:55:56.191761+00:00
[2024-12-17T11:25:56.192+0530] {logging_mixin.py:190} INFO - [2024-12-17T11:25:56.192+0530] {dagrun.py:905} INFO - DagRun Finished: dag_id=movie_pipeline, execution_date=2024-12-17 05:55:50.798572+00:00, run_id=manual__2024-12-17T05:55:50.798572+00:00, run_start_date=2024-12-17 05:55:50.798572+00:00, run_end_date=2024-12-17 05:55:56.191761+00:00, run_duration=5.393189, state=success, external_trigger=False, run_type=manual, data_interval_start=2024-12-16 05:55:50.798572+00:00, data_interval_end=2024-12-17 05:55:50.798572+00:00, dag_hash=None
[2024-12-17T11:25:56.200+0530] {processor.py:925} INFO - DAG(s) 'movie_pipeline' retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T11:25:56.213+0530] {logging_mixin.py:190} INFO - [2024-12-17T11:25:56.213+0530] {dag.py:3239} INFO - Sync 1 DAGs
[2024-12-17T11:25:56.230+0530] {logging_mixin.py:190} INFO - [2024-12-17T11:25:56.230+0530] {dag.py:4180} INFO - Setting next_dagrun for movie_pipeline to 2024-12-17 01:30:00+00:00, run_after=2024-12-18 01:30:00+00:00
[2024-12-17T11:25:56.249+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 5.499 seconds
[2024-12-17T11:28:53.189+0530] {processor.py:186} INFO - Started process (PID=54233) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T11:28:53.191+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-17T11:28:53.193+0530] {logging_mixin.py:190} INFO - [2024-12-17T11:28:53.192+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T11:28:53.417+0530] {logging_mixin.py:190} INFO - [2024-12-17T11:28:53.416+0530] {dag.py:4435} INFO - dagrun id: movie_pipeline
[2024-12-17T11:28:53.437+0530] {logging_mixin.py:190} INFO - [2024-12-17T11:28:53.436+0530] {dag.py:4451} INFO - created dagrun <DagRun movie_pipeline @ 2024-12-17 05:58:53.241503+00:00: manual__2024-12-17T05:58:53.241503+00:00, state:running, queued_at: None. externally triggered: False>
[2024-12-17T11:28:53.469+0530] {logging_mixin.py:190} INFO - [2024-12-17T11:28:53.469+0530] {dag.py:4396} INFO - [DAG TEST] starting task_id=scrape_collection_page map_index=-1
[2024-12-17T11:28:53.470+0530] {logging_mixin.py:190} INFO - [2024-12-17T11:28:53.469+0530] {dag.py:4399} INFO - [DAG TEST] running task <TaskInstance: movie_pipeline.scrape_collection_page manual__2024-12-17T05:58:53.241503+00:00 [scheduled]>
[2024-12-17T11:28:58.586+0530] {logging_mixin.py:190} INFO - [2024-12-17T11:28:58.586+0530] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-12-17T11:28:58.650+0530] {logging_mixin.py:190} INFO - [2024-12-17 11:28:58,650] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T05:58:53.241503+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T05:58:53.241503+00:00'
[2024-12-17T11:28:58.651+0530] {logging_mixin.py:190} INFO - [2024-12-17T11:28:58.650+0530] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T05:58:53.241503+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T05:58:53.241503+00:00'
[2024-12-17T11:28:58.654+0530] {logging_mixin.py:190} INFO - Task instance is in running state
[2024-12-17T11:28:58.655+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: queued
[2024-12-17T11:28:58.655+0530] {logging_mixin.py:190} INFO - Current task name:scrape_collection_page state:scheduled start_date:None
[2024-12-17T11:28:58.656+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline and current dag run status:running
[2024-12-17T11:28:58.656+0530] {logging_mixin.py:190} INFO - [2024-12-17T11:28:58.656+0530] {taskinstance.py:731} INFO - ::endgroup::
[2024-12-17T11:28:58.658+0530] {logging_mixin.py:190} INFO - 2024-12-17
[2024-12-17T11:28:58.659+0530] {logging_mixin.py:190} INFO - [2024-12-17 11:28:58,658] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T11:28:58.659+0530] {logging_mixin.py:190} INFO - [2024-12-17T11:28:58.658+0530] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T11:28:58.664+0530] {logging_mixin.py:190} INFO - [2024-12-17T11:28:58.664+0530] {taskinstance.py:340} INFO - ::group::Post task execution logs
[2024-12-17T11:28:58.665+0530] {logging_mixin.py:190} INFO - [2024-12-17T11:28:58.665+0530] {taskinstance.py:352} INFO - Marking task as SUCCESS. dag_id=movie_pipeline, task_id=scrape_collection_page, run_id=manual__2024-12-17T05:58:53.241503+00:00, execution_date=20241217T055853, start_date=, end_date=20241217T055858
[2024-12-17T11:28:58.676+0530] {logging_mixin.py:190} INFO - Task instance in success state
[2024-12-17T11:28:58.677+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: running
[2024-12-17T11:28:58.677+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline queued_at:None
[2024-12-17T11:28:58.677+0530] {logging_mixin.py:190} INFO - Task hostname:Jayeshs-MacBook-Air.local operator:PythonOperator
[2024-12-17T11:28:58.678+0530] {logging_mixin.py:190} INFO - [2024-12-17T11:28:58.678+0530] {dag.py:4410} INFO - [DAG TEST] end task task_id=scrape_collection_page map_index=-1
[2024-12-17T11:28:58.683+0530] {logging_mixin.py:190} INFO - [2024-12-17T11:28:58.682+0530] {dagrun.py:854} INFO - Marking run <DagRun movie_pipeline @ 2024-12-17 05:58:53.241503+00:00: manual__2024-12-17T05:58:53.241503+00:00, state:running, queued_at: None. externally triggered: False> successful
[2024-12-17T11:28:58.683+0530] {logging_mixin.py:190} INFO - Dag run in success state
[2024-12-17T11:28:58.683+0530] {logging_mixin.py:190} INFO - Dag run start:2024-12-17 05:58:53.241503+00:00 end:2024-12-17 05:58:58.683385+00:00
[2024-12-17T11:28:58.684+0530] {logging_mixin.py:190} INFO - [2024-12-17T11:28:58.684+0530] {dagrun.py:905} INFO - DagRun Finished: dag_id=movie_pipeline, execution_date=2024-12-17 05:58:53.241503+00:00, run_id=manual__2024-12-17T05:58:53.241503+00:00, run_start_date=2024-12-17 05:58:53.241503+00:00, run_end_date=2024-12-17 05:58:58.683385+00:00, run_duration=5.441882, state=success, external_trigger=False, run_type=manual, data_interval_start=2024-12-16 05:58:53.241503+00:00, data_interval_end=2024-12-17 05:58:53.241503+00:00, dag_hash=None
[2024-12-17T11:28:58.692+0530] {processor.py:925} INFO - DAG(s) 'movie_pipeline' retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T11:28:58.705+0530] {logging_mixin.py:190} INFO - [2024-12-17T11:28:58.704+0530] {dag.py:3239} INFO - Sync 1 DAGs
[2024-12-17T11:28:58.724+0530] {logging_mixin.py:190} INFO - [2024-12-17T11:28:58.723+0530] {dag.py:4180} INFO - Setting next_dagrun for movie_pipeline to 2024-12-17 01:30:00+00:00, run_after=2024-12-18 01:30:00+00:00
[2024-12-17T11:28:58.743+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 5.559 seconds
[2024-12-17T11:31:55.850+0530] {processor.py:186} INFO - Started process (PID=54435) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T11:31:55.852+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-17T11:31:55.854+0530] {logging_mixin.py:190} INFO - [2024-12-17T11:31:55.853+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T11:31:56.035+0530] {logging_mixin.py:190} INFO - [2024-12-17T11:31:56.035+0530] {dag.py:4435} INFO - dagrun id: movie_pipeline
[2024-12-17T11:31:56.051+0530] {logging_mixin.py:190} INFO - [2024-12-17T11:31:56.051+0530] {dag.py:4451} INFO - created dagrun <DagRun movie_pipeline @ 2024-12-17 06:01:55.898253+00:00: manual__2024-12-17T06:01:55.898253+00:00, state:running, queued_at: None. externally triggered: False>
[2024-12-17T11:31:56.079+0530] {logging_mixin.py:190} INFO - [2024-12-17T11:31:56.079+0530] {dag.py:4396} INFO - [DAG TEST] starting task_id=scrape_collection_page map_index=-1
[2024-12-17T11:31:56.079+0530] {logging_mixin.py:190} INFO - [2024-12-17T11:31:56.079+0530] {dag.py:4399} INFO - [DAG TEST] running task <TaskInstance: movie_pipeline.scrape_collection_page manual__2024-12-17T06:01:55.898253+00:00 [scheduled]>
[2024-12-17T11:32:01.185+0530] {logging_mixin.py:190} INFO - [2024-12-17T11:32:01.185+0530] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-12-17T11:32:01.249+0530] {logging_mixin.py:190} INFO - [2024-12-17 11:32:01,248] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T06:01:55.898253+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T06:01:55.898253+00:00'
[2024-12-17T11:32:01.249+0530] {logging_mixin.py:190} INFO - [2024-12-17T11:32:01.248+0530] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T06:01:55.898253+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T06:01:55.898253+00:00'
[2024-12-17T11:32:01.252+0530] {logging_mixin.py:190} INFO - Task instance is in running state
[2024-12-17T11:32:01.253+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: queued
[2024-12-17T11:32:01.253+0530] {logging_mixin.py:190} INFO - Current task name:scrape_collection_page state:scheduled start_date:None
[2024-12-17T11:32:01.253+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline and current dag run status:running
[2024-12-17T11:32:01.253+0530] {logging_mixin.py:190} INFO - [2024-12-17T11:32:01.253+0530] {taskinstance.py:731} INFO - ::endgroup::
[2024-12-17T11:32:01.255+0530] {logging_mixin.py:190} INFO - 2024-12-17
[2024-12-17T11:32:01.256+0530] {logging_mixin.py:190} INFO - [2024-12-17 11:32:01,255] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T11:32:01.256+0530] {logging_mixin.py:190} INFO - [2024-12-17T11:32:01.255+0530] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T11:32:01.262+0530] {logging_mixin.py:190} INFO - [2024-12-17T11:32:01.262+0530] {taskinstance.py:340} INFO - ::group::Post task execution logs
[2024-12-17T11:32:01.263+0530] {logging_mixin.py:190} INFO - [2024-12-17T11:32:01.263+0530] {taskinstance.py:352} INFO - Marking task as SUCCESS. dag_id=movie_pipeline, task_id=scrape_collection_page, run_id=manual__2024-12-17T06:01:55.898253+00:00, execution_date=20241217T060155, start_date=, end_date=20241217T060201
[2024-12-17T11:32:01.273+0530] {logging_mixin.py:190} INFO - Task instance in success state
[2024-12-17T11:32:01.274+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: running
[2024-12-17T11:32:01.274+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline queued_at:None
[2024-12-17T11:32:01.274+0530] {logging_mixin.py:190} INFO - Task hostname:Jayeshs-MacBook-Air.local operator:PythonOperator
[2024-12-17T11:32:01.275+0530] {logging_mixin.py:190} INFO - [2024-12-17T11:32:01.275+0530] {dag.py:4410} INFO - [DAG TEST] end task task_id=scrape_collection_page map_index=-1
[2024-12-17T11:32:01.279+0530] {logging_mixin.py:190} INFO - [2024-12-17T11:32:01.279+0530] {dagrun.py:854} INFO - Marking run <DagRun movie_pipeline @ 2024-12-17 06:01:55.898253+00:00: manual__2024-12-17T06:01:55.898253+00:00, state:running, queued_at: None. externally triggered: False> successful
[2024-12-17T11:32:01.279+0530] {logging_mixin.py:190} INFO - Dag run in success state
[2024-12-17T11:32:01.280+0530] {logging_mixin.py:190} INFO - Dag run start:2024-12-17 06:01:55.898253+00:00 end:2024-12-17 06:02:01.279919+00:00
[2024-12-17T11:32:01.280+0530] {logging_mixin.py:190} INFO - [2024-12-17T11:32:01.280+0530] {dagrun.py:905} INFO - DagRun Finished: dag_id=movie_pipeline, execution_date=2024-12-17 06:01:55.898253+00:00, run_id=manual__2024-12-17T06:01:55.898253+00:00, run_start_date=2024-12-17 06:01:55.898253+00:00, run_end_date=2024-12-17 06:02:01.279919+00:00, run_duration=5.381666, state=success, external_trigger=False, run_type=manual, data_interval_start=2024-12-16 06:01:55.898253+00:00, data_interval_end=2024-12-17 06:01:55.898253+00:00, dag_hash=None
[2024-12-17T11:32:01.290+0530] {processor.py:925} INFO - DAG(s) 'movie_pipeline' retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T11:32:01.302+0530] {logging_mixin.py:190} INFO - [2024-12-17T11:32:01.302+0530] {dag.py:3239} INFO - Sync 1 DAGs
[2024-12-17T11:32:01.320+0530] {logging_mixin.py:190} INFO - [2024-12-17T11:32:01.320+0530] {dag.py:4180} INFO - Setting next_dagrun for movie_pipeline to 2024-12-17 01:30:00+00:00, run_after=2024-12-18 01:30:00+00:00
[2024-12-17T11:32:01.340+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 5.494 seconds
[2024-12-17T11:35:04.661+0530] {processor.py:186} INFO - Started process (PID=54662) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T11:35:04.664+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-17T11:35:04.665+0530] {logging_mixin.py:190} INFO - [2024-12-17T11:35:04.665+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T11:35:04.845+0530] {logging_mixin.py:190} INFO - [2024-12-17T11:35:04.844+0530] {dag.py:4435} INFO - dagrun id: movie_pipeline
[2024-12-17T11:35:04.858+0530] {logging_mixin.py:190} INFO - [2024-12-17T11:35:04.858+0530] {dag.py:4451} INFO - created dagrun <DagRun movie_pipeline @ 2024-12-17 06:05:04.705192+00:00: manual__2024-12-17T06:05:04.705192+00:00, state:running, queued_at: None. externally triggered: False>
[2024-12-17T11:35:04.889+0530] {logging_mixin.py:190} INFO - [2024-12-17T11:35:04.888+0530] {dag.py:4396} INFO - [DAG TEST] starting task_id=scrape_collection_page map_index=-1
[2024-12-17T11:35:04.890+0530] {logging_mixin.py:190} INFO - [2024-12-17T11:35:04.889+0530] {dag.py:4399} INFO - [DAG TEST] running task <TaskInstance: movie_pipeline.scrape_collection_page manual__2024-12-17T06:05:04.705192+00:00 [scheduled]>
[2024-12-17T11:35:09.991+0530] {logging_mixin.py:190} INFO - [2024-12-17T11:35:09.991+0530] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-12-17T11:35:10.074+0530] {logging_mixin.py:190} INFO - [2024-12-17 11:35:10,073] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T06:05:04.705192+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T06:05:04.705192+00:00'
[2024-12-17T11:35:10.074+0530] {logging_mixin.py:190} INFO - [2024-12-17T11:35:10.073+0530] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T06:05:04.705192+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T06:05:04.705192+00:00'
[2024-12-17T11:35:10.087+0530] {logging_mixin.py:190} INFO - Task instance is in running state
[2024-12-17T11:35:10.088+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: queued
[2024-12-17T11:35:10.088+0530] {logging_mixin.py:190} INFO - Current task name:scrape_collection_page state:scheduled start_date:None
[2024-12-17T11:35:10.088+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline and current dag run status:running
[2024-12-17T11:35:10.089+0530] {logging_mixin.py:190} INFO - [2024-12-17T11:35:10.089+0530] {taskinstance.py:731} INFO - ::endgroup::
[2024-12-17T11:35:10.093+0530] {logging_mixin.py:190} INFO - 2024-12-17
[2024-12-17T11:35:10.094+0530] {logging_mixin.py:190} INFO - [2024-12-17 11:35:10,093] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T11:35:10.094+0530] {logging_mixin.py:190} INFO - [2024-12-17T11:35:10.093+0530] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T11:35:10.099+0530] {logging_mixin.py:190} INFO - [2024-12-17T11:35:10.098+0530] {taskinstance.py:340} INFO - ::group::Post task execution logs
[2024-12-17T11:35:10.099+0530] {logging_mixin.py:190} INFO - [2024-12-17T11:35:10.099+0530] {taskinstance.py:352} INFO - Marking task as SUCCESS. dag_id=movie_pipeline, task_id=scrape_collection_page, run_id=manual__2024-12-17T06:05:04.705192+00:00, execution_date=20241217T060504, start_date=, end_date=20241217T060510
[2024-12-17T11:35:10.108+0530] {logging_mixin.py:190} INFO - Task instance in success state
[2024-12-17T11:35:10.109+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: running
[2024-12-17T11:35:10.109+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline queued_at:None
[2024-12-17T11:35:10.109+0530] {logging_mixin.py:190} INFO - Task hostname:Jayeshs-MacBook-Air.local operator:PythonOperator
[2024-12-17T11:35:10.110+0530] {logging_mixin.py:190} INFO - [2024-12-17T11:35:10.110+0530] {dag.py:4410} INFO - [DAG TEST] end task task_id=scrape_collection_page map_index=-1
[2024-12-17T11:35:10.114+0530] {logging_mixin.py:190} INFO - [2024-12-17T11:35:10.114+0530] {dagrun.py:854} INFO - Marking run <DagRun movie_pipeline @ 2024-12-17 06:05:04.705192+00:00: manual__2024-12-17T06:05:04.705192+00:00, state:running, queued_at: None. externally triggered: False> successful
[2024-12-17T11:35:10.115+0530] {logging_mixin.py:190} INFO - Dag run in success state
[2024-12-17T11:35:10.115+0530] {logging_mixin.py:190} INFO - Dag run start:2024-12-17 06:05:04.705192+00:00 end:2024-12-17 06:05:10.115259+00:00
[2024-12-17T11:35:10.116+0530] {logging_mixin.py:190} INFO - [2024-12-17T11:35:10.115+0530] {dagrun.py:905} INFO - DagRun Finished: dag_id=movie_pipeline, execution_date=2024-12-17 06:05:04.705192+00:00, run_id=manual__2024-12-17T06:05:04.705192+00:00, run_start_date=2024-12-17 06:05:04.705192+00:00, run_end_date=2024-12-17 06:05:10.115259+00:00, run_duration=5.410067, state=success, external_trigger=False, run_type=manual, data_interval_start=2024-12-16 06:05:04.705192+00:00, data_interval_end=2024-12-17 06:05:04.705192+00:00, dag_hash=None
[2024-12-17T11:35:10.126+0530] {processor.py:925} INFO - DAG(s) 'movie_pipeline' retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T11:35:10.138+0530] {logging_mixin.py:190} INFO - [2024-12-17T11:35:10.138+0530] {dag.py:3239} INFO - Sync 1 DAGs
[2024-12-17T11:35:10.157+0530] {logging_mixin.py:190} INFO - [2024-12-17T11:35:10.156+0530] {dag.py:4180} INFO - Setting next_dagrun for movie_pipeline to 2024-12-17 01:30:00+00:00, run_after=2024-12-18 01:30:00+00:00
[2024-12-17T11:35:10.175+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 5.519 seconds
[2024-12-17T11:38:01.344+0530] {processor.py:186} INFO - Started process (PID=54864) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T11:38:01.345+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-17T11:38:01.347+0530] {logging_mixin.py:190} INFO - [2024-12-17T11:38:01.346+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T11:38:01.524+0530] {logging_mixin.py:190} INFO - [2024-12-17T11:38:01.523+0530] {dag.py:4435} INFO - dagrun id: movie_pipeline
[2024-12-17T11:38:01.540+0530] {logging_mixin.py:190} INFO - [2024-12-17T11:38:01.539+0530] {dag.py:4451} INFO - created dagrun <DagRun movie_pipeline @ 2024-12-17 06:08:01.391016+00:00: manual__2024-12-17T06:08:01.391016+00:00, state:running, queued_at: None. externally triggered: False>
[2024-12-17T11:38:01.567+0530] {logging_mixin.py:190} INFO - [2024-12-17T11:38:01.567+0530] {dag.py:4396} INFO - [DAG TEST] starting task_id=scrape_collection_page map_index=-1
[2024-12-17T11:38:01.567+0530] {logging_mixin.py:190} INFO - [2024-12-17T11:38:01.567+0530] {dag.py:4399} INFO - [DAG TEST] running task <TaskInstance: movie_pipeline.scrape_collection_page manual__2024-12-17T06:08:01.391016+00:00 [scheduled]>
[2024-12-17T11:38:06.669+0530] {logging_mixin.py:190} INFO - [2024-12-17T11:38:06.669+0530] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-12-17T11:38:06.732+0530] {logging_mixin.py:190} INFO - [2024-12-17 11:38:06,732] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T06:08:01.391016+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T06:08:01.391016+00:00'
[2024-12-17T11:38:06.733+0530] {logging_mixin.py:190} INFO - [2024-12-17T11:38:06.732+0530] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T06:08:01.391016+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T06:08:01.391016+00:00'
[2024-12-17T11:38:06.738+0530] {logging_mixin.py:190} INFO - Task instance is in running state
[2024-12-17T11:38:06.738+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: queued
[2024-12-17T11:38:06.739+0530] {logging_mixin.py:190} INFO - Current task name:scrape_collection_page state:scheduled start_date:None
[2024-12-17T11:38:06.739+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline and current dag run status:running
[2024-12-17T11:38:06.740+0530] {logging_mixin.py:190} INFO - [2024-12-17T11:38:06.739+0530] {taskinstance.py:731} INFO - ::endgroup::
[2024-12-17T11:38:06.742+0530] {logging_mixin.py:190} INFO - 2024-12-17
[2024-12-17T11:38:06.742+0530] {logging_mixin.py:190} INFO - [2024-12-17 11:38:06,742] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T11:38:06.743+0530] {logging_mixin.py:190} INFO - [2024-12-17T11:38:06.742+0530] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T11:38:06.749+0530] {logging_mixin.py:190} INFO - [2024-12-17T11:38:06.749+0530] {taskinstance.py:340} INFO - ::group::Post task execution logs
[2024-12-17T11:38:06.750+0530] {logging_mixin.py:190} INFO - [2024-12-17T11:38:06.750+0530] {taskinstance.py:352} INFO - Marking task as SUCCESS. dag_id=movie_pipeline, task_id=scrape_collection_page, run_id=manual__2024-12-17T06:08:01.391016+00:00, execution_date=20241217T060801, start_date=, end_date=20241217T060806
[2024-12-17T11:38:06.760+0530] {logging_mixin.py:190} INFO - Task instance in success state
[2024-12-17T11:38:06.760+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: running
[2024-12-17T11:38:06.760+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline queued_at:None
[2024-12-17T11:38:06.761+0530] {logging_mixin.py:190} INFO - Task hostname:Jayeshs-MacBook-Air.local operator:PythonOperator
[2024-12-17T11:38:06.762+0530] {logging_mixin.py:190} INFO - [2024-12-17T11:38:06.761+0530] {dag.py:4410} INFO - [DAG TEST] end task task_id=scrape_collection_page map_index=-1
[2024-12-17T11:38:06.766+0530] {logging_mixin.py:190} INFO - [2024-12-17T11:38:06.765+0530] {dagrun.py:854} INFO - Marking run <DagRun movie_pipeline @ 2024-12-17 06:08:01.391016+00:00: manual__2024-12-17T06:08:01.391016+00:00, state:running, queued_at: None. externally triggered: False> successful
[2024-12-17T11:38:06.766+0530] {logging_mixin.py:190} INFO - Dag run in success state
[2024-12-17T11:38:06.766+0530] {logging_mixin.py:190} INFO - Dag run start:2024-12-17 06:08:01.391016+00:00 end:2024-12-17 06:08:06.766453+00:00
[2024-12-17T11:38:06.767+0530] {logging_mixin.py:190} INFO - [2024-12-17T11:38:06.767+0530] {dagrun.py:905} INFO - DagRun Finished: dag_id=movie_pipeline, execution_date=2024-12-17 06:08:01.391016+00:00, run_id=manual__2024-12-17T06:08:01.391016+00:00, run_start_date=2024-12-17 06:08:01.391016+00:00, run_end_date=2024-12-17 06:08:06.766453+00:00, run_duration=5.375437, state=success, external_trigger=False, run_type=manual, data_interval_start=2024-12-16 06:08:01.391016+00:00, data_interval_end=2024-12-17 06:08:01.391016+00:00, dag_hash=None
[2024-12-17T11:38:06.777+0530] {processor.py:925} INFO - DAG(s) 'movie_pipeline' retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T11:38:06.790+0530] {logging_mixin.py:190} INFO - [2024-12-17T11:38:06.790+0530] {dag.py:3239} INFO - Sync 1 DAGs
[2024-12-17T11:38:06.808+0530] {logging_mixin.py:190} INFO - [2024-12-17T11:38:06.807+0530] {dag.py:4180} INFO - Setting next_dagrun for movie_pipeline to 2024-12-17 01:30:00+00:00, run_after=2024-12-18 01:30:00+00:00
[2024-12-17T11:38:06.829+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 5.488 seconds
[2024-12-17T11:41:14.604+0530] {processor.py:186} INFO - Started process (PID=55090) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T11:41:14.606+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-17T11:41:14.608+0530] {logging_mixin.py:190} INFO - [2024-12-17T11:41:14.608+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T11:41:14.847+0530] {logging_mixin.py:190} INFO - [2024-12-17T11:41:14.846+0530] {dag.py:4435} INFO - dagrun id: movie_pipeline
[2024-12-17T11:41:14.865+0530] {logging_mixin.py:190} INFO - [2024-12-17T11:41:14.865+0530] {dag.py:4451} INFO - created dagrun <DagRun movie_pipeline @ 2024-12-17 06:11:14.654255+00:00: manual__2024-12-17T06:11:14.654255+00:00, state:running, queued_at: None. externally triggered: False>
[2024-12-17T11:41:14.905+0530] {logging_mixin.py:190} INFO - [2024-12-17T11:41:14.903+0530] {dag.py:4396} INFO - [DAG TEST] starting task_id=scrape_collection_page map_index=-1
[2024-12-17T11:41:14.906+0530] {logging_mixin.py:190} INFO - [2024-12-17T11:41:14.906+0530] {dag.py:4399} INFO - [DAG TEST] running task <TaskInstance: movie_pipeline.scrape_collection_page manual__2024-12-17T06:11:14.654255+00:00 [scheduled]>
[2024-12-17T11:41:20.028+0530] {logging_mixin.py:190} INFO - [2024-12-17T11:41:20.028+0530] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-12-17T11:41:20.115+0530] {logging_mixin.py:190} INFO - [2024-12-17 11:41:20,114] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T06:11:14.654255+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T06:11:14.654255+00:00'
[2024-12-17T11:41:20.115+0530] {logging_mixin.py:190} INFO - [2024-12-17T11:41:20.114+0530] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T06:11:14.654255+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T06:11:14.654255+00:00'
[2024-12-17T11:41:20.127+0530] {logging_mixin.py:190} INFO - Task instance is in running state
[2024-12-17T11:41:20.127+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: queued
[2024-12-17T11:41:20.127+0530] {logging_mixin.py:190} INFO - Current task name:scrape_collection_page state:scheduled start_date:None
[2024-12-17T11:41:20.128+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline and current dag run status:running
[2024-12-17T11:41:20.128+0530] {logging_mixin.py:190} INFO - [2024-12-17T11:41:20.128+0530] {taskinstance.py:731} INFO - ::endgroup::
[2024-12-17T11:41:20.132+0530] {logging_mixin.py:190} INFO - 2024-12-17
[2024-12-17T11:41:20.132+0530] {logging_mixin.py:190} INFO - [2024-12-17 11:41:20,132] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T11:41:20.133+0530] {logging_mixin.py:190} INFO - [2024-12-17T11:41:20.132+0530] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T11:41:20.138+0530] {logging_mixin.py:190} INFO - [2024-12-17T11:41:20.138+0530] {taskinstance.py:340} INFO - ::group::Post task execution logs
[2024-12-17T11:41:20.139+0530] {logging_mixin.py:190} INFO - [2024-12-17T11:41:20.139+0530] {taskinstance.py:352} INFO - Marking task as SUCCESS. dag_id=movie_pipeline, task_id=scrape_collection_page, run_id=manual__2024-12-17T06:11:14.654255+00:00, execution_date=20241217T061114, start_date=, end_date=20241217T061120
[2024-12-17T11:41:20.151+0530] {logging_mixin.py:190} INFO - Task instance in success state
[2024-12-17T11:41:20.151+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: running
[2024-12-17T11:41:20.152+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline queued_at:None
[2024-12-17T11:41:20.152+0530] {logging_mixin.py:190} INFO - Task hostname:Jayeshs-MacBook-Air.local operator:PythonOperator
[2024-12-17T11:41:20.152+0530] {logging_mixin.py:190} INFO - [2024-12-17T11:41:20.152+0530] {dag.py:4410} INFO - [DAG TEST] end task task_id=scrape_collection_page map_index=-1
[2024-12-17T11:41:20.159+0530] {logging_mixin.py:190} INFO - [2024-12-17T11:41:20.158+0530] {dagrun.py:854} INFO - Marking run <DagRun movie_pipeline @ 2024-12-17 06:11:14.654255+00:00: manual__2024-12-17T06:11:14.654255+00:00, state:running, queued_at: None. externally triggered: False> successful
[2024-12-17T11:41:20.159+0530] {logging_mixin.py:190} INFO - Dag run in success state
[2024-12-17T11:41:20.159+0530] {logging_mixin.py:190} INFO - Dag run start:2024-12-17 06:11:14.654255+00:00 end:2024-12-17 06:11:20.159416+00:00
[2024-12-17T11:41:20.160+0530] {logging_mixin.py:190} INFO - [2024-12-17T11:41:20.159+0530] {dagrun.py:905} INFO - DagRun Finished: dag_id=movie_pipeline, execution_date=2024-12-17 06:11:14.654255+00:00, run_id=manual__2024-12-17T06:11:14.654255+00:00, run_start_date=2024-12-17 06:11:14.654255+00:00, run_end_date=2024-12-17 06:11:20.159416+00:00, run_duration=5.505161, state=success, external_trigger=False, run_type=manual, data_interval_start=2024-12-16 06:11:14.654255+00:00, data_interval_end=2024-12-17 06:11:14.654255+00:00, dag_hash=None
[2024-12-17T11:41:20.169+0530] {processor.py:925} INFO - DAG(s) 'movie_pipeline' retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T11:41:20.184+0530] {logging_mixin.py:190} INFO - [2024-12-17T11:41:20.184+0530] {dag.py:3239} INFO - Sync 1 DAGs
[2024-12-17T11:41:20.206+0530] {logging_mixin.py:190} INFO - [2024-12-17T11:41:20.205+0530] {dag.py:4180} INFO - Setting next_dagrun for movie_pipeline to 2024-12-17 01:30:00+00:00, run_after=2024-12-18 01:30:00+00:00
[2024-12-17T11:41:20.228+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 5.627 seconds
[2024-12-17T11:44:29.601+0530] {processor.py:186} INFO - Started process (PID=55324) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T11:44:29.603+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-17T11:44:29.605+0530] {logging_mixin.py:190} INFO - [2024-12-17T11:44:29.605+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T11:44:29.814+0530] {logging_mixin.py:190} INFO - [2024-12-17T11:44:29.814+0530] {dag.py:4435} INFO - dagrun id: movie_pipeline
[2024-12-17T11:44:29.831+0530] {logging_mixin.py:190} INFO - [2024-12-17T11:44:29.830+0530] {dag.py:4451} INFO - created dagrun <DagRun movie_pipeline @ 2024-12-17 06:14:29.655612+00:00: manual__2024-12-17T06:14:29.655612+00:00, state:running, queued_at: None. externally triggered: False>
[2024-12-17T11:44:29.867+0530] {logging_mixin.py:190} INFO - [2024-12-17T11:44:29.867+0530] {dag.py:4396} INFO - [DAG TEST] starting task_id=scrape_collection_page map_index=-1
[2024-12-17T11:44:29.868+0530] {logging_mixin.py:190} INFO - [2024-12-17T11:44:29.867+0530] {dag.py:4399} INFO - [DAG TEST] running task <TaskInstance: movie_pipeline.scrape_collection_page manual__2024-12-17T06:14:29.655612+00:00 [scheduled]>
[2024-12-17T11:44:35.066+0530] {logging_mixin.py:190} INFO - [2024-12-17T11:44:35.066+0530] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-12-17T11:44:35.158+0530] {logging_mixin.py:190} INFO - [2024-12-17 11:44:35,157] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T06:14:29.655612+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T06:14:29.655612+00:00'
[2024-12-17T11:44:35.158+0530] {logging_mixin.py:190} INFO - [2024-12-17T11:44:35.157+0530] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T06:14:29.655612+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T06:14:29.655612+00:00'
[2024-12-17T11:44:35.172+0530] {logging_mixin.py:190} INFO - Task instance is in running state
[2024-12-17T11:44:35.172+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: queued
[2024-12-17T11:44:35.173+0530] {logging_mixin.py:190} INFO - Current task name:scrape_collection_page state:scheduled start_date:None
[2024-12-17T11:44:35.173+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline and current dag run status:running
[2024-12-17T11:44:35.173+0530] {logging_mixin.py:190} INFO - [2024-12-17T11:44:35.173+0530] {taskinstance.py:731} INFO - ::endgroup::
[2024-12-17T11:44:35.177+0530] {logging_mixin.py:190} INFO - 2024-12-17
[2024-12-17T11:44:35.178+0530] {logging_mixin.py:190} INFO - [2024-12-17 11:44:35,178] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T11:44:35.178+0530] {logging_mixin.py:190} INFO - [2024-12-17T11:44:35.178+0530] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T11:44:35.184+0530] {logging_mixin.py:190} INFO - [2024-12-17T11:44:35.183+0530] {taskinstance.py:340} INFO - ::group::Post task execution logs
[2024-12-17T11:44:35.184+0530] {logging_mixin.py:190} INFO - [2024-12-17T11:44:35.184+0530] {taskinstance.py:352} INFO - Marking task as SUCCESS. dag_id=movie_pipeline, task_id=scrape_collection_page, run_id=manual__2024-12-17T06:14:29.655612+00:00, execution_date=20241217T061429, start_date=, end_date=20241217T061435
[2024-12-17T11:44:35.195+0530] {logging_mixin.py:190} INFO - Task instance in success state
[2024-12-17T11:44:35.196+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: running
[2024-12-17T11:44:35.196+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline queued_at:None
[2024-12-17T11:44:35.196+0530] {logging_mixin.py:190} INFO - Task hostname:Jayeshs-MacBook-Air.local operator:PythonOperator
[2024-12-17T11:44:35.196+0530] {logging_mixin.py:190} INFO - [2024-12-17T11:44:35.196+0530] {dag.py:4410} INFO - [DAG TEST] end task task_id=scrape_collection_page map_index=-1
[2024-12-17T11:44:35.201+0530] {logging_mixin.py:190} INFO - [2024-12-17T11:44:35.200+0530] {dagrun.py:854} INFO - Marking run <DagRun movie_pipeline @ 2024-12-17 06:14:29.655612+00:00: manual__2024-12-17T06:14:29.655612+00:00, state:running, queued_at: None. externally triggered: False> successful
[2024-12-17T11:44:35.201+0530] {logging_mixin.py:190} INFO - Dag run in success state
[2024-12-17T11:44:35.201+0530] {logging_mixin.py:190} INFO - Dag run start:2024-12-17 06:14:29.655612+00:00 end:2024-12-17 06:14:35.201351+00:00
[2024-12-17T11:44:35.202+0530] {logging_mixin.py:190} INFO - [2024-12-17T11:44:35.201+0530] {dagrun.py:905} INFO - DagRun Finished: dag_id=movie_pipeline, execution_date=2024-12-17 06:14:29.655612+00:00, run_id=manual__2024-12-17T06:14:29.655612+00:00, run_start_date=2024-12-17 06:14:29.655612+00:00, run_end_date=2024-12-17 06:14:35.201351+00:00, run_duration=5.545739, state=success, external_trigger=False, run_type=manual, data_interval_start=2024-12-16 06:14:29.655612+00:00, data_interval_end=2024-12-17 06:14:29.655612+00:00, dag_hash=None
[2024-12-17T11:44:35.210+0530] {processor.py:925} INFO - DAG(s) 'movie_pipeline' retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T11:44:35.222+0530] {logging_mixin.py:190} INFO - [2024-12-17T11:44:35.222+0530] {dag.py:3239} INFO - Sync 1 DAGs
[2024-12-17T11:44:35.242+0530] {logging_mixin.py:190} INFO - [2024-12-17T11:44:35.242+0530] {dag.py:4180} INFO - Setting next_dagrun for movie_pipeline to 2024-12-17 01:30:00+00:00, run_after=2024-12-18 01:30:00+00:00
[2024-12-17T11:44:35.263+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 5.665 seconds
[2024-12-17T11:47:47.913+0530] {processor.py:186} INFO - Started process (PID=55547) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T11:47:47.915+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-17T11:47:47.918+0530] {logging_mixin.py:190} INFO - [2024-12-17T11:47:47.918+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T11:47:48.099+0530] {logging_mixin.py:190} INFO - [2024-12-17T11:47:48.099+0530] {dag.py:4435} INFO - dagrun id: movie_pipeline
[2024-12-17T11:47:48.114+0530] {logging_mixin.py:190} INFO - [2024-12-17T11:47:48.113+0530] {dag.py:4451} INFO - created dagrun <DagRun movie_pipeline @ 2024-12-17 06:17:47.964221+00:00: manual__2024-12-17T06:17:47.964221+00:00, state:running, queued_at: None. externally triggered: False>
[2024-12-17T11:47:48.142+0530] {logging_mixin.py:190} INFO - [2024-12-17T11:47:48.142+0530] {dag.py:4396} INFO - [DAG TEST] starting task_id=scrape_collection_page map_index=-1
[2024-12-17T11:47:48.143+0530] {logging_mixin.py:190} INFO - [2024-12-17T11:47:48.142+0530] {dag.py:4399} INFO - [DAG TEST] running task <TaskInstance: movie_pipeline.scrape_collection_page manual__2024-12-17T06:17:47.964221+00:00 [scheduled]>
[2024-12-17T11:47:53.244+0530] {logging_mixin.py:190} INFO - [2024-12-17T11:47:53.243+0530] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-12-17T11:47:53.308+0530] {logging_mixin.py:190} INFO - [2024-12-17 11:47:53,308] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T06:17:47.964221+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T06:17:47.964221+00:00'
[2024-12-17T11:47:53.308+0530] {logging_mixin.py:190} INFO - [2024-12-17T11:47:53.308+0530] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T06:17:47.964221+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T06:17:47.964221+00:00'
[2024-12-17T11:47:53.321+0530] {logging_mixin.py:190} INFO - Task instance is in running state
[2024-12-17T11:47:53.322+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: queued
[2024-12-17T11:47:53.322+0530] {logging_mixin.py:190} INFO - Current task name:scrape_collection_page state:scheduled start_date:None
[2024-12-17T11:47:53.322+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline and current dag run status:running
[2024-12-17T11:47:53.323+0530] {logging_mixin.py:190} INFO - [2024-12-17T11:47:53.323+0530] {taskinstance.py:731} INFO - ::endgroup::
[2024-12-17T11:47:53.327+0530] {logging_mixin.py:190} INFO - 2024-12-17
[2024-12-17T11:47:53.327+0530] {logging_mixin.py:190} INFO - [2024-12-17 11:47:53,327] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T11:47:53.328+0530] {logging_mixin.py:190} INFO - [2024-12-17T11:47:53.327+0530] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T11:47:53.334+0530] {logging_mixin.py:190} INFO - [2024-12-17T11:47:53.334+0530] {taskinstance.py:340} INFO - ::group::Post task execution logs
[2024-12-17T11:47:53.335+0530] {logging_mixin.py:190} INFO - [2024-12-17T11:47:53.335+0530] {taskinstance.py:352} INFO - Marking task as SUCCESS. dag_id=movie_pipeline, task_id=scrape_collection_page, run_id=manual__2024-12-17T06:17:47.964221+00:00, execution_date=20241217T061747, start_date=, end_date=20241217T061753
[2024-12-17T11:47:53.345+0530] {logging_mixin.py:190} INFO - Task instance in success state
[2024-12-17T11:47:53.345+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: running
[2024-12-17T11:47:53.345+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline queued_at:None
[2024-12-17T11:47:53.345+0530] {logging_mixin.py:190} INFO - Task hostname:Jayeshs-MacBook-Air.local operator:PythonOperator
[2024-12-17T11:47:53.346+0530] {logging_mixin.py:190} INFO - [2024-12-17T11:47:53.345+0530] {dag.py:4410} INFO - [DAG TEST] end task task_id=scrape_collection_page map_index=-1
[2024-12-17T11:47:53.349+0530] {logging_mixin.py:190} INFO - [2024-12-17T11:47:53.349+0530] {dagrun.py:854} INFO - Marking run <DagRun movie_pipeline @ 2024-12-17 06:17:47.964221+00:00: manual__2024-12-17T06:17:47.964221+00:00, state:running, queued_at: None. externally triggered: False> successful
[2024-12-17T11:47:53.350+0530] {logging_mixin.py:190} INFO - Dag run in success state
[2024-12-17T11:47:53.350+0530] {logging_mixin.py:190} INFO - Dag run start:2024-12-17 06:17:47.964221+00:00 end:2024-12-17 06:17:53.350088+00:00
[2024-12-17T11:47:53.350+0530] {logging_mixin.py:190} INFO - [2024-12-17T11:47:53.350+0530] {dagrun.py:905} INFO - DagRun Finished: dag_id=movie_pipeline, execution_date=2024-12-17 06:17:47.964221+00:00, run_id=manual__2024-12-17T06:17:47.964221+00:00, run_start_date=2024-12-17 06:17:47.964221+00:00, run_end_date=2024-12-17 06:17:53.350088+00:00, run_duration=5.385867, state=success, external_trigger=False, run_type=manual, data_interval_start=2024-12-16 06:17:47.964221+00:00, data_interval_end=2024-12-17 06:17:47.964221+00:00, dag_hash=None
[2024-12-17T11:47:53.360+0530] {processor.py:925} INFO - DAG(s) 'movie_pipeline' retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T11:47:53.372+0530] {logging_mixin.py:190} INFO - [2024-12-17T11:47:53.371+0530] {dag.py:3239} INFO - Sync 1 DAGs
[2024-12-17T11:47:53.394+0530] {logging_mixin.py:190} INFO - [2024-12-17T11:47:53.394+0530] {dag.py:4180} INFO - Setting next_dagrun for movie_pipeline to 2024-12-17 01:30:00+00:00, run_after=2024-12-18 01:30:00+00:00
[2024-12-17T11:47:53.413+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 5.504 seconds
[2024-12-17T11:50:50.388+0530] {processor.py:186} INFO - Started process (PID=55760) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T11:50:50.391+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-17T11:50:50.393+0530] {logging_mixin.py:190} INFO - [2024-12-17T11:50:50.393+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T11:50:50.635+0530] {logging_mixin.py:190} INFO - [2024-12-17T11:50:50.635+0530] {dag.py:4435} INFO - dagrun id: movie_pipeline
[2024-12-17T11:50:50.655+0530] {logging_mixin.py:190} INFO - [2024-12-17T11:50:50.654+0530] {dag.py:4451} INFO - created dagrun <DagRun movie_pipeline @ 2024-12-17 06:20:50.496724+00:00: manual__2024-12-17T06:20:50.496724+00:00, state:running, queued_at: None. externally triggered: False>
[2024-12-17T11:50:50.686+0530] {logging_mixin.py:190} INFO - [2024-12-17T11:50:50.686+0530] {dag.py:4396} INFO - [DAG TEST] starting task_id=scrape_collection_page map_index=-1
[2024-12-17T11:50:50.687+0530] {logging_mixin.py:190} INFO - [2024-12-17T11:50:50.687+0530] {dag.py:4399} INFO - [DAG TEST] running task <TaskInstance: movie_pipeline.scrape_collection_page manual__2024-12-17T06:20:50.496724+00:00 [scheduled]>
[2024-12-17T11:50:55.795+0530] {logging_mixin.py:190} INFO - [2024-12-17T11:50:55.795+0530] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-12-17T11:50:55.869+0530] {logging_mixin.py:190} INFO - [2024-12-17 11:50:55,869] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T06:20:50.496724+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T06:20:50.496724+00:00'
[2024-12-17T11:50:55.870+0530] {logging_mixin.py:190} INFO - [2024-12-17T11:50:55.869+0530] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T06:20:50.496724+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T06:20:50.496724+00:00'
[2024-12-17T11:50:55.882+0530] {logging_mixin.py:190} INFO - Task instance is in running state
[2024-12-17T11:50:55.883+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: queued
[2024-12-17T11:50:55.883+0530] {logging_mixin.py:190} INFO - Current task name:scrape_collection_page state:scheduled start_date:None
[2024-12-17T11:50:55.883+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline and current dag run status:running
[2024-12-17T11:50:55.883+0530] {logging_mixin.py:190} INFO - [2024-12-17T11:50:55.883+0530] {taskinstance.py:731} INFO - ::endgroup::
[2024-12-17T11:50:55.887+0530] {logging_mixin.py:190} INFO - 2024-12-17
[2024-12-17T11:50:55.887+0530] {logging_mixin.py:190} INFO - [2024-12-17 11:50:55,887] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T11:50:55.888+0530] {logging_mixin.py:190} INFO - [2024-12-17T11:50:55.887+0530] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T11:50:55.893+0530] {logging_mixin.py:190} INFO - [2024-12-17T11:50:55.892+0530] {taskinstance.py:340} INFO - ::group::Post task execution logs
[2024-12-17T11:50:55.893+0530] {logging_mixin.py:190} INFO - [2024-12-17T11:50:55.893+0530] {taskinstance.py:352} INFO - Marking task as SUCCESS. dag_id=movie_pipeline, task_id=scrape_collection_page, run_id=manual__2024-12-17T06:20:50.496724+00:00, execution_date=20241217T062050, start_date=, end_date=20241217T062055
[2024-12-17T11:50:55.902+0530] {logging_mixin.py:190} INFO - Task instance in success state
[2024-12-17T11:50:55.902+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: running
[2024-12-17T11:50:55.902+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline queued_at:None
[2024-12-17T11:50:55.902+0530] {logging_mixin.py:190} INFO - Task hostname:Jayeshs-MacBook-Air.local operator:PythonOperator
[2024-12-17T11:50:55.903+0530] {logging_mixin.py:190} INFO - [2024-12-17T11:50:55.903+0530] {dag.py:4410} INFO - [DAG TEST] end task task_id=scrape_collection_page map_index=-1
[2024-12-17T11:50:55.907+0530] {logging_mixin.py:190} INFO - [2024-12-17T11:50:55.907+0530] {dagrun.py:854} INFO - Marking run <DagRun movie_pipeline @ 2024-12-17 06:20:50.496724+00:00: manual__2024-12-17T06:20:50.496724+00:00, state:running, queued_at: None. externally triggered: False> successful
[2024-12-17T11:50:55.907+0530] {logging_mixin.py:190} INFO - Dag run in success state
[2024-12-17T11:50:55.907+0530] {logging_mixin.py:190} INFO - Dag run start:2024-12-17 06:20:50.496724+00:00 end:2024-12-17 06:20:55.907567+00:00
[2024-12-17T11:50:55.908+0530] {logging_mixin.py:190} INFO - [2024-12-17T11:50:55.908+0530] {dagrun.py:905} INFO - DagRun Finished: dag_id=movie_pipeline, execution_date=2024-12-17 06:20:50.496724+00:00, run_id=manual__2024-12-17T06:20:50.496724+00:00, run_start_date=2024-12-17 06:20:50.496724+00:00, run_end_date=2024-12-17 06:20:55.907567+00:00, run_duration=5.410843, state=success, external_trigger=False, run_type=manual, data_interval_start=2024-12-16 06:20:50.496724+00:00, data_interval_end=2024-12-17 06:20:50.496724+00:00, dag_hash=None
[2024-12-17T11:50:55.917+0530] {processor.py:925} INFO - DAG(s) 'movie_pipeline' retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T11:50:55.933+0530] {logging_mixin.py:190} INFO - [2024-12-17T11:50:55.933+0530] {dag.py:3239} INFO - Sync 1 DAGs
[2024-12-17T11:50:55.959+0530] {logging_mixin.py:190} INFO - [2024-12-17T11:50:55.958+0530] {dag.py:4180} INFO - Setting next_dagrun for movie_pipeline to 2024-12-17 01:30:00+00:00, run_after=2024-12-18 01:30:00+00:00
[2024-12-17T11:50:55.981+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 5.600 seconds
[2024-12-17T11:54:00.348+0530] {processor.py:186} INFO - Started process (PID=55990) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T11:54:00.352+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-17T11:54:00.353+0530] {logging_mixin.py:190} INFO - [2024-12-17T11:54:00.353+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T11:54:00.558+0530] {logging_mixin.py:190} INFO - [2024-12-17T11:54:00.557+0530] {dag.py:4435} INFO - dagrun id: movie_pipeline
[2024-12-17T11:54:00.574+0530] {logging_mixin.py:190} INFO - [2024-12-17T11:54:00.574+0530] {dag.py:4451} INFO - created dagrun <DagRun movie_pipeline @ 2024-12-17 06:24:00.402829+00:00: manual__2024-12-17T06:24:00.402829+00:00, state:running, queued_at: None. externally triggered: False>
[2024-12-17T11:54:00.609+0530] {logging_mixin.py:190} INFO - [2024-12-17T11:54:00.608+0530] {dag.py:4396} INFO - [DAG TEST] starting task_id=scrape_collection_page map_index=-1
[2024-12-17T11:54:00.609+0530] {logging_mixin.py:190} INFO - [2024-12-17T11:54:00.609+0530] {dag.py:4399} INFO - [DAG TEST] running task <TaskInstance: movie_pipeline.scrape_collection_page manual__2024-12-17T06:24:00.402829+00:00 [scheduled]>
[2024-12-17T11:54:05.716+0530] {logging_mixin.py:190} INFO - [2024-12-17T11:54:05.715+0530] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-12-17T11:54:05.811+0530] {logging_mixin.py:190} INFO - [2024-12-17 11:54:05,811] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T06:24:00.402829+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T06:24:00.402829+00:00'
[2024-12-17T11:54:05.811+0530] {logging_mixin.py:190} INFO - [2024-12-17T11:54:05.811+0530] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T06:24:00.402829+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T06:24:00.402829+00:00'
[2024-12-17T11:54:05.825+0530] {logging_mixin.py:190} INFO - Task instance is in running state
[2024-12-17T11:54:05.826+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: queued
[2024-12-17T11:54:05.827+0530] {logging_mixin.py:190} INFO - Current task name:scrape_collection_page state:scheduled start_date:None
[2024-12-17T11:54:05.827+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline and current dag run status:running
[2024-12-17T11:54:05.828+0530] {logging_mixin.py:190} INFO - [2024-12-17T11:54:05.828+0530] {taskinstance.py:731} INFO - ::endgroup::
[2024-12-17T11:54:05.831+0530] {logging_mixin.py:190} INFO - 2024-12-17
[2024-12-17T11:54:05.832+0530] {logging_mixin.py:190} INFO - [2024-12-17 11:54:05,831] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T11:54:05.832+0530] {logging_mixin.py:190} INFO - [2024-12-17T11:54:05.831+0530] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T11:54:05.840+0530] {logging_mixin.py:190} INFO - [2024-12-17T11:54:05.840+0530] {taskinstance.py:340} INFO - ::group::Post task execution logs
[2024-12-17T11:54:05.840+0530] {logging_mixin.py:190} INFO - [2024-12-17T11:54:05.840+0530] {taskinstance.py:352} INFO - Marking task as SUCCESS. dag_id=movie_pipeline, task_id=scrape_collection_page, run_id=manual__2024-12-17T06:24:00.402829+00:00, execution_date=20241217T062400, start_date=, end_date=20241217T062405
[2024-12-17T11:54:05.854+0530] {logging_mixin.py:190} INFO - Task instance in success state
[2024-12-17T11:54:05.854+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: running
[2024-12-17T11:54:05.855+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline queued_at:None
[2024-12-17T11:54:05.855+0530] {logging_mixin.py:190} INFO - Task hostname:Jayeshs-MacBook-Air.local operator:PythonOperator
[2024-12-17T11:54:05.856+0530] {logging_mixin.py:190} INFO - [2024-12-17T11:54:05.856+0530] {dag.py:4410} INFO - [DAG TEST] end task task_id=scrape_collection_page map_index=-1
[2024-12-17T11:54:05.861+0530] {logging_mixin.py:190} INFO - [2024-12-17T11:54:05.861+0530] {dagrun.py:854} INFO - Marking run <DagRun movie_pipeline @ 2024-12-17 06:24:00.402829+00:00: manual__2024-12-17T06:24:00.402829+00:00, state:running, queued_at: None. externally triggered: False> successful
[2024-12-17T11:54:05.861+0530] {logging_mixin.py:190} INFO - Dag run in success state
[2024-12-17T11:54:05.862+0530] {logging_mixin.py:190} INFO - Dag run start:2024-12-17 06:24:00.402829+00:00 end:2024-12-17 06:24:05.861919+00:00
[2024-12-17T11:54:05.862+0530] {logging_mixin.py:190} INFO - [2024-12-17T11:54:05.862+0530] {dagrun.py:905} INFO - DagRun Finished: dag_id=movie_pipeline, execution_date=2024-12-17 06:24:00.402829+00:00, run_id=manual__2024-12-17T06:24:00.402829+00:00, run_start_date=2024-12-17 06:24:00.402829+00:00, run_end_date=2024-12-17 06:24:05.861919+00:00, run_duration=5.45909, state=success, external_trigger=False, run_type=manual, data_interval_start=2024-12-16 06:24:00.402829+00:00, data_interval_end=2024-12-17 06:24:00.402829+00:00, dag_hash=None
[2024-12-17T11:54:05.871+0530] {processor.py:925} INFO - DAG(s) 'movie_pipeline' retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T11:54:05.887+0530] {logging_mixin.py:190} INFO - [2024-12-17T11:54:05.887+0530] {dag.py:3239} INFO - Sync 1 DAGs
[2024-12-17T11:54:05.908+0530] {logging_mixin.py:190} INFO - [2024-12-17T11:54:05.908+0530] {dag.py:4180} INFO - Setting next_dagrun for movie_pipeline to 2024-12-17 01:30:00+00:00, run_after=2024-12-18 01:30:00+00:00
[2024-12-17T11:54:05.928+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 5.584 seconds
[2024-12-17T11:57:31.057+0530] {processor.py:186} INFO - Started process (PID=56262) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T11:57:31.060+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-17T11:57:31.061+0530] {logging_mixin.py:190} INFO - [2024-12-17T11:57:31.061+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T11:57:31.248+0530] {logging_mixin.py:190} INFO - [2024-12-17T11:57:31.248+0530] {dag.py:4435} INFO - dagrun id: movie_pipeline
[2024-12-17T11:57:31.262+0530] {logging_mixin.py:190} INFO - [2024-12-17T11:57:31.262+0530] {dag.py:4451} INFO - created dagrun <DagRun movie_pipeline @ 2024-12-17 06:27:31.104278+00:00: manual__2024-12-17T06:27:31.104278+00:00, state:running, queued_at: None. externally triggered: False>
[2024-12-17T11:57:31.293+0530] {logging_mixin.py:190} INFO - [2024-12-17T11:57:31.293+0530] {dag.py:4396} INFO - [DAG TEST] starting task_id=scrape_collection_page map_index=-1
[2024-12-17T11:57:31.294+0530] {logging_mixin.py:190} INFO - [2024-12-17T11:57:31.293+0530] {dag.py:4399} INFO - [DAG TEST] running task <TaskInstance: movie_pipeline.scrape_collection_page manual__2024-12-17T06:27:31.104278+00:00 [scheduled]>
[2024-12-17T11:57:36.392+0530] {logging_mixin.py:190} INFO - [2024-12-17T11:57:36.392+0530] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-12-17T11:57:36.463+0530] {logging_mixin.py:190} INFO - [2024-12-17 11:57:36,463] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T06:27:31.104278+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T06:27:31.104278+00:00'
[2024-12-17T11:57:36.463+0530] {logging_mixin.py:190} INFO - [2024-12-17T11:57:36.463+0530] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T06:27:31.104278+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T06:27:31.104278+00:00'
[2024-12-17T11:57:36.476+0530] {logging_mixin.py:190} INFO - Task instance is in running state
[2024-12-17T11:57:36.477+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: queued
[2024-12-17T11:57:36.477+0530] {logging_mixin.py:190} INFO - Current task name:scrape_collection_page state:scheduled start_date:None
[2024-12-17T11:57:36.478+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline and current dag run status:running
[2024-12-17T11:57:36.478+0530] {logging_mixin.py:190} INFO - [2024-12-17T11:57:36.478+0530] {taskinstance.py:731} INFO - ::endgroup::
[2024-12-17T11:57:36.482+0530] {logging_mixin.py:190} INFO - 2024-12-17
[2024-12-17T11:57:36.482+0530] {logging_mixin.py:190} INFO - [2024-12-17 11:57:36,482] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T11:57:36.483+0530] {logging_mixin.py:190} INFO - [2024-12-17T11:57:36.482+0530] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T11:57:36.488+0530] {logging_mixin.py:190} INFO - [2024-12-17T11:57:36.488+0530] {taskinstance.py:340} INFO - ::group::Post task execution logs
[2024-12-17T11:57:36.488+0530] {logging_mixin.py:190} INFO - [2024-12-17T11:57:36.488+0530] {taskinstance.py:352} INFO - Marking task as SUCCESS. dag_id=movie_pipeline, task_id=scrape_collection_page, run_id=manual__2024-12-17T06:27:31.104278+00:00, execution_date=20241217T062731, start_date=, end_date=20241217T062736
[2024-12-17T11:57:36.498+0530] {logging_mixin.py:190} INFO - Task instance in success state
[2024-12-17T11:57:36.498+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: running
[2024-12-17T11:57:36.499+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline queued_at:None
[2024-12-17T11:57:36.500+0530] {logging_mixin.py:190} INFO - Task hostname:Jayeshs-MacBook-Air.local operator:PythonOperator
[2024-12-17T11:57:36.500+0530] {logging_mixin.py:190} INFO - [2024-12-17T11:57:36.500+0530] {dag.py:4410} INFO - [DAG TEST] end task task_id=scrape_collection_page map_index=-1
[2024-12-17T11:57:36.505+0530] {logging_mixin.py:190} INFO - [2024-12-17T11:57:36.505+0530] {dagrun.py:854} INFO - Marking run <DagRun movie_pipeline @ 2024-12-17 06:27:31.104278+00:00: manual__2024-12-17T06:27:31.104278+00:00, state:running, queued_at: None. externally triggered: False> successful
[2024-12-17T11:57:36.506+0530] {logging_mixin.py:190} INFO - Dag run in success state
[2024-12-17T11:57:36.506+0530] {logging_mixin.py:190} INFO - Dag run start:2024-12-17 06:27:31.104278+00:00 end:2024-12-17 06:27:36.505904+00:00
[2024-12-17T11:57:36.507+0530] {logging_mixin.py:190} INFO - [2024-12-17T11:57:36.506+0530] {dagrun.py:905} INFO - DagRun Finished: dag_id=movie_pipeline, execution_date=2024-12-17 06:27:31.104278+00:00, run_id=manual__2024-12-17T06:27:31.104278+00:00, run_start_date=2024-12-17 06:27:31.104278+00:00, run_end_date=2024-12-17 06:27:36.505904+00:00, run_duration=5.401626, state=success, external_trigger=False, run_type=manual, data_interval_start=2024-12-16 06:27:31.104278+00:00, data_interval_end=2024-12-17 06:27:31.104278+00:00, dag_hash=None
[2024-12-17T11:57:36.516+0530] {processor.py:925} INFO - DAG(s) 'movie_pipeline' retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T11:57:36.529+0530] {logging_mixin.py:190} INFO - [2024-12-17T11:57:36.528+0530] {dag.py:3239} INFO - Sync 1 DAGs
[2024-12-17T11:57:36.547+0530] {logging_mixin.py:190} INFO - [2024-12-17T11:57:36.546+0530] {dag.py:4180} INFO - Setting next_dagrun for movie_pipeline to 2024-12-17 01:30:00+00:00, run_after=2024-12-18 01:30:00+00:00
[2024-12-17T11:57:36.565+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 5.512 seconds
[2024-12-17T12:00:30.663+0530] {processor.py:186} INFO - Started process (PID=56472) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T12:00:30.664+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-17T12:00:30.667+0530] {logging_mixin.py:190} INFO - [2024-12-17T12:00:30.667+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T12:00:30.843+0530] {logging_mixin.py:190} INFO - [2024-12-17T12:00:30.843+0530] {dag.py:4435} INFO - dagrun id: movie_pipeline
[2024-12-17T12:00:30.858+0530] {logging_mixin.py:190} INFO - [2024-12-17T12:00:30.858+0530] {dag.py:4451} INFO - created dagrun <DagRun movie_pipeline @ 2024-12-17 06:30:30.709582+00:00: manual__2024-12-17T06:30:30.709582+00:00, state:running, queued_at: None. externally triggered: False>
[2024-12-17T12:00:30.883+0530] {logging_mixin.py:190} INFO - [2024-12-17T12:00:30.882+0530] {dag.py:4396} INFO - [DAG TEST] starting task_id=scrape_collection_page map_index=-1
[2024-12-17T12:00:30.883+0530] {logging_mixin.py:190} INFO - [2024-12-17T12:00:30.883+0530] {dag.py:4399} INFO - [DAG TEST] running task <TaskInstance: movie_pipeline.scrape_collection_page manual__2024-12-17T06:30:30.709582+00:00 [scheduled]>
[2024-12-17T12:00:35.978+0530] {logging_mixin.py:190} INFO - [2024-12-17T12:00:35.978+0530] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-12-17T12:00:36.056+0530] {logging_mixin.py:190} INFO - [2024-12-17 12:00:36,056] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T06:30:30.709582+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T06:30:30.709582+00:00'
[2024-12-17T12:00:36.057+0530] {logging_mixin.py:190} INFO - [2024-12-17T12:00:36.056+0530] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T06:30:30.709582+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T06:30:30.709582+00:00'
[2024-12-17T12:00:36.063+0530] {logging_mixin.py:190} INFO - Task instance is in running state
[2024-12-17T12:00:36.063+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: queued
[2024-12-17T12:00:36.064+0530] {logging_mixin.py:190} INFO - Current task name:scrape_collection_page state:scheduled start_date:None
[2024-12-17T12:00:36.064+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline and current dag run status:running
[2024-12-17T12:00:36.065+0530] {logging_mixin.py:190} INFO - [2024-12-17T12:00:36.065+0530] {taskinstance.py:731} INFO - ::endgroup::
[2024-12-17T12:00:36.068+0530] {logging_mixin.py:190} INFO - 2024-12-17
[2024-12-17T12:00:36.068+0530] {logging_mixin.py:190} INFO - [2024-12-17 12:00:36,068] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T12:00:36.069+0530] {logging_mixin.py:190} INFO - [2024-12-17T12:00:36.068+0530] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T12:00:36.075+0530] {logging_mixin.py:190} INFO - [2024-12-17T12:00:36.074+0530] {taskinstance.py:340} INFO - ::group::Post task execution logs
[2024-12-17T12:00:36.075+0530] {logging_mixin.py:190} INFO - [2024-12-17T12:00:36.075+0530] {taskinstance.py:352} INFO - Marking task as SUCCESS. dag_id=movie_pipeline, task_id=scrape_collection_page, run_id=manual__2024-12-17T06:30:30.709582+00:00, execution_date=20241217T063030, start_date=, end_date=20241217T063036
[2024-12-17T12:00:36.086+0530] {logging_mixin.py:190} INFO - Task instance in success state
[2024-12-17T12:00:36.086+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: running
[2024-12-17T12:00:36.087+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline queued_at:None
[2024-12-17T12:00:36.087+0530] {logging_mixin.py:190} INFO - Task hostname:Jayeshs-MacBook-Air.local operator:PythonOperator
[2024-12-17T12:00:36.088+0530] {logging_mixin.py:190} INFO - [2024-12-17T12:00:36.087+0530] {dag.py:4410} INFO - [DAG TEST] end task task_id=scrape_collection_page map_index=-1
[2024-12-17T12:00:36.092+0530] {logging_mixin.py:190} INFO - [2024-12-17T12:00:36.092+0530] {dagrun.py:854} INFO - Marking run <DagRun movie_pipeline @ 2024-12-17 06:30:30.709582+00:00: manual__2024-12-17T06:30:30.709582+00:00, state:running, queued_at: None. externally triggered: False> successful
[2024-12-17T12:00:36.093+0530] {logging_mixin.py:190} INFO - Dag run in success state
[2024-12-17T12:00:36.093+0530] {logging_mixin.py:190} INFO - Dag run start:2024-12-17 06:30:30.709582+00:00 end:2024-12-17 06:30:36.093057+00:00
[2024-12-17T12:00:36.094+0530] {logging_mixin.py:190} INFO - [2024-12-17T12:00:36.093+0530] {dagrun.py:905} INFO - DagRun Finished: dag_id=movie_pipeline, execution_date=2024-12-17 06:30:30.709582+00:00, run_id=manual__2024-12-17T06:30:30.709582+00:00, run_start_date=2024-12-17 06:30:30.709582+00:00, run_end_date=2024-12-17 06:30:36.093057+00:00, run_duration=5.383475, state=success, external_trigger=False, run_type=manual, data_interval_start=2024-12-16 06:30:30.709582+00:00, data_interval_end=2024-12-17 06:30:30.709582+00:00, dag_hash=None
[2024-12-17T12:00:36.102+0530] {processor.py:925} INFO - DAG(s) 'movie_pipeline' retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T12:00:36.114+0530] {logging_mixin.py:190} INFO - [2024-12-17T12:00:36.113+0530] {dag.py:3239} INFO - Sync 1 DAGs
[2024-12-17T12:00:36.133+0530] {logging_mixin.py:190} INFO - [2024-12-17T12:00:36.132+0530] {dag.py:4180} INFO - Setting next_dagrun for movie_pipeline to 2024-12-17 01:30:00+00:00, run_after=2024-12-18 01:30:00+00:00
[2024-12-17T12:00:36.153+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 5.494 seconds
[2024-12-17T12:03:26.353+0530] {processor.py:186} INFO - Started process (PID=56675) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T12:03:26.355+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-17T12:03:26.357+0530] {logging_mixin.py:190} INFO - [2024-12-17T12:03:26.357+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T12:03:26.536+0530] {logging_mixin.py:190} INFO - [2024-12-17T12:03:26.536+0530] {dag.py:4435} INFO - dagrun id: movie_pipeline
[2024-12-17T12:03:26.551+0530] {logging_mixin.py:190} INFO - [2024-12-17T12:03:26.551+0530] {dag.py:4451} INFO - created dagrun <DagRun movie_pipeline @ 2024-12-17 06:33:26.402934+00:00: manual__2024-12-17T06:33:26.402934+00:00, state:running, queued_at: None. externally triggered: False>
[2024-12-17T12:03:26.582+0530] {logging_mixin.py:190} INFO - [2024-12-17T12:03:26.582+0530] {dag.py:4396} INFO - [DAG TEST] starting task_id=scrape_collection_page map_index=-1
[2024-12-17T12:03:26.583+0530] {logging_mixin.py:190} INFO - [2024-12-17T12:03:26.583+0530] {dag.py:4399} INFO - [DAG TEST] running task <TaskInstance: movie_pipeline.scrape_collection_page manual__2024-12-17T06:33:26.402934+00:00 [scheduled]>
[2024-12-17T12:03:31.695+0530] {logging_mixin.py:190} INFO - [2024-12-17T12:03:31.695+0530] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-12-17T12:03:31.764+0530] {logging_mixin.py:190} INFO - [2024-12-17 12:03:31,763] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T06:33:26.402934+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T06:33:26.402934+00:00'
[2024-12-17T12:03:31.764+0530] {logging_mixin.py:190} INFO - [2024-12-17T12:03:31.763+0530] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T06:33:26.402934+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T06:33:26.402934+00:00'
[2024-12-17T12:03:31.777+0530] {logging_mixin.py:190} INFO - Task instance is in running state
[2024-12-17T12:03:31.778+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: queued
[2024-12-17T12:03:31.778+0530] {logging_mixin.py:190} INFO - Current task name:scrape_collection_page state:scheduled start_date:None
[2024-12-17T12:03:31.779+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline and current dag run status:running
[2024-12-17T12:03:31.779+0530] {logging_mixin.py:190} INFO - [2024-12-17T12:03:31.779+0530] {taskinstance.py:731} INFO - ::endgroup::
[2024-12-17T12:03:31.781+0530] {logging_mixin.py:190} INFO - 2024-12-17
[2024-12-17T12:03:31.782+0530] {logging_mixin.py:190} INFO - [2024-12-17 12:03:31,782] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T12:03:31.782+0530] {logging_mixin.py:190} INFO - [2024-12-17T12:03:31.782+0530] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T12:03:31.787+0530] {logging_mixin.py:190} INFO - [2024-12-17T12:03:31.787+0530] {taskinstance.py:340} INFO - ::group::Post task execution logs
[2024-12-17T12:03:31.788+0530] {logging_mixin.py:190} INFO - [2024-12-17T12:03:31.787+0530] {taskinstance.py:352} INFO - Marking task as SUCCESS. dag_id=movie_pipeline, task_id=scrape_collection_page, run_id=manual__2024-12-17T06:33:26.402934+00:00, execution_date=20241217T063326, start_date=, end_date=20241217T063331
[2024-12-17T12:03:31.798+0530] {logging_mixin.py:190} INFO - Task instance in success state
[2024-12-17T12:03:31.798+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: running
[2024-12-17T12:03:31.798+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline queued_at:None
[2024-12-17T12:03:31.799+0530] {logging_mixin.py:190} INFO - Task hostname:Jayeshs-MacBook-Air.local operator:PythonOperator
[2024-12-17T12:03:31.799+0530] {logging_mixin.py:190} INFO - [2024-12-17T12:03:31.799+0530] {dag.py:4410} INFO - [DAG TEST] end task task_id=scrape_collection_page map_index=-1
[2024-12-17T12:03:31.803+0530] {logging_mixin.py:190} INFO - [2024-12-17T12:03:31.803+0530] {dagrun.py:854} INFO - Marking run <DagRun movie_pipeline @ 2024-12-17 06:33:26.402934+00:00: manual__2024-12-17T06:33:26.402934+00:00, state:running, queued_at: None. externally triggered: False> successful
[2024-12-17T12:03:31.804+0530] {logging_mixin.py:190} INFO - Dag run in success state
[2024-12-17T12:03:31.804+0530] {logging_mixin.py:190} INFO - Dag run start:2024-12-17 06:33:26.402934+00:00 end:2024-12-17 06:33:31.804132+00:00
[2024-12-17T12:03:31.804+0530] {logging_mixin.py:190} INFO - [2024-12-17T12:03:31.804+0530] {dagrun.py:905} INFO - DagRun Finished: dag_id=movie_pipeline, execution_date=2024-12-17 06:33:26.402934+00:00, run_id=manual__2024-12-17T06:33:26.402934+00:00, run_start_date=2024-12-17 06:33:26.402934+00:00, run_end_date=2024-12-17 06:33:31.804132+00:00, run_duration=5.401198, state=success, external_trigger=False, run_type=manual, data_interval_start=2024-12-16 06:33:26.402934+00:00, data_interval_end=2024-12-17 06:33:26.402934+00:00, dag_hash=None
[2024-12-17T12:03:31.813+0530] {processor.py:925} INFO - DAG(s) 'movie_pipeline' retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T12:03:31.825+0530] {logging_mixin.py:190} INFO - [2024-12-17T12:03:31.825+0530] {dag.py:3239} INFO - Sync 1 DAGs
[2024-12-17T12:03:31.844+0530] {logging_mixin.py:190} INFO - [2024-12-17T12:03:31.844+0530] {dag.py:4180} INFO - Setting next_dagrun for movie_pipeline to 2024-12-17 01:30:00+00:00, run_after=2024-12-18 01:30:00+00:00
[2024-12-17T12:03:31.864+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 5.514 seconds
[2024-12-17T12:06:23.586+0530] {processor.py:186} INFO - Started process (PID=56880) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T12:06:23.589+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-17T12:06:23.591+0530] {logging_mixin.py:190} INFO - [2024-12-17T12:06:23.590+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T12:06:23.769+0530] {logging_mixin.py:190} INFO - [2024-12-17T12:06:23.769+0530] {dag.py:4435} INFO - dagrun id: movie_pipeline
[2024-12-17T12:06:23.783+0530] {logging_mixin.py:190} INFO - [2024-12-17T12:06:23.783+0530] {dag.py:4451} INFO - created dagrun <DagRun movie_pipeline @ 2024-12-17 06:36:23.636306+00:00: manual__2024-12-17T06:36:23.636306+00:00, state:running, queued_at: None. externally triggered: False>
[2024-12-17T12:06:23.813+0530] {logging_mixin.py:190} INFO - [2024-12-17T12:06:23.813+0530] {dag.py:4396} INFO - [DAG TEST] starting task_id=scrape_collection_page map_index=-1
[2024-12-17T12:06:23.814+0530] {logging_mixin.py:190} INFO - [2024-12-17T12:06:23.814+0530] {dag.py:4399} INFO - [DAG TEST] running task <TaskInstance: movie_pipeline.scrape_collection_page manual__2024-12-17T06:36:23.636306+00:00 [scheduled]>
[2024-12-17T12:06:28.913+0530] {logging_mixin.py:190} INFO - [2024-12-17T12:06:28.913+0530] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-12-17T12:06:28.981+0530] {logging_mixin.py:190} INFO - [2024-12-17 12:06:28,981] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T06:36:23.636306+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T06:36:23.636306+00:00'
[2024-12-17T12:06:28.981+0530] {logging_mixin.py:190} INFO - [2024-12-17T12:06:28.981+0530] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T06:36:23.636306+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T06:36:23.636306+00:00'
[2024-12-17T12:06:28.994+0530] {logging_mixin.py:190} INFO - Task instance is in running state
[2024-12-17T12:06:28.995+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: queued
[2024-12-17T12:06:28.995+0530] {logging_mixin.py:190} INFO - Current task name:scrape_collection_page state:scheduled start_date:None
[2024-12-17T12:06:28.995+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline and current dag run status:running
[2024-12-17T12:06:28.996+0530] {logging_mixin.py:190} INFO - [2024-12-17T12:06:28.995+0530] {taskinstance.py:731} INFO - ::endgroup::
[2024-12-17T12:06:28.999+0530] {logging_mixin.py:190} INFO - 2024-12-17
[2024-12-17T12:06:29.000+0530] {logging_mixin.py:190} INFO - [2024-12-17 12:06:28,999] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T12:06:29.000+0530] {logging_mixin.py:190} INFO - [2024-12-17T12:06:28.999+0530] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T12:06:29.006+0530] {logging_mixin.py:190} INFO - [2024-12-17T12:06:29.005+0530] {taskinstance.py:340} INFO - ::group::Post task execution logs
[2024-12-17T12:06:29.007+0530] {logging_mixin.py:190} INFO - [2024-12-17T12:06:29.006+0530] {taskinstance.py:352} INFO - Marking task as SUCCESS. dag_id=movie_pipeline, task_id=scrape_collection_page, run_id=manual__2024-12-17T06:36:23.636306+00:00, execution_date=20241217T063623, start_date=, end_date=20241217T063629
[2024-12-17T12:06:29.017+0530] {logging_mixin.py:190} INFO - Task instance in success state
[2024-12-17T12:06:29.017+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: running
[2024-12-17T12:06:29.017+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline queued_at:None
[2024-12-17T12:06:29.018+0530] {logging_mixin.py:190} INFO - Task hostname:Jayeshs-MacBook-Air.local operator:PythonOperator
[2024-12-17T12:06:29.018+0530] {logging_mixin.py:190} INFO - [2024-12-17T12:06:29.018+0530] {dag.py:4410} INFO - [DAG TEST] end task task_id=scrape_collection_page map_index=-1
[2024-12-17T12:06:29.022+0530] {logging_mixin.py:190} INFO - [2024-12-17T12:06:29.022+0530] {dagrun.py:854} INFO - Marking run <DagRun movie_pipeline @ 2024-12-17 06:36:23.636306+00:00: manual__2024-12-17T06:36:23.636306+00:00, state:running, queued_at: None. externally triggered: False> successful
[2024-12-17T12:06:29.022+0530] {logging_mixin.py:190} INFO - Dag run in success state
[2024-12-17T12:06:29.022+0530] {logging_mixin.py:190} INFO - Dag run start:2024-12-17 06:36:23.636306+00:00 end:2024-12-17 06:36:29.022621+00:00
[2024-12-17T12:06:29.023+0530] {logging_mixin.py:190} INFO - [2024-12-17T12:06:29.023+0530] {dagrun.py:905} INFO - DagRun Finished: dag_id=movie_pipeline, execution_date=2024-12-17 06:36:23.636306+00:00, run_id=manual__2024-12-17T06:36:23.636306+00:00, run_start_date=2024-12-17 06:36:23.636306+00:00, run_end_date=2024-12-17 06:36:29.022621+00:00, run_duration=5.386315, state=success, external_trigger=False, run_type=manual, data_interval_start=2024-12-16 06:36:23.636306+00:00, data_interval_end=2024-12-17 06:36:23.636306+00:00, dag_hash=None
[2024-12-17T12:06:29.032+0530] {processor.py:925} INFO - DAG(s) 'movie_pipeline' retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T12:06:29.044+0530] {logging_mixin.py:190} INFO - [2024-12-17T12:06:29.044+0530] {dag.py:3239} INFO - Sync 1 DAGs
[2024-12-17T12:06:29.062+0530] {logging_mixin.py:190} INFO - [2024-12-17T12:06:29.062+0530] {dag.py:4180} INFO - Setting next_dagrun for movie_pipeline to 2024-12-17 01:30:00+00:00, run_after=2024-12-18 01:30:00+00:00
[2024-12-17T12:06:29.082+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 5.500 seconds
[2024-12-17T12:09:22.970+0530] {processor.py:186} INFO - Started process (PID=57078) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T12:09:22.971+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-17T12:09:22.972+0530] {logging_mixin.py:190} INFO - [2024-12-17T12:09:22.972+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T12:09:23.151+0530] {logging_mixin.py:190} INFO - [2024-12-17T12:09:23.150+0530] {dag.py:4435} INFO - dagrun id: movie_pipeline
[2024-12-17T12:09:23.165+0530] {logging_mixin.py:190} INFO - [2024-12-17T12:09:23.165+0530] {dag.py:4451} INFO - created dagrun <DagRun movie_pipeline @ 2024-12-17 06:39:23.015809+00:00: manual__2024-12-17T06:39:23.015809+00:00, state:running, queued_at: None. externally triggered: False>
[2024-12-17T12:09:23.191+0530] {logging_mixin.py:190} INFO - [2024-12-17T12:09:23.191+0530] {dag.py:4396} INFO - [DAG TEST] starting task_id=scrape_collection_page map_index=-1
[2024-12-17T12:09:23.192+0530] {logging_mixin.py:190} INFO - [2024-12-17T12:09:23.192+0530] {dag.py:4399} INFO - [DAG TEST] running task <TaskInstance: movie_pipeline.scrape_collection_page manual__2024-12-17T06:39:23.015809+00:00 [scheduled]>
[2024-12-17T12:09:28.290+0530] {logging_mixin.py:190} INFO - [2024-12-17T12:09:28.290+0530] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-12-17T12:09:28.353+0530] {logging_mixin.py:190} INFO - [2024-12-17 12:09:28,353] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T06:39:23.015809+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T06:39:23.015809+00:00'
[2024-12-17T12:09:28.354+0530] {logging_mixin.py:190} INFO - [2024-12-17T12:09:28.353+0530] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T06:39:23.015809+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T06:39:23.015809+00:00'
[2024-12-17T12:09:28.356+0530] {logging_mixin.py:190} INFO - Task instance is in running state
[2024-12-17T12:09:28.357+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: queued
[2024-12-17T12:09:28.357+0530] {logging_mixin.py:190} INFO - Current task name:scrape_collection_page state:scheduled start_date:None
[2024-12-17T12:09:28.357+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline and current dag run status:running
[2024-12-17T12:09:28.358+0530] {logging_mixin.py:190} INFO - [2024-12-17T12:09:28.357+0530] {taskinstance.py:731} INFO - ::endgroup::
[2024-12-17T12:09:28.360+0530] {logging_mixin.py:190} INFO - 2024-12-17
[2024-12-17T12:09:28.360+0530] {logging_mixin.py:190} INFO - [2024-12-17 12:09:28,360] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T12:09:28.360+0530] {logging_mixin.py:190} INFO - [2024-12-17T12:09:28.360+0530] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T12:09:28.365+0530] {logging_mixin.py:190} INFO - [2024-12-17T12:09:28.365+0530] {taskinstance.py:340} INFO - ::group::Post task execution logs
[2024-12-17T12:09:28.366+0530] {logging_mixin.py:190} INFO - [2024-12-17T12:09:28.366+0530] {taskinstance.py:352} INFO - Marking task as SUCCESS. dag_id=movie_pipeline, task_id=scrape_collection_page, run_id=manual__2024-12-17T06:39:23.015809+00:00, execution_date=20241217T063923, start_date=, end_date=20241217T063928
[2024-12-17T12:09:28.376+0530] {logging_mixin.py:190} INFO - Task instance in success state
[2024-12-17T12:09:28.376+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: running
[2024-12-17T12:09:28.377+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline queued_at:None
[2024-12-17T12:09:28.377+0530] {logging_mixin.py:190} INFO - Task hostname:Jayeshs-MacBook-Air.local operator:PythonOperator
[2024-12-17T12:09:28.379+0530] {logging_mixin.py:190} INFO - [2024-12-17T12:09:28.378+0530] {dag.py:4410} INFO - [DAG TEST] end task task_id=scrape_collection_page map_index=-1
[2024-12-17T12:09:28.384+0530] {logging_mixin.py:190} INFO - [2024-12-17T12:09:28.384+0530] {dagrun.py:854} INFO - Marking run <DagRun movie_pipeline @ 2024-12-17 06:39:23.015809+00:00: manual__2024-12-17T06:39:23.015809+00:00, state:running, queued_at: None. externally triggered: False> successful
[2024-12-17T12:09:28.384+0530] {logging_mixin.py:190} INFO - Dag run in success state
[2024-12-17T12:09:28.384+0530] {logging_mixin.py:190} INFO - Dag run start:2024-12-17 06:39:23.015809+00:00 end:2024-12-17 06:39:28.384545+00:00
[2024-12-17T12:09:28.385+0530] {logging_mixin.py:190} INFO - [2024-12-17T12:09:28.385+0530] {dagrun.py:905} INFO - DagRun Finished: dag_id=movie_pipeline, execution_date=2024-12-17 06:39:23.015809+00:00, run_id=manual__2024-12-17T06:39:23.015809+00:00, run_start_date=2024-12-17 06:39:23.015809+00:00, run_end_date=2024-12-17 06:39:28.384545+00:00, run_duration=5.368736, state=success, external_trigger=False, run_type=manual, data_interval_start=2024-12-16 06:39:23.015809+00:00, data_interval_end=2024-12-17 06:39:23.015809+00:00, dag_hash=None
[2024-12-17T12:09:28.394+0530] {processor.py:925} INFO - DAG(s) 'movie_pipeline' retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T12:09:28.407+0530] {logging_mixin.py:190} INFO - [2024-12-17T12:09:28.406+0530] {dag.py:3239} INFO - Sync 1 DAGs
[2024-12-17T12:09:28.425+0530] {logging_mixin.py:190} INFO - [2024-12-17T12:09:28.425+0530] {dag.py:4180} INFO - Setting next_dagrun for movie_pipeline to 2024-12-17 01:30:00+00:00, run_after=2024-12-18 01:30:00+00:00
[2024-12-17T12:09:28.443+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 5.477 seconds
[2024-12-17T12:12:18.589+0530] {processor.py:186} INFO - Started process (PID=57286) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T12:12:18.591+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-17T12:12:18.592+0530] {logging_mixin.py:190} INFO - [2024-12-17T12:12:18.592+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T12:12:18.769+0530] {logging_mixin.py:190} INFO - [2024-12-17T12:12:18.768+0530] {dag.py:4435} INFO - dagrun id: movie_pipeline
[2024-12-17T12:12:18.784+0530] {logging_mixin.py:190} INFO - [2024-12-17T12:12:18.783+0530] {dag.py:4451} INFO - created dagrun <DagRun movie_pipeline @ 2024-12-17 06:42:18.636025+00:00: manual__2024-12-17T06:42:18.636025+00:00, state:running, queued_at: None. externally triggered: False>
[2024-12-17T12:12:18.811+0530] {logging_mixin.py:190} INFO - [2024-12-17T12:12:18.811+0530] {dag.py:4396} INFO - [DAG TEST] starting task_id=scrape_collection_page map_index=-1
[2024-12-17T12:12:18.812+0530] {logging_mixin.py:190} INFO - [2024-12-17T12:12:18.811+0530] {dag.py:4399} INFO - [DAG TEST] running task <TaskInstance: movie_pipeline.scrape_collection_page manual__2024-12-17T06:42:18.636025+00:00 [scheduled]>
[2024-12-17T12:12:23.912+0530] {logging_mixin.py:190} INFO - [2024-12-17T12:12:23.912+0530] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-12-17T12:12:23.977+0530] {logging_mixin.py:190} INFO - [2024-12-17 12:12:23,977] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T06:42:18.636025+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T06:42:18.636025+00:00'
[2024-12-17T12:12:23.978+0530] {logging_mixin.py:190} INFO - [2024-12-17T12:12:23.977+0530] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T06:42:18.636025+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T06:42:18.636025+00:00'
[2024-12-17T12:12:23.980+0530] {logging_mixin.py:190} INFO - Task instance is in running state
[2024-12-17T12:12:23.980+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: queued
[2024-12-17T12:12:23.981+0530] {logging_mixin.py:190} INFO - Current task name:scrape_collection_page state:scheduled start_date:None
[2024-12-17T12:12:23.981+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline and current dag run status:running
[2024-12-17T12:12:23.982+0530] {logging_mixin.py:190} INFO - [2024-12-17T12:12:23.981+0530] {taskinstance.py:731} INFO - ::endgroup::
[2024-12-17T12:12:23.983+0530] {logging_mixin.py:190} INFO - 2024-12-17
[2024-12-17T12:12:23.984+0530] {logging_mixin.py:190} INFO - [2024-12-17 12:12:23,984] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T12:12:23.984+0530] {logging_mixin.py:190} INFO - [2024-12-17T12:12:23.984+0530] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T12:12:23.989+0530] {logging_mixin.py:190} INFO - [2024-12-17T12:12:23.989+0530] {taskinstance.py:340} INFO - ::group::Post task execution logs
[2024-12-17T12:12:23.990+0530] {logging_mixin.py:190} INFO - [2024-12-17T12:12:23.989+0530] {taskinstance.py:352} INFO - Marking task as SUCCESS. dag_id=movie_pipeline, task_id=scrape_collection_page, run_id=manual__2024-12-17T06:42:18.636025+00:00, execution_date=20241217T064218, start_date=, end_date=20241217T064223
[2024-12-17T12:12:24.002+0530] {logging_mixin.py:190} INFO - Task instance in success state
[2024-12-17T12:12:24.002+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: running
[2024-12-17T12:12:24.002+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline queued_at:None
[2024-12-17T12:12:24.002+0530] {logging_mixin.py:190} INFO - Task hostname:Jayeshs-MacBook-Air.local operator:PythonOperator
[2024-12-17T12:12:24.003+0530] {logging_mixin.py:190} INFO - [2024-12-17T12:12:24.002+0530] {dag.py:4410} INFO - [DAG TEST] end task task_id=scrape_collection_page map_index=-1
[2024-12-17T12:12:24.006+0530] {logging_mixin.py:190} INFO - [2024-12-17T12:12:24.006+0530] {dagrun.py:854} INFO - Marking run <DagRun movie_pipeline @ 2024-12-17 06:42:18.636025+00:00: manual__2024-12-17T06:42:18.636025+00:00, state:running, queued_at: None. externally triggered: False> successful
[2024-12-17T12:12:24.007+0530] {logging_mixin.py:190} INFO - Dag run in success state
[2024-12-17T12:12:24.007+0530] {logging_mixin.py:190} INFO - Dag run start:2024-12-17 06:42:18.636025+00:00 end:2024-12-17 06:42:24.007253+00:00
[2024-12-17T12:12:24.008+0530] {logging_mixin.py:190} INFO - [2024-12-17T12:12:24.008+0530] {dagrun.py:905} INFO - DagRun Finished: dag_id=movie_pipeline, execution_date=2024-12-17 06:42:18.636025+00:00, run_id=manual__2024-12-17T06:42:18.636025+00:00, run_start_date=2024-12-17 06:42:18.636025+00:00, run_end_date=2024-12-17 06:42:24.007253+00:00, run_duration=5.371228, state=success, external_trigger=False, run_type=manual, data_interval_start=2024-12-16 06:42:18.636025+00:00, data_interval_end=2024-12-17 06:42:18.636025+00:00, dag_hash=None
[2024-12-17T12:12:24.017+0530] {processor.py:925} INFO - DAG(s) 'movie_pipeline' retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T12:12:24.029+0530] {logging_mixin.py:190} INFO - [2024-12-17T12:12:24.029+0530] {dag.py:3239} INFO - Sync 1 DAGs
[2024-12-17T12:12:24.047+0530] {logging_mixin.py:190} INFO - [2024-12-17T12:12:24.046+0530] {dag.py:4180} INFO - Setting next_dagrun for movie_pipeline to 2024-12-17 01:30:00+00:00, run_after=2024-12-18 01:30:00+00:00
[2024-12-17T12:12:24.065+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 5.479 seconds
[2024-12-17T12:15:15.126+0530] {processor.py:186} INFO - Started process (PID=57483) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T12:15:15.128+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-17T12:15:15.129+0530] {logging_mixin.py:190} INFO - [2024-12-17T12:15:15.129+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T12:15:15.301+0530] {logging_mixin.py:190} INFO - [2024-12-17T12:15:15.301+0530] {dag.py:4435} INFO - dagrun id: movie_pipeline
[2024-12-17T12:15:15.317+0530] {logging_mixin.py:190} INFO - [2024-12-17T12:15:15.317+0530] {dag.py:4451} INFO - created dagrun <DagRun movie_pipeline @ 2024-12-17 06:45:15.170831+00:00: manual__2024-12-17T06:45:15.170831+00:00, state:running, queued_at: None. externally triggered: False>
[2024-12-17T12:15:15.343+0530] {logging_mixin.py:190} INFO - [2024-12-17T12:15:15.342+0530] {dag.py:4396} INFO - [DAG TEST] starting task_id=scrape_collection_page map_index=-1
[2024-12-17T12:15:15.344+0530] {logging_mixin.py:190} INFO - [2024-12-17T12:15:15.343+0530] {dag.py:4399} INFO - [DAG TEST] running task <TaskInstance: movie_pipeline.scrape_collection_page manual__2024-12-17T06:45:15.170831+00:00 [scheduled]>
[2024-12-17T12:15:20.442+0530] {logging_mixin.py:190} INFO - [2024-12-17T12:15:20.442+0530] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-12-17T12:15:20.505+0530] {logging_mixin.py:190} INFO - [2024-12-17 12:15:20,505] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T06:45:15.170831+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T06:45:15.170831+00:00'
[2024-12-17T12:15:20.505+0530] {logging_mixin.py:190} INFO - [2024-12-17T12:15:20.505+0530] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T06:45:15.170831+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T06:45:15.170831+00:00'
[2024-12-17T12:15:20.508+0530] {logging_mixin.py:190} INFO - Task instance is in running state
[2024-12-17T12:15:20.509+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: queued
[2024-12-17T12:15:20.509+0530] {logging_mixin.py:190} INFO - Current task name:scrape_collection_page state:scheduled start_date:None
[2024-12-17T12:15:20.509+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline and current dag run status:running
[2024-12-17T12:15:20.510+0530] {logging_mixin.py:190} INFO - [2024-12-17T12:15:20.509+0530] {taskinstance.py:731} INFO - ::endgroup::
[2024-12-17T12:15:20.511+0530] {logging_mixin.py:190} INFO - 2024-12-17
[2024-12-17T12:15:20.512+0530] {logging_mixin.py:190} INFO - [2024-12-17 12:15:20,512] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T12:15:20.513+0530] {logging_mixin.py:190} INFO - [2024-12-17T12:15:20.512+0530] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T12:15:20.517+0530] {logging_mixin.py:190} INFO - [2024-12-17T12:15:20.517+0530] {taskinstance.py:340} INFO - ::group::Post task execution logs
[2024-12-17T12:15:20.518+0530] {logging_mixin.py:190} INFO - [2024-12-17T12:15:20.518+0530] {taskinstance.py:352} INFO - Marking task as SUCCESS. dag_id=movie_pipeline, task_id=scrape_collection_page, run_id=manual__2024-12-17T06:45:15.170831+00:00, execution_date=20241217T064515, start_date=, end_date=20241217T064520
[2024-12-17T12:15:20.527+0530] {logging_mixin.py:190} INFO - Task instance in success state
[2024-12-17T12:15:20.527+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: running
[2024-12-17T12:15:20.528+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline queued_at:None
[2024-12-17T12:15:20.528+0530] {logging_mixin.py:190} INFO - Task hostname:Jayeshs-MacBook-Air.local operator:PythonOperator
[2024-12-17T12:15:20.528+0530] {logging_mixin.py:190} INFO - [2024-12-17T12:15:20.528+0530] {dag.py:4410} INFO - [DAG TEST] end task task_id=scrape_collection_page map_index=-1
[2024-12-17T12:15:20.532+0530] {logging_mixin.py:190} INFO - [2024-12-17T12:15:20.532+0530] {dagrun.py:854} INFO - Marking run <DagRun movie_pipeline @ 2024-12-17 06:45:15.170831+00:00: manual__2024-12-17T06:45:15.170831+00:00, state:running, queued_at: None. externally triggered: False> successful
[2024-12-17T12:15:20.532+0530] {logging_mixin.py:190} INFO - Dag run in success state
[2024-12-17T12:15:20.533+0530] {logging_mixin.py:190} INFO - Dag run start:2024-12-17 06:45:15.170831+00:00 end:2024-12-17 06:45:20.532660+00:00
[2024-12-17T12:15:20.534+0530] {logging_mixin.py:190} INFO - [2024-12-17T12:15:20.533+0530] {dagrun.py:905} INFO - DagRun Finished: dag_id=movie_pipeline, execution_date=2024-12-17 06:45:15.170831+00:00, run_id=manual__2024-12-17T06:45:15.170831+00:00, run_start_date=2024-12-17 06:45:15.170831+00:00, run_end_date=2024-12-17 06:45:20.532660+00:00, run_duration=5.361829, state=success, external_trigger=False, run_type=manual, data_interval_start=2024-12-16 06:45:15.170831+00:00, data_interval_end=2024-12-17 06:45:15.170831+00:00, dag_hash=None
[2024-12-17T12:15:20.544+0530] {processor.py:925} INFO - DAG(s) 'movie_pipeline' retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T12:15:20.557+0530] {logging_mixin.py:190} INFO - [2024-12-17T12:15:20.556+0530] {dag.py:3239} INFO - Sync 1 DAGs
[2024-12-17T12:15:20.574+0530] {logging_mixin.py:190} INFO - [2024-12-17T12:15:20.574+0530] {dag.py:4180} INFO - Setting next_dagrun for movie_pipeline to 2024-12-17 01:30:00+00:00, run_after=2024-12-18 01:30:00+00:00
[2024-12-17T12:15:20.592+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 5.469 seconds
[2024-12-17T12:18:11.449+0530] {processor.py:186} INFO - Started process (PID=57693) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T12:18:11.450+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-17T12:18:11.452+0530] {logging_mixin.py:190} INFO - [2024-12-17T12:18:11.451+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T12:18:11.629+0530] {logging_mixin.py:190} INFO - [2024-12-17T12:18:11.629+0530] {dag.py:4435} INFO - dagrun id: movie_pipeline
[2024-12-17T12:18:11.644+0530] {logging_mixin.py:190} INFO - [2024-12-17T12:18:11.644+0530] {dag.py:4451} INFO - created dagrun <DagRun movie_pipeline @ 2024-12-17 06:48:11.495511+00:00: manual__2024-12-17T06:48:11.495511+00:00, state:running, queued_at: None. externally triggered: False>
[2024-12-17T12:18:11.670+0530] {logging_mixin.py:190} INFO - [2024-12-17T12:18:11.670+0530] {dag.py:4396} INFO - [DAG TEST] starting task_id=scrape_collection_page map_index=-1
[2024-12-17T12:18:11.670+0530] {logging_mixin.py:190} INFO - [2024-12-17T12:18:11.670+0530] {dag.py:4399} INFO - [DAG TEST] running task <TaskInstance: movie_pipeline.scrape_collection_page manual__2024-12-17T06:48:11.495511+00:00 [scheduled]>
[2024-12-17T12:18:16.773+0530] {logging_mixin.py:190} INFO - [2024-12-17T12:18:16.773+0530] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-12-17T12:18:16.836+0530] {logging_mixin.py:190} INFO - [2024-12-17 12:18:16,836] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T06:48:11.495511+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T06:48:11.495511+00:00'
[2024-12-17T12:18:16.837+0530] {logging_mixin.py:190} INFO - [2024-12-17T12:18:16.836+0530] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T06:48:11.495511+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T06:48:11.495511+00:00'
[2024-12-17T12:18:16.839+0530] {logging_mixin.py:190} INFO - Task instance is in running state
[2024-12-17T12:18:16.839+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: queued
[2024-12-17T12:18:16.840+0530] {logging_mixin.py:190} INFO - Current task name:scrape_collection_page state:scheduled start_date:None
[2024-12-17T12:18:16.840+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline and current dag run status:running
[2024-12-17T12:18:16.841+0530] {logging_mixin.py:190} INFO - [2024-12-17T12:18:16.841+0530] {taskinstance.py:731} INFO - ::endgroup::
[2024-12-17T12:18:16.843+0530] {logging_mixin.py:190} INFO - 2024-12-17
[2024-12-17T12:18:16.844+0530] {logging_mixin.py:190} INFO - [2024-12-17 12:18:16,843] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T12:18:16.844+0530] {logging_mixin.py:190} INFO - [2024-12-17T12:18:16.843+0530] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T12:18:16.849+0530] {logging_mixin.py:190} INFO - [2024-12-17T12:18:16.849+0530] {taskinstance.py:340} INFO - ::group::Post task execution logs
[2024-12-17T12:18:16.850+0530] {logging_mixin.py:190} INFO - [2024-12-17T12:18:16.850+0530] {taskinstance.py:352} INFO - Marking task as SUCCESS. dag_id=movie_pipeline, task_id=scrape_collection_page, run_id=manual__2024-12-17T06:48:11.495511+00:00, execution_date=20241217T064811, start_date=, end_date=20241217T064816
[2024-12-17T12:18:16.860+0530] {logging_mixin.py:190} INFO - Task instance in success state
[2024-12-17T12:18:16.860+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: running
[2024-12-17T12:18:16.860+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline queued_at:None
[2024-12-17T12:18:16.861+0530] {logging_mixin.py:190} INFO - Task hostname:Jayeshs-MacBook-Air.local operator:PythonOperator
[2024-12-17T12:18:16.861+0530] {logging_mixin.py:190} INFO - [2024-12-17T12:18:16.861+0530] {dag.py:4410} INFO - [DAG TEST] end task task_id=scrape_collection_page map_index=-1
[2024-12-17T12:18:16.866+0530] {logging_mixin.py:190} INFO - [2024-12-17T12:18:16.865+0530] {dagrun.py:854} INFO - Marking run <DagRun movie_pipeline @ 2024-12-17 06:48:11.495511+00:00: manual__2024-12-17T06:48:11.495511+00:00, state:running, queued_at: None. externally triggered: False> successful
[2024-12-17T12:18:16.866+0530] {logging_mixin.py:190} INFO - Dag run in success state
[2024-12-17T12:18:16.866+0530] {logging_mixin.py:190} INFO - Dag run start:2024-12-17 06:48:11.495511+00:00 end:2024-12-17 06:48:16.866291+00:00
[2024-12-17T12:18:16.866+0530] {logging_mixin.py:190} INFO - [2024-12-17T12:18:16.866+0530] {dagrun.py:905} INFO - DagRun Finished: dag_id=movie_pipeline, execution_date=2024-12-17 06:48:11.495511+00:00, run_id=manual__2024-12-17T06:48:11.495511+00:00, run_start_date=2024-12-17 06:48:11.495511+00:00, run_end_date=2024-12-17 06:48:16.866291+00:00, run_duration=5.37078, state=success, external_trigger=False, run_type=manual, data_interval_start=2024-12-16 06:48:11.495511+00:00, data_interval_end=2024-12-17 06:48:11.495511+00:00, dag_hash=None
[2024-12-17T12:18:16.877+0530] {processor.py:925} INFO - DAG(s) 'movie_pipeline' retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T12:18:16.889+0530] {logging_mixin.py:190} INFO - [2024-12-17T12:18:16.889+0530] {dag.py:3239} INFO - Sync 1 DAGs
[2024-12-17T12:18:16.906+0530] {logging_mixin.py:190} INFO - [2024-12-17T12:18:16.906+0530] {dag.py:4180} INFO - Setting next_dagrun for movie_pipeline to 2024-12-17 01:30:00+00:00, run_after=2024-12-18 01:30:00+00:00
[2024-12-17T12:18:16.932+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 5.486 seconds
[2024-12-17T12:21:07.125+0530] {processor.py:186} INFO - Started process (PID=57885) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T12:21:07.126+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-17T12:21:07.127+0530] {logging_mixin.py:190} INFO - [2024-12-17T12:21:07.127+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T12:21:07.302+0530] {logging_mixin.py:190} INFO - [2024-12-17T12:21:07.302+0530] {dag.py:4435} INFO - dagrun id: movie_pipeline
[2024-12-17T12:21:07.318+0530] {logging_mixin.py:190} INFO - [2024-12-17T12:21:07.317+0530] {dag.py:4451} INFO - created dagrun <DagRun movie_pipeline @ 2024-12-17 06:51:07.171212+00:00: manual__2024-12-17T06:51:07.171212+00:00, state:running, queued_at: None. externally triggered: False>
[2024-12-17T12:21:07.345+0530] {logging_mixin.py:190} INFO - [2024-12-17T12:21:07.344+0530] {dag.py:4396} INFO - [DAG TEST] starting task_id=scrape_collection_page map_index=-1
[2024-12-17T12:21:07.345+0530] {logging_mixin.py:190} INFO - [2024-12-17T12:21:07.345+0530] {dag.py:4399} INFO - [DAG TEST] running task <TaskInstance: movie_pipeline.scrape_collection_page manual__2024-12-17T06:51:07.171212+00:00 [scheduled]>
[2024-12-17T12:21:12.445+0530] {logging_mixin.py:190} INFO - [2024-12-17T12:21:12.445+0530] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-12-17T12:21:12.509+0530] {logging_mixin.py:190} INFO - [2024-12-17 12:21:12,509] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T06:51:07.171212+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T06:51:07.171212+00:00'
[2024-12-17T12:21:12.510+0530] {logging_mixin.py:190} INFO - [2024-12-17T12:21:12.509+0530] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T06:51:07.171212+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T06:51:07.171212+00:00'
[2024-12-17T12:21:12.512+0530] {logging_mixin.py:190} INFO - Task instance is in running state
[2024-12-17T12:21:12.513+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: queued
[2024-12-17T12:21:12.513+0530] {logging_mixin.py:190} INFO - Current task name:scrape_collection_page state:scheduled start_date:None
[2024-12-17T12:21:12.513+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline and current dag run status:running
[2024-12-17T12:21:12.514+0530] {logging_mixin.py:190} INFO - [2024-12-17T12:21:12.513+0530] {taskinstance.py:731} INFO - ::endgroup::
[2024-12-17T12:21:12.515+0530] {logging_mixin.py:190} INFO - 2024-12-17
[2024-12-17T12:21:12.516+0530] {logging_mixin.py:190} INFO - [2024-12-17 12:21:12,516] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T12:21:12.516+0530] {logging_mixin.py:190} INFO - [2024-12-17T12:21:12.516+0530] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T12:21:12.521+0530] {logging_mixin.py:190} INFO - [2024-12-17T12:21:12.521+0530] {taskinstance.py:340} INFO - ::group::Post task execution logs
[2024-12-17T12:21:12.521+0530] {logging_mixin.py:190} INFO - [2024-12-17T12:21:12.521+0530] {taskinstance.py:352} INFO - Marking task as SUCCESS. dag_id=movie_pipeline, task_id=scrape_collection_page, run_id=manual__2024-12-17T06:51:07.171212+00:00, execution_date=20241217T065107, start_date=, end_date=20241217T065112
[2024-12-17T12:21:12.532+0530] {logging_mixin.py:190} INFO - Task instance in success state
[2024-12-17T12:21:12.532+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: running
[2024-12-17T12:21:12.532+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline queued_at:None
[2024-12-17T12:21:12.532+0530] {logging_mixin.py:190} INFO - Task hostname:Jayeshs-MacBook-Air.local operator:PythonOperator
[2024-12-17T12:21:12.533+0530] {logging_mixin.py:190} INFO - [2024-12-17T12:21:12.532+0530] {dag.py:4410} INFO - [DAG TEST] end task task_id=scrape_collection_page map_index=-1
[2024-12-17T12:21:12.536+0530] {logging_mixin.py:190} INFO - [2024-12-17T12:21:12.536+0530] {dagrun.py:854} INFO - Marking run <DagRun movie_pipeline @ 2024-12-17 06:51:07.171212+00:00: manual__2024-12-17T06:51:07.171212+00:00, state:running, queued_at: None. externally triggered: False> successful
[2024-12-17T12:21:12.537+0530] {logging_mixin.py:190} INFO - Dag run in success state
[2024-12-17T12:21:12.537+0530] {logging_mixin.py:190} INFO - Dag run start:2024-12-17 06:51:07.171212+00:00 end:2024-12-17 06:51:12.537091+00:00
[2024-12-17T12:21:12.538+0530] {logging_mixin.py:190} INFO - [2024-12-17T12:21:12.538+0530] {dagrun.py:905} INFO - DagRun Finished: dag_id=movie_pipeline, execution_date=2024-12-17 06:51:07.171212+00:00, run_id=manual__2024-12-17T06:51:07.171212+00:00, run_start_date=2024-12-17 06:51:07.171212+00:00, run_end_date=2024-12-17 06:51:12.537091+00:00, run_duration=5.365879, state=success, external_trigger=False, run_type=manual, data_interval_start=2024-12-16 06:51:07.171212+00:00, data_interval_end=2024-12-17 06:51:07.171212+00:00, dag_hash=None
[2024-12-17T12:21:12.548+0530] {processor.py:925} INFO - DAG(s) 'movie_pipeline' retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T12:21:12.561+0530] {logging_mixin.py:190} INFO - [2024-12-17T12:21:12.560+0530] {dag.py:3239} INFO - Sync 1 DAGs
[2024-12-17T12:21:12.579+0530] {logging_mixin.py:190} INFO - [2024-12-17T12:21:12.578+0530] {dag.py:4180} INFO - Setting next_dagrun for movie_pipeline to 2024-12-17 01:30:00+00:00, run_after=2024-12-18 01:30:00+00:00
[2024-12-17T12:21:12.602+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 5.481 seconds
[2024-12-17T12:24:02.994+0530] {processor.py:186} INFO - Started process (PID=58091) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T12:24:02.995+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-17T12:24:02.996+0530] {logging_mixin.py:190} INFO - [2024-12-17T12:24:02.996+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T12:24:03.173+0530] {logging_mixin.py:190} INFO - [2024-12-17T12:24:03.172+0530] {dag.py:4435} INFO - dagrun id: movie_pipeline
[2024-12-17T12:24:03.188+0530] {logging_mixin.py:190} INFO - [2024-12-17T12:24:03.188+0530] {dag.py:4451} INFO - created dagrun <DagRun movie_pipeline @ 2024-12-17 06:54:03.040031+00:00: manual__2024-12-17T06:54:03.040031+00:00, state:running, queued_at: None. externally triggered: False>
[2024-12-17T12:24:03.213+0530] {logging_mixin.py:190} INFO - [2024-12-17T12:24:03.213+0530] {dag.py:4396} INFO - [DAG TEST] starting task_id=scrape_collection_page map_index=-1
[2024-12-17T12:24:03.214+0530] {logging_mixin.py:190} INFO - [2024-12-17T12:24:03.214+0530] {dag.py:4399} INFO - [DAG TEST] running task <TaskInstance: movie_pipeline.scrape_collection_page manual__2024-12-17T06:54:03.040031+00:00 [scheduled]>
[2024-12-17T12:24:08.313+0530] {logging_mixin.py:190} INFO - [2024-12-17T12:24:08.313+0530] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-12-17T12:24:08.376+0530] {logging_mixin.py:190} INFO - [2024-12-17 12:24:08,376] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T06:54:03.040031+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T06:54:03.040031+00:00'
[2024-12-17T12:24:08.377+0530] {logging_mixin.py:190} INFO - [2024-12-17T12:24:08.376+0530] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T06:54:03.040031+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T06:54:03.040031+00:00'
[2024-12-17T12:24:08.380+0530] {logging_mixin.py:190} INFO - Task instance is in running state
[2024-12-17T12:24:08.380+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: queued
[2024-12-17T12:24:08.381+0530] {logging_mixin.py:190} INFO - Current task name:scrape_collection_page state:scheduled start_date:None
[2024-12-17T12:24:08.381+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline and current dag run status:running
[2024-12-17T12:24:08.382+0530] {logging_mixin.py:190} INFO - [2024-12-17T12:24:08.381+0530] {taskinstance.py:731} INFO - ::endgroup::
[2024-12-17T12:24:08.383+0530] {logging_mixin.py:190} INFO - 2024-12-17
[2024-12-17T12:24:08.384+0530] {logging_mixin.py:190} INFO - [2024-12-17 12:24:08,383] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T12:24:08.384+0530] {logging_mixin.py:190} INFO - [2024-12-17T12:24:08.383+0530] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T12:24:08.389+0530] {logging_mixin.py:190} INFO - [2024-12-17T12:24:08.389+0530] {taskinstance.py:340} INFO - ::group::Post task execution logs
[2024-12-17T12:24:08.390+0530] {logging_mixin.py:190} INFO - [2024-12-17T12:24:08.390+0530] {taskinstance.py:352} INFO - Marking task as SUCCESS. dag_id=movie_pipeline, task_id=scrape_collection_page, run_id=manual__2024-12-17T06:54:03.040031+00:00, execution_date=20241217T065403, start_date=, end_date=20241217T065408
[2024-12-17T12:24:08.403+0530] {logging_mixin.py:190} INFO - Task instance in success state
[2024-12-17T12:24:08.403+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: running
[2024-12-17T12:24:08.403+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline queued_at:None
[2024-12-17T12:24:08.404+0530] {logging_mixin.py:190} INFO - Task hostname:Jayeshs-MacBook-Air.local operator:PythonOperator
[2024-12-17T12:24:08.404+0530] {logging_mixin.py:190} INFO - [2024-12-17T12:24:08.404+0530] {dag.py:4410} INFO - [DAG TEST] end task task_id=scrape_collection_page map_index=-1
[2024-12-17T12:24:08.408+0530] {logging_mixin.py:190} INFO - [2024-12-17T12:24:08.408+0530] {dagrun.py:854} INFO - Marking run <DagRun movie_pipeline @ 2024-12-17 06:54:03.040031+00:00: manual__2024-12-17T06:54:03.040031+00:00, state:running, queued_at: None. externally triggered: False> successful
[2024-12-17T12:24:08.409+0530] {logging_mixin.py:190} INFO - Dag run in success state
[2024-12-17T12:24:08.409+0530] {logging_mixin.py:190} INFO - Dag run start:2024-12-17 06:54:03.040031+00:00 end:2024-12-17 06:54:08.409138+00:00
[2024-12-17T12:24:08.409+0530] {logging_mixin.py:190} INFO - [2024-12-17T12:24:08.409+0530] {dagrun.py:905} INFO - DagRun Finished: dag_id=movie_pipeline, execution_date=2024-12-17 06:54:03.040031+00:00, run_id=manual__2024-12-17T06:54:03.040031+00:00, run_start_date=2024-12-17 06:54:03.040031+00:00, run_end_date=2024-12-17 06:54:08.409138+00:00, run_duration=5.369107, state=success, external_trigger=False, run_type=manual, data_interval_start=2024-12-16 06:54:03.040031+00:00, data_interval_end=2024-12-17 06:54:03.040031+00:00, dag_hash=None
[2024-12-17T12:24:08.418+0530] {processor.py:925} INFO - DAG(s) 'movie_pipeline' retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T12:24:08.431+0530] {logging_mixin.py:190} INFO - [2024-12-17T12:24:08.430+0530] {dag.py:3239} INFO - Sync 1 DAGs
[2024-12-17T12:24:08.448+0530] {logging_mixin.py:190} INFO - [2024-12-17T12:24:08.447+0530] {dag.py:4180} INFO - Setting next_dagrun for movie_pipeline to 2024-12-17 01:30:00+00:00, run_after=2024-12-18 01:30:00+00:00
[2024-12-17T12:24:08.474+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 5.483 seconds
[2024-12-17T12:26:59.171+0530] {processor.py:186} INFO - Started process (PID=58289) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T12:26:59.173+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-17T12:26:59.174+0530] {logging_mixin.py:190} INFO - [2024-12-17T12:26:59.174+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T12:26:59.350+0530] {logging_mixin.py:190} INFO - [2024-12-17T12:26:59.349+0530] {dag.py:4435} INFO - dagrun id: movie_pipeline
[2024-12-17T12:26:59.364+0530] {logging_mixin.py:190} INFO - [2024-12-17T12:26:59.364+0530] {dag.py:4451} INFO - created dagrun <DagRun movie_pipeline @ 2024-12-17 06:56:59.216704+00:00: manual__2024-12-17T06:56:59.216704+00:00, state:running, queued_at: None. externally triggered: False>
[2024-12-17T12:26:59.390+0530] {logging_mixin.py:190} INFO - [2024-12-17T12:26:59.390+0530] {dag.py:4396} INFO - [DAG TEST] starting task_id=scrape_collection_page map_index=-1
[2024-12-17T12:26:59.391+0530] {logging_mixin.py:190} INFO - [2024-12-17T12:26:59.390+0530] {dag.py:4399} INFO - [DAG TEST] running task <TaskInstance: movie_pipeline.scrape_collection_page manual__2024-12-17T06:56:59.216704+00:00 [scheduled]>
[2024-12-17T12:27:04.490+0530] {logging_mixin.py:190} INFO - [2024-12-17T12:27:04.490+0530] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-12-17T12:27:04.552+0530] {logging_mixin.py:190} INFO - [2024-12-17 12:27:04,552] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T06:56:59.216704+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T06:56:59.216704+00:00'
[2024-12-17T12:27:04.552+0530] {logging_mixin.py:190} INFO - [2024-12-17T12:27:04.552+0530] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T06:56:59.216704+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T06:56:59.216704+00:00'
[2024-12-17T12:27:04.556+0530] {logging_mixin.py:190} INFO - Task instance is in running state
[2024-12-17T12:27:04.556+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: queued
[2024-12-17T12:27:04.557+0530] {logging_mixin.py:190} INFO - Current task name:scrape_collection_page state:scheduled start_date:None
[2024-12-17T12:27:04.557+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline and current dag run status:running
[2024-12-17T12:27:04.557+0530] {logging_mixin.py:190} INFO - [2024-12-17T12:27:04.557+0530] {taskinstance.py:731} INFO - ::endgroup::
[2024-12-17T12:27:04.559+0530] {logging_mixin.py:190} INFO - 2024-12-17
[2024-12-17T12:27:04.560+0530] {logging_mixin.py:190} INFO - [2024-12-17 12:27:04,559] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T12:27:04.560+0530] {logging_mixin.py:190} INFO - [2024-12-17T12:27:04.559+0530] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T12:27:04.565+0530] {logging_mixin.py:190} INFO - [2024-12-17T12:27:04.565+0530] {taskinstance.py:340} INFO - ::group::Post task execution logs
[2024-12-17T12:27:04.566+0530] {logging_mixin.py:190} INFO - [2024-12-17T12:27:04.566+0530] {taskinstance.py:352} INFO - Marking task as SUCCESS. dag_id=movie_pipeline, task_id=scrape_collection_page, run_id=manual__2024-12-17T06:56:59.216704+00:00, execution_date=20241217T065659, start_date=, end_date=20241217T065704
[2024-12-17T12:27:04.576+0530] {logging_mixin.py:190} INFO - Task instance in success state
[2024-12-17T12:27:04.577+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: running
[2024-12-17T12:27:04.577+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline queued_at:None
[2024-12-17T12:27:04.577+0530] {logging_mixin.py:190} INFO - Task hostname:Jayeshs-MacBook-Air.local operator:PythonOperator
[2024-12-17T12:27:04.577+0530] {logging_mixin.py:190} INFO - [2024-12-17T12:27:04.577+0530] {dag.py:4410} INFO - [DAG TEST] end task task_id=scrape_collection_page map_index=-1
[2024-12-17T12:27:04.581+0530] {logging_mixin.py:190} INFO - [2024-12-17T12:27:04.581+0530] {dagrun.py:854} INFO - Marking run <DagRun movie_pipeline @ 2024-12-17 06:56:59.216704+00:00: manual__2024-12-17T06:56:59.216704+00:00, state:running, queued_at: None. externally triggered: False> successful
[2024-12-17T12:27:04.581+0530] {logging_mixin.py:190} INFO - Dag run in success state
[2024-12-17T12:27:04.582+0530] {logging_mixin.py:190} INFO - Dag run start:2024-12-17 06:56:59.216704+00:00 end:2024-12-17 06:57:04.581818+00:00
[2024-12-17T12:27:04.582+0530] {logging_mixin.py:190} INFO - [2024-12-17T12:27:04.582+0530] {dagrun.py:905} INFO - DagRun Finished: dag_id=movie_pipeline, execution_date=2024-12-17 06:56:59.216704+00:00, run_id=manual__2024-12-17T06:56:59.216704+00:00, run_start_date=2024-12-17 06:56:59.216704+00:00, run_end_date=2024-12-17 06:57:04.581818+00:00, run_duration=5.365114, state=success, external_trigger=False, run_type=manual, data_interval_start=2024-12-16 06:56:59.216704+00:00, data_interval_end=2024-12-17 06:56:59.216704+00:00, dag_hash=None
[2024-12-17T12:27:04.592+0530] {processor.py:925} INFO - DAG(s) 'movie_pipeline' retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T12:27:04.604+0530] {logging_mixin.py:190} INFO - [2024-12-17T12:27:04.604+0530] {dag.py:3239} INFO - Sync 1 DAGs
[2024-12-17T12:27:04.623+0530] {logging_mixin.py:190} INFO - [2024-12-17T12:27:04.622+0530] {dag.py:4180} INFO - Setting next_dagrun for movie_pipeline to 2024-12-17 01:30:00+00:00, run_after=2024-12-18 01:30:00+00:00
[2024-12-17T12:27:04.648+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 5.480 seconds
[2024-12-17T12:29:54.297+0530] {processor.py:186} INFO - Started process (PID=58480) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T12:29:54.299+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-17T12:29:54.300+0530] {logging_mixin.py:190} INFO - [2024-12-17T12:29:54.299+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T12:29:54.474+0530] {logging_mixin.py:190} INFO - [2024-12-17T12:29:54.474+0530] {dag.py:4435} INFO - dagrun id: movie_pipeline
[2024-12-17T12:29:54.492+0530] {logging_mixin.py:190} INFO - [2024-12-17T12:29:54.492+0530] {dag.py:4451} INFO - created dagrun <DagRun movie_pipeline @ 2024-12-17 06:59:54.341493+00:00: manual__2024-12-17T06:59:54.341493+00:00, state:running, queued_at: None. externally triggered: False>
[2024-12-17T12:29:54.520+0530] {logging_mixin.py:190} INFO - [2024-12-17T12:29:54.520+0530] {dag.py:4396} INFO - [DAG TEST] starting task_id=scrape_collection_page map_index=-1
[2024-12-17T12:29:54.520+0530] {logging_mixin.py:190} INFO - [2024-12-17T12:29:54.520+0530] {dag.py:4399} INFO - [DAG TEST] running task <TaskInstance: movie_pipeline.scrape_collection_page manual__2024-12-17T06:59:54.341493+00:00 [scheduled]>
[2024-12-17T12:29:59.614+0530] {logging_mixin.py:190} INFO - [2024-12-17T12:29:59.614+0530] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-12-17T12:29:59.698+0530] {logging_mixin.py:190} INFO - [2024-12-17 12:29:59,698] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T06:59:54.341493+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T06:59:54.341493+00:00'
[2024-12-17T12:29:59.699+0530] {logging_mixin.py:190} INFO - [2024-12-17T12:29:59.698+0530] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T06:59:54.341493+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T06:59:54.341493+00:00'
[2024-12-17T12:29:59.705+0530] {logging_mixin.py:190} INFO - Task instance is in running state
[2024-12-17T12:29:59.705+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: queued
[2024-12-17T12:29:59.705+0530] {logging_mixin.py:190} INFO - Current task name:scrape_collection_page state:scheduled start_date:None
[2024-12-17T12:29:59.706+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline and current dag run status:running
[2024-12-17T12:29:59.706+0530] {logging_mixin.py:190} INFO - [2024-12-17T12:29:59.706+0530] {taskinstance.py:731} INFO - ::endgroup::
[2024-12-17T12:29:59.709+0530] {logging_mixin.py:190} INFO - 2024-12-17
[2024-12-17T12:29:59.710+0530] {logging_mixin.py:190} INFO - [2024-12-17 12:29:59,709] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T12:29:59.710+0530] {logging_mixin.py:190} INFO - [2024-12-17T12:29:59.709+0530] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T12:29:59.718+0530] {logging_mixin.py:190} INFO - [2024-12-17T12:29:59.718+0530] {taskinstance.py:340} INFO - ::group::Post task execution logs
[2024-12-17T12:29:59.719+0530] {logging_mixin.py:190} INFO - [2024-12-17T12:29:59.719+0530] {taskinstance.py:352} INFO - Marking task as SUCCESS. dag_id=movie_pipeline, task_id=scrape_collection_page, run_id=manual__2024-12-17T06:59:54.341493+00:00, execution_date=20241217T065954, start_date=, end_date=20241217T065959
[2024-12-17T12:29:59.732+0530] {logging_mixin.py:190} INFO - Task instance in success state
[2024-12-17T12:29:59.732+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: running
[2024-12-17T12:29:59.732+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline queued_at:None
[2024-12-17T12:29:59.732+0530] {logging_mixin.py:190} INFO - Task hostname:Jayeshs-MacBook-Air.local operator:PythonOperator
[2024-12-17T12:29:59.733+0530] {logging_mixin.py:190} INFO - [2024-12-17T12:29:59.733+0530] {dag.py:4410} INFO - [DAG TEST] end task task_id=scrape_collection_page map_index=-1
[2024-12-17T12:29:59.737+0530] {logging_mixin.py:190} INFO - [2024-12-17T12:29:59.736+0530] {dagrun.py:854} INFO - Marking run <DagRun movie_pipeline @ 2024-12-17 06:59:54.341493+00:00: manual__2024-12-17T06:59:54.341493+00:00, state:running, queued_at: None. externally triggered: False> successful
[2024-12-17T12:29:59.737+0530] {logging_mixin.py:190} INFO - Dag run in success state
[2024-12-17T12:29:59.738+0530] {logging_mixin.py:190} INFO - Dag run start:2024-12-17 06:59:54.341493+00:00 end:2024-12-17 06:59:59.737542+00:00
[2024-12-17T12:29:59.739+0530] {logging_mixin.py:190} INFO - [2024-12-17T12:29:59.738+0530] {dagrun.py:905} INFO - DagRun Finished: dag_id=movie_pipeline, execution_date=2024-12-17 06:59:54.341493+00:00, run_id=manual__2024-12-17T06:59:54.341493+00:00, run_start_date=2024-12-17 06:59:54.341493+00:00, run_end_date=2024-12-17 06:59:59.737542+00:00, run_duration=5.396049, state=success, external_trigger=False, run_type=manual, data_interval_start=2024-12-16 06:59:54.341493+00:00, data_interval_end=2024-12-17 06:59:54.341493+00:00, dag_hash=None
[2024-12-17T12:29:59.751+0530] {processor.py:925} INFO - DAG(s) 'movie_pipeline' retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T12:29:59.766+0530] {logging_mixin.py:190} INFO - [2024-12-17T12:29:59.765+0530] {dag.py:3239} INFO - Sync 1 DAGs
[2024-12-17T12:29:59.789+0530] {logging_mixin.py:190} INFO - [2024-12-17T12:29:59.789+0530] {dag.py:4180} INFO - Setting next_dagrun for movie_pipeline to 2024-12-17 01:30:00+00:00, run_after=2024-12-18 01:30:00+00:00
[2024-12-17T12:29:59.816+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 5.522 seconds
[2024-12-17T12:32:49.276+0530] {processor.py:186} INFO - Started process (PID=58690) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T12:32:49.277+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-17T12:32:49.278+0530] {logging_mixin.py:190} INFO - [2024-12-17T12:32:49.278+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T12:32:49.454+0530] {logging_mixin.py:190} INFO - [2024-12-17T12:32:49.454+0530] {dag.py:4435} INFO - dagrun id: movie_pipeline
[2024-12-17T12:32:49.468+0530] {logging_mixin.py:190} INFO - [2024-12-17T12:32:49.468+0530] {dag.py:4451} INFO - created dagrun <DagRun movie_pipeline @ 2024-12-17 07:02:49.317220+00:00: manual__2024-12-17T07:02:49.317220+00:00, state:running, queued_at: None. externally triggered: False>
[2024-12-17T12:32:49.496+0530] {logging_mixin.py:190} INFO - [2024-12-17T12:32:49.496+0530] {dag.py:4396} INFO - [DAG TEST] starting task_id=scrape_collection_page map_index=-1
[2024-12-17T12:32:49.497+0530] {logging_mixin.py:190} INFO - [2024-12-17T12:32:49.496+0530] {dag.py:4399} INFO - [DAG TEST] running task <TaskInstance: movie_pipeline.scrape_collection_page manual__2024-12-17T07:02:49.317220+00:00 [scheduled]>
[2024-12-17T12:32:54.599+0530] {logging_mixin.py:190} INFO - [2024-12-17T12:32:54.599+0530] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-12-17T12:32:54.662+0530] {logging_mixin.py:190} INFO - [2024-12-17 12:32:54,662] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T07:02:49.317220+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T07:02:49.317220+00:00'
[2024-12-17T12:32:54.662+0530] {logging_mixin.py:190} INFO - [2024-12-17T12:32:54.662+0530] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T07:02:49.317220+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T07:02:49.317220+00:00'
[2024-12-17T12:32:54.666+0530] {logging_mixin.py:190} INFO - Task instance is in running state
[2024-12-17T12:32:54.666+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: queued
[2024-12-17T12:32:54.667+0530] {logging_mixin.py:190} INFO - Current task name:scrape_collection_page state:scheduled start_date:None
[2024-12-17T12:32:54.667+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline and current dag run status:running
[2024-12-17T12:32:54.668+0530] {logging_mixin.py:190} INFO - [2024-12-17T12:32:54.667+0530] {taskinstance.py:731} INFO - ::endgroup::
[2024-12-17T12:32:54.669+0530] {logging_mixin.py:190} INFO - 2024-12-17
[2024-12-17T12:32:54.670+0530] {logging_mixin.py:190} INFO - [2024-12-17 12:32:54,669] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T12:32:54.670+0530] {logging_mixin.py:190} INFO - [2024-12-17T12:32:54.669+0530] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T12:32:54.675+0530] {logging_mixin.py:190} INFO - [2024-12-17T12:32:54.675+0530] {taskinstance.py:340} INFO - ::group::Post task execution logs
[2024-12-17T12:32:54.676+0530] {logging_mixin.py:190} INFO - [2024-12-17T12:32:54.676+0530] {taskinstance.py:352} INFO - Marking task as SUCCESS. dag_id=movie_pipeline, task_id=scrape_collection_page, run_id=manual__2024-12-17T07:02:49.317220+00:00, execution_date=20241217T070249, start_date=, end_date=20241217T070254
[2024-12-17T12:32:54.685+0530] {logging_mixin.py:190} INFO - Task instance in success state
[2024-12-17T12:32:54.686+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: running
[2024-12-17T12:32:54.686+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline queued_at:None
[2024-12-17T12:32:54.686+0530] {logging_mixin.py:190} INFO - Task hostname:Jayeshs-MacBook-Air.local operator:PythonOperator
[2024-12-17T12:32:54.687+0530] {logging_mixin.py:190} INFO - [2024-12-17T12:32:54.686+0530] {dag.py:4410} INFO - [DAG TEST] end task task_id=scrape_collection_page map_index=-1
[2024-12-17T12:32:54.691+0530] {logging_mixin.py:190} INFO - [2024-12-17T12:32:54.691+0530] {dagrun.py:854} INFO - Marking run <DagRun movie_pipeline @ 2024-12-17 07:02:49.317220+00:00: manual__2024-12-17T07:02:49.317220+00:00, state:running, queued_at: None. externally triggered: False> successful
[2024-12-17T12:32:54.692+0530] {logging_mixin.py:190} INFO - Dag run in success state
[2024-12-17T12:32:54.692+0530] {logging_mixin.py:190} INFO - Dag run start:2024-12-17 07:02:49.317220+00:00 end:2024-12-17 07:02:54.691989+00:00
[2024-12-17T12:32:54.692+0530] {logging_mixin.py:190} INFO - [2024-12-17T12:32:54.692+0530] {dagrun.py:905} INFO - DagRun Finished: dag_id=movie_pipeline, execution_date=2024-12-17 07:02:49.317220+00:00, run_id=manual__2024-12-17T07:02:49.317220+00:00, run_start_date=2024-12-17 07:02:49.317220+00:00, run_end_date=2024-12-17 07:02:54.691989+00:00, run_duration=5.374769, state=success, external_trigger=False, run_type=manual, data_interval_start=2024-12-16 07:02:49.317220+00:00, data_interval_end=2024-12-17 07:02:49.317220+00:00, dag_hash=None
[2024-12-17T12:32:54.701+0530] {processor.py:925} INFO - DAG(s) 'movie_pipeline' retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T12:32:54.715+0530] {logging_mixin.py:190} INFO - [2024-12-17T12:32:54.715+0530] {dag.py:3239} INFO - Sync 1 DAGs
[2024-12-17T12:32:54.734+0530] {logging_mixin.py:190} INFO - [2024-12-17T12:32:54.733+0530] {dag.py:4180} INFO - Setting next_dagrun for movie_pipeline to 2024-12-17 01:30:00+00:00, run_after=2024-12-18 01:30:00+00:00
[2024-12-17T12:32:54.753+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 5.480 seconds
[2024-12-17T12:35:48.290+0530] {processor.py:186} INFO - Started process (PID=58900) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T12:35:48.293+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-17T12:35:48.294+0530] {logging_mixin.py:190} INFO - [2024-12-17T12:35:48.294+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T12:35:48.466+0530] {logging_mixin.py:190} INFO - [2024-12-17T12:35:48.466+0530] {dag.py:4435} INFO - dagrun id: movie_pipeline
[2024-12-17T12:35:48.481+0530] {logging_mixin.py:190} INFO - [2024-12-17T12:35:48.480+0530] {dag.py:4451} INFO - created dagrun <DagRun movie_pipeline @ 2024-12-17 07:05:48.335497+00:00: manual__2024-12-17T07:05:48.335497+00:00, state:running, queued_at: None. externally triggered: False>
[2024-12-17T12:35:48.511+0530] {logging_mixin.py:190} INFO - [2024-12-17T12:35:48.511+0530] {dag.py:4396} INFO - [DAG TEST] starting task_id=scrape_collection_page map_index=-1
[2024-12-17T12:35:48.512+0530] {logging_mixin.py:190} INFO - [2024-12-17T12:35:48.512+0530] {dag.py:4399} INFO - [DAG TEST] running task <TaskInstance: movie_pipeline.scrape_collection_page manual__2024-12-17T07:05:48.335497+00:00 [scheduled]>
[2024-12-17T12:35:53.614+0530] {logging_mixin.py:190} INFO - [2024-12-17T12:35:53.613+0530] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-12-17T12:35:53.681+0530] {logging_mixin.py:190} INFO - [2024-12-17 12:35:53,681] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T07:05:48.335497+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T07:05:48.335497+00:00'
[2024-12-17T12:35:53.682+0530] {logging_mixin.py:190} INFO - [2024-12-17T12:35:53.681+0530] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T07:05:48.335497+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T07:05:48.335497+00:00'
[2024-12-17T12:35:53.699+0530] {logging_mixin.py:190} INFO - Task instance is in running state
[2024-12-17T12:35:53.699+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: queued
[2024-12-17T12:35:53.700+0530] {logging_mixin.py:190} INFO - Current task name:scrape_collection_page state:scheduled start_date:None
[2024-12-17T12:35:53.700+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline and current dag run status:running
[2024-12-17T12:35:53.701+0530] {logging_mixin.py:190} INFO - [2024-12-17T12:35:53.701+0530] {taskinstance.py:731} INFO - ::endgroup::
[2024-12-17T12:35:53.705+0530] {logging_mixin.py:190} INFO - 2024-12-17
[2024-12-17T12:35:53.706+0530] {logging_mixin.py:190} INFO - [2024-12-17 12:35:53,705] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T12:35:53.706+0530] {logging_mixin.py:190} INFO - [2024-12-17T12:35:53.705+0530] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T12:35:53.713+0530] {logging_mixin.py:190} INFO - [2024-12-17T12:35:53.712+0530] {taskinstance.py:340} INFO - ::group::Post task execution logs
[2024-12-17T12:35:53.713+0530] {logging_mixin.py:190} INFO - [2024-12-17T12:35:53.713+0530] {taskinstance.py:352} INFO - Marking task as SUCCESS. dag_id=movie_pipeline, task_id=scrape_collection_page, run_id=manual__2024-12-17T07:05:48.335497+00:00, execution_date=20241217T070548, start_date=, end_date=20241217T070553
[2024-12-17T12:35:53.723+0530] {logging_mixin.py:190} INFO - Task instance in success state
[2024-12-17T12:35:53.723+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: running
[2024-12-17T12:35:53.724+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline queued_at:None
[2024-12-17T12:35:53.724+0530] {logging_mixin.py:190} INFO - Task hostname:Jayeshs-MacBook-Air.local operator:PythonOperator
[2024-12-17T12:35:53.725+0530] {logging_mixin.py:190} INFO - [2024-12-17T12:35:53.724+0530] {dag.py:4410} INFO - [DAG TEST] end task task_id=scrape_collection_page map_index=-1
[2024-12-17T12:35:53.729+0530] {logging_mixin.py:190} INFO - [2024-12-17T12:35:53.728+0530] {dagrun.py:854} INFO - Marking run <DagRun movie_pipeline @ 2024-12-17 07:05:48.335497+00:00: manual__2024-12-17T07:05:48.335497+00:00, state:running, queued_at: None. externally triggered: False> successful
[2024-12-17T12:35:53.729+0530] {logging_mixin.py:190} INFO - Dag run in success state
[2024-12-17T12:35:53.729+0530] {logging_mixin.py:190} INFO - Dag run start:2024-12-17 07:05:48.335497+00:00 end:2024-12-17 07:05:53.729476+00:00
[2024-12-17T12:35:53.730+0530] {logging_mixin.py:190} INFO - [2024-12-17T12:35:53.730+0530] {dagrun.py:905} INFO - DagRun Finished: dag_id=movie_pipeline, execution_date=2024-12-17 07:05:48.335497+00:00, run_id=manual__2024-12-17T07:05:48.335497+00:00, run_start_date=2024-12-17 07:05:48.335497+00:00, run_end_date=2024-12-17 07:05:53.729476+00:00, run_duration=5.393979, state=success, external_trigger=False, run_type=manual, data_interval_start=2024-12-16 07:05:48.335497+00:00, data_interval_end=2024-12-17 07:05:48.335497+00:00, dag_hash=None
[2024-12-17T12:35:53.739+0530] {processor.py:925} INFO - DAG(s) 'movie_pipeline' retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T12:35:53.751+0530] {logging_mixin.py:190} INFO - [2024-12-17T12:35:53.751+0530] {dag.py:3239} INFO - Sync 1 DAGs
[2024-12-17T12:35:53.769+0530] {logging_mixin.py:190} INFO - [2024-12-17T12:35:53.769+0530] {dag.py:4180} INFO - Setting next_dagrun for movie_pipeline to 2024-12-17 01:30:00+00:00, run_after=2024-12-18 01:30:00+00:00
[2024-12-17T12:35:53.789+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 5.503 seconds
[2024-12-17T12:38:47.052+0530] {processor.py:186} INFO - Started process (PID=59108) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T12:38:47.053+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-17T12:38:47.055+0530] {logging_mixin.py:190} INFO - [2024-12-17T12:38:47.054+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T12:38:47.230+0530] {logging_mixin.py:190} INFO - [2024-12-17T12:38:47.230+0530] {dag.py:4435} INFO - dagrun id: movie_pipeline
[2024-12-17T12:38:47.244+0530] {logging_mixin.py:190} INFO - [2024-12-17T12:38:47.244+0530] {dag.py:4451} INFO - created dagrun <DagRun movie_pipeline @ 2024-12-17 07:08:47.094678+00:00: manual__2024-12-17T07:08:47.094678+00:00, state:running, queued_at: None. externally triggered: False>
[2024-12-17T12:38:47.270+0530] {logging_mixin.py:190} INFO - [2024-12-17T12:38:47.270+0530] {dag.py:4396} INFO - [DAG TEST] starting task_id=scrape_collection_page map_index=-1
[2024-12-17T12:38:47.271+0530] {logging_mixin.py:190} INFO - [2024-12-17T12:38:47.271+0530] {dag.py:4399} INFO - [DAG TEST] running task <TaskInstance: movie_pipeline.scrape_collection_page manual__2024-12-17T07:08:47.094678+00:00 [scheduled]>
[2024-12-17T12:38:52.373+0530] {logging_mixin.py:190} INFO - [2024-12-17T12:38:52.373+0530] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-12-17T12:38:52.437+0530] {logging_mixin.py:190} INFO - [2024-12-17 12:38:52,437] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T07:08:47.094678+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T07:08:47.094678+00:00'
[2024-12-17T12:38:52.437+0530] {logging_mixin.py:190} INFO - [2024-12-17T12:38:52.437+0530] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T07:08:47.094678+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T07:08:47.094678+00:00'
[2024-12-17T12:38:52.440+0530] {logging_mixin.py:190} INFO - Task instance is in running state
[2024-12-17T12:38:52.440+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: queued
[2024-12-17T12:38:52.442+0530] {logging_mixin.py:190} INFO - Current task name:scrape_collection_page state:scheduled start_date:None
[2024-12-17T12:38:52.442+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline and current dag run status:running
[2024-12-17T12:38:52.442+0530] {logging_mixin.py:190} INFO - [2024-12-17T12:38:52.442+0530] {taskinstance.py:731} INFO - ::endgroup::
[2024-12-17T12:38:52.444+0530] {logging_mixin.py:190} INFO - 2024-12-17
[2024-12-17T12:38:52.445+0530] {logging_mixin.py:190} INFO - [2024-12-17 12:38:52,445] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T12:38:52.445+0530] {logging_mixin.py:190} INFO - [2024-12-17T12:38:52.445+0530] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T12:38:52.450+0530] {logging_mixin.py:190} INFO - [2024-12-17T12:38:52.450+0530] {taskinstance.py:340} INFO - ::group::Post task execution logs
[2024-12-17T12:38:52.451+0530] {logging_mixin.py:190} INFO - [2024-12-17T12:38:52.450+0530] {taskinstance.py:352} INFO - Marking task as SUCCESS. dag_id=movie_pipeline, task_id=scrape_collection_page, run_id=manual__2024-12-17T07:08:47.094678+00:00, execution_date=20241217T070847, start_date=, end_date=20241217T070852
[2024-12-17T12:38:52.462+0530] {logging_mixin.py:190} INFO - Task instance in success state
[2024-12-17T12:38:52.462+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: running
[2024-12-17T12:38:52.462+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline queued_at:None
[2024-12-17T12:38:52.462+0530] {logging_mixin.py:190} INFO - Task hostname:Jayeshs-MacBook-Air.local operator:PythonOperator
[2024-12-17T12:38:52.463+0530] {logging_mixin.py:190} INFO - [2024-12-17T12:38:52.462+0530] {dag.py:4410} INFO - [DAG TEST] end task task_id=scrape_collection_page map_index=-1
[2024-12-17T12:38:52.466+0530] {logging_mixin.py:190} INFO - [2024-12-17T12:38:52.466+0530] {dagrun.py:854} INFO - Marking run <DagRun movie_pipeline @ 2024-12-17 07:08:47.094678+00:00: manual__2024-12-17T07:08:47.094678+00:00, state:running, queued_at: None. externally triggered: False> successful
[2024-12-17T12:38:52.467+0530] {logging_mixin.py:190} INFO - Dag run in success state
[2024-12-17T12:38:52.467+0530] {logging_mixin.py:190} INFO - Dag run start:2024-12-17 07:08:47.094678+00:00 end:2024-12-17 07:08:52.466947+00:00
[2024-12-17T12:38:52.467+0530] {logging_mixin.py:190} INFO - [2024-12-17T12:38:52.467+0530] {dagrun.py:905} INFO - DagRun Finished: dag_id=movie_pipeline, execution_date=2024-12-17 07:08:47.094678+00:00, run_id=manual__2024-12-17T07:08:47.094678+00:00, run_start_date=2024-12-17 07:08:47.094678+00:00, run_end_date=2024-12-17 07:08:52.466947+00:00, run_duration=5.372269, state=success, external_trigger=False, run_type=manual, data_interval_start=2024-12-16 07:08:47.094678+00:00, data_interval_end=2024-12-17 07:08:47.094678+00:00, dag_hash=None
[2024-12-17T12:38:52.477+0530] {processor.py:925} INFO - DAG(s) 'movie_pipeline' retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T12:38:52.489+0530] {logging_mixin.py:190} INFO - [2024-12-17T12:38:52.489+0530] {dag.py:3239} INFO - Sync 1 DAGs
[2024-12-17T12:38:52.510+0530] {logging_mixin.py:190} INFO - [2024-12-17T12:38:52.509+0530] {dag.py:4180} INFO - Setting next_dagrun for movie_pipeline to 2024-12-17 01:30:00+00:00, run_after=2024-12-18 01:30:00+00:00
[2024-12-17T12:38:52.530+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 5.481 seconds
[2024-12-17T12:41:41.804+0530] {processor.py:186} INFO - Started process (PID=59301) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T12:41:41.805+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-17T12:41:41.807+0530] {logging_mixin.py:190} INFO - [2024-12-17T12:41:41.807+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T12:41:41.984+0530] {logging_mixin.py:190} INFO - [2024-12-17T12:41:41.984+0530] {dag.py:4435} INFO - dagrun id: movie_pipeline
[2024-12-17T12:41:41.998+0530] {logging_mixin.py:190} INFO - [2024-12-17T12:41:41.998+0530] {dag.py:4451} INFO - created dagrun <DagRun movie_pipeline @ 2024-12-17 07:11:41.848719+00:00: manual__2024-12-17T07:11:41.848719+00:00, state:running, queued_at: None. externally triggered: False>
[2024-12-17T12:41:42.023+0530] {logging_mixin.py:190} INFO - [2024-12-17T12:41:42.023+0530] {dag.py:4396} INFO - [DAG TEST] starting task_id=scrape_collection_page map_index=-1
[2024-12-17T12:41:42.024+0530] {logging_mixin.py:190} INFO - [2024-12-17T12:41:42.024+0530] {dag.py:4399} INFO - [DAG TEST] running task <TaskInstance: movie_pipeline.scrape_collection_page manual__2024-12-17T07:11:41.848719+00:00 [scheduled]>
[2024-12-17T12:41:47.121+0530] {logging_mixin.py:190} INFO - [2024-12-17T12:41:47.121+0530] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-12-17T12:41:47.182+0530] {logging_mixin.py:190} INFO - [2024-12-17 12:41:47,182] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T07:11:41.848719+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T07:11:41.848719+00:00'
[2024-12-17T12:41:47.183+0530] {logging_mixin.py:190} INFO - [2024-12-17T12:41:47.182+0530] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T07:11:41.848719+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T07:11:41.848719+00:00'
[2024-12-17T12:41:47.187+0530] {logging_mixin.py:190} INFO - Task instance is in running state
[2024-12-17T12:41:47.187+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: queued
[2024-12-17T12:41:47.187+0530] {logging_mixin.py:190} INFO - Current task name:scrape_collection_page state:scheduled start_date:None
[2024-12-17T12:41:47.187+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline and current dag run status:running
[2024-12-17T12:41:47.188+0530] {logging_mixin.py:190} INFO - [2024-12-17T12:41:47.188+0530] {taskinstance.py:731} INFO - ::endgroup::
[2024-12-17T12:41:47.190+0530] {logging_mixin.py:190} INFO - 2024-12-17
[2024-12-17T12:41:47.190+0530] {logging_mixin.py:190} INFO - [2024-12-17 12:41:47,190] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T12:41:47.191+0530] {logging_mixin.py:190} INFO - [2024-12-17T12:41:47.190+0530] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T12:41:47.196+0530] {logging_mixin.py:190} INFO - [2024-12-17T12:41:47.196+0530] {taskinstance.py:340} INFO - ::group::Post task execution logs
[2024-12-17T12:41:47.197+0530] {logging_mixin.py:190} INFO - [2024-12-17T12:41:47.197+0530] {taskinstance.py:352} INFO - Marking task as SUCCESS. dag_id=movie_pipeline, task_id=scrape_collection_page, run_id=manual__2024-12-17T07:11:41.848719+00:00, execution_date=20241217T071141, start_date=, end_date=20241217T071147
[2024-12-17T12:41:47.206+0530] {logging_mixin.py:190} INFO - Task instance in success state
[2024-12-17T12:41:47.207+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: running
[2024-12-17T12:41:47.207+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline queued_at:None
[2024-12-17T12:41:47.207+0530] {logging_mixin.py:190} INFO - Task hostname:Jayeshs-MacBook-Air.local operator:PythonOperator
[2024-12-17T12:41:47.208+0530] {logging_mixin.py:190} INFO - [2024-12-17T12:41:47.208+0530] {dag.py:4410} INFO - [DAG TEST] end task task_id=scrape_collection_page map_index=-1
[2024-12-17T12:41:47.213+0530] {logging_mixin.py:190} INFO - [2024-12-17T12:41:47.211+0530] {dagrun.py:854} INFO - Marking run <DagRun movie_pipeline @ 2024-12-17 07:11:41.848719+00:00: manual__2024-12-17T07:11:41.848719+00:00, state:running, queued_at: None. externally triggered: False> successful
[2024-12-17T12:41:47.213+0530] {logging_mixin.py:190} INFO - Dag run in success state
[2024-12-17T12:41:47.214+0530] {logging_mixin.py:190} INFO - Dag run start:2024-12-17 07:11:41.848719+00:00 end:2024-12-17 07:11:47.213634+00:00
[2024-12-17T12:41:47.214+0530] {logging_mixin.py:190} INFO - [2024-12-17T12:41:47.214+0530] {dagrun.py:905} INFO - DagRun Finished: dag_id=movie_pipeline, execution_date=2024-12-17 07:11:41.848719+00:00, run_id=manual__2024-12-17T07:11:41.848719+00:00, run_start_date=2024-12-17 07:11:41.848719+00:00, run_end_date=2024-12-17 07:11:47.213634+00:00, run_duration=5.364915, state=success, external_trigger=False, run_type=manual, data_interval_start=2024-12-16 07:11:41.848719+00:00, data_interval_end=2024-12-17 07:11:41.848719+00:00, dag_hash=None
[2024-12-17T12:41:47.223+0530] {processor.py:925} INFO - DAG(s) 'movie_pipeline' retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T12:41:47.235+0530] {logging_mixin.py:190} INFO - [2024-12-17T12:41:47.234+0530] {dag.py:3239} INFO - Sync 1 DAGs
[2024-12-17T12:41:47.252+0530] {logging_mixin.py:190} INFO - [2024-12-17T12:41:47.252+0530] {dag.py:4180} INFO - Setting next_dagrun for movie_pipeline to 2024-12-17 01:30:00+00:00, run_after=2024-12-18 01:30:00+00:00
[2024-12-17T12:41:47.271+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 5.472 seconds
[2024-12-17T12:44:38.255+0530] {processor.py:186} INFO - Started process (PID=59520) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T12:44:38.257+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-17T12:44:38.258+0530] {logging_mixin.py:190} INFO - [2024-12-17T12:44:38.258+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T12:44:38.436+0530] {logging_mixin.py:190} INFO - [2024-12-17T12:44:38.436+0530] {dag.py:4435} INFO - dagrun id: movie_pipeline
[2024-12-17T12:44:38.452+0530] {logging_mixin.py:190} INFO - [2024-12-17T12:44:38.452+0530] {dag.py:4451} INFO - created dagrun <DagRun movie_pipeline @ 2024-12-17 07:14:38.303028+00:00: manual__2024-12-17T07:14:38.303028+00:00, state:running, queued_at: None. externally triggered: False>
[2024-12-17T12:44:38.482+0530] {logging_mixin.py:190} INFO - [2024-12-17T12:44:38.481+0530] {dag.py:4396} INFO - [DAG TEST] starting task_id=scrape_collection_page map_index=-1
[2024-12-17T12:44:38.482+0530] {logging_mixin.py:190} INFO - [2024-12-17T12:44:38.482+0530] {dag.py:4399} INFO - [DAG TEST] running task <TaskInstance: movie_pipeline.scrape_collection_page manual__2024-12-17T07:14:38.303028+00:00 [scheduled]>
[2024-12-17T12:44:43.589+0530] {logging_mixin.py:190} INFO - [2024-12-17T12:44:43.589+0530] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-12-17T12:44:43.662+0530] {logging_mixin.py:190} INFO - [2024-12-17 12:44:43,662] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T07:14:38.303028+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T07:14:38.303028+00:00'
[2024-12-17T12:44:43.663+0530] {logging_mixin.py:190} INFO - [2024-12-17T12:44:43.662+0530] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T07:14:38.303028+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T07:14:38.303028+00:00'
[2024-12-17T12:44:43.677+0530] {logging_mixin.py:190} INFO - Task instance is in running state
[2024-12-17T12:44:43.678+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: queued
[2024-12-17T12:44:43.678+0530] {logging_mixin.py:190} INFO - Current task name:scrape_collection_page state:scheduled start_date:None
[2024-12-17T12:44:43.678+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline and current dag run status:running
[2024-12-17T12:44:43.678+0530] {logging_mixin.py:190} INFO - [2024-12-17T12:44:43.678+0530] {taskinstance.py:731} INFO - ::endgroup::
[2024-12-17T12:44:43.682+0530] {logging_mixin.py:190} INFO - 2024-12-17
[2024-12-17T12:44:43.683+0530] {logging_mixin.py:190} INFO - [2024-12-17 12:44:43,683] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T12:44:43.684+0530] {logging_mixin.py:190} INFO - [2024-12-17T12:44:43.683+0530] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T12:44:43.690+0530] {logging_mixin.py:190} INFO - [2024-12-17T12:44:43.690+0530] {taskinstance.py:340} INFO - ::group::Post task execution logs
[2024-12-17T12:44:43.691+0530] {logging_mixin.py:190} INFO - [2024-12-17T12:44:43.690+0530] {taskinstance.py:352} INFO - Marking task as SUCCESS. dag_id=movie_pipeline, task_id=scrape_collection_page, run_id=manual__2024-12-17T07:14:38.303028+00:00, execution_date=20241217T071438, start_date=, end_date=20241217T071443
[2024-12-17T12:44:43.700+0530] {logging_mixin.py:190} INFO - Task instance in success state
[2024-12-17T12:44:43.700+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: running
[2024-12-17T12:44:43.700+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline queued_at:None
[2024-12-17T12:44:43.700+0530] {logging_mixin.py:190} INFO - Task hostname:Jayeshs-MacBook-Air.local operator:PythonOperator
[2024-12-17T12:44:43.701+0530] {logging_mixin.py:190} INFO - [2024-12-17T12:44:43.701+0530] {dag.py:4410} INFO - [DAG TEST] end task task_id=scrape_collection_page map_index=-1
[2024-12-17T12:44:43.705+0530] {logging_mixin.py:190} INFO - [2024-12-17T12:44:43.705+0530] {dagrun.py:854} INFO - Marking run <DagRun movie_pipeline @ 2024-12-17 07:14:38.303028+00:00: manual__2024-12-17T07:14:38.303028+00:00, state:running, queued_at: None. externally triggered: False> successful
[2024-12-17T12:44:43.705+0530] {logging_mixin.py:190} INFO - Dag run in success state
[2024-12-17T12:44:43.705+0530] {logging_mixin.py:190} INFO - Dag run start:2024-12-17 07:14:38.303028+00:00 end:2024-12-17 07:14:43.705535+00:00
[2024-12-17T12:44:43.706+0530] {logging_mixin.py:190} INFO - [2024-12-17T12:44:43.705+0530] {dagrun.py:905} INFO - DagRun Finished: dag_id=movie_pipeline, execution_date=2024-12-17 07:14:38.303028+00:00, run_id=manual__2024-12-17T07:14:38.303028+00:00, run_start_date=2024-12-17 07:14:38.303028+00:00, run_end_date=2024-12-17 07:14:43.705535+00:00, run_duration=5.402507, state=success, external_trigger=False, run_type=manual, data_interval_start=2024-12-16 07:14:38.303028+00:00, data_interval_end=2024-12-17 07:14:38.303028+00:00, dag_hash=None
[2024-12-17T12:44:43.714+0530] {processor.py:925} INFO - DAG(s) 'movie_pipeline' retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T12:44:43.727+0530] {logging_mixin.py:190} INFO - [2024-12-17T12:44:43.726+0530] {dag.py:3239} INFO - Sync 1 DAGs
[2024-12-17T12:44:43.746+0530] {logging_mixin.py:190} INFO - [2024-12-17T12:44:43.746+0530] {dag.py:4180} INFO - Setting next_dagrun for movie_pipeline to 2024-12-17 01:30:00+00:00, run_after=2024-12-18 01:30:00+00:00
[2024-12-17T12:44:43.765+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 5.514 seconds
[2024-12-17T12:47:36.870+0530] {processor.py:186} INFO - Started process (PID=59717) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T12:47:36.871+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-17T12:47:36.872+0530] {logging_mixin.py:190} INFO - [2024-12-17T12:47:36.872+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T12:47:37.048+0530] {logging_mixin.py:190} INFO - [2024-12-17T12:47:37.047+0530] {dag.py:4435} INFO - dagrun id: movie_pipeline
[2024-12-17T12:47:37.061+0530] {logging_mixin.py:190} INFO - [2024-12-17T12:47:37.061+0530] {dag.py:4451} INFO - created dagrun <DagRun movie_pipeline @ 2024-12-17 07:17:36.912621+00:00: manual__2024-12-17T07:17:36.912621+00:00, state:running, queued_at: None. externally triggered: False>
[2024-12-17T12:47:37.092+0530] {logging_mixin.py:190} INFO - [2024-12-17T12:47:37.092+0530] {dag.py:4396} INFO - [DAG TEST] starting task_id=scrape_collection_page map_index=-1
[2024-12-17T12:47:37.093+0530] {logging_mixin.py:190} INFO - [2024-12-17T12:47:37.092+0530] {dag.py:4399} INFO - [DAG TEST] running task <TaskInstance: movie_pipeline.scrape_collection_page manual__2024-12-17T07:17:36.912621+00:00 [scheduled]>
[2024-12-17T12:47:42.187+0530] {logging_mixin.py:190} INFO - [2024-12-17T12:47:42.187+0530] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-12-17T12:47:42.251+0530] {logging_mixin.py:190} INFO - [2024-12-17 12:47:42,251] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T07:17:36.912621+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T07:17:36.912621+00:00'
[2024-12-17T12:47:42.251+0530] {logging_mixin.py:190} INFO - [2024-12-17T12:47:42.251+0530] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T07:17:36.912621+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T07:17:36.912621+00:00'
[2024-12-17T12:47:42.255+0530] {logging_mixin.py:190} INFO - Task instance is in running state
[2024-12-17T12:47:42.255+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: queued
[2024-12-17T12:47:42.255+0530] {logging_mixin.py:190} INFO - Current task name:scrape_collection_page state:scheduled start_date:None
[2024-12-17T12:47:42.255+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline and current dag run status:running
[2024-12-17T12:47:42.256+0530] {logging_mixin.py:190} INFO - [2024-12-17T12:47:42.255+0530] {taskinstance.py:731} INFO - ::endgroup::
[2024-12-17T12:47:42.257+0530] {logging_mixin.py:190} INFO - 2024-12-17
[2024-12-17T12:47:42.258+0530] {logging_mixin.py:190} INFO - [2024-12-17 12:47:42,258] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T12:47:42.259+0530] {logging_mixin.py:190} INFO - [2024-12-17T12:47:42.258+0530] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T12:47:42.264+0530] {logging_mixin.py:190} INFO - [2024-12-17T12:47:42.264+0530] {taskinstance.py:340} INFO - ::group::Post task execution logs
[2024-12-17T12:47:42.264+0530] {logging_mixin.py:190} INFO - [2024-12-17T12:47:42.264+0530] {taskinstance.py:352} INFO - Marking task as SUCCESS. dag_id=movie_pipeline, task_id=scrape_collection_page, run_id=manual__2024-12-17T07:17:36.912621+00:00, execution_date=20241217T071736, start_date=, end_date=20241217T071742
[2024-12-17T12:47:42.274+0530] {logging_mixin.py:190} INFO - Task instance in success state
[2024-12-17T12:47:42.274+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: running
[2024-12-17T12:47:42.275+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline queued_at:None
[2024-12-17T12:47:42.275+0530] {logging_mixin.py:190} INFO - Task hostname:Jayeshs-MacBook-Air.local operator:PythonOperator
[2024-12-17T12:47:42.275+0530] {logging_mixin.py:190} INFO - [2024-12-17T12:47:42.275+0530] {dag.py:4410} INFO - [DAG TEST] end task task_id=scrape_collection_page map_index=-1
[2024-12-17T12:47:42.280+0530] {logging_mixin.py:190} INFO - [2024-12-17T12:47:42.280+0530] {dagrun.py:854} INFO - Marking run <DagRun movie_pipeline @ 2024-12-17 07:17:36.912621+00:00: manual__2024-12-17T07:17:36.912621+00:00, state:running, queued_at: None. externally triggered: False> successful
[2024-12-17T12:47:42.280+0530] {logging_mixin.py:190} INFO - Dag run in success state
[2024-12-17T12:47:42.281+0530] {logging_mixin.py:190} INFO - Dag run start:2024-12-17 07:17:36.912621+00:00 end:2024-12-17 07:17:42.280796+00:00
[2024-12-17T12:47:42.281+0530] {logging_mixin.py:190} INFO - [2024-12-17T12:47:42.281+0530] {dagrun.py:905} INFO - DagRun Finished: dag_id=movie_pipeline, execution_date=2024-12-17 07:17:36.912621+00:00, run_id=manual__2024-12-17T07:17:36.912621+00:00, run_start_date=2024-12-17 07:17:36.912621+00:00, run_end_date=2024-12-17 07:17:42.280796+00:00, run_duration=5.368175, state=success, external_trigger=False, run_type=manual, data_interval_start=2024-12-16 07:17:36.912621+00:00, data_interval_end=2024-12-17 07:17:36.912621+00:00, dag_hash=None
[2024-12-17T12:47:42.290+0530] {processor.py:925} INFO - DAG(s) 'movie_pipeline' retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T12:47:42.302+0530] {logging_mixin.py:190} INFO - [2024-12-17T12:47:42.301+0530] {dag.py:3239} INFO - Sync 1 DAGs
[2024-12-17T12:47:42.323+0530] {logging_mixin.py:190} INFO - [2024-12-17T12:47:42.323+0530] {dag.py:4180} INFO - Setting next_dagrun for movie_pipeline to 2024-12-17 01:30:00+00:00, run_after=2024-12-18 01:30:00+00:00
[2024-12-17T12:47:42.343+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 5.477 seconds
[2024-12-17T12:50:33.434+0530] {processor.py:186} INFO - Started process (PID=59921) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T12:50:33.436+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-17T12:50:33.438+0530] {logging_mixin.py:190} INFO - [2024-12-17T12:50:33.438+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T12:50:33.624+0530] {logging_mixin.py:190} INFO - [2024-12-17T12:50:33.624+0530] {dag.py:4435} INFO - dagrun id: movie_pipeline
[2024-12-17T12:50:33.640+0530] {logging_mixin.py:190} INFO - [2024-12-17T12:50:33.639+0530] {dag.py:4451} INFO - created dagrun <DagRun movie_pipeline @ 2024-12-17 07:20:33.481261+00:00: manual__2024-12-17T07:20:33.481261+00:00, state:running, queued_at: None. externally triggered: False>
[2024-12-17T12:50:33.671+0530] {logging_mixin.py:190} INFO - [2024-12-17T12:50:33.671+0530] {dag.py:4396} INFO - [DAG TEST] starting task_id=scrape_collection_page map_index=-1
[2024-12-17T12:50:33.672+0530] {logging_mixin.py:190} INFO - [2024-12-17T12:50:33.672+0530] {dag.py:4399} INFO - [DAG TEST] running task <TaskInstance: movie_pipeline.scrape_collection_page manual__2024-12-17T07:20:33.481261+00:00 [scheduled]>
[2024-12-17T12:50:38.772+0530] {logging_mixin.py:190} INFO - [2024-12-17T12:50:38.772+0530] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-12-17T12:50:38.842+0530] {logging_mixin.py:190} INFO - [2024-12-17 12:50:38,842] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T07:20:33.481261+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T07:20:33.481261+00:00'
[2024-12-17T12:50:38.843+0530] {logging_mixin.py:190} INFO - [2024-12-17T12:50:38.842+0530] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T07:20:33.481261+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T07:20:33.481261+00:00'
[2024-12-17T12:50:38.853+0530] {logging_mixin.py:190} INFO - Task instance is in running state
[2024-12-17T12:50:38.853+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: queued
[2024-12-17T12:50:38.853+0530] {logging_mixin.py:190} INFO - Current task name:scrape_collection_page state:scheduled start_date:None
[2024-12-17T12:50:38.854+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline and current dag run status:running
[2024-12-17T12:50:38.854+0530] {logging_mixin.py:190} INFO - [2024-12-17T12:50:38.854+0530] {taskinstance.py:731} INFO - ::endgroup::
[2024-12-17T12:50:38.856+0530] {logging_mixin.py:190} INFO - 2024-12-17
[2024-12-17T12:50:38.856+0530] {logging_mixin.py:190} INFO - [2024-12-17 12:50:38,856] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T12:50:38.856+0530] {logging_mixin.py:190} INFO - [2024-12-17T12:50:38.856+0530] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T12:50:38.861+0530] {logging_mixin.py:190} INFO - [2024-12-17T12:50:38.861+0530] {taskinstance.py:340} INFO - ::group::Post task execution logs
[2024-12-17T12:50:38.862+0530] {logging_mixin.py:190} INFO - [2024-12-17T12:50:38.862+0530] {taskinstance.py:352} INFO - Marking task as SUCCESS. dag_id=movie_pipeline, task_id=scrape_collection_page, run_id=manual__2024-12-17T07:20:33.481261+00:00, execution_date=20241217T072033, start_date=, end_date=20241217T072038
[2024-12-17T12:50:38.873+0530] {logging_mixin.py:190} INFO - Task instance in success state
[2024-12-17T12:50:38.874+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: running
[2024-12-17T12:50:38.874+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline queued_at:None
[2024-12-17T12:50:38.874+0530] {logging_mixin.py:190} INFO - Task hostname:Jayeshs-MacBook-Air.local operator:PythonOperator
[2024-12-17T12:50:38.874+0530] {logging_mixin.py:190} INFO - [2024-12-17T12:50:38.874+0530] {dag.py:4410} INFO - [DAG TEST] end task task_id=scrape_collection_page map_index=-1
[2024-12-17T12:50:38.878+0530] {logging_mixin.py:190} INFO - [2024-12-17T12:50:38.878+0530] {dagrun.py:854} INFO - Marking run <DagRun movie_pipeline @ 2024-12-17 07:20:33.481261+00:00: manual__2024-12-17T07:20:33.481261+00:00, state:running, queued_at: None. externally triggered: False> successful
[2024-12-17T12:50:38.878+0530] {logging_mixin.py:190} INFO - Dag run in success state
[2024-12-17T12:50:38.879+0530] {logging_mixin.py:190} INFO - Dag run start:2024-12-17 07:20:33.481261+00:00 end:2024-12-17 07:20:38.878814+00:00
[2024-12-17T12:50:38.880+0530] {logging_mixin.py:190} INFO - [2024-12-17T12:50:38.880+0530] {dagrun.py:905} INFO - DagRun Finished: dag_id=movie_pipeline, execution_date=2024-12-17 07:20:33.481261+00:00, run_id=manual__2024-12-17T07:20:33.481261+00:00, run_start_date=2024-12-17 07:20:33.481261+00:00, run_end_date=2024-12-17 07:20:38.878814+00:00, run_duration=5.397553, state=success, external_trigger=False, run_type=manual, data_interval_start=2024-12-16 07:20:33.481261+00:00, data_interval_end=2024-12-17 07:20:33.481261+00:00, dag_hash=None
[2024-12-17T12:50:38.889+0530] {processor.py:925} INFO - DAG(s) 'movie_pipeline' retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T12:50:38.901+0530] {logging_mixin.py:190} INFO - [2024-12-17T12:50:38.901+0530] {dag.py:3239} INFO - Sync 1 DAGs
[2024-12-17T12:50:38.919+0530] {logging_mixin.py:190} INFO - [2024-12-17T12:50:38.919+0530] {dag.py:4180} INFO - Setting next_dagrun for movie_pipeline to 2024-12-17 01:30:00+00:00, run_after=2024-12-18 01:30:00+00:00
[2024-12-17T12:50:38.942+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 5.512 seconds
[2024-12-17T12:53:28.322+0530] {processor.py:186} INFO - Started process (PID=60130) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T12:53:28.324+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-17T12:53:28.325+0530] {logging_mixin.py:190} INFO - [2024-12-17T12:53:28.325+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T12:53:28.503+0530] {logging_mixin.py:190} INFO - [2024-12-17T12:53:28.502+0530] {dag.py:4435} INFO - dagrun id: movie_pipeline
[2024-12-17T12:53:28.517+0530] {logging_mixin.py:190} INFO - [2024-12-17T12:53:28.516+0530] {dag.py:4451} INFO - created dagrun <DagRun movie_pipeline @ 2024-12-17 07:23:28.367931+00:00: manual__2024-12-17T07:23:28.367931+00:00, state:running, queued_at: None. externally triggered: False>
[2024-12-17T12:53:28.546+0530] {logging_mixin.py:190} INFO - [2024-12-17T12:53:28.545+0530] {dag.py:4396} INFO - [DAG TEST] starting task_id=scrape_collection_page map_index=-1
[2024-12-17T12:53:28.547+0530] {logging_mixin.py:190} INFO - [2024-12-17T12:53:28.546+0530] {dag.py:4399} INFO - [DAG TEST] running task <TaskInstance: movie_pipeline.scrape_collection_page manual__2024-12-17T07:23:28.367931+00:00 [scheduled]>
[2024-12-17T12:53:33.646+0530] {logging_mixin.py:190} INFO - [2024-12-17T12:53:33.645+0530] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-12-17T12:53:33.715+0530] {logging_mixin.py:190} INFO - [2024-12-17 12:53:33,715] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T07:23:28.367931+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T07:23:28.367931+00:00'
[2024-12-17T12:53:33.716+0530] {logging_mixin.py:190} INFO - [2024-12-17T12:53:33.715+0530] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T07:23:28.367931+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T07:23:28.367931+00:00'
[2024-12-17T12:53:33.728+0530] {logging_mixin.py:190} INFO - Task instance is in running state
[2024-12-17T12:53:33.728+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: queued
[2024-12-17T12:53:33.729+0530] {logging_mixin.py:190} INFO - Current task name:scrape_collection_page state:scheduled start_date:None
[2024-12-17T12:53:33.729+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline and current dag run status:running
[2024-12-17T12:53:33.730+0530] {logging_mixin.py:190} INFO - [2024-12-17T12:53:33.730+0530] {taskinstance.py:731} INFO - ::endgroup::
[2024-12-17T12:53:33.735+0530] {logging_mixin.py:190} INFO - 2024-12-17
[2024-12-17T12:53:33.736+0530] {logging_mixin.py:190} INFO - [2024-12-17 12:53:33,735] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T12:53:33.736+0530] {logging_mixin.py:190} INFO - [2024-12-17T12:53:33.735+0530] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T12:53:33.741+0530] {logging_mixin.py:190} INFO - [2024-12-17T12:53:33.741+0530] {taskinstance.py:340} INFO - ::group::Post task execution logs
[2024-12-17T12:53:33.741+0530] {logging_mixin.py:190} INFO - [2024-12-17T12:53:33.741+0530] {taskinstance.py:352} INFO - Marking task as SUCCESS. dag_id=movie_pipeline, task_id=scrape_collection_page, run_id=manual__2024-12-17T07:23:28.367931+00:00, execution_date=20241217T072328, start_date=, end_date=20241217T072333
[2024-12-17T12:53:33.751+0530] {logging_mixin.py:190} INFO - Task instance in success state
[2024-12-17T12:53:33.751+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: running
[2024-12-17T12:53:33.751+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline queued_at:None
[2024-12-17T12:53:33.751+0530] {logging_mixin.py:190} INFO - Task hostname:Jayeshs-MacBook-Air.local operator:PythonOperator
[2024-12-17T12:53:33.752+0530] {logging_mixin.py:190} INFO - [2024-12-17T12:53:33.751+0530] {dag.py:4410} INFO - [DAG TEST] end task task_id=scrape_collection_page map_index=-1
[2024-12-17T12:53:33.756+0530] {logging_mixin.py:190} INFO - [2024-12-17T12:53:33.755+0530] {dagrun.py:854} INFO - Marking run <DagRun movie_pipeline @ 2024-12-17 07:23:28.367931+00:00: manual__2024-12-17T07:23:28.367931+00:00, state:running, queued_at: None. externally triggered: False> successful
[2024-12-17T12:53:33.756+0530] {logging_mixin.py:190} INFO - Dag run in success state
[2024-12-17T12:53:33.756+0530] {logging_mixin.py:190} INFO - Dag run start:2024-12-17 07:23:28.367931+00:00 end:2024-12-17 07:23:33.756425+00:00
[2024-12-17T12:53:33.757+0530] {logging_mixin.py:190} INFO - [2024-12-17T12:53:33.756+0530] {dagrun.py:905} INFO - DagRun Finished: dag_id=movie_pipeline, execution_date=2024-12-17 07:23:28.367931+00:00, run_id=manual__2024-12-17T07:23:28.367931+00:00, run_start_date=2024-12-17 07:23:28.367931+00:00, run_end_date=2024-12-17 07:23:33.756425+00:00, run_duration=5.388494, state=success, external_trigger=False, run_type=manual, data_interval_start=2024-12-16 07:23:28.367931+00:00, data_interval_end=2024-12-17 07:23:28.367931+00:00, dag_hash=None
[2024-12-17T12:53:33.765+0530] {processor.py:925} INFO - DAG(s) 'movie_pipeline' retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T12:53:33.780+0530] {logging_mixin.py:190} INFO - [2024-12-17T12:53:33.780+0530] {dag.py:3239} INFO - Sync 1 DAGs
[2024-12-17T12:53:33.800+0530] {logging_mixin.py:190} INFO - [2024-12-17T12:53:33.800+0530] {dag.py:4180} INFO - Setting next_dagrun for movie_pipeline to 2024-12-17 01:30:00+00:00, run_after=2024-12-18 01:30:00+00:00
[2024-12-17T12:53:33.821+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 5.504 seconds
[2024-12-17T12:56:27.668+0530] {processor.py:186} INFO - Started process (PID=60356) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T12:56:27.669+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-17T12:56:27.670+0530] {logging_mixin.py:190} INFO - [2024-12-17T12:56:27.670+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T12:56:27.861+0530] {logging_mixin.py:190} INFO - [2024-12-17T12:56:27.860+0530] {dag.py:4435} INFO - dagrun id: movie_pipeline
[2024-12-17T12:56:27.876+0530] {logging_mixin.py:190} INFO - [2024-12-17T12:56:27.876+0530] {dag.py:4451} INFO - created dagrun <DagRun movie_pipeline @ 2024-12-17 07:26:27.710477+00:00: manual__2024-12-17T07:26:27.710477+00:00, state:running, queued_at: None. externally triggered: False>
[2024-12-17T12:56:27.906+0530] {logging_mixin.py:190} INFO - [2024-12-17T12:56:27.906+0530] {dag.py:4396} INFO - [DAG TEST] starting task_id=scrape_collection_page map_index=-1
[2024-12-17T12:56:27.907+0530] {logging_mixin.py:190} INFO - [2024-12-17T12:56:27.906+0530] {dag.py:4399} INFO - [DAG TEST] running task <TaskInstance: movie_pipeline.scrape_collection_page manual__2024-12-17T07:26:27.710477+00:00 [scheduled]>
[2024-12-17T12:56:33.008+0530] {logging_mixin.py:190} INFO - [2024-12-17T12:56:33.007+0530] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-12-17T12:56:33.074+0530] {logging_mixin.py:190} INFO - [2024-12-17 12:56:33,074] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T07:26:27.710477+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T07:26:27.710477+00:00'
[2024-12-17T12:56:33.074+0530] {logging_mixin.py:190} INFO - [2024-12-17T12:56:33.074+0530] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T07:26:27.710477+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T07:26:27.710477+00:00'
[2024-12-17T12:56:33.086+0530] {logging_mixin.py:190} INFO - Task instance is in running state
[2024-12-17T12:56:33.087+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: queued
[2024-12-17T12:56:33.088+0530] {logging_mixin.py:190} INFO - Current task name:scrape_collection_page state:scheduled start_date:None
[2024-12-17T12:56:33.089+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline and current dag run status:running
[2024-12-17T12:56:33.090+0530] {logging_mixin.py:190} INFO - [2024-12-17T12:56:33.089+0530] {taskinstance.py:731} INFO - ::endgroup::
[2024-12-17T12:56:33.092+0530] {logging_mixin.py:190} INFO - 2024-12-17
[2024-12-17T12:56:33.093+0530] {logging_mixin.py:190} INFO - [2024-12-17 12:56:33,092] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T12:56:33.093+0530] {logging_mixin.py:190} INFO - [2024-12-17T12:56:33.092+0530] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T12:56:33.100+0530] {logging_mixin.py:190} INFO - [2024-12-17T12:56:33.100+0530] {taskinstance.py:340} INFO - ::group::Post task execution logs
[2024-12-17T12:56:33.100+0530] {logging_mixin.py:190} INFO - [2024-12-17T12:56:33.100+0530] {taskinstance.py:352} INFO - Marking task as SUCCESS. dag_id=movie_pipeline, task_id=scrape_collection_page, run_id=manual__2024-12-17T07:26:27.710477+00:00, execution_date=20241217T072627, start_date=, end_date=20241217T072633
[2024-12-17T12:56:33.112+0530] {logging_mixin.py:190} INFO - Task instance in success state
[2024-12-17T12:56:33.112+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: running
[2024-12-17T12:56:33.112+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline queued_at:None
[2024-12-17T12:56:33.113+0530] {logging_mixin.py:190} INFO - Task hostname:Jayeshs-MacBook-Air.local operator:PythonOperator
[2024-12-17T12:56:33.113+0530] {logging_mixin.py:190} INFO - [2024-12-17T12:56:33.113+0530] {dag.py:4410} INFO - [DAG TEST] end task task_id=scrape_collection_page map_index=-1
[2024-12-17T12:56:33.118+0530] {logging_mixin.py:190} INFO - [2024-12-17T12:56:33.117+0530] {dagrun.py:854} INFO - Marking run <DagRun movie_pipeline @ 2024-12-17 07:26:27.710477+00:00: manual__2024-12-17T07:26:27.710477+00:00, state:running, queued_at: None. externally triggered: False> successful
[2024-12-17T12:56:33.118+0530] {logging_mixin.py:190} INFO - Dag run in success state
[2024-12-17T12:56:33.118+0530] {logging_mixin.py:190} INFO - Dag run start:2024-12-17 07:26:27.710477+00:00 end:2024-12-17 07:26:33.118400+00:00
[2024-12-17T12:56:33.119+0530] {logging_mixin.py:190} INFO - [2024-12-17T12:56:33.118+0530] {dagrun.py:905} INFO - DagRun Finished: dag_id=movie_pipeline, execution_date=2024-12-17 07:26:27.710477+00:00, run_id=manual__2024-12-17T07:26:27.710477+00:00, run_start_date=2024-12-17 07:26:27.710477+00:00, run_end_date=2024-12-17 07:26:33.118400+00:00, run_duration=5.407923, state=success, external_trigger=False, run_type=manual, data_interval_start=2024-12-16 07:26:27.710477+00:00, data_interval_end=2024-12-17 07:26:27.710477+00:00, dag_hash=None
[2024-12-17T12:56:33.128+0530] {processor.py:925} INFO - DAG(s) 'movie_pipeline' retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T12:56:33.141+0530] {logging_mixin.py:190} INFO - [2024-12-17T12:56:33.140+0530] {dag.py:3239} INFO - Sync 1 DAGs
[2024-12-17T12:56:33.158+0530] {logging_mixin.py:190} INFO - [2024-12-17T12:56:33.157+0530] {dag.py:4180} INFO - Setting next_dagrun for movie_pipeline to 2024-12-17 01:30:00+00:00, run_after=2024-12-18 01:30:00+00:00
[2024-12-17T12:56:33.176+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 5.512 seconds
[2024-12-17T12:59:24.293+0530] {processor.py:186} INFO - Started process (PID=60550) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T12:59:24.294+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-17T12:59:24.295+0530] {logging_mixin.py:190} INFO - [2024-12-17T12:59:24.295+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T12:59:24.471+0530] {logging_mixin.py:190} INFO - [2024-12-17T12:59:24.470+0530] {dag.py:4435} INFO - dagrun id: movie_pipeline
[2024-12-17T12:59:24.487+0530] {logging_mixin.py:190} INFO - [2024-12-17T12:59:24.486+0530] {dag.py:4451} INFO - created dagrun <DagRun movie_pipeline @ 2024-12-17 07:29:24.336653+00:00: manual__2024-12-17T07:29:24.336653+00:00, state:running, queued_at: None. externally triggered: False>
[2024-12-17T12:59:24.513+0530] {logging_mixin.py:190} INFO - [2024-12-17T12:59:24.512+0530] {dag.py:4396} INFO - [DAG TEST] starting task_id=scrape_collection_page map_index=-1
[2024-12-17T12:59:24.513+0530] {logging_mixin.py:190} INFO - [2024-12-17T12:59:24.513+0530] {dag.py:4399} INFO - [DAG TEST] running task <TaskInstance: movie_pipeline.scrape_collection_page manual__2024-12-17T07:29:24.336653+00:00 [scheduled]>
[2024-12-17T12:59:29.616+0530] {logging_mixin.py:190} INFO - [2024-12-17T12:59:29.615+0530] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-12-17T12:59:29.682+0530] {logging_mixin.py:190} INFO - [2024-12-17 12:59:29,682] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T07:29:24.336653+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T07:29:24.336653+00:00'
[2024-12-17T12:59:29.682+0530] {logging_mixin.py:190} INFO - [2024-12-17T12:59:29.682+0530] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T07:29:24.336653+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T07:29:24.336653+00:00'
[2024-12-17T12:59:29.685+0530] {logging_mixin.py:190} INFO - Task instance is in running state
[2024-12-17T12:59:29.685+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: queued
[2024-12-17T12:59:29.686+0530] {logging_mixin.py:190} INFO - Current task name:scrape_collection_page state:scheduled start_date:None
[2024-12-17T12:59:29.686+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline and current dag run status:running
[2024-12-17T12:59:29.686+0530] {logging_mixin.py:190} INFO - [2024-12-17T12:59:29.686+0530] {taskinstance.py:731} INFO - ::endgroup::
[2024-12-17T12:59:29.688+0530] {logging_mixin.py:190} INFO - 2024-12-17
[2024-12-17T12:59:29.689+0530] {logging_mixin.py:190} INFO - [2024-12-17 12:59:29,688] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T12:59:29.689+0530] {logging_mixin.py:190} INFO - [2024-12-17T12:59:29.688+0530] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T12:59:29.694+0530] {logging_mixin.py:190} INFO - [2024-12-17T12:59:29.693+0530] {taskinstance.py:340} INFO - ::group::Post task execution logs
[2024-12-17T12:59:29.694+0530] {logging_mixin.py:190} INFO - [2024-12-17T12:59:29.694+0530] {taskinstance.py:352} INFO - Marking task as SUCCESS. dag_id=movie_pipeline, task_id=scrape_collection_page, run_id=manual__2024-12-17T07:29:24.336653+00:00, execution_date=20241217T072924, start_date=, end_date=20241217T072929
[2024-12-17T12:59:29.705+0530] {logging_mixin.py:190} INFO - Task instance in success state
[2024-12-17T12:59:29.706+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: running
[2024-12-17T12:59:29.706+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline queued_at:None
[2024-12-17T12:59:29.706+0530] {logging_mixin.py:190} INFO - Task hostname:Jayeshs-MacBook-Air.local operator:PythonOperator
[2024-12-17T12:59:29.706+0530] {logging_mixin.py:190} INFO - [2024-12-17T12:59:29.706+0530] {dag.py:4410} INFO - [DAG TEST] end task task_id=scrape_collection_page map_index=-1
[2024-12-17T12:59:29.711+0530] {logging_mixin.py:190} INFO - [2024-12-17T12:59:29.711+0530] {dagrun.py:854} INFO - Marking run <DagRun movie_pipeline @ 2024-12-17 07:29:24.336653+00:00: manual__2024-12-17T07:29:24.336653+00:00, state:running, queued_at: None. externally triggered: False> successful
[2024-12-17T12:59:29.711+0530] {logging_mixin.py:190} INFO - Dag run in success state
[2024-12-17T12:59:29.711+0530] {logging_mixin.py:190} INFO - Dag run start:2024-12-17 07:29:24.336653+00:00 end:2024-12-17 07:29:29.711623+00:00
[2024-12-17T12:59:29.713+0530] {logging_mixin.py:190} INFO - [2024-12-17T12:59:29.712+0530] {dagrun.py:905} INFO - DagRun Finished: dag_id=movie_pipeline, execution_date=2024-12-17 07:29:24.336653+00:00, run_id=manual__2024-12-17T07:29:24.336653+00:00, run_start_date=2024-12-17 07:29:24.336653+00:00, run_end_date=2024-12-17 07:29:29.711623+00:00, run_duration=5.37497, state=success, external_trigger=False, run_type=manual, data_interval_start=2024-12-16 07:29:24.336653+00:00, data_interval_end=2024-12-17 07:29:24.336653+00:00, dag_hash=None
[2024-12-17T12:59:29.722+0530] {processor.py:925} INFO - DAG(s) 'movie_pipeline' retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T12:59:29.736+0530] {logging_mixin.py:190} INFO - [2024-12-17T12:59:29.735+0530] {dag.py:3239} INFO - Sync 1 DAGs
[2024-12-17T12:59:29.754+0530] {logging_mixin.py:190} INFO - [2024-12-17T12:59:29.753+0530] {dag.py:4180} INFO - Setting next_dagrun for movie_pipeline to 2024-12-17 01:30:00+00:00, run_after=2024-12-18 01:30:00+00:00
[2024-12-17T12:59:29.771+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 5.482 seconds
[2024-12-17T13:02:26.433+0530] {processor.py:186} INFO - Started process (PID=60768) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T13:02:26.436+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-17T13:02:26.438+0530] {logging_mixin.py:190} INFO - [2024-12-17T13:02:26.437+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T13:02:26.675+0530] {logging_mixin.py:190} INFO - [2024-12-17T13:02:26.675+0530] {dag.py:4435} INFO - dagrun id: movie_pipeline
[2024-12-17T13:02:26.690+0530] {logging_mixin.py:190} INFO - [2024-12-17T13:02:26.689+0530] {dag.py:4451} INFO - created dagrun <DagRun movie_pipeline @ 2024-12-17 07:32:26.530421+00:00: manual__2024-12-17T07:32:26.530421+00:00, state:running, queued_at: None. externally triggered: False>
[2024-12-17T13:02:26.730+0530] {logging_mixin.py:190} INFO - [2024-12-17T13:02:26.730+0530] {dag.py:4396} INFO - [DAG TEST] starting task_id=scrape_collection_page map_index=-1
[2024-12-17T13:02:26.731+0530] {logging_mixin.py:190} INFO - [2024-12-17T13:02:26.730+0530] {dag.py:4399} INFO - [DAG TEST] running task <TaskInstance: movie_pipeline.scrape_collection_page manual__2024-12-17T07:32:26.530421+00:00 [scheduled]>
[2024-12-17T13:02:31.856+0530] {logging_mixin.py:190} INFO - [2024-12-17T13:02:31.856+0530] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-12-17T13:02:31.935+0530] {logging_mixin.py:190} INFO - [2024-12-17 13:02:31,935] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T07:32:26.530421+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T07:32:26.530421+00:00'
[2024-12-17T13:02:31.935+0530] {logging_mixin.py:190} INFO - [2024-12-17T13:02:31.935+0530] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T07:32:26.530421+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T07:32:26.530421+00:00'
[2024-12-17T13:02:31.949+0530] {logging_mixin.py:190} INFO - Task instance is in running state
[2024-12-17T13:02:31.949+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: queued
[2024-12-17T13:02:31.950+0530] {logging_mixin.py:190} INFO - Current task name:scrape_collection_page state:scheduled start_date:None
[2024-12-17T13:02:31.950+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline and current dag run status:running
[2024-12-17T13:02:31.950+0530] {logging_mixin.py:190} INFO - [2024-12-17T13:02:31.950+0530] {taskinstance.py:731} INFO - ::endgroup::
[2024-12-17T13:02:31.954+0530] {logging_mixin.py:190} INFO - 2024-12-17
[2024-12-17T13:02:31.955+0530] {logging_mixin.py:190} INFO - [2024-12-17 13:02:31,955] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T13:02:31.955+0530] {logging_mixin.py:190} INFO - [2024-12-17T13:02:31.955+0530] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T13:02:31.961+0530] {logging_mixin.py:190} INFO - [2024-12-17T13:02:31.960+0530] {taskinstance.py:340} INFO - ::group::Post task execution logs
[2024-12-17T13:02:31.961+0530] {logging_mixin.py:190} INFO - [2024-12-17T13:02:31.961+0530] {taskinstance.py:352} INFO - Marking task as SUCCESS. dag_id=movie_pipeline, task_id=scrape_collection_page, run_id=manual__2024-12-17T07:32:26.530421+00:00, execution_date=20241217T073226, start_date=, end_date=20241217T073231
[2024-12-17T13:02:31.972+0530] {logging_mixin.py:190} INFO - Task instance in success state
[2024-12-17T13:02:31.972+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: running
[2024-12-17T13:02:31.973+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline queued_at:None
[2024-12-17T13:02:31.973+0530] {logging_mixin.py:190} INFO - Task hostname:Jayeshs-MacBook-Air.local operator:PythonOperator
[2024-12-17T13:02:31.974+0530] {logging_mixin.py:190} INFO - [2024-12-17T13:02:31.974+0530] {dag.py:4410} INFO - [DAG TEST] end task task_id=scrape_collection_page map_index=-1
[2024-12-17T13:02:31.978+0530] {logging_mixin.py:190} INFO - [2024-12-17T13:02:31.978+0530] {dagrun.py:854} INFO - Marking run <DagRun movie_pipeline @ 2024-12-17 07:32:26.530421+00:00: manual__2024-12-17T07:32:26.530421+00:00, state:running, queued_at: None. externally triggered: False> successful
[2024-12-17T13:02:31.978+0530] {logging_mixin.py:190} INFO - Dag run in success state
[2024-12-17T13:02:31.979+0530] {logging_mixin.py:190} INFO - Dag run start:2024-12-17 07:32:26.530421+00:00 end:2024-12-17 07:32:31.978915+00:00
[2024-12-17T13:02:31.980+0530] {logging_mixin.py:190} INFO - [2024-12-17T13:02:31.979+0530] {dagrun.py:905} INFO - DagRun Finished: dag_id=movie_pipeline, execution_date=2024-12-17 07:32:26.530421+00:00, run_id=manual__2024-12-17T07:32:26.530421+00:00, run_start_date=2024-12-17 07:32:26.530421+00:00, run_end_date=2024-12-17 07:32:31.978915+00:00, run_duration=5.448494, state=success, external_trigger=False, run_type=manual, data_interval_start=2024-12-16 07:32:26.530421+00:00, data_interval_end=2024-12-17 07:32:26.530421+00:00, dag_hash=None
[2024-12-17T13:02:31.988+0530] {processor.py:925} INFO - DAG(s) 'movie_pipeline' retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T13:02:32.001+0530] {logging_mixin.py:190} INFO - [2024-12-17T13:02:32.001+0530] {dag.py:3239} INFO - Sync 1 DAGs
[2024-12-17T13:02:32.018+0530] {logging_mixin.py:190} INFO - [2024-12-17T13:02:32.018+0530] {dag.py:4180} INFO - Setting next_dagrun for movie_pipeline to 2024-12-17 01:30:00+00:00, run_after=2024-12-18 01:30:00+00:00
[2024-12-17T13:02:32.036+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 5.609 seconds
[2024-12-17T13:05:24.575+0530] {processor.py:186} INFO - Started process (PID=60982) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T13:05:24.578+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-17T13:05:24.580+0530] {logging_mixin.py:190} INFO - [2024-12-17T13:05:24.579+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T13:05:24.757+0530] {logging_mixin.py:190} INFO - [2024-12-17T13:05:24.757+0530] {dag.py:4435} INFO - dagrun id: movie_pipeline
[2024-12-17T13:05:24.771+0530] {logging_mixin.py:190} INFO - [2024-12-17T13:05:24.771+0530] {dag.py:4451} INFO - created dagrun <DagRun movie_pipeline @ 2024-12-17 07:35:24.622748+00:00: manual__2024-12-17T07:35:24.622748+00:00, state:running, queued_at: None. externally triggered: False>
[2024-12-17T13:05:24.803+0530] {logging_mixin.py:190} INFO - [2024-12-17T13:05:24.803+0530] {dag.py:4396} INFO - [DAG TEST] starting task_id=scrape_collection_page map_index=-1
[2024-12-17T13:05:24.804+0530] {logging_mixin.py:190} INFO - [2024-12-17T13:05:24.803+0530] {dag.py:4399} INFO - [DAG TEST] running task <TaskInstance: movie_pipeline.scrape_collection_page manual__2024-12-17T07:35:24.622748+00:00 [scheduled]>
[2024-12-17T13:05:29.904+0530] {logging_mixin.py:190} INFO - [2024-12-17T13:05:29.904+0530] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-12-17T13:05:29.969+0530] {logging_mixin.py:190} INFO - [2024-12-17 13:05:29,968] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T07:35:24.622748+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T07:35:24.622748+00:00'
[2024-12-17T13:05:29.969+0530] {logging_mixin.py:190} INFO - [2024-12-17T13:05:29.968+0530] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T07:35:24.622748+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T07:35:24.622748+00:00'
[2024-12-17T13:05:29.982+0530] {logging_mixin.py:190} INFO - Task instance is in running state
[2024-12-17T13:05:29.983+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: queued
[2024-12-17T13:05:29.983+0530] {logging_mixin.py:190} INFO - Current task name:scrape_collection_page state:scheduled start_date:None
[2024-12-17T13:05:29.984+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline and current dag run status:running
[2024-12-17T13:05:29.984+0530] {logging_mixin.py:190} INFO - [2024-12-17T13:05:29.984+0530] {taskinstance.py:731} INFO - ::endgroup::
[2024-12-17T13:05:29.986+0530] {logging_mixin.py:190} INFO - 2024-12-17
[2024-12-17T13:05:29.987+0530] {logging_mixin.py:190} INFO - [2024-12-17 13:05:29,986] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T13:05:29.987+0530] {logging_mixin.py:190} INFO - [2024-12-17T13:05:29.986+0530] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T13:05:29.992+0530] {logging_mixin.py:190} INFO - [2024-12-17T13:05:29.992+0530] {taskinstance.py:340} INFO - ::group::Post task execution logs
[2024-12-17T13:05:29.993+0530] {logging_mixin.py:190} INFO - [2024-12-17T13:05:29.993+0530] {taskinstance.py:352} INFO - Marking task as SUCCESS. dag_id=movie_pipeline, task_id=scrape_collection_page, run_id=manual__2024-12-17T07:35:24.622748+00:00, execution_date=20241217T073524, start_date=, end_date=20241217T073529
[2024-12-17T13:05:30.003+0530] {logging_mixin.py:190} INFO - Task instance in success state
[2024-12-17T13:05:30.003+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: running
[2024-12-17T13:05:30.004+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline queued_at:None
[2024-12-17T13:05:30.004+0530] {logging_mixin.py:190} INFO - Task hostname:Jayeshs-MacBook-Air.local operator:PythonOperator
[2024-12-17T13:05:30.004+0530] {logging_mixin.py:190} INFO - [2024-12-17T13:05:30.004+0530] {dag.py:4410} INFO - [DAG TEST] end task task_id=scrape_collection_page map_index=-1
[2024-12-17T13:05:30.008+0530] {logging_mixin.py:190} INFO - [2024-12-17T13:05:30.008+0530] {dagrun.py:854} INFO - Marking run <DagRun movie_pipeline @ 2024-12-17 07:35:24.622748+00:00: manual__2024-12-17T07:35:24.622748+00:00, state:running, queued_at: None. externally triggered: False> successful
[2024-12-17T13:05:30.009+0530] {logging_mixin.py:190} INFO - Dag run in success state
[2024-12-17T13:05:30.009+0530] {logging_mixin.py:190} INFO - Dag run start:2024-12-17 07:35:24.622748+00:00 end:2024-12-17 07:35:30.009087+00:00
[2024-12-17T13:05:30.009+0530] {logging_mixin.py:190} INFO - [2024-12-17T13:05:30.009+0530] {dagrun.py:905} INFO - DagRun Finished: dag_id=movie_pipeline, execution_date=2024-12-17 07:35:24.622748+00:00, run_id=manual__2024-12-17T07:35:24.622748+00:00, run_start_date=2024-12-17 07:35:24.622748+00:00, run_end_date=2024-12-17 07:35:30.009087+00:00, run_duration=5.386339, state=success, external_trigger=False, run_type=manual, data_interval_start=2024-12-16 07:35:24.622748+00:00, data_interval_end=2024-12-17 07:35:24.622748+00:00, dag_hash=None
[2024-12-17T13:05:30.018+0530] {processor.py:925} INFO - DAG(s) 'movie_pipeline' retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T13:05:30.031+0530] {logging_mixin.py:190} INFO - [2024-12-17T13:05:30.030+0530] {dag.py:3239} INFO - Sync 1 DAGs
[2024-12-17T13:05:30.050+0530] {logging_mixin.py:190} INFO - [2024-12-17T13:05:30.049+0530] {dag.py:4180} INFO - Setting next_dagrun for movie_pipeline to 2024-12-17 01:30:00+00:00, run_after=2024-12-18 01:30:00+00:00
[2024-12-17T13:05:30.072+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 5.501 seconds
[2024-12-17T13:08:24.175+0530] {processor.py:186} INFO - Started process (PID=61193) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T13:08:24.176+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-17T13:08:24.178+0530] {logging_mixin.py:190} INFO - [2024-12-17T13:08:24.177+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T13:08:24.354+0530] {logging_mixin.py:190} INFO - [2024-12-17T13:08:24.354+0530] {dag.py:4435} INFO - dagrun id: movie_pipeline
[2024-12-17T13:08:24.369+0530] {logging_mixin.py:190} INFO - [2024-12-17T13:08:24.369+0530] {dag.py:4451} INFO - created dagrun <DagRun movie_pipeline @ 2024-12-17 07:38:24.221555+00:00: manual__2024-12-17T07:38:24.221555+00:00, state:running, queued_at: None. externally triggered: False>
[2024-12-17T13:08:24.396+0530] {logging_mixin.py:190} INFO - [2024-12-17T13:08:24.396+0530] {dag.py:4396} INFO - [DAG TEST] starting task_id=scrape_collection_page map_index=-1
[2024-12-17T13:08:24.397+0530] {logging_mixin.py:190} INFO - [2024-12-17T13:08:24.397+0530] {dag.py:4399} INFO - [DAG TEST] running task <TaskInstance: movie_pipeline.scrape_collection_page manual__2024-12-17T07:38:24.221555+00:00 [scheduled]>
[2024-12-17T13:08:29.497+0530] {logging_mixin.py:190} INFO - [2024-12-17T13:08:29.497+0530] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-12-17T13:08:29.565+0530] {logging_mixin.py:190} INFO - [2024-12-17 13:08:29,565] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T07:38:24.221555+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T07:38:24.221555+00:00'
[2024-12-17T13:08:29.566+0530] {logging_mixin.py:190} INFO - [2024-12-17T13:08:29.565+0530] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T07:38:24.221555+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T07:38:24.221555+00:00'
[2024-12-17T13:08:29.578+0530] {logging_mixin.py:190} INFO - Task instance is in running state
[2024-12-17T13:08:29.578+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: queued
[2024-12-17T13:08:29.579+0530] {logging_mixin.py:190} INFO - Current task name:scrape_collection_page state:scheduled start_date:None
[2024-12-17T13:08:29.579+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline and current dag run status:running
[2024-12-17T13:08:29.579+0530] {logging_mixin.py:190} INFO - [2024-12-17T13:08:29.579+0530] {taskinstance.py:731} INFO - ::endgroup::
[2024-12-17T13:08:29.581+0530] {logging_mixin.py:190} INFO - 2024-12-17
[2024-12-17T13:08:29.582+0530] {logging_mixin.py:190} INFO - [2024-12-17 13:08:29,582] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T13:08:29.582+0530] {logging_mixin.py:190} INFO - [2024-12-17T13:08:29.582+0530] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T13:08:29.588+0530] {logging_mixin.py:190} INFO - [2024-12-17T13:08:29.587+0530] {taskinstance.py:340} INFO - ::group::Post task execution logs
[2024-12-17T13:08:29.588+0530] {logging_mixin.py:190} INFO - [2024-12-17T13:08:29.588+0530] {taskinstance.py:352} INFO - Marking task as SUCCESS. dag_id=movie_pipeline, task_id=scrape_collection_page, run_id=manual__2024-12-17T07:38:24.221555+00:00, execution_date=20241217T073824, start_date=, end_date=20241217T073829
[2024-12-17T13:08:29.600+0530] {logging_mixin.py:190} INFO - Task instance in success state
[2024-12-17T13:08:29.601+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: running
[2024-12-17T13:08:29.601+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline queued_at:None
[2024-12-17T13:08:29.601+0530] {logging_mixin.py:190} INFO - Task hostname:Jayeshs-MacBook-Air.local operator:PythonOperator
[2024-12-17T13:08:29.601+0530] {logging_mixin.py:190} INFO - [2024-12-17T13:08:29.601+0530] {dag.py:4410} INFO - [DAG TEST] end task task_id=scrape_collection_page map_index=-1
[2024-12-17T13:08:29.606+0530] {logging_mixin.py:190} INFO - [2024-12-17T13:08:29.605+0530] {dagrun.py:854} INFO - Marking run <DagRun movie_pipeline @ 2024-12-17 07:38:24.221555+00:00: manual__2024-12-17T07:38:24.221555+00:00, state:running, queued_at: None. externally triggered: False> successful
[2024-12-17T13:08:29.606+0530] {logging_mixin.py:190} INFO - Dag run in success state
[2024-12-17T13:08:29.606+0530] {logging_mixin.py:190} INFO - Dag run start:2024-12-17 07:38:24.221555+00:00 end:2024-12-17 07:38:29.606595+00:00
[2024-12-17T13:08:29.607+0530] {logging_mixin.py:190} INFO - [2024-12-17T13:08:29.607+0530] {dagrun.py:905} INFO - DagRun Finished: dag_id=movie_pipeline, execution_date=2024-12-17 07:38:24.221555+00:00, run_id=manual__2024-12-17T07:38:24.221555+00:00, run_start_date=2024-12-17 07:38:24.221555+00:00, run_end_date=2024-12-17 07:38:29.606595+00:00, run_duration=5.38504, state=success, external_trigger=False, run_type=manual, data_interval_start=2024-12-16 07:38:24.221555+00:00, data_interval_end=2024-12-17 07:38:24.221555+00:00, dag_hash=None
[2024-12-17T13:08:29.616+0530] {processor.py:925} INFO - DAG(s) 'movie_pipeline' retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T13:08:29.628+0530] {logging_mixin.py:190} INFO - [2024-12-17T13:08:29.628+0530] {dag.py:3239} INFO - Sync 1 DAGs
[2024-12-17T13:08:29.645+0530] {logging_mixin.py:190} INFO - [2024-12-17T13:08:29.645+0530] {dag.py:4180} INFO - Setting next_dagrun for movie_pipeline to 2024-12-17 01:30:00+00:00, run_after=2024-12-18 01:30:00+00:00
[2024-12-17T13:08:29.664+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 5.493 seconds
[2024-12-17T13:11:20.880+0530] {processor.py:186} INFO - Started process (PID=61390) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T13:11:20.882+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-17T13:11:20.884+0530] {logging_mixin.py:190} INFO - [2024-12-17T13:11:20.884+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T13:11:21.066+0530] {logging_mixin.py:190} INFO - [2024-12-17T13:11:21.065+0530] {dag.py:4435} INFO - dagrun id: movie_pipeline
[2024-12-17T13:11:21.082+0530] {logging_mixin.py:190} INFO - [2024-12-17T13:11:21.081+0530] {dag.py:4451} INFO - created dagrun <DagRun movie_pipeline @ 2024-12-17 07:41:20.928266+00:00: manual__2024-12-17T07:41:20.928266+00:00, state:running, queued_at: None. externally triggered: False>
[2024-12-17T13:11:21.114+0530] {logging_mixin.py:190} INFO - [2024-12-17T13:11:21.113+0530] {dag.py:4396} INFO - [DAG TEST] starting task_id=scrape_collection_page map_index=-1
[2024-12-17T13:11:21.114+0530] {logging_mixin.py:190} INFO - [2024-12-17T13:11:21.114+0530] {dag.py:4399} INFO - [DAG TEST] running task <TaskInstance: movie_pipeline.scrape_collection_page manual__2024-12-17T07:41:20.928266+00:00 [scheduled]>
[2024-12-17T13:11:26.212+0530] {logging_mixin.py:190} INFO - [2024-12-17T13:11:26.212+0530] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-12-17T13:11:26.278+0530] {logging_mixin.py:190} INFO - [2024-12-17 13:11:26,278] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T07:41:20.928266+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T07:41:20.928266+00:00'
[2024-12-17T13:11:26.279+0530] {logging_mixin.py:190} INFO - [2024-12-17T13:11:26.278+0530] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T07:41:20.928266+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T07:41:20.928266+00:00'
[2024-12-17T13:11:26.292+0530] {logging_mixin.py:190} INFO - Task instance is in running state
[2024-12-17T13:11:26.293+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: queued
[2024-12-17T13:11:26.293+0530] {logging_mixin.py:190} INFO - Current task name:scrape_collection_page state:scheduled start_date:None
[2024-12-17T13:11:26.294+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline and current dag run status:running
[2024-12-17T13:11:26.295+0530] {logging_mixin.py:190} INFO - [2024-12-17T13:11:26.294+0530] {taskinstance.py:731} INFO - ::endgroup::
[2024-12-17T13:11:26.296+0530] {logging_mixin.py:190} INFO - 2024-12-17
[2024-12-17T13:11:26.297+0530] {logging_mixin.py:190} INFO - [2024-12-17 13:11:26,297] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T13:11:26.297+0530] {logging_mixin.py:190} INFO - [2024-12-17T13:11:26.297+0530] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T13:11:26.302+0530] {logging_mixin.py:190} INFO - [2024-12-17T13:11:26.302+0530] {taskinstance.py:340} INFO - ::group::Post task execution logs
[2024-12-17T13:11:26.303+0530] {logging_mixin.py:190} INFO - [2024-12-17T13:11:26.302+0530] {taskinstance.py:352} INFO - Marking task as SUCCESS. dag_id=movie_pipeline, task_id=scrape_collection_page, run_id=manual__2024-12-17T07:41:20.928266+00:00, execution_date=20241217T074120, start_date=, end_date=20241217T074126
[2024-12-17T13:11:26.314+0530] {logging_mixin.py:190} INFO - Task instance in success state
[2024-12-17T13:11:26.314+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: running
[2024-12-17T13:11:26.315+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline queued_at:None
[2024-12-17T13:11:26.315+0530] {logging_mixin.py:190} INFO - Task hostname:Jayeshs-MacBook-Air.local operator:PythonOperator
[2024-12-17T13:11:26.315+0530] {logging_mixin.py:190} INFO - [2024-12-17T13:11:26.315+0530] {dag.py:4410} INFO - [DAG TEST] end task task_id=scrape_collection_page map_index=-1
[2024-12-17T13:11:26.319+0530] {logging_mixin.py:190} INFO - [2024-12-17T13:11:26.319+0530] {dagrun.py:854} INFO - Marking run <DagRun movie_pipeline @ 2024-12-17 07:41:20.928266+00:00: manual__2024-12-17T07:41:20.928266+00:00, state:running, queued_at: None. externally triggered: False> successful
[2024-12-17T13:11:26.320+0530] {logging_mixin.py:190} INFO - Dag run in success state
[2024-12-17T13:11:26.320+0530] {logging_mixin.py:190} INFO - Dag run start:2024-12-17 07:41:20.928266+00:00 end:2024-12-17 07:41:26.319983+00:00
[2024-12-17T13:11:26.320+0530] {logging_mixin.py:190} INFO - [2024-12-17T13:11:26.320+0530] {dagrun.py:905} INFO - DagRun Finished: dag_id=movie_pipeline, execution_date=2024-12-17 07:41:20.928266+00:00, run_id=manual__2024-12-17T07:41:20.928266+00:00, run_start_date=2024-12-17 07:41:20.928266+00:00, run_end_date=2024-12-17 07:41:26.319983+00:00, run_duration=5.391717, state=success, external_trigger=False, run_type=manual, data_interval_start=2024-12-16 07:41:20.928266+00:00, data_interval_end=2024-12-17 07:41:20.928266+00:00, dag_hash=None
[2024-12-17T13:11:26.330+0530] {processor.py:925} INFO - DAG(s) 'movie_pipeline' retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T13:11:26.343+0530] {logging_mixin.py:190} INFO - [2024-12-17T13:11:26.342+0530] {dag.py:3239} INFO - Sync 1 DAGs
[2024-12-17T13:11:26.362+0530] {logging_mixin.py:190} INFO - [2024-12-17T13:11:26.362+0530] {dag.py:4180} INFO - Setting next_dagrun for movie_pipeline to 2024-12-17 01:30:00+00:00, run_after=2024-12-18 01:30:00+00:00
[2024-12-17T13:11:26.382+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 5.506 seconds
[2024-12-17T13:14:20.128+0530] {processor.py:186} INFO - Started process (PID=61596) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T13:14:20.129+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-17T13:14:20.130+0530] {logging_mixin.py:190} INFO - [2024-12-17T13:14:20.130+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T13:14:20.306+0530] {logging_mixin.py:190} INFO - [2024-12-17T13:14:20.306+0530] {dag.py:4435} INFO - dagrun id: movie_pipeline
[2024-12-17T13:14:20.320+0530] {logging_mixin.py:190} INFO - [2024-12-17T13:14:20.319+0530] {dag.py:4451} INFO - created dagrun <DagRun movie_pipeline @ 2024-12-17 07:44:20.170553+00:00: manual__2024-12-17T07:44:20.170553+00:00, state:running, queued_at: None. externally triggered: False>
[2024-12-17T13:14:20.348+0530] {logging_mixin.py:190} INFO - [2024-12-17T13:14:20.348+0530] {dag.py:4396} INFO - [DAG TEST] starting task_id=scrape_collection_page map_index=-1
[2024-12-17T13:14:20.349+0530] {logging_mixin.py:190} INFO - [2024-12-17T13:14:20.349+0530] {dag.py:4399} INFO - [DAG TEST] running task <TaskInstance: movie_pipeline.scrape_collection_page manual__2024-12-17T07:44:20.170553+00:00 [scheduled]>
[2024-12-17T13:14:25.442+0530] {logging_mixin.py:190} INFO - [2024-12-17T13:14:25.441+0530] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-12-17T13:14:25.506+0530] {logging_mixin.py:190} INFO - [2024-12-17 13:14:25,505] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T07:44:20.170553+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T07:44:20.170553+00:00'
[2024-12-17T13:14:25.506+0530] {logging_mixin.py:190} INFO - [2024-12-17T13:14:25.505+0530] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T07:44:20.170553+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T07:44:20.170553+00:00'
[2024-12-17T13:14:25.510+0530] {logging_mixin.py:190} INFO - Task instance is in running state
[2024-12-17T13:14:25.510+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: queued
[2024-12-17T13:14:25.511+0530] {logging_mixin.py:190} INFO - Current task name:scrape_collection_page state:scheduled start_date:None
[2024-12-17T13:14:25.511+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline and current dag run status:running
[2024-12-17T13:14:25.511+0530] {logging_mixin.py:190} INFO - [2024-12-17T13:14:25.511+0530] {taskinstance.py:731} INFO - ::endgroup::
[2024-12-17T13:14:25.513+0530] {logging_mixin.py:190} INFO - 2024-12-17
[2024-12-17T13:14:25.513+0530] {logging_mixin.py:190} INFO - [2024-12-17 13:14:25,513] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T13:14:25.514+0530] {logging_mixin.py:190} INFO - [2024-12-17T13:14:25.513+0530] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T13:14:25.518+0530] {logging_mixin.py:190} INFO - [2024-12-17T13:14:25.518+0530] {taskinstance.py:340} INFO - ::group::Post task execution logs
[2024-12-17T13:14:25.519+0530] {logging_mixin.py:190} INFO - [2024-12-17T13:14:25.519+0530] {taskinstance.py:352} INFO - Marking task as SUCCESS. dag_id=movie_pipeline, task_id=scrape_collection_page, run_id=manual__2024-12-17T07:44:20.170553+00:00, execution_date=20241217T074420, start_date=, end_date=20241217T074425
[2024-12-17T13:14:25.530+0530] {logging_mixin.py:190} INFO - Task instance in success state
[2024-12-17T13:14:25.531+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: running
[2024-12-17T13:14:25.531+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline queued_at:None
[2024-12-17T13:14:25.531+0530] {logging_mixin.py:190} INFO - Task hostname:Jayeshs-MacBook-Air.local operator:PythonOperator
[2024-12-17T13:14:25.532+0530] {logging_mixin.py:190} INFO - [2024-12-17T13:14:25.531+0530] {dag.py:4410} INFO - [DAG TEST] end task task_id=scrape_collection_page map_index=-1
[2024-12-17T13:14:25.536+0530] {logging_mixin.py:190} INFO - [2024-12-17T13:14:25.535+0530] {dagrun.py:854} INFO - Marking run <DagRun movie_pipeline @ 2024-12-17 07:44:20.170553+00:00: manual__2024-12-17T07:44:20.170553+00:00, state:running, queued_at: None. externally triggered: False> successful
[2024-12-17T13:14:25.536+0530] {logging_mixin.py:190} INFO - Dag run in success state
[2024-12-17T13:14:25.536+0530] {logging_mixin.py:190} INFO - Dag run start:2024-12-17 07:44:20.170553+00:00 end:2024-12-17 07:44:25.536417+00:00
[2024-12-17T13:14:25.537+0530] {logging_mixin.py:190} INFO - [2024-12-17T13:14:25.536+0530] {dagrun.py:905} INFO - DagRun Finished: dag_id=movie_pipeline, execution_date=2024-12-17 07:44:20.170553+00:00, run_id=manual__2024-12-17T07:44:20.170553+00:00, run_start_date=2024-12-17 07:44:20.170553+00:00, run_end_date=2024-12-17 07:44:25.536417+00:00, run_duration=5.365864, state=success, external_trigger=False, run_type=manual, data_interval_start=2024-12-16 07:44:20.170553+00:00, data_interval_end=2024-12-17 07:44:20.170553+00:00, dag_hash=None
[2024-12-17T13:14:25.545+0530] {processor.py:925} INFO - DAG(s) 'movie_pipeline' retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T13:14:25.557+0530] {logging_mixin.py:190} INFO - [2024-12-17T13:14:25.557+0530] {dag.py:3239} INFO - Sync 1 DAGs
[2024-12-17T13:14:25.575+0530] {logging_mixin.py:190} INFO - [2024-12-17T13:14:25.575+0530] {dag.py:4180} INFO - Setting next_dagrun for movie_pipeline to 2024-12-17 01:30:00+00:00, run_after=2024-12-18 01:30:00+00:00
[2024-12-17T13:14:25.599+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 5.475 seconds
[2024-12-17T13:17:16.585+0530] {processor.py:186} INFO - Started process (PID=61792) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T13:17:16.586+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-17T13:17:16.588+0530] {logging_mixin.py:190} INFO - [2024-12-17T13:17:16.587+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T13:17:16.766+0530] {logging_mixin.py:190} INFO - [2024-12-17T13:17:16.766+0530] {dag.py:4435} INFO - dagrun id: movie_pipeline
[2024-12-17T13:17:16.783+0530] {logging_mixin.py:190} INFO - [2024-12-17T13:17:16.782+0530] {dag.py:4451} INFO - created dagrun <DagRun movie_pipeline @ 2024-12-17 07:47:16.631840+00:00: manual__2024-12-17T07:47:16.631840+00:00, state:running, queued_at: None. externally triggered: False>
[2024-12-17T13:17:16.808+0530] {logging_mixin.py:190} INFO - [2024-12-17T13:17:16.808+0530] {dag.py:4396} INFO - [DAG TEST] starting task_id=scrape_collection_page map_index=-1
[2024-12-17T13:17:16.809+0530] {logging_mixin.py:190} INFO - [2024-12-17T13:17:16.808+0530] {dag.py:4399} INFO - [DAG TEST] running task <TaskInstance: movie_pipeline.scrape_collection_page manual__2024-12-17T07:47:16.631840+00:00 [scheduled]>
[2024-12-17T13:17:21.911+0530] {logging_mixin.py:190} INFO - [2024-12-17T13:17:21.911+0530] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-12-17T13:17:21.973+0530] {logging_mixin.py:190} INFO - [2024-12-17 13:17:21,973] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T07:47:16.631840+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T07:47:16.631840+00:00'
[2024-12-17T13:17:21.973+0530] {logging_mixin.py:190} INFO - [2024-12-17T13:17:21.973+0530] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T07:47:16.631840+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T07:47:16.631840+00:00'
[2024-12-17T13:17:21.977+0530] {logging_mixin.py:190} INFO - Task instance is in running state
[2024-12-17T13:17:21.977+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: queued
[2024-12-17T13:17:21.978+0530] {logging_mixin.py:190} INFO - Current task name:scrape_collection_page state:scheduled start_date:None
[2024-12-17T13:17:21.978+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline and current dag run status:running
[2024-12-17T13:17:21.978+0530] {logging_mixin.py:190} INFO - [2024-12-17T13:17:21.978+0530] {taskinstance.py:731} INFO - ::endgroup::
[2024-12-17T13:17:21.980+0530] {logging_mixin.py:190} INFO - 2024-12-17
[2024-12-17T13:17:21.981+0530] {logging_mixin.py:190} INFO - [2024-12-17 13:17:21,981] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T13:17:21.982+0530] {logging_mixin.py:190} INFO - [2024-12-17T13:17:21.981+0530] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T13:17:21.987+0530] {logging_mixin.py:190} INFO - [2024-12-17T13:17:21.987+0530] {taskinstance.py:340} INFO - ::group::Post task execution logs
[2024-12-17T13:17:21.988+0530] {logging_mixin.py:190} INFO - [2024-12-17T13:17:21.988+0530] {taskinstance.py:352} INFO - Marking task as SUCCESS. dag_id=movie_pipeline, task_id=scrape_collection_page, run_id=manual__2024-12-17T07:47:16.631840+00:00, execution_date=20241217T074716, start_date=, end_date=20241217T074721
[2024-12-17T13:17:21.999+0530] {logging_mixin.py:190} INFO - Task instance in success state
[2024-12-17T13:17:21.999+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: running
[2024-12-17T13:17:22.000+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline queued_at:None
[2024-12-17T13:17:22.000+0530] {logging_mixin.py:190} INFO - Task hostname:Jayeshs-MacBook-Air.local operator:PythonOperator
[2024-12-17T13:17:22.000+0530] {logging_mixin.py:190} INFO - [2024-12-17T13:17:22.000+0530] {dag.py:4410} INFO - [DAG TEST] end task task_id=scrape_collection_page map_index=-1
[2024-12-17T13:17:22.004+0530] {logging_mixin.py:190} INFO - [2024-12-17T13:17:22.004+0530] {dagrun.py:854} INFO - Marking run <DagRun movie_pipeline @ 2024-12-17 07:47:16.631840+00:00: manual__2024-12-17T07:47:16.631840+00:00, state:running, queued_at: None. externally triggered: False> successful
[2024-12-17T13:17:22.005+0530] {logging_mixin.py:190} INFO - Dag run in success state
[2024-12-17T13:17:22.005+0530] {logging_mixin.py:190} INFO - Dag run start:2024-12-17 07:47:16.631840+00:00 end:2024-12-17 07:47:22.005102+00:00
[2024-12-17T13:17:22.006+0530] {logging_mixin.py:190} INFO - [2024-12-17T13:17:22.005+0530] {dagrun.py:905} INFO - DagRun Finished: dag_id=movie_pipeline, execution_date=2024-12-17 07:47:16.631840+00:00, run_id=manual__2024-12-17T07:47:16.631840+00:00, run_start_date=2024-12-17 07:47:16.631840+00:00, run_end_date=2024-12-17 07:47:22.005102+00:00, run_duration=5.373262, state=success, external_trigger=False, run_type=manual, data_interval_start=2024-12-16 07:47:16.631840+00:00, data_interval_end=2024-12-17 07:47:16.631840+00:00, dag_hash=None
[2024-12-17T13:17:22.015+0530] {processor.py:925} INFO - DAG(s) 'movie_pipeline' retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T13:17:22.027+0530] {logging_mixin.py:190} INFO - [2024-12-17T13:17:22.027+0530] {dag.py:3239} INFO - Sync 1 DAGs
[2024-12-17T13:17:22.044+0530] {logging_mixin.py:190} INFO - [2024-12-17T13:17:22.044+0530] {dag.py:4180} INFO - Setting next_dagrun for movie_pipeline to 2024-12-17 01:30:00+00:00, run_after=2024-12-18 01:30:00+00:00
[2024-12-17T13:17:22.064+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 5.483 seconds
[2024-12-17T13:20:11.450+0530] {processor.py:186} INFO - Started process (PID=61998) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T13:20:11.452+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-17T13:20:11.454+0530] {logging_mixin.py:190} INFO - [2024-12-17T13:20:11.454+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T13:20:11.636+0530] {logging_mixin.py:190} INFO - [2024-12-17T13:20:11.635+0530] {dag.py:4435} INFO - dagrun id: movie_pipeline
[2024-12-17T13:20:11.652+0530] {logging_mixin.py:190} INFO - [2024-12-17T13:20:11.651+0530] {dag.py:4451} INFO - created dagrun <DagRun movie_pipeline @ 2024-12-17 07:50:11.496812+00:00: manual__2024-12-17T07:50:11.496812+00:00, state:running, queued_at: None. externally triggered: False>
[2024-12-17T13:20:11.683+0530] {logging_mixin.py:190} INFO - [2024-12-17T13:20:11.683+0530] {dag.py:4396} INFO - [DAG TEST] starting task_id=scrape_collection_page map_index=-1
[2024-12-17T13:20:11.684+0530] {logging_mixin.py:190} INFO - [2024-12-17T13:20:11.683+0530] {dag.py:4399} INFO - [DAG TEST] running task <TaskInstance: movie_pipeline.scrape_collection_page manual__2024-12-17T07:50:11.496812+00:00 [scheduled]>
[2024-12-17T13:20:16.779+0530] {logging_mixin.py:190} INFO - [2024-12-17T13:20:16.779+0530] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-12-17T13:20:16.862+0530] {logging_mixin.py:190} INFO - [2024-12-17 13:20:16,862] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T07:50:11.496812+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T07:50:11.496812+00:00'
[2024-12-17T13:20:16.863+0530] {logging_mixin.py:190} INFO - [2024-12-17T13:20:16.862+0530] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T07:50:11.496812+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T07:50:11.496812+00:00'
[2024-12-17T13:20:16.873+0530] {logging_mixin.py:190} INFO - Task instance is in running state
[2024-12-17T13:20:16.873+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: queued
[2024-12-17T13:20:16.874+0530] {logging_mixin.py:190} INFO - Current task name:scrape_collection_page state:scheduled start_date:None
[2024-12-17T13:20:16.875+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline and current dag run status:running
[2024-12-17T13:20:16.875+0530] {logging_mixin.py:190} INFO - [2024-12-17T13:20:16.875+0530] {taskinstance.py:731} INFO - ::endgroup::
[2024-12-17T13:20:16.878+0530] {logging_mixin.py:190} INFO - 2024-12-17
[2024-12-17T13:20:16.879+0530] {logging_mixin.py:190} INFO - [2024-12-17 13:20:16,878] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T13:20:16.879+0530] {logging_mixin.py:190} INFO - [2024-12-17T13:20:16.878+0530] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T13:20:16.884+0530] {logging_mixin.py:190} INFO - [2024-12-17T13:20:16.884+0530] {taskinstance.py:340} INFO - ::group::Post task execution logs
[2024-12-17T13:20:16.885+0530] {logging_mixin.py:190} INFO - [2024-12-17T13:20:16.884+0530] {taskinstance.py:352} INFO - Marking task as SUCCESS. dag_id=movie_pipeline, task_id=scrape_collection_page, run_id=manual__2024-12-17T07:50:11.496812+00:00, execution_date=20241217T075011, start_date=, end_date=20241217T075016
[2024-12-17T13:20:16.896+0530] {logging_mixin.py:190} INFO - Task instance in success state
[2024-12-17T13:20:16.896+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: running
[2024-12-17T13:20:16.897+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline queued_at:None
[2024-12-17T13:20:16.897+0530] {logging_mixin.py:190} INFO - Task hostname:Jayeshs-MacBook-Air.local operator:PythonOperator
[2024-12-17T13:20:16.897+0530] {logging_mixin.py:190} INFO - [2024-12-17T13:20:16.897+0530] {dag.py:4410} INFO - [DAG TEST] end task task_id=scrape_collection_page map_index=-1
[2024-12-17T13:20:16.901+0530] {logging_mixin.py:190} INFO - [2024-12-17T13:20:16.901+0530] {dagrun.py:854} INFO - Marking run <DagRun movie_pipeline @ 2024-12-17 07:50:11.496812+00:00: manual__2024-12-17T07:50:11.496812+00:00, state:running, queued_at: None. externally triggered: False> successful
[2024-12-17T13:20:16.902+0530] {logging_mixin.py:190} INFO - Dag run in success state
[2024-12-17T13:20:16.902+0530] {logging_mixin.py:190} INFO - Dag run start:2024-12-17 07:50:11.496812+00:00 end:2024-12-17 07:50:16.902263+00:00
[2024-12-17T13:20:16.903+0530] {logging_mixin.py:190} INFO - [2024-12-17T13:20:16.902+0530] {dagrun.py:905} INFO - DagRun Finished: dag_id=movie_pipeline, execution_date=2024-12-17 07:50:11.496812+00:00, run_id=manual__2024-12-17T07:50:11.496812+00:00, run_start_date=2024-12-17 07:50:11.496812+00:00, run_end_date=2024-12-17 07:50:16.902263+00:00, run_duration=5.405451, state=success, external_trigger=False, run_type=manual, data_interval_start=2024-12-16 07:50:11.496812+00:00, data_interval_end=2024-12-17 07:50:11.496812+00:00, dag_hash=None
[2024-12-17T13:20:16.913+0530] {processor.py:925} INFO - DAG(s) 'movie_pipeline' retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T13:20:16.926+0530] {logging_mixin.py:190} INFO - [2024-12-17T13:20:16.926+0530] {dag.py:3239} INFO - Sync 1 DAGs
[2024-12-17T13:20:16.945+0530] {logging_mixin.py:190} INFO - [2024-12-17T13:20:16.945+0530] {dag.py:4180} INFO - Setting next_dagrun for movie_pipeline to 2024-12-17 01:30:00+00:00, run_after=2024-12-18 01:30:00+00:00
[2024-12-17T13:20:16.963+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 5.517 seconds
[2024-12-17T13:23:07.634+0530] {processor.py:186} INFO - Started process (PID=62197) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T13:23:07.635+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-17T13:23:07.636+0530] {logging_mixin.py:190} INFO - [2024-12-17T13:23:07.636+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T13:23:07.816+0530] {logging_mixin.py:190} INFO - [2024-12-17T13:23:07.815+0530] {dag.py:4435} INFO - dagrun id: movie_pipeline
[2024-12-17T13:23:07.829+0530] {logging_mixin.py:190} INFO - [2024-12-17T13:23:07.829+0530] {dag.py:4451} INFO - created dagrun <DagRun movie_pipeline @ 2024-12-17 07:53:07.679757+00:00: manual__2024-12-17T07:53:07.679757+00:00, state:running, queued_at: None. externally triggered: False>
[2024-12-17T13:23:07.861+0530] {logging_mixin.py:190} INFO - [2024-12-17T13:23:07.861+0530] {dag.py:4396} INFO - [DAG TEST] starting task_id=scrape_collection_page map_index=-1
[2024-12-17T13:23:07.862+0530] {logging_mixin.py:190} INFO - [2024-12-17T13:23:07.861+0530] {dag.py:4399} INFO - [DAG TEST] running task <TaskInstance: movie_pipeline.scrape_collection_page manual__2024-12-17T07:53:07.679757+00:00 [scheduled]>
[2024-12-17T13:23:12.963+0530] {logging_mixin.py:190} INFO - [2024-12-17T13:23:12.962+0530] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-12-17T13:23:13.030+0530] {logging_mixin.py:190} INFO - [2024-12-17 13:23:13,030] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T07:53:07.679757+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T07:53:07.679757+00:00'
[2024-12-17T13:23:13.030+0530] {logging_mixin.py:190} INFO - [2024-12-17T13:23:13.030+0530] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T07:53:07.679757+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T07:53:07.679757+00:00'
[2024-12-17T13:23:13.043+0530] {logging_mixin.py:190} INFO - Task instance is in running state
[2024-12-17T13:23:13.043+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: queued
[2024-12-17T13:23:13.044+0530] {logging_mixin.py:190} INFO - Current task name:scrape_collection_page state:scheduled start_date:None
[2024-12-17T13:23:13.044+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline and current dag run status:running
[2024-12-17T13:23:13.045+0530] {logging_mixin.py:190} INFO - [2024-12-17T13:23:13.044+0530] {taskinstance.py:731} INFO - ::endgroup::
[2024-12-17T13:23:13.047+0530] {logging_mixin.py:190} INFO - 2024-12-17
[2024-12-17T13:23:13.048+0530] {logging_mixin.py:190} INFO - [2024-12-17 13:23:13,048] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T13:23:13.048+0530] {logging_mixin.py:190} INFO - [2024-12-17T13:23:13.048+0530] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T13:23:13.053+0530] {logging_mixin.py:190} INFO - [2024-12-17T13:23:13.053+0530] {taskinstance.py:340} INFO - ::group::Post task execution logs
[2024-12-17T13:23:13.054+0530] {logging_mixin.py:190} INFO - [2024-12-17T13:23:13.054+0530] {taskinstance.py:352} INFO - Marking task as SUCCESS. dag_id=movie_pipeline, task_id=scrape_collection_page, run_id=manual__2024-12-17T07:53:07.679757+00:00, execution_date=20241217T075307, start_date=, end_date=20241217T075313
[2024-12-17T13:23:13.063+0530] {logging_mixin.py:190} INFO - Task instance in success state
[2024-12-17T13:23:13.064+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: running
[2024-12-17T13:23:13.064+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline queued_at:None
[2024-12-17T13:23:13.064+0530] {logging_mixin.py:190} INFO - Task hostname:Jayeshs-MacBook-Air.local operator:PythonOperator
[2024-12-17T13:23:13.065+0530] {logging_mixin.py:190} INFO - [2024-12-17T13:23:13.064+0530] {dag.py:4410} INFO - [DAG TEST] end task task_id=scrape_collection_page map_index=-1
[2024-12-17T13:23:13.069+0530] {logging_mixin.py:190} INFO - [2024-12-17T13:23:13.069+0530] {dagrun.py:854} INFO - Marking run <DagRun movie_pipeline @ 2024-12-17 07:53:07.679757+00:00: manual__2024-12-17T07:53:07.679757+00:00, state:running, queued_at: None. externally triggered: False> successful
[2024-12-17T13:23:13.069+0530] {logging_mixin.py:190} INFO - Dag run in success state
[2024-12-17T13:23:13.070+0530] {logging_mixin.py:190} INFO - Dag run start:2024-12-17 07:53:07.679757+00:00 end:2024-12-17 07:53:13.069867+00:00
[2024-12-17T13:23:13.070+0530] {logging_mixin.py:190} INFO - [2024-12-17T13:23:13.070+0530] {dagrun.py:905} INFO - DagRun Finished: dag_id=movie_pipeline, execution_date=2024-12-17 07:53:07.679757+00:00, run_id=manual__2024-12-17T07:53:07.679757+00:00, run_start_date=2024-12-17 07:53:07.679757+00:00, run_end_date=2024-12-17 07:53:13.069867+00:00, run_duration=5.39011, state=success, external_trigger=False, run_type=manual, data_interval_start=2024-12-16 07:53:07.679757+00:00, data_interval_end=2024-12-17 07:53:07.679757+00:00, dag_hash=None
[2024-12-17T13:23:13.080+0530] {processor.py:925} INFO - DAG(s) 'movie_pipeline' retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T13:23:13.092+0530] {logging_mixin.py:190} INFO - [2024-12-17T13:23:13.091+0530] {dag.py:3239} INFO - Sync 1 DAGs
[2024-12-17T13:23:13.110+0530] {logging_mixin.py:190} INFO - [2024-12-17T13:23:13.109+0530] {dag.py:4180} INFO - Setting next_dagrun for movie_pipeline to 2024-12-17 01:30:00+00:00, run_after=2024-12-18 01:30:00+00:00
[2024-12-17T13:23:13.133+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 5.503 seconds
[2024-12-17T13:26:05.894+0530] {processor.py:186} INFO - Started process (PID=62412) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T13:26:05.896+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-17T13:26:05.897+0530] {logging_mixin.py:190} INFO - [2024-12-17T13:26:05.897+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T13:26:06.072+0530] {logging_mixin.py:190} INFO - [2024-12-17T13:26:06.071+0530] {dag.py:4435} INFO - dagrun id: movie_pipeline
[2024-12-17T13:26:06.087+0530] {logging_mixin.py:190} INFO - [2024-12-17T13:26:06.087+0530] {dag.py:4451} INFO - created dagrun <DagRun movie_pipeline @ 2024-12-17 07:56:05.941187+00:00: manual__2024-12-17T07:56:05.941187+00:00, state:running, queued_at: None. externally triggered: False>
[2024-12-17T13:26:06.113+0530] {logging_mixin.py:190} INFO - [2024-12-17T13:26:06.113+0530] {dag.py:4396} INFO - [DAG TEST] starting task_id=scrape_collection_page map_index=-1
[2024-12-17T13:26:06.114+0530] {logging_mixin.py:190} INFO - [2024-12-17T13:26:06.113+0530] {dag.py:4399} INFO - [DAG TEST] running task <TaskInstance: movie_pipeline.scrape_collection_page manual__2024-12-17T07:56:05.941187+00:00 [scheduled]>
[2024-12-17T13:26:11.213+0530] {logging_mixin.py:190} INFO - [2024-12-17T13:26:11.213+0530] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-12-17T13:26:11.277+0530] {logging_mixin.py:190} INFO - [2024-12-17 13:26:11,277] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T07:56:05.941187+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T07:56:05.941187+00:00'
[2024-12-17T13:26:11.277+0530] {logging_mixin.py:190} INFO - [2024-12-17T13:26:11.277+0530] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T07:56:05.941187+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T07:56:05.941187+00:00'
[2024-12-17T13:26:11.280+0530] {logging_mixin.py:190} INFO - Task instance is in running state
[2024-12-17T13:26:11.281+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: queued
[2024-12-17T13:26:11.281+0530] {logging_mixin.py:190} INFO - Current task name:scrape_collection_page state:scheduled start_date:None
[2024-12-17T13:26:11.281+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline and current dag run status:running
[2024-12-17T13:26:11.282+0530] {logging_mixin.py:190} INFO - [2024-12-17T13:26:11.282+0530] {taskinstance.py:731} INFO - ::endgroup::
[2024-12-17T13:26:11.283+0530] {logging_mixin.py:190} INFO - 2024-12-17
[2024-12-17T13:26:11.284+0530] {logging_mixin.py:190} INFO - [2024-12-17 13:26:11,284] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T13:26:11.284+0530] {logging_mixin.py:190} INFO - [2024-12-17T13:26:11.284+0530] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T13:26:11.289+0530] {logging_mixin.py:190} INFO - [2024-12-17T13:26:11.289+0530] {taskinstance.py:340} INFO - ::group::Post task execution logs
[2024-12-17T13:26:11.290+0530] {logging_mixin.py:190} INFO - [2024-12-17T13:26:11.290+0530] {taskinstance.py:352} INFO - Marking task as SUCCESS. dag_id=movie_pipeline, task_id=scrape_collection_page, run_id=manual__2024-12-17T07:56:05.941187+00:00, execution_date=20241217T075605, start_date=, end_date=20241217T075611
[2024-12-17T13:26:11.299+0530] {logging_mixin.py:190} INFO - Task instance in success state
[2024-12-17T13:26:11.299+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: running
[2024-12-17T13:26:11.300+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline queued_at:None
[2024-12-17T13:26:11.300+0530] {logging_mixin.py:190} INFO - Task hostname:Jayeshs-MacBook-Air.local operator:PythonOperator
[2024-12-17T13:26:11.300+0530] {logging_mixin.py:190} INFO - [2024-12-17T13:26:11.300+0530] {dag.py:4410} INFO - [DAG TEST] end task task_id=scrape_collection_page map_index=-1
[2024-12-17T13:26:11.307+0530] {logging_mixin.py:190} INFO - [2024-12-17T13:26:11.307+0530] {dagrun.py:854} INFO - Marking run <DagRun movie_pipeline @ 2024-12-17 07:56:05.941187+00:00: manual__2024-12-17T07:56:05.941187+00:00, state:running, queued_at: None. externally triggered: False> successful
[2024-12-17T13:26:11.307+0530] {logging_mixin.py:190} INFO - Dag run in success state
[2024-12-17T13:26:11.308+0530] {logging_mixin.py:190} INFO - Dag run start:2024-12-17 07:56:05.941187+00:00 end:2024-12-17 07:56:11.307630+00:00
[2024-12-17T13:26:11.308+0530] {logging_mixin.py:190} INFO - [2024-12-17T13:26:11.308+0530] {dagrun.py:905} INFO - DagRun Finished: dag_id=movie_pipeline, execution_date=2024-12-17 07:56:05.941187+00:00, run_id=manual__2024-12-17T07:56:05.941187+00:00, run_start_date=2024-12-17 07:56:05.941187+00:00, run_end_date=2024-12-17 07:56:11.307630+00:00, run_duration=5.366443, state=success, external_trigger=False, run_type=manual, data_interval_start=2024-12-16 07:56:05.941187+00:00, data_interval_end=2024-12-17 07:56:05.941187+00:00, dag_hash=None
[2024-12-17T13:26:11.319+0530] {processor.py:925} INFO - DAG(s) 'movie_pipeline' retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T13:26:11.331+0530] {logging_mixin.py:190} INFO - [2024-12-17T13:26:11.331+0530] {dag.py:3239} INFO - Sync 1 DAGs
[2024-12-17T13:26:11.350+0530] {logging_mixin.py:190} INFO - [2024-12-17T13:26:11.350+0530] {dag.py:4180} INFO - Setting next_dagrun for movie_pipeline to 2024-12-17 01:30:00+00:00, run_after=2024-12-18 01:30:00+00:00
[2024-12-17T13:26:11.369+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 5.478 seconds
[2024-12-17T13:29:01.654+0530] {processor.py:186} INFO - Started process (PID=62606) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T13:29:01.655+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-17T13:29:01.657+0530] {logging_mixin.py:190} INFO - [2024-12-17T13:29:01.656+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T13:29:01.832+0530] {logging_mixin.py:190} INFO - [2024-12-17T13:29:01.832+0530] {dag.py:4435} INFO - dagrun id: movie_pipeline
[2024-12-17T13:29:01.847+0530] {logging_mixin.py:190} INFO - [2024-12-17T13:29:01.847+0530] {dag.py:4451} INFO - created dagrun <DagRun movie_pipeline @ 2024-12-17 07:59:01.699152+00:00: manual__2024-12-17T07:59:01.699152+00:00, state:running, queued_at: None. externally triggered: False>
[2024-12-17T13:29:01.879+0530] {logging_mixin.py:190} INFO - [2024-12-17T13:29:01.878+0530] {dag.py:4396} INFO - [DAG TEST] starting task_id=scrape_collection_page map_index=-1
[2024-12-17T13:29:01.879+0530] {logging_mixin.py:190} INFO - [2024-12-17T13:29:01.879+0530] {dag.py:4399} INFO - [DAG TEST] running task <TaskInstance: movie_pipeline.scrape_collection_page manual__2024-12-17T07:59:01.699152+00:00 [scheduled]>
[2024-12-17T13:29:06.979+0530] {logging_mixin.py:190} INFO - [2024-12-17T13:29:06.979+0530] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-12-17T13:29:07.043+0530] {logging_mixin.py:190} INFO - [2024-12-17 13:29:07,043] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T07:59:01.699152+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T07:59:01.699152+00:00'
[2024-12-17T13:29:07.044+0530] {logging_mixin.py:190} INFO - [2024-12-17T13:29:07.043+0530] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T07:59:01.699152+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T07:59:01.699152+00:00'
[2024-12-17T13:29:07.047+0530] {logging_mixin.py:190} INFO - Task instance is in running state
[2024-12-17T13:29:07.047+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: queued
[2024-12-17T13:29:07.047+0530] {logging_mixin.py:190} INFO - Current task name:scrape_collection_page state:scheduled start_date:None
[2024-12-17T13:29:07.047+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline and current dag run status:running
[2024-12-17T13:29:07.048+0530] {logging_mixin.py:190} INFO - [2024-12-17T13:29:07.048+0530] {taskinstance.py:731} INFO - ::endgroup::
[2024-12-17T13:29:07.050+0530] {logging_mixin.py:190} INFO - 2024-12-17
[2024-12-17T13:29:07.050+0530] {logging_mixin.py:190} INFO - [2024-12-17 13:29:07,050] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T13:29:07.050+0530] {logging_mixin.py:190} INFO - [2024-12-17T13:29:07.050+0530] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T13:29:07.056+0530] {logging_mixin.py:190} INFO - [2024-12-17T13:29:07.055+0530] {taskinstance.py:340} INFO - ::group::Post task execution logs
[2024-12-17T13:29:07.056+0530] {logging_mixin.py:190} INFO - [2024-12-17T13:29:07.056+0530] {taskinstance.py:352} INFO - Marking task as SUCCESS. dag_id=movie_pipeline, task_id=scrape_collection_page, run_id=manual__2024-12-17T07:59:01.699152+00:00, execution_date=20241217T075901, start_date=, end_date=20241217T075907
[2024-12-17T13:29:07.065+0530] {logging_mixin.py:190} INFO - Task instance in success state
[2024-12-17T13:29:07.065+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: running
[2024-12-17T13:29:07.065+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline queued_at:None
[2024-12-17T13:29:07.066+0530] {logging_mixin.py:190} INFO - Task hostname:Jayeshs-MacBook-Air.local operator:PythonOperator
[2024-12-17T13:29:07.066+0530] {logging_mixin.py:190} INFO - [2024-12-17T13:29:07.066+0530] {dag.py:4410} INFO - [DAG TEST] end task task_id=scrape_collection_page map_index=-1
[2024-12-17T13:29:07.071+0530] {logging_mixin.py:190} INFO - [2024-12-17T13:29:07.071+0530] {dagrun.py:854} INFO - Marking run <DagRun movie_pipeline @ 2024-12-17 07:59:01.699152+00:00: manual__2024-12-17T07:59:01.699152+00:00, state:running, queued_at: None. externally triggered: False> successful
[2024-12-17T13:29:07.071+0530] {logging_mixin.py:190} INFO - Dag run in success state
[2024-12-17T13:29:07.071+0530] {logging_mixin.py:190} INFO - Dag run start:2024-12-17 07:59:01.699152+00:00 end:2024-12-17 07:59:07.071727+00:00
[2024-12-17T13:29:07.072+0530] {logging_mixin.py:190} INFO - [2024-12-17T13:29:07.072+0530] {dagrun.py:905} INFO - DagRun Finished: dag_id=movie_pipeline, execution_date=2024-12-17 07:59:01.699152+00:00, run_id=manual__2024-12-17T07:59:01.699152+00:00, run_start_date=2024-12-17 07:59:01.699152+00:00, run_end_date=2024-12-17 07:59:07.071727+00:00, run_duration=5.372575, state=success, external_trigger=False, run_type=manual, data_interval_start=2024-12-16 07:59:01.699152+00:00, data_interval_end=2024-12-17 07:59:01.699152+00:00, dag_hash=None
[2024-12-17T13:29:07.082+0530] {processor.py:925} INFO - DAG(s) 'movie_pipeline' retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T13:29:07.095+0530] {logging_mixin.py:190} INFO - [2024-12-17T13:29:07.094+0530] {dag.py:3239} INFO - Sync 1 DAGs
[2024-12-17T13:29:07.115+0530] {logging_mixin.py:190} INFO - [2024-12-17T13:29:07.115+0530] {dag.py:4180} INFO - Setting next_dagrun for movie_pipeline to 2024-12-17 01:30:00+00:00, run_after=2024-12-18 01:30:00+00:00
[2024-12-17T13:29:07.136+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 5.486 seconds
[2024-12-17T13:31:58.130+0530] {processor.py:186} INFO - Started process (PID=62806) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T13:31:58.131+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-17T13:31:58.132+0530] {logging_mixin.py:190} INFO - [2024-12-17T13:31:58.132+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T13:31:58.319+0530] {logging_mixin.py:190} INFO - [2024-12-17T13:31:58.319+0530] {dag.py:4435} INFO - dagrun id: movie_pipeline
[2024-12-17T13:31:58.333+0530] {logging_mixin.py:190} INFO - [2024-12-17T13:31:58.333+0530] {dag.py:4451} INFO - created dagrun <DagRun movie_pipeline @ 2024-12-17 08:01:58.180905+00:00: manual__2024-12-17T08:01:58.180905+00:00, state:running, queued_at: None. externally triggered: False>
[2024-12-17T13:31:58.361+0530] {logging_mixin.py:190} INFO - [2024-12-17T13:31:58.361+0530] {dag.py:4396} INFO - [DAG TEST] starting task_id=scrape_collection_page map_index=-1
[2024-12-17T13:31:58.362+0530] {logging_mixin.py:190} INFO - [2024-12-17T13:31:58.361+0530] {dag.py:4399} INFO - [DAG TEST] running task <TaskInstance: movie_pipeline.scrape_collection_page manual__2024-12-17T08:01:58.180905+00:00 [scheduled]>
[2024-12-17T13:32:03.458+0530] {logging_mixin.py:190} INFO - [2024-12-17T13:32:03.458+0530] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-12-17T13:32:03.522+0530] {logging_mixin.py:190} INFO - [2024-12-17 13:32:03,522] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T08:01:58.180905+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T08:01:58.180905+00:00'
[2024-12-17T13:32:03.523+0530] {logging_mixin.py:190} INFO - [2024-12-17T13:32:03.522+0530] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T08:01:58.180905+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T08:01:58.180905+00:00'
[2024-12-17T13:32:03.527+0530] {logging_mixin.py:190} INFO - Task instance is in running state
[2024-12-17T13:32:03.528+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: queued
[2024-12-17T13:32:03.528+0530] {logging_mixin.py:190} INFO - Current task name:scrape_collection_page state:scheduled start_date:None
[2024-12-17T13:32:03.528+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline and current dag run status:running
[2024-12-17T13:32:03.528+0530] {logging_mixin.py:190} INFO - [2024-12-17T13:32:03.528+0530] {taskinstance.py:731} INFO - ::endgroup::
[2024-12-17T13:32:03.530+0530] {logging_mixin.py:190} INFO - 2024-12-17
[2024-12-17T13:32:03.531+0530] {logging_mixin.py:190} INFO - [2024-12-17 13:32:03,530] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T13:32:03.531+0530] {logging_mixin.py:190} INFO - [2024-12-17T13:32:03.530+0530] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T13:32:03.536+0530] {logging_mixin.py:190} INFO - [2024-12-17T13:32:03.536+0530] {taskinstance.py:340} INFO - ::group::Post task execution logs
[2024-12-17T13:32:03.537+0530] {logging_mixin.py:190} INFO - [2024-12-17T13:32:03.537+0530] {taskinstance.py:352} INFO - Marking task as SUCCESS. dag_id=movie_pipeline, task_id=scrape_collection_page, run_id=manual__2024-12-17T08:01:58.180905+00:00, execution_date=20241217T080158, start_date=, end_date=20241217T080203
[2024-12-17T13:32:03.546+0530] {logging_mixin.py:190} INFO - Task instance in success state
[2024-12-17T13:32:03.547+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: running
[2024-12-17T13:32:03.547+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline queued_at:None
[2024-12-17T13:32:03.547+0530] {logging_mixin.py:190} INFO - Task hostname:Jayeshs-MacBook-Air.local operator:PythonOperator
[2024-12-17T13:32:03.548+0530] {logging_mixin.py:190} INFO - [2024-12-17T13:32:03.547+0530] {dag.py:4410} INFO - [DAG TEST] end task task_id=scrape_collection_page map_index=-1
[2024-12-17T13:32:03.552+0530] {logging_mixin.py:190} INFO - [2024-12-17T13:32:03.551+0530] {dagrun.py:854} INFO - Marking run <DagRun movie_pipeline @ 2024-12-17 08:01:58.180905+00:00: manual__2024-12-17T08:01:58.180905+00:00, state:running, queued_at: None. externally triggered: False> successful
[2024-12-17T13:32:03.552+0530] {logging_mixin.py:190} INFO - Dag run in success state
[2024-12-17T13:32:03.553+0530] {logging_mixin.py:190} INFO - Dag run start:2024-12-17 08:01:58.180905+00:00 end:2024-12-17 08:02:03.552777+00:00
[2024-12-17T13:32:03.553+0530] {logging_mixin.py:190} INFO - [2024-12-17T13:32:03.553+0530] {dagrun.py:905} INFO - DagRun Finished: dag_id=movie_pipeline, execution_date=2024-12-17 08:01:58.180905+00:00, run_id=manual__2024-12-17T08:01:58.180905+00:00, run_start_date=2024-12-17 08:01:58.180905+00:00, run_end_date=2024-12-17 08:02:03.552777+00:00, run_duration=5.371872, state=success, external_trigger=False, run_type=manual, data_interval_start=2024-12-16 08:01:58.180905+00:00, data_interval_end=2024-12-17 08:01:58.180905+00:00, dag_hash=None
[2024-12-17T13:32:03.562+0530] {processor.py:925} INFO - DAG(s) 'movie_pipeline' retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T13:32:03.574+0530] {logging_mixin.py:190} INFO - [2024-12-17T13:32:03.573+0530] {dag.py:3239} INFO - Sync 1 DAGs
[2024-12-17T13:32:03.592+0530] {logging_mixin.py:190} INFO - [2024-12-17T13:32:03.592+0530] {dag.py:4180} INFO - Setting next_dagrun for movie_pipeline to 2024-12-17 01:30:00+00:00, run_after=2024-12-18 01:30:00+00:00
[2024-12-17T13:32:03.613+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 5.484 seconds
[2024-12-17T13:34:55.723+0530] {processor.py:186} INFO - Started process (PID=63011) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T13:34:55.725+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-17T13:34:55.727+0530] {logging_mixin.py:190} INFO - [2024-12-17T13:34:55.727+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T13:34:55.906+0530] {logging_mixin.py:190} INFO - [2024-12-17T13:34:55.905+0530] {dag.py:4435} INFO - dagrun id: movie_pipeline
[2024-12-17T13:34:55.921+0530] {logging_mixin.py:190} INFO - [2024-12-17T13:34:55.920+0530] {dag.py:4451} INFO - created dagrun <DagRun movie_pipeline @ 2024-12-17 08:04:55.774914+00:00: manual__2024-12-17T08:04:55.774914+00:00, state:running, queued_at: None. externally triggered: False>
[2024-12-17T13:34:55.952+0530] {logging_mixin.py:190} INFO - [2024-12-17T13:34:55.951+0530] {dag.py:4396} INFO - [DAG TEST] starting task_id=scrape_collection_page map_index=-1
[2024-12-17T13:34:55.952+0530] {logging_mixin.py:190} INFO - [2024-12-17T13:34:55.952+0530] {dag.py:4399} INFO - [DAG TEST] running task <TaskInstance: movie_pipeline.scrape_collection_page manual__2024-12-17T08:04:55.774914+00:00 [scheduled]>
[2024-12-17T13:35:01.054+0530] {logging_mixin.py:190} INFO - [2024-12-17T13:35:01.053+0530] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-12-17T13:35:01.122+0530] {logging_mixin.py:190} INFO - [2024-12-17 13:35:01,122] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T08:04:55.774914+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T08:04:55.774914+00:00'
[2024-12-17T13:35:01.123+0530] {logging_mixin.py:190} INFO - [2024-12-17T13:35:01.122+0530] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T08:04:55.774914+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T08:04:55.774914+00:00'
[2024-12-17T13:35:01.135+0530] {logging_mixin.py:190} INFO - Task instance is in running state
[2024-12-17T13:35:01.135+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: queued
[2024-12-17T13:35:01.135+0530] {logging_mixin.py:190} INFO - Current task name:scrape_collection_page state:scheduled start_date:None
[2024-12-17T13:35:01.136+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline and current dag run status:running
[2024-12-17T13:35:01.136+0530] {logging_mixin.py:190} INFO - [2024-12-17T13:35:01.136+0530] {taskinstance.py:731} INFO - ::endgroup::
[2024-12-17T13:35:01.138+0530] {logging_mixin.py:190} INFO - 2024-12-17
[2024-12-17T13:35:01.138+0530] {logging_mixin.py:190} INFO - [2024-12-17 13:35:01,138] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T13:35:01.139+0530] {logging_mixin.py:190} INFO - [2024-12-17T13:35:01.138+0530] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T13:35:01.144+0530] {logging_mixin.py:190} INFO - [2024-12-17T13:35:01.144+0530] {taskinstance.py:340} INFO - ::group::Post task execution logs
[2024-12-17T13:35:01.147+0530] {logging_mixin.py:190} INFO - [2024-12-17T13:35:01.144+0530] {taskinstance.py:352} INFO - Marking task as SUCCESS. dag_id=movie_pipeline, task_id=scrape_collection_page, run_id=manual__2024-12-17T08:04:55.774914+00:00, execution_date=20241217T080455, start_date=, end_date=20241217T080501
[2024-12-17T13:35:01.157+0530] {logging_mixin.py:190} INFO - Task instance in success state
[2024-12-17T13:35:01.158+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: running
[2024-12-17T13:35:01.158+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline queued_at:None
[2024-12-17T13:35:01.158+0530] {logging_mixin.py:190} INFO - Task hostname:Jayeshs-MacBook-Air.local operator:PythonOperator
[2024-12-17T13:35:01.159+0530] {logging_mixin.py:190} INFO - [2024-12-17T13:35:01.158+0530] {dag.py:4410} INFO - [DAG TEST] end task task_id=scrape_collection_page map_index=-1
[2024-12-17T13:35:01.163+0530] {logging_mixin.py:190} INFO - [2024-12-17T13:35:01.163+0530] {dagrun.py:854} INFO - Marking run <DagRun movie_pipeline @ 2024-12-17 08:04:55.774914+00:00: manual__2024-12-17T08:04:55.774914+00:00, state:running, queued_at: None. externally triggered: False> successful
[2024-12-17T13:35:01.164+0530] {logging_mixin.py:190} INFO - Dag run in success state
[2024-12-17T13:35:01.164+0530] {logging_mixin.py:190} INFO - Dag run start:2024-12-17 08:04:55.774914+00:00 end:2024-12-17 08:05:01.164014+00:00
[2024-12-17T13:35:01.164+0530] {logging_mixin.py:190} INFO - [2024-12-17T13:35:01.164+0530] {dagrun.py:905} INFO - DagRun Finished: dag_id=movie_pipeline, execution_date=2024-12-17 08:04:55.774914+00:00, run_id=manual__2024-12-17T08:04:55.774914+00:00, run_start_date=2024-12-17 08:04:55.774914+00:00, run_end_date=2024-12-17 08:05:01.164014+00:00, run_duration=5.3891, state=success, external_trigger=False, run_type=manual, data_interval_start=2024-12-16 08:04:55.774914+00:00, data_interval_end=2024-12-17 08:04:55.774914+00:00, dag_hash=None
[2024-12-17T13:35:01.173+0530] {processor.py:925} INFO - DAG(s) 'movie_pipeline' retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T13:35:01.186+0530] {logging_mixin.py:190} INFO - [2024-12-17T13:35:01.185+0530] {dag.py:3239} INFO - Sync 1 DAGs
[2024-12-17T13:35:01.203+0530] {logging_mixin.py:190} INFO - [2024-12-17T13:35:01.202+0530] {dag.py:4180} INFO - Setting next_dagrun for movie_pipeline to 2024-12-17 01:30:00+00:00, run_after=2024-12-18 01:30:00+00:00
[2024-12-17T13:35:01.223+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 5.503 seconds
[2024-12-17T13:37:56.515+0530] {processor.py:186} INFO - Started process (PID=63208) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T13:37:56.517+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-17T13:37:56.519+0530] {logging_mixin.py:190} INFO - [2024-12-17T13:37:56.519+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T13:37:56.696+0530] {logging_mixin.py:190} INFO - [2024-12-17T13:37:56.696+0530] {dag.py:4435} INFO - dagrun id: movie_pipeline
[2024-12-17T13:37:56.711+0530] {logging_mixin.py:190} INFO - [2024-12-17T13:37:56.710+0530] {dag.py:4451} INFO - created dagrun <DagRun movie_pipeline @ 2024-12-17 08:07:56.562435+00:00: manual__2024-12-17T08:07:56.562435+00:00, state:running, queued_at: None. externally triggered: False>
[2024-12-17T13:37:56.742+0530] {logging_mixin.py:190} INFO - [2024-12-17T13:37:56.742+0530] {dag.py:4396} INFO - [DAG TEST] starting task_id=scrape_collection_page map_index=-1
[2024-12-17T13:37:56.742+0530] {logging_mixin.py:190} INFO - [2024-12-17T13:37:56.742+0530] {dag.py:4399} INFO - [DAG TEST] running task <TaskInstance: movie_pipeline.scrape_collection_page manual__2024-12-17T08:07:56.562435+00:00 [scheduled]>
[2024-12-17T13:38:01.845+0530] {logging_mixin.py:190} INFO - [2024-12-17T13:38:01.844+0530] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-12-17T13:38:01.912+0530] {logging_mixin.py:190} INFO - [2024-12-17 13:38:01,912] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T08:07:56.562435+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T08:07:56.562435+00:00'
[2024-12-17T13:38:01.913+0530] {logging_mixin.py:190} INFO - [2024-12-17T13:38:01.912+0530] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T08:07:56.562435+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T08:07:56.562435+00:00'
[2024-12-17T13:38:01.924+0530] {logging_mixin.py:190} INFO - Task instance is in running state
[2024-12-17T13:38:01.924+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: queued
[2024-12-17T13:38:01.924+0530] {logging_mixin.py:190} INFO - Current task name:scrape_collection_page state:scheduled start_date:None
[2024-12-17T13:38:01.924+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline and current dag run status:running
[2024-12-17T13:38:01.925+0530] {logging_mixin.py:190} INFO - [2024-12-17T13:38:01.925+0530] {taskinstance.py:731} INFO - ::endgroup::
[2024-12-17T13:38:01.926+0530] {logging_mixin.py:190} INFO - 2024-12-17
[2024-12-17T13:38:01.927+0530] {logging_mixin.py:190} INFO - [2024-12-17 13:38:01,927] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T13:38:01.927+0530] {logging_mixin.py:190} INFO - [2024-12-17T13:38:01.927+0530] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T13:38:01.935+0530] {logging_mixin.py:190} INFO - [2024-12-17T13:38:01.935+0530] {taskinstance.py:340} INFO - ::group::Post task execution logs
[2024-12-17T13:38:01.935+0530] {logging_mixin.py:190} INFO - [2024-12-17T13:38:01.935+0530] {taskinstance.py:352} INFO - Marking task as SUCCESS. dag_id=movie_pipeline, task_id=scrape_collection_page, run_id=manual__2024-12-17T08:07:56.562435+00:00, execution_date=20241217T080756, start_date=, end_date=20241217T080801
[2024-12-17T13:38:01.945+0530] {logging_mixin.py:190} INFO - Task instance in success state
[2024-12-17T13:38:01.945+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: running
[2024-12-17T13:38:01.947+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline queued_at:None
[2024-12-17T13:38:01.947+0530] {logging_mixin.py:190} INFO - Task hostname:Jayeshs-MacBook-Air.local operator:PythonOperator
[2024-12-17T13:38:01.947+0530] {logging_mixin.py:190} INFO - [2024-12-17T13:38:01.947+0530] {dag.py:4410} INFO - [DAG TEST] end task task_id=scrape_collection_page map_index=-1
[2024-12-17T13:38:01.951+0530] {logging_mixin.py:190} INFO - [2024-12-17T13:38:01.951+0530] {dagrun.py:854} INFO - Marking run <DagRun movie_pipeline @ 2024-12-17 08:07:56.562435+00:00: manual__2024-12-17T08:07:56.562435+00:00, state:running, queued_at: None. externally triggered: False> successful
[2024-12-17T13:38:01.952+0530] {logging_mixin.py:190} INFO - Dag run in success state
[2024-12-17T13:38:01.952+0530] {logging_mixin.py:190} INFO - Dag run start:2024-12-17 08:07:56.562435+00:00 end:2024-12-17 08:08:01.952399+00:00
[2024-12-17T13:38:01.953+0530] {logging_mixin.py:190} INFO - [2024-12-17T13:38:01.952+0530] {dagrun.py:905} INFO - DagRun Finished: dag_id=movie_pipeline, execution_date=2024-12-17 08:07:56.562435+00:00, run_id=manual__2024-12-17T08:07:56.562435+00:00, run_start_date=2024-12-17 08:07:56.562435+00:00, run_end_date=2024-12-17 08:08:01.952399+00:00, run_duration=5.389964, state=success, external_trigger=False, run_type=manual, data_interval_start=2024-12-16 08:07:56.562435+00:00, data_interval_end=2024-12-17 08:07:56.562435+00:00, dag_hash=None
[2024-12-17T13:38:01.963+0530] {processor.py:925} INFO - DAG(s) 'movie_pipeline' retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T13:38:01.975+0530] {logging_mixin.py:190} INFO - [2024-12-17T13:38:01.974+0530] {dag.py:3239} INFO - Sync 1 DAGs
[2024-12-17T13:38:01.997+0530] {logging_mixin.py:190} INFO - [2024-12-17T13:38:01.996+0530] {dag.py:4180} INFO - Setting next_dagrun for movie_pipeline to 2024-12-17 01:30:00+00:00, run_after=2024-12-18 01:30:00+00:00
[2024-12-17T13:38:02.021+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 5.510 seconds
[2024-12-17T13:40:54.717+0530] {processor.py:186} INFO - Started process (PID=63415) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T13:40:54.719+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-17T13:40:54.721+0530] {logging_mixin.py:190} INFO - [2024-12-17T13:40:54.720+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T13:40:54.898+0530] {logging_mixin.py:190} INFO - [2024-12-17T13:40:54.898+0530] {dag.py:4435} INFO - dagrun id: movie_pipeline
[2024-12-17T13:40:54.913+0530] {logging_mixin.py:190} INFO - [2024-12-17T13:40:54.912+0530] {dag.py:4451} INFO - created dagrun <DagRun movie_pipeline @ 2024-12-17 08:10:54.764999+00:00: manual__2024-12-17T08:10:54.764999+00:00, state:running, queued_at: None. externally triggered: False>
[2024-12-17T13:40:54.946+0530] {logging_mixin.py:190} INFO - [2024-12-17T13:40:54.946+0530] {dag.py:4396} INFO - [DAG TEST] starting task_id=scrape_collection_page map_index=-1
[2024-12-17T13:40:54.947+0530] {logging_mixin.py:190} INFO - [2024-12-17T13:40:54.947+0530] {dag.py:4399} INFO - [DAG TEST] running task <TaskInstance: movie_pipeline.scrape_collection_page manual__2024-12-17T08:10:54.764999+00:00 [scheduled]>
[2024-12-17T13:41:00.060+0530] {logging_mixin.py:190} INFO - [2024-12-17T13:41:00.060+0530] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-12-17T13:41:00.131+0530] {logging_mixin.py:190} INFO - [2024-12-17 13:41:00,131] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T08:10:54.764999+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T08:10:54.764999+00:00'
[2024-12-17T13:41:00.131+0530] {logging_mixin.py:190} INFO - [2024-12-17T13:41:00.131+0530] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T08:10:54.764999+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T08:10:54.764999+00:00'
[2024-12-17T13:41:00.144+0530] {logging_mixin.py:190} INFO - Task instance is in running state
[2024-12-17T13:41:00.145+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: queued
[2024-12-17T13:41:00.145+0530] {logging_mixin.py:190} INFO - Current task name:scrape_collection_page state:scheduled start_date:None
[2024-12-17T13:41:00.145+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline and current dag run status:running
[2024-12-17T13:41:00.146+0530] {logging_mixin.py:190} INFO - [2024-12-17T13:41:00.146+0530] {taskinstance.py:731} INFO - ::endgroup::
[2024-12-17T13:41:00.149+0530] {logging_mixin.py:190} INFO - 2024-12-17
[2024-12-17T13:41:00.150+0530] {logging_mixin.py:190} INFO - [2024-12-17 13:41:00,149] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T13:41:00.150+0530] {logging_mixin.py:190} INFO - [2024-12-17T13:41:00.149+0530] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T13:41:00.155+0530] {logging_mixin.py:190} INFO - [2024-12-17T13:41:00.155+0530] {taskinstance.py:340} INFO - ::group::Post task execution logs
[2024-12-17T13:41:00.156+0530] {logging_mixin.py:190} INFO - [2024-12-17T13:41:00.155+0530] {taskinstance.py:352} INFO - Marking task as SUCCESS. dag_id=movie_pipeline, task_id=scrape_collection_page, run_id=manual__2024-12-17T08:10:54.764999+00:00, execution_date=20241217T081054, start_date=, end_date=20241217T081100
[2024-12-17T13:41:00.165+0530] {logging_mixin.py:190} INFO - Task instance in success state
[2024-12-17T13:41:00.166+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: running
[2024-12-17T13:41:00.166+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline queued_at:None
[2024-12-17T13:41:00.166+0530] {logging_mixin.py:190} INFO - Task hostname:Jayeshs-MacBook-Air.local operator:PythonOperator
[2024-12-17T13:41:00.166+0530] {logging_mixin.py:190} INFO - [2024-12-17T13:41:00.166+0530] {dag.py:4410} INFO - [DAG TEST] end task task_id=scrape_collection_page map_index=-1
[2024-12-17T13:41:00.170+0530] {logging_mixin.py:190} INFO - [2024-12-17T13:41:00.170+0530] {dagrun.py:854} INFO - Marking run <DagRun movie_pipeline @ 2024-12-17 08:10:54.764999+00:00: manual__2024-12-17T08:10:54.764999+00:00, state:running, queued_at: None. externally triggered: False> successful
[2024-12-17T13:41:00.170+0530] {logging_mixin.py:190} INFO - Dag run in success state
[2024-12-17T13:41:00.171+0530] {logging_mixin.py:190} INFO - Dag run start:2024-12-17 08:10:54.764999+00:00 end:2024-12-17 08:11:00.170824+00:00
[2024-12-17T13:41:00.171+0530] {logging_mixin.py:190} INFO - [2024-12-17T13:41:00.171+0530] {dagrun.py:905} INFO - DagRun Finished: dag_id=movie_pipeline, execution_date=2024-12-17 08:10:54.764999+00:00, run_id=manual__2024-12-17T08:10:54.764999+00:00, run_start_date=2024-12-17 08:10:54.764999+00:00, run_end_date=2024-12-17 08:11:00.170824+00:00, run_duration=5.405825, state=success, external_trigger=False, run_type=manual, data_interval_start=2024-12-16 08:10:54.764999+00:00, data_interval_end=2024-12-17 08:10:54.764999+00:00, dag_hash=None
[2024-12-17T13:41:00.181+0530] {processor.py:925} INFO - DAG(s) 'movie_pipeline' retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T13:41:00.193+0530] {logging_mixin.py:190} INFO - [2024-12-17T13:41:00.192+0530] {dag.py:3239} INFO - Sync 1 DAGs
[2024-12-17T13:41:00.211+0530] {logging_mixin.py:190} INFO - [2024-12-17T13:41:00.211+0530] {dag.py:4180} INFO - Setting next_dagrun for movie_pipeline to 2024-12-17 01:30:00+00:00, run_after=2024-12-18 01:30:00+00:00
[2024-12-17T13:41:00.230+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 5.518 seconds
[2024-12-17T13:43:54.488+0530] {processor.py:186} INFO - Started process (PID=63611) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T13:43:54.491+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-17T13:43:54.492+0530] {logging_mixin.py:190} INFO - [2024-12-17T13:43:54.492+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T13:43:54.667+0530] {logging_mixin.py:190} INFO - [2024-12-17T13:43:54.667+0530] {dag.py:4435} INFO - dagrun id: movie_pipeline
[2024-12-17T13:43:54.681+0530] {logging_mixin.py:190} INFO - [2024-12-17T13:43:54.680+0530] {dag.py:4451} INFO - created dagrun <DagRun movie_pipeline @ 2024-12-17 08:13:54.534376+00:00: manual__2024-12-17T08:13:54.534376+00:00, state:running, queued_at: None. externally triggered: False>
[2024-12-17T13:43:54.711+0530] {logging_mixin.py:190} INFO - [2024-12-17T13:43:54.711+0530] {dag.py:4396} INFO - [DAG TEST] starting task_id=scrape_collection_page map_index=-1
[2024-12-17T13:43:54.712+0530] {logging_mixin.py:190} INFO - [2024-12-17T13:43:54.712+0530] {dag.py:4399} INFO - [DAG TEST] running task <TaskInstance: movie_pipeline.scrape_collection_page manual__2024-12-17T08:13:54.534376+00:00 [scheduled]>
[2024-12-17T13:43:59.820+0530] {logging_mixin.py:190} INFO - [2024-12-17T13:43:59.820+0530] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-12-17T13:43:59.896+0530] {logging_mixin.py:190} INFO - [2024-12-17 13:43:59,896] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T08:13:54.534376+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T08:13:54.534376+00:00'
[2024-12-17T13:43:59.896+0530] {logging_mixin.py:190} INFO - [2024-12-17T13:43:59.896+0530] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T08:13:54.534376+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T08:13:54.534376+00:00'
[2024-12-17T13:43:59.903+0530] {logging_mixin.py:190} INFO - Task instance is in running state
[2024-12-17T13:43:59.903+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: queued
[2024-12-17T13:43:59.904+0530] {logging_mixin.py:190} INFO - Current task name:scrape_collection_page state:scheduled start_date:None
[2024-12-17T13:43:59.904+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline and current dag run status:running
[2024-12-17T13:43:59.905+0530] {logging_mixin.py:190} INFO - [2024-12-17T13:43:59.904+0530] {taskinstance.py:731} INFO - ::endgroup::
[2024-12-17T13:43:59.908+0530] {logging_mixin.py:190} INFO - 2024-12-17
[2024-12-17T13:43:59.909+0530] {logging_mixin.py:190} INFO - [2024-12-17 13:43:59,908] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T13:43:59.909+0530] {logging_mixin.py:190} INFO - [2024-12-17T13:43:59.908+0530] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T13:43:59.915+0530] {logging_mixin.py:190} INFO - [2024-12-17T13:43:59.914+0530] {taskinstance.py:340} INFO - ::group::Post task execution logs
[2024-12-17T13:43:59.915+0530] {logging_mixin.py:190} INFO - [2024-12-17T13:43:59.915+0530] {taskinstance.py:352} INFO - Marking task as SUCCESS. dag_id=movie_pipeline, task_id=scrape_collection_page, run_id=manual__2024-12-17T08:13:54.534376+00:00, execution_date=20241217T081354, start_date=, end_date=20241217T081359
[2024-12-17T13:43:59.928+0530] {logging_mixin.py:190} INFO - Task instance in success state
[2024-12-17T13:43:59.928+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: running
[2024-12-17T13:43:59.928+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline queued_at:None
[2024-12-17T13:43:59.929+0530] {logging_mixin.py:190} INFO - Task hostname:Jayeshs-MacBook-Air.local operator:PythonOperator
[2024-12-17T13:43:59.929+0530] {logging_mixin.py:190} INFO - [2024-12-17T13:43:59.929+0530] {dag.py:4410} INFO - [DAG TEST] end task task_id=scrape_collection_page map_index=-1
[2024-12-17T13:43:59.934+0530] {logging_mixin.py:190} INFO - [2024-12-17T13:43:59.933+0530] {dagrun.py:854} INFO - Marking run <DagRun movie_pipeline @ 2024-12-17 08:13:54.534376+00:00: manual__2024-12-17T08:13:54.534376+00:00, state:running, queued_at: None. externally triggered: False> successful
[2024-12-17T13:43:59.934+0530] {logging_mixin.py:190} INFO - Dag run in success state
[2024-12-17T13:43:59.935+0530] {logging_mixin.py:190} INFO - Dag run start:2024-12-17 08:13:54.534376+00:00 end:2024-12-17 08:13:59.934702+00:00
[2024-12-17T13:43:59.935+0530] {logging_mixin.py:190} INFO - [2024-12-17T13:43:59.935+0530] {dagrun.py:905} INFO - DagRun Finished: dag_id=movie_pipeline, execution_date=2024-12-17 08:13:54.534376+00:00, run_id=manual__2024-12-17T08:13:54.534376+00:00, run_start_date=2024-12-17 08:13:54.534376+00:00, run_end_date=2024-12-17 08:13:59.934702+00:00, run_duration=5.400326, state=success, external_trigger=False, run_type=manual, data_interval_start=2024-12-16 08:13:54.534376+00:00, data_interval_end=2024-12-17 08:13:54.534376+00:00, dag_hash=None
[2024-12-17T13:43:59.944+0530] {processor.py:925} INFO - DAG(s) 'movie_pipeline' retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T13:43:59.957+0530] {logging_mixin.py:190} INFO - [2024-12-17T13:43:59.956+0530] {dag.py:3239} INFO - Sync 1 DAGs
[2024-12-17T13:43:59.973+0530] {logging_mixin.py:190} INFO - [2024-12-17T13:43:59.973+0530] {dag.py:4180} INFO - Setting next_dagrun for movie_pipeline to 2024-12-17 01:30:00+00:00, run_after=2024-12-18 01:30:00+00:00
[2024-12-17T13:43:59.995+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 5.510 seconds
[2024-12-17T13:46:51.548+0530] {processor.py:186} INFO - Started process (PID=63822) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T13:46:51.551+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-17T13:46:51.553+0530] {logging_mixin.py:190} INFO - [2024-12-17T13:46:51.553+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T13:46:51.735+0530] {logging_mixin.py:190} INFO - [2024-12-17T13:46:51.734+0530] {dag.py:4435} INFO - dagrun id: movie_pipeline
[2024-12-17T13:46:51.749+0530] {logging_mixin.py:190} INFO - [2024-12-17T13:46:51.749+0530] {dag.py:4451} INFO - created dagrun <DagRun movie_pipeline @ 2024-12-17 08:16:51.597162+00:00: manual__2024-12-17T08:16:51.597162+00:00, state:running, queued_at: None. externally triggered: False>
[2024-12-17T13:46:51.780+0530] {logging_mixin.py:190} INFO - [2024-12-17T13:46:51.780+0530] {dag.py:4396} INFO - [DAG TEST] starting task_id=scrape_collection_page map_index=-1
[2024-12-17T13:46:51.781+0530] {logging_mixin.py:190} INFO - [2024-12-17T13:46:51.780+0530] {dag.py:4399} INFO - [DAG TEST] running task <TaskInstance: movie_pipeline.scrape_collection_page manual__2024-12-17T08:16:51.597162+00:00 [scheduled]>
[2024-12-17T13:46:56.883+0530] {logging_mixin.py:190} INFO - [2024-12-17T13:46:56.883+0530] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-12-17T13:46:56.967+0530] {logging_mixin.py:190} INFO - [2024-12-17 13:46:56,967] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T08:16:51.597162+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T08:16:51.597162+00:00'
[2024-12-17T13:46:56.968+0530] {logging_mixin.py:190} INFO - [2024-12-17T13:46:56.967+0530] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T08:16:51.597162+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T08:16:51.597162+00:00'
[2024-12-17T13:46:56.979+0530] {logging_mixin.py:190} INFO - Task instance is in running state
[2024-12-17T13:46:56.980+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: queued
[2024-12-17T13:46:56.980+0530] {logging_mixin.py:190} INFO - Current task name:scrape_collection_page state:scheduled start_date:None
[2024-12-17T13:46:56.980+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline and current dag run status:running
[2024-12-17T13:46:56.981+0530] {logging_mixin.py:190} INFO - [2024-12-17T13:46:56.980+0530] {taskinstance.py:731} INFO - ::endgroup::
[2024-12-17T13:46:56.984+0530] {logging_mixin.py:190} INFO - 2024-12-17
[2024-12-17T13:46:56.985+0530] {logging_mixin.py:190} INFO - [2024-12-17 13:46:56,984] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T13:46:56.985+0530] {logging_mixin.py:190} INFO - [2024-12-17T13:46:56.984+0530] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T13:46:56.991+0530] {logging_mixin.py:190} INFO - [2024-12-17T13:46:56.990+0530] {taskinstance.py:340} INFO - ::group::Post task execution logs
[2024-12-17T13:46:56.991+0530] {logging_mixin.py:190} INFO - [2024-12-17T13:46:56.991+0530] {taskinstance.py:352} INFO - Marking task as SUCCESS. dag_id=movie_pipeline, task_id=scrape_collection_page, run_id=manual__2024-12-17T08:16:51.597162+00:00, execution_date=20241217T081651, start_date=, end_date=20241217T081656
[2024-12-17T13:46:57.000+0530] {logging_mixin.py:190} INFO - Task instance in success state
[2024-12-17T13:46:57.001+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: running
[2024-12-17T13:46:57.001+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline queued_at:None
[2024-12-17T13:46:57.001+0530] {logging_mixin.py:190} INFO - Task hostname:Jayeshs-MacBook-Air.local operator:PythonOperator
[2024-12-17T13:46:57.002+0530] {logging_mixin.py:190} INFO - [2024-12-17T13:46:57.001+0530] {dag.py:4410} INFO - [DAG TEST] end task task_id=scrape_collection_page map_index=-1
[2024-12-17T13:46:57.006+0530] {logging_mixin.py:190} INFO - [2024-12-17T13:46:57.005+0530] {dagrun.py:854} INFO - Marking run <DagRun movie_pipeline @ 2024-12-17 08:16:51.597162+00:00: manual__2024-12-17T08:16:51.597162+00:00, state:running, queued_at: None. externally triggered: False> successful
[2024-12-17T13:46:57.006+0530] {logging_mixin.py:190} INFO - Dag run in success state
[2024-12-17T13:46:57.007+0530] {logging_mixin.py:190} INFO - Dag run start:2024-12-17 08:16:51.597162+00:00 end:2024-12-17 08:16:57.006262+00:00
[2024-12-17T13:46:57.008+0530] {logging_mixin.py:190} INFO - [2024-12-17T13:46:57.007+0530] {dagrun.py:905} INFO - DagRun Finished: dag_id=movie_pipeline, execution_date=2024-12-17 08:16:51.597162+00:00, run_id=manual__2024-12-17T08:16:51.597162+00:00, run_start_date=2024-12-17 08:16:51.597162+00:00, run_end_date=2024-12-17 08:16:57.006262+00:00, run_duration=5.4091, state=success, external_trigger=False, run_type=manual, data_interval_start=2024-12-16 08:16:51.597162+00:00, data_interval_end=2024-12-17 08:16:51.597162+00:00, dag_hash=None
[2024-12-17T13:46:57.016+0530] {processor.py:925} INFO - DAG(s) 'movie_pipeline' retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T13:46:57.031+0530] {logging_mixin.py:190} INFO - [2024-12-17T13:46:57.030+0530] {dag.py:3239} INFO - Sync 1 DAGs
[2024-12-17T13:46:57.049+0530] {logging_mixin.py:190} INFO - [2024-12-17T13:46:57.048+0530] {dag.py:4180} INFO - Setting next_dagrun for movie_pipeline to 2024-12-17 01:30:00+00:00, run_after=2024-12-18 01:30:00+00:00
[2024-12-17T13:46:57.066+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 5.522 seconds
[2024-12-17T13:49:47.543+0530] {processor.py:186} INFO - Started process (PID=64015) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T13:49:47.545+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-17T13:49:47.547+0530] {logging_mixin.py:190} INFO - [2024-12-17T13:49:47.547+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T13:49:47.727+0530] {logging_mixin.py:190} INFO - [2024-12-17T13:49:47.727+0530] {dag.py:4435} INFO - dagrun id: movie_pipeline
[2024-12-17T13:49:47.742+0530] {logging_mixin.py:190} INFO - [2024-12-17T13:49:47.741+0530] {dag.py:4451} INFO - created dagrun <DagRun movie_pipeline @ 2024-12-17 08:19:47.592426+00:00: manual__2024-12-17T08:19:47.592426+00:00, state:running, queued_at: None. externally triggered: False>
[2024-12-17T13:49:47.771+0530] {logging_mixin.py:190} INFO - [2024-12-17T13:49:47.771+0530] {dag.py:4396} INFO - [DAG TEST] starting task_id=scrape_collection_page map_index=-1
[2024-12-17T13:49:47.772+0530] {logging_mixin.py:190} INFO - [2024-12-17T13:49:47.771+0530] {dag.py:4399} INFO - [DAG TEST] running task <TaskInstance: movie_pipeline.scrape_collection_page manual__2024-12-17T08:19:47.592426+00:00 [scheduled]>
[2024-12-17T13:49:52.879+0530] {logging_mixin.py:190} INFO - [2024-12-17T13:49:52.878+0530] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-12-17T13:49:52.958+0530] {logging_mixin.py:190} INFO - [2024-12-17 13:49:52,958] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T08:19:47.592426+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T08:19:47.592426+00:00'
[2024-12-17T13:49:52.959+0530] {logging_mixin.py:190} INFO - [2024-12-17T13:49:52.958+0530] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T08:19:47.592426+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T08:19:47.592426+00:00'
[2024-12-17T13:49:52.972+0530] {logging_mixin.py:190} INFO - Task instance is in running state
[2024-12-17T13:49:52.972+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: queued
[2024-12-17T13:49:52.973+0530] {logging_mixin.py:190} INFO - Current task name:scrape_collection_page state:scheduled start_date:None
[2024-12-17T13:49:52.973+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline and current dag run status:running
[2024-12-17T13:49:52.974+0530] {logging_mixin.py:190} INFO - [2024-12-17T13:49:52.973+0530] {taskinstance.py:731} INFO - ::endgroup::
[2024-12-17T13:49:52.977+0530] {logging_mixin.py:190} INFO - 2024-12-17
[2024-12-17T13:49:52.977+0530] {logging_mixin.py:190} INFO - [2024-12-17 13:49:52,977] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T13:49:52.978+0530] {logging_mixin.py:190} INFO - [2024-12-17T13:49:52.977+0530] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T13:49:52.983+0530] {logging_mixin.py:190} INFO - [2024-12-17T13:49:52.983+0530] {taskinstance.py:340} INFO - ::group::Post task execution logs
[2024-12-17T13:49:52.984+0530] {logging_mixin.py:190} INFO - [2024-12-17T13:49:52.984+0530] {taskinstance.py:352} INFO - Marking task as SUCCESS. dag_id=movie_pipeline, task_id=scrape_collection_page, run_id=manual__2024-12-17T08:19:47.592426+00:00, execution_date=20241217T081947, start_date=, end_date=20241217T081952
[2024-12-17T13:49:52.993+0530] {logging_mixin.py:190} INFO - Task instance in success state
[2024-12-17T13:49:52.994+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: running
[2024-12-17T13:49:52.994+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline queued_at:None
[2024-12-17T13:49:52.994+0530] {logging_mixin.py:190} INFO - Task hostname:Jayeshs-MacBook-Air.local operator:PythonOperator
[2024-12-17T13:49:52.995+0530] {logging_mixin.py:190} INFO - [2024-12-17T13:49:52.995+0530] {dag.py:4410} INFO - [DAG TEST] end task task_id=scrape_collection_page map_index=-1
[2024-12-17T13:49:52.999+0530] {logging_mixin.py:190} INFO - [2024-12-17T13:49:52.999+0530] {dagrun.py:854} INFO - Marking run <DagRun movie_pipeline @ 2024-12-17 08:19:47.592426+00:00: manual__2024-12-17T08:19:47.592426+00:00, state:running, queued_at: None. externally triggered: False> successful
[2024-12-17T13:49:52.999+0530] {logging_mixin.py:190} INFO - Dag run in success state
[2024-12-17T13:49:53.000+0530] {logging_mixin.py:190} INFO - Dag run start:2024-12-17 08:19:47.592426+00:00 end:2024-12-17 08:19:52.999690+00:00
[2024-12-17T13:49:53.000+0530] {logging_mixin.py:190} INFO - [2024-12-17T13:49:53.000+0530] {dagrun.py:905} INFO - DagRun Finished: dag_id=movie_pipeline, execution_date=2024-12-17 08:19:47.592426+00:00, run_id=manual__2024-12-17T08:19:47.592426+00:00, run_start_date=2024-12-17 08:19:47.592426+00:00, run_end_date=2024-12-17 08:19:52.999690+00:00, run_duration=5.407264, state=success, external_trigger=False, run_type=manual, data_interval_start=2024-12-16 08:19:47.592426+00:00, data_interval_end=2024-12-17 08:19:47.592426+00:00, dag_hash=None
[2024-12-17T13:49:53.010+0530] {processor.py:925} INFO - DAG(s) 'movie_pipeline' retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T13:49:53.023+0530] {logging_mixin.py:190} INFO - [2024-12-17T13:49:53.023+0530] {dag.py:3239} INFO - Sync 1 DAGs
[2024-12-17T13:49:53.043+0530] {logging_mixin.py:190} INFO - [2024-12-17T13:49:53.042+0530] {dag.py:4180} INFO - Setting next_dagrun for movie_pipeline to 2024-12-17 01:30:00+00:00, run_after=2024-12-18 01:30:00+00:00
[2024-12-17T13:49:53.061+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 5.522 seconds
[2024-12-17T13:52:42.035+0530] {processor.py:186} INFO - Started process (PID=64224) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T13:52:42.038+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-17T13:52:42.040+0530] {logging_mixin.py:190} INFO - [2024-12-17T13:52:42.039+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T13:52:42.238+0530] {logging_mixin.py:190} INFO - [2024-12-17T13:52:42.238+0530] {dag.py:4435} INFO - dagrun id: movie_pipeline
[2024-12-17T13:52:42.254+0530] {logging_mixin.py:190} INFO - [2024-12-17T13:52:42.253+0530] {dag.py:4451} INFO - created dagrun <DagRun movie_pipeline @ 2024-12-17 08:22:42.084990+00:00: manual__2024-12-17T08:22:42.084990+00:00, state:running, queued_at: None. externally triggered: False>
[2024-12-17T13:52:42.288+0530] {logging_mixin.py:190} INFO - [2024-12-17T13:52:42.288+0530] {dag.py:4396} INFO - [DAG TEST] starting task_id=scrape_collection_page map_index=-1
[2024-12-17T13:52:42.289+0530] {logging_mixin.py:190} INFO - [2024-12-17T13:52:42.289+0530] {dag.py:4399} INFO - [DAG TEST] running task <TaskInstance: movie_pipeline.scrape_collection_page manual__2024-12-17T08:22:42.084990+00:00 [scheduled]>
[2024-12-17T13:52:47.396+0530] {logging_mixin.py:190} INFO - [2024-12-17T13:52:47.396+0530] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-12-17T13:52:47.469+0530] {logging_mixin.py:190} INFO - [2024-12-17 13:52:47,469] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T08:22:42.084990+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T08:22:42.084990+00:00'
[2024-12-17T13:52:47.470+0530] {logging_mixin.py:190} INFO - [2024-12-17T13:52:47.469+0530] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T08:22:42.084990+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T08:22:42.084990+00:00'
[2024-12-17T13:52:47.482+0530] {logging_mixin.py:190} INFO - Task instance is in running state
[2024-12-17T13:52:47.482+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: queued
[2024-12-17T13:52:47.483+0530] {logging_mixin.py:190} INFO - Current task name:scrape_collection_page state:scheduled start_date:None
[2024-12-17T13:52:47.483+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline and current dag run status:running
[2024-12-17T13:52:47.483+0530] {logging_mixin.py:190} INFO - [2024-12-17T13:52:47.483+0530] {taskinstance.py:731} INFO - ::endgroup::
[2024-12-17T13:52:47.487+0530] {logging_mixin.py:190} INFO - 2024-12-17
[2024-12-17T13:52:47.487+0530] {logging_mixin.py:190} INFO - [2024-12-17 13:52:47,487] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T13:52:47.487+0530] {logging_mixin.py:190} INFO - [2024-12-17T13:52:47.487+0530] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T13:52:47.494+0530] {logging_mixin.py:190} INFO - [2024-12-17T13:52:47.494+0530] {taskinstance.py:340} INFO - ::group::Post task execution logs
[2024-12-17T13:52:47.494+0530] {logging_mixin.py:190} INFO - [2024-12-17T13:52:47.494+0530] {taskinstance.py:352} INFO - Marking task as SUCCESS. dag_id=movie_pipeline, task_id=scrape_collection_page, run_id=manual__2024-12-17T08:22:42.084990+00:00, execution_date=20241217T082242, start_date=, end_date=20241217T082247
[2024-12-17T13:52:47.503+0530] {logging_mixin.py:190} INFO - Task instance in success state
[2024-12-17T13:52:47.503+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: running
[2024-12-17T13:52:47.504+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline queued_at:None
[2024-12-17T13:52:47.504+0530] {logging_mixin.py:190} INFO - Task hostname:Jayeshs-MacBook-Air.local operator:PythonOperator
[2024-12-17T13:52:47.504+0530] {logging_mixin.py:190} INFO - [2024-12-17T13:52:47.504+0530] {dag.py:4410} INFO - [DAG TEST] end task task_id=scrape_collection_page map_index=-1
[2024-12-17T13:52:47.509+0530] {logging_mixin.py:190} INFO - [2024-12-17T13:52:47.509+0530] {dagrun.py:854} INFO - Marking run <DagRun movie_pipeline @ 2024-12-17 08:22:42.084990+00:00: manual__2024-12-17T08:22:42.084990+00:00, state:running, queued_at: None. externally triggered: False> successful
[2024-12-17T13:52:47.509+0530] {logging_mixin.py:190} INFO - Dag run in success state
[2024-12-17T13:52:47.510+0530] {logging_mixin.py:190} INFO - Dag run start:2024-12-17 08:22:42.084990+00:00 end:2024-12-17 08:22:47.509724+00:00
[2024-12-17T13:52:47.511+0530] {logging_mixin.py:190} INFO - [2024-12-17T13:52:47.510+0530] {dagrun.py:905} INFO - DagRun Finished: dag_id=movie_pipeline, execution_date=2024-12-17 08:22:42.084990+00:00, run_id=manual__2024-12-17T08:22:42.084990+00:00, run_start_date=2024-12-17 08:22:42.084990+00:00, run_end_date=2024-12-17 08:22:47.509724+00:00, run_duration=5.424734, state=success, external_trigger=False, run_type=manual, data_interval_start=2024-12-16 08:22:42.084990+00:00, data_interval_end=2024-12-17 08:22:42.084990+00:00, dag_hash=None
[2024-12-17T13:52:47.520+0530] {processor.py:925} INFO - DAG(s) 'movie_pipeline' retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T13:52:47.533+0530] {logging_mixin.py:190} INFO - [2024-12-17T13:52:47.532+0530] {dag.py:3239} INFO - Sync 1 DAGs
[2024-12-17T13:52:47.549+0530] {logging_mixin.py:190} INFO - [2024-12-17T13:52:47.549+0530] {dag.py:4180} INFO - Setting next_dagrun for movie_pipeline to 2024-12-17 01:30:00+00:00, run_after=2024-12-18 01:30:00+00:00
[2024-12-17T13:52:47.569+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 5.538 seconds
[2024-12-17T13:55:36.370+0530] {processor.py:186} INFO - Started process (PID=64442) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T13:55:36.373+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-17T13:55:36.375+0530] {logging_mixin.py:190} INFO - [2024-12-17T13:55:36.375+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T13:55:36.553+0530] {logging_mixin.py:190} INFO - [2024-12-17T13:55:36.552+0530] {dag.py:4435} INFO - dagrun id: movie_pipeline
[2024-12-17T13:55:36.568+0530] {logging_mixin.py:190} INFO - [2024-12-17T13:55:36.567+0530] {dag.py:4451} INFO - created dagrun <DagRun movie_pipeline @ 2024-12-17 08:25:36.417548+00:00: manual__2024-12-17T08:25:36.417548+00:00, state:running, queued_at: None. externally triggered: False>
[2024-12-17T13:55:36.599+0530] {logging_mixin.py:190} INFO - [2024-12-17T13:55:36.599+0530] {dag.py:4396} INFO - [DAG TEST] starting task_id=scrape_collection_page map_index=-1
[2024-12-17T13:55:36.599+0530] {logging_mixin.py:190} INFO - [2024-12-17T13:55:36.599+0530] {dag.py:4399} INFO - [DAG TEST] running task <TaskInstance: movie_pipeline.scrape_collection_page manual__2024-12-17T08:25:36.417548+00:00 [scheduled]>
[2024-12-17T13:55:41.696+0530] {logging_mixin.py:190} INFO - [2024-12-17T13:55:41.696+0530] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-12-17T13:55:41.764+0530] {logging_mixin.py:190} INFO - [2024-12-17 13:55:41,764] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T08:25:36.417548+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T08:25:36.417548+00:00'
[2024-12-17T13:55:41.765+0530] {logging_mixin.py:190} INFO - [2024-12-17T13:55:41.764+0530] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T08:25:36.417548+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T08:25:36.417548+00:00'
[2024-12-17T13:55:41.778+0530] {logging_mixin.py:190} INFO - Task instance is in running state
[2024-12-17T13:55:41.779+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: queued
[2024-12-17T13:55:41.779+0530] {logging_mixin.py:190} INFO - Current task name:scrape_collection_page state:scheduled start_date:None
[2024-12-17T13:55:41.779+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline and current dag run status:running
[2024-12-17T13:55:41.780+0530] {logging_mixin.py:190} INFO - [2024-12-17T13:55:41.780+0530] {taskinstance.py:731} INFO - ::endgroup::
[2024-12-17T13:55:41.782+0530] {logging_mixin.py:190} INFO - 2024-12-17
[2024-12-17T13:55:41.782+0530] {logging_mixin.py:190} INFO - [2024-12-17 13:55:41,782] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T13:55:41.783+0530] {logging_mixin.py:190} INFO - [2024-12-17T13:55:41.782+0530] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T13:55:41.788+0530] {logging_mixin.py:190} INFO - [2024-12-17T13:55:41.787+0530] {taskinstance.py:340} INFO - ::group::Post task execution logs
[2024-12-17T13:55:41.788+0530] {logging_mixin.py:190} INFO - [2024-12-17T13:55:41.788+0530] {taskinstance.py:352} INFO - Marking task as SUCCESS. dag_id=movie_pipeline, task_id=scrape_collection_page, run_id=manual__2024-12-17T08:25:36.417548+00:00, execution_date=20241217T082536, start_date=, end_date=20241217T082541
[2024-12-17T13:55:41.800+0530] {logging_mixin.py:190} INFO - Task instance in success state
[2024-12-17T13:55:41.800+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: running
[2024-12-17T13:55:41.801+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline queued_at:None
[2024-12-17T13:55:41.801+0530] {logging_mixin.py:190} INFO - Task hostname:Jayeshs-MacBook-Air.local operator:PythonOperator
[2024-12-17T13:55:41.801+0530] {logging_mixin.py:190} INFO - [2024-12-17T13:55:41.801+0530] {dag.py:4410} INFO - [DAG TEST] end task task_id=scrape_collection_page map_index=-1
[2024-12-17T13:55:41.805+0530] {logging_mixin.py:190} INFO - [2024-12-17T13:55:41.805+0530] {dagrun.py:854} INFO - Marking run <DagRun movie_pipeline @ 2024-12-17 08:25:36.417548+00:00: manual__2024-12-17T08:25:36.417548+00:00, state:running, queued_at: None. externally triggered: False> successful
[2024-12-17T13:55:41.805+0530] {logging_mixin.py:190} INFO - Dag run in success state
[2024-12-17T13:55:41.806+0530] {logging_mixin.py:190} INFO - Dag run start:2024-12-17 08:25:36.417548+00:00 end:2024-12-17 08:25:41.805914+00:00
[2024-12-17T13:55:41.806+0530] {logging_mixin.py:190} INFO - [2024-12-17T13:55:41.806+0530] {dagrun.py:905} INFO - DagRun Finished: dag_id=movie_pipeline, execution_date=2024-12-17 08:25:36.417548+00:00, run_id=manual__2024-12-17T08:25:36.417548+00:00, run_start_date=2024-12-17 08:25:36.417548+00:00, run_end_date=2024-12-17 08:25:41.805914+00:00, run_duration=5.388366, state=success, external_trigger=False, run_type=manual, data_interval_start=2024-12-16 08:25:36.417548+00:00, data_interval_end=2024-12-17 08:25:36.417548+00:00, dag_hash=None
[2024-12-17T13:55:41.816+0530] {processor.py:925} INFO - DAG(s) 'movie_pipeline' retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T13:55:41.828+0530] {logging_mixin.py:190} INFO - [2024-12-17T13:55:41.828+0530] {dag.py:3239} INFO - Sync 1 DAGs
[2024-12-17T13:55:41.845+0530] {logging_mixin.py:190} INFO - [2024-12-17T13:55:41.845+0530] {dag.py:4180} INFO - Setting next_dagrun for movie_pipeline to 2024-12-17 01:30:00+00:00, run_after=2024-12-18 01:30:00+00:00
[2024-12-17T13:55:41.864+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 5.497 seconds
[2024-12-17T13:58:34.118+0530] {processor.py:186} INFO - Started process (PID=64646) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T13:58:34.121+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-17T13:58:34.123+0530] {logging_mixin.py:190} INFO - [2024-12-17T13:58:34.123+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T13:58:34.301+0530] {logging_mixin.py:190} INFO - [2024-12-17T13:58:34.301+0530] {dag.py:4435} INFO - dagrun id: movie_pipeline
[2024-12-17T13:58:34.316+0530] {logging_mixin.py:190} INFO - [2024-12-17T13:58:34.316+0530] {dag.py:4451} INFO - created dagrun <DagRun movie_pipeline @ 2024-12-17 08:28:34.166490+00:00: manual__2024-12-17T08:28:34.166490+00:00, state:running, queued_at: None. externally triggered: False>
[2024-12-17T13:58:34.348+0530] {logging_mixin.py:190} INFO - [2024-12-17T13:58:34.347+0530] {dag.py:4396} INFO - [DAG TEST] starting task_id=scrape_collection_page map_index=-1
[2024-12-17T13:58:34.348+0530] {logging_mixin.py:190} INFO - [2024-12-17T13:58:34.348+0530] {dag.py:4399} INFO - [DAG TEST] running task <TaskInstance: movie_pipeline.scrape_collection_page manual__2024-12-17T08:28:34.166490+00:00 [scheduled]>
[2024-12-17T13:58:39.443+0530] {logging_mixin.py:190} INFO - [2024-12-17T13:58:39.443+0530] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-12-17T13:58:39.515+0530] {logging_mixin.py:190} INFO - [2024-12-17 13:58:39,515] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T08:28:34.166490+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T08:28:34.166490+00:00'
[2024-12-17T13:58:39.516+0530] {logging_mixin.py:190} INFO - [2024-12-17T13:58:39.515+0530] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T08:28:34.166490+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T08:28:34.166490+00:00'
[2024-12-17T13:58:39.531+0530] {logging_mixin.py:190} INFO - Task instance is in running state
[2024-12-17T13:58:39.531+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: queued
[2024-12-17T13:58:39.531+0530] {logging_mixin.py:190} INFO - Current task name:scrape_collection_page state:scheduled start_date:None
[2024-12-17T13:58:39.532+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline and current dag run status:running
[2024-12-17T13:58:39.532+0530] {logging_mixin.py:190} INFO - [2024-12-17T13:58:39.532+0530] {taskinstance.py:731} INFO - ::endgroup::
[2024-12-17T13:58:39.534+0530] {logging_mixin.py:190} INFO - 2024-12-17
[2024-12-17T13:58:39.534+0530] {logging_mixin.py:190} INFO - [2024-12-17 13:58:39,534] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T13:58:39.535+0530] {logging_mixin.py:190} INFO - [2024-12-17T13:58:39.534+0530] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T13:58:39.539+0530] {logging_mixin.py:190} INFO - [2024-12-17T13:58:39.539+0530] {taskinstance.py:340} INFO - ::group::Post task execution logs
[2024-12-17T13:58:39.540+0530] {logging_mixin.py:190} INFO - [2024-12-17T13:58:39.540+0530] {taskinstance.py:352} INFO - Marking task as SUCCESS. dag_id=movie_pipeline, task_id=scrape_collection_page, run_id=manual__2024-12-17T08:28:34.166490+00:00, execution_date=20241217T082834, start_date=, end_date=20241217T082839
[2024-12-17T13:58:39.549+0530] {logging_mixin.py:190} INFO - Task instance in success state
[2024-12-17T13:58:39.550+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: running
[2024-12-17T13:58:39.550+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline queued_at:None
[2024-12-17T13:58:39.550+0530] {logging_mixin.py:190} INFO - Task hostname:Jayeshs-MacBook-Air.local operator:PythonOperator
[2024-12-17T13:58:39.550+0530] {logging_mixin.py:190} INFO - [2024-12-17T13:58:39.550+0530] {dag.py:4410} INFO - [DAG TEST] end task task_id=scrape_collection_page map_index=-1
[2024-12-17T13:58:39.558+0530] {logging_mixin.py:190} INFO - [2024-12-17T13:58:39.557+0530] {dagrun.py:854} INFO - Marking run <DagRun movie_pipeline @ 2024-12-17 08:28:34.166490+00:00: manual__2024-12-17T08:28:34.166490+00:00, state:running, queued_at: None. externally triggered: False> successful
[2024-12-17T13:58:39.558+0530] {logging_mixin.py:190} INFO - Dag run in success state
[2024-12-17T13:58:39.559+0530] {logging_mixin.py:190} INFO - Dag run start:2024-12-17 08:28:34.166490+00:00 end:2024-12-17 08:28:39.558692+00:00
[2024-12-17T13:58:39.560+0530] {logging_mixin.py:190} INFO - [2024-12-17T13:58:39.559+0530] {dagrun.py:905} INFO - DagRun Finished: dag_id=movie_pipeline, execution_date=2024-12-17 08:28:34.166490+00:00, run_id=manual__2024-12-17T08:28:34.166490+00:00, run_start_date=2024-12-17 08:28:34.166490+00:00, run_end_date=2024-12-17 08:28:39.558692+00:00, run_duration=5.392202, state=success, external_trigger=False, run_type=manual, data_interval_start=2024-12-16 08:28:34.166490+00:00, data_interval_end=2024-12-17 08:28:34.166490+00:00, dag_hash=None
[2024-12-17T13:58:39.572+0530] {processor.py:925} INFO - DAG(s) 'movie_pipeline' retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T13:58:39.585+0530] {logging_mixin.py:190} INFO - [2024-12-17T13:58:39.584+0530] {dag.py:3239} INFO - Sync 1 DAGs
[2024-12-17T13:58:39.603+0530] {logging_mixin.py:190} INFO - [2024-12-17T13:58:39.602+0530] {dag.py:4180} INFO - Setting next_dagrun for movie_pipeline to 2024-12-17 01:30:00+00:00, run_after=2024-12-18 01:30:00+00:00
[2024-12-17T13:58:39.621+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 5.507 seconds
[2024-12-17T14:01:33.437+0530] {processor.py:186} INFO - Started process (PID=64841) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T14:01:33.440+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-17T14:01:33.442+0530] {logging_mixin.py:190} INFO - [2024-12-17T14:01:33.442+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T14:01:33.622+0530] {logging_mixin.py:190} INFO - [2024-12-17T14:01:33.622+0530] {dag.py:4435} INFO - dagrun id: movie_pipeline
[2024-12-17T14:01:33.636+0530] {logging_mixin.py:190} INFO - [2024-12-17T14:01:33.636+0530] {dag.py:4451} INFO - created dagrun <DagRun movie_pipeline @ 2024-12-17 08:31:33.484407+00:00: manual__2024-12-17T08:31:33.484407+00:00, state:running, queued_at: None. externally triggered: False>
[2024-12-17T14:01:33.666+0530] {logging_mixin.py:190} INFO - [2024-12-17T14:01:33.666+0530] {dag.py:4396} INFO - [DAG TEST] starting task_id=scrape_collection_page map_index=-1
[2024-12-17T14:01:33.667+0530] {logging_mixin.py:190} INFO - [2024-12-17T14:01:33.666+0530] {dag.py:4399} INFO - [DAG TEST] running task <TaskInstance: movie_pipeline.scrape_collection_page manual__2024-12-17T08:31:33.484407+00:00 [scheduled]>
[2024-12-17T14:01:38.772+0530] {logging_mixin.py:190} INFO - [2024-12-17T14:01:38.772+0530] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-12-17T14:01:38.842+0530] {logging_mixin.py:190} INFO - [2024-12-17 14:01:38,842] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T08:31:33.484407+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T08:31:33.484407+00:00'
[2024-12-17T14:01:38.842+0530] {logging_mixin.py:190} INFO - [2024-12-17T14:01:38.842+0530] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T08:31:33.484407+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T08:31:33.484407+00:00'
[2024-12-17T14:01:38.856+0530] {logging_mixin.py:190} INFO - Task instance is in running state
[2024-12-17T14:01:38.856+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: queued
[2024-12-17T14:01:38.856+0530] {logging_mixin.py:190} INFO - Current task name:scrape_collection_page state:scheduled start_date:None
[2024-12-17T14:01:38.857+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline and current dag run status:running
[2024-12-17T14:01:38.857+0530] {logging_mixin.py:190} INFO - [2024-12-17T14:01:38.857+0530] {taskinstance.py:731} INFO - ::endgroup::
[2024-12-17T14:01:38.860+0530] {logging_mixin.py:190} INFO - 2024-12-17
[2024-12-17T14:01:38.861+0530] {logging_mixin.py:190} INFO - [2024-12-17 14:01:38,860] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T14:01:38.861+0530] {logging_mixin.py:190} INFO - [2024-12-17T14:01:38.860+0530] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T14:01:38.866+0530] {logging_mixin.py:190} INFO - [2024-12-17T14:01:38.866+0530] {taskinstance.py:340} INFO - ::group::Post task execution logs
[2024-12-17T14:01:38.866+0530] {logging_mixin.py:190} INFO - [2024-12-17T14:01:38.866+0530] {taskinstance.py:352} INFO - Marking task as SUCCESS. dag_id=movie_pipeline, task_id=scrape_collection_page, run_id=manual__2024-12-17T08:31:33.484407+00:00, execution_date=20241217T083133, start_date=, end_date=20241217T083138
[2024-12-17T14:01:38.876+0530] {logging_mixin.py:190} INFO - Task instance in success state
[2024-12-17T14:01:38.876+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: running
[2024-12-17T14:01:38.876+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline queued_at:None
[2024-12-17T14:01:38.876+0530] {logging_mixin.py:190} INFO - Task hostname:Jayeshs-MacBook-Air.local operator:PythonOperator
[2024-12-17T14:01:38.877+0530] {logging_mixin.py:190} INFO - [2024-12-17T14:01:38.877+0530] {dag.py:4410} INFO - [DAG TEST] end task task_id=scrape_collection_page map_index=-1
[2024-12-17T14:01:38.881+0530] {logging_mixin.py:190} INFO - [2024-12-17T14:01:38.880+0530] {dagrun.py:854} INFO - Marking run <DagRun movie_pipeline @ 2024-12-17 08:31:33.484407+00:00: manual__2024-12-17T08:31:33.484407+00:00, state:running, queued_at: None. externally triggered: False> successful
[2024-12-17T14:01:38.881+0530] {logging_mixin.py:190} INFO - Dag run in success state
[2024-12-17T14:01:38.881+0530] {logging_mixin.py:190} INFO - Dag run start:2024-12-17 08:31:33.484407+00:00 end:2024-12-17 08:31:38.881229+00:00
[2024-12-17T14:01:38.881+0530] {logging_mixin.py:190} INFO - [2024-12-17T14:01:38.881+0530] {dagrun.py:905} INFO - DagRun Finished: dag_id=movie_pipeline, execution_date=2024-12-17 08:31:33.484407+00:00, run_id=manual__2024-12-17T08:31:33.484407+00:00, run_start_date=2024-12-17 08:31:33.484407+00:00, run_end_date=2024-12-17 08:31:38.881229+00:00, run_duration=5.396822, state=success, external_trigger=False, run_type=manual, data_interval_start=2024-12-16 08:31:33.484407+00:00, data_interval_end=2024-12-17 08:31:33.484407+00:00, dag_hash=None
[2024-12-17T14:01:38.891+0530] {processor.py:925} INFO - DAG(s) 'movie_pipeline' retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T14:01:38.904+0530] {logging_mixin.py:190} INFO - [2024-12-17T14:01:38.904+0530] {dag.py:3239} INFO - Sync 1 DAGs
[2024-12-17T14:01:38.922+0530] {logging_mixin.py:190} INFO - [2024-12-17T14:01:38.922+0530] {dag.py:4180} INFO - Setting next_dagrun for movie_pipeline to 2024-12-17 01:30:00+00:00, run_after=2024-12-18 01:30:00+00:00
[2024-12-17T14:01:38.941+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 5.503 seconds
[2024-12-17T14:04:29.342+0530] {processor.py:186} INFO - Started process (PID=65049) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T14:04:29.344+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-17T14:04:29.346+0530] {logging_mixin.py:190} INFO - [2024-12-17T14:04:29.346+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T14:04:29.523+0530] {logging_mixin.py:190} INFO - [2024-12-17T14:04:29.522+0530] {dag.py:4435} INFO - dagrun id: movie_pipeline
[2024-12-17T14:04:29.538+0530] {logging_mixin.py:190} INFO - [2024-12-17T14:04:29.538+0530] {dag.py:4451} INFO - created dagrun <DagRun movie_pipeline @ 2024-12-17 08:34:29.389913+00:00: manual__2024-12-17T08:34:29.389913+00:00, state:running, queued_at: None. externally triggered: False>
[2024-12-17T14:04:29.569+0530] {logging_mixin.py:190} INFO - [2024-12-17T14:04:29.569+0530] {dag.py:4396} INFO - [DAG TEST] starting task_id=scrape_collection_page map_index=-1
[2024-12-17T14:04:29.570+0530] {logging_mixin.py:190} INFO - [2024-12-17T14:04:29.569+0530] {dag.py:4399} INFO - [DAG TEST] running task <TaskInstance: movie_pipeline.scrape_collection_page manual__2024-12-17T08:34:29.389913+00:00 [scheduled]>
[2024-12-17T14:04:34.668+0530] {logging_mixin.py:190} INFO - [2024-12-17T14:04:34.668+0530] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-12-17T14:04:34.735+0530] {logging_mixin.py:190} INFO - [2024-12-17 14:04:34,735] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T08:34:29.389913+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T08:34:29.389913+00:00'
[2024-12-17T14:04:34.736+0530] {logging_mixin.py:190} INFO - [2024-12-17T14:04:34.735+0530] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T08:34:29.389913+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T08:34:29.389913+00:00'
[2024-12-17T14:04:34.746+0530] {logging_mixin.py:190} INFO - Task instance is in running state
[2024-12-17T14:04:34.746+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: queued
[2024-12-17T14:04:34.747+0530] {logging_mixin.py:190} INFO - Current task name:scrape_collection_page state:scheduled start_date:None
[2024-12-17T14:04:34.747+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline and current dag run status:running
[2024-12-17T14:04:34.747+0530] {logging_mixin.py:190} INFO - [2024-12-17T14:04:34.747+0530] {taskinstance.py:731} INFO - ::endgroup::
[2024-12-17T14:04:34.751+0530] {logging_mixin.py:190} INFO - 2024-12-17
[2024-12-17T14:04:34.752+0530] {logging_mixin.py:190} INFO - [2024-12-17 14:04:34,751] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T14:04:34.752+0530] {logging_mixin.py:190} INFO - [2024-12-17T14:04:34.751+0530] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T14:04:34.757+0530] {logging_mixin.py:190} INFO - [2024-12-17T14:04:34.757+0530] {taskinstance.py:340} INFO - ::group::Post task execution logs
[2024-12-17T14:04:34.759+0530] {logging_mixin.py:190} INFO - [2024-12-17T14:04:34.758+0530] {taskinstance.py:352} INFO - Marking task as SUCCESS. dag_id=movie_pipeline, task_id=scrape_collection_page, run_id=manual__2024-12-17T08:34:29.389913+00:00, execution_date=20241217T083429, start_date=, end_date=20241217T083434
[2024-12-17T14:04:34.770+0530] {logging_mixin.py:190} INFO - Task instance in success state
[2024-12-17T14:04:34.770+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: running
[2024-12-17T14:04:34.770+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline queued_at:None
[2024-12-17T14:04:34.771+0530] {logging_mixin.py:190} INFO - Task hostname:Jayeshs-MacBook-Air.local operator:PythonOperator
[2024-12-17T14:04:34.771+0530] {logging_mixin.py:190} INFO - [2024-12-17T14:04:34.771+0530] {dag.py:4410} INFO - [DAG TEST] end task task_id=scrape_collection_page map_index=-1
[2024-12-17T14:04:34.775+0530] {logging_mixin.py:190} INFO - [2024-12-17T14:04:34.774+0530] {dagrun.py:854} INFO - Marking run <DagRun movie_pipeline @ 2024-12-17 08:34:29.389913+00:00: manual__2024-12-17T08:34:29.389913+00:00, state:running, queued_at: None. externally triggered: False> successful
[2024-12-17T14:04:34.775+0530] {logging_mixin.py:190} INFO - Dag run in success state
[2024-12-17T14:04:34.776+0530] {logging_mixin.py:190} INFO - Dag run start:2024-12-17 08:34:29.389913+00:00 end:2024-12-17 08:34:34.775432+00:00
[2024-12-17T14:04:34.776+0530] {logging_mixin.py:190} INFO - [2024-12-17T14:04:34.776+0530] {dagrun.py:905} INFO - DagRun Finished: dag_id=movie_pipeline, execution_date=2024-12-17 08:34:29.389913+00:00, run_id=manual__2024-12-17T08:34:29.389913+00:00, run_start_date=2024-12-17 08:34:29.389913+00:00, run_end_date=2024-12-17 08:34:34.775432+00:00, run_duration=5.385519, state=success, external_trigger=False, run_type=manual, data_interval_start=2024-12-16 08:34:29.389913+00:00, data_interval_end=2024-12-17 08:34:29.389913+00:00, dag_hash=None
[2024-12-17T14:04:34.786+0530] {processor.py:925} INFO - DAG(s) 'movie_pipeline' retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T14:04:34.799+0530] {logging_mixin.py:190} INFO - [2024-12-17T14:04:34.799+0530] {dag.py:3239} INFO - Sync 1 DAGs
[2024-12-17T14:04:34.816+0530] {logging_mixin.py:190} INFO - [2024-12-17T14:04:34.816+0530] {dag.py:4180} INFO - Setting next_dagrun for movie_pipeline to 2024-12-17 01:30:00+00:00, run_after=2024-12-18 01:30:00+00:00
[2024-12-17T14:04:34.837+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 5.499 seconds
[2024-12-17T14:07:29.441+0530] {processor.py:186} INFO - Started process (PID=65257) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T14:07:29.443+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-17T14:07:29.445+0530] {logging_mixin.py:190} INFO - [2024-12-17T14:07:29.445+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T14:07:29.620+0530] {logging_mixin.py:190} INFO - [2024-12-17T14:07:29.620+0530] {dag.py:4435} INFO - dagrun id: movie_pipeline
[2024-12-17T14:07:29.635+0530] {logging_mixin.py:190} INFO - [2024-12-17T14:07:29.634+0530] {dag.py:4451} INFO - created dagrun <DagRun movie_pipeline @ 2024-12-17 08:37:29.489787+00:00: manual__2024-12-17T08:37:29.489787+00:00, state:running, queued_at: None. externally triggered: False>
[2024-12-17T14:07:29.664+0530] {logging_mixin.py:190} INFO - [2024-12-17T14:07:29.663+0530] {dag.py:4396} INFO - [DAG TEST] starting task_id=scrape_collection_page map_index=-1
[2024-12-17T14:07:29.664+0530] {logging_mixin.py:190} INFO - [2024-12-17T14:07:29.664+0530] {dag.py:4399} INFO - [DAG TEST] running task <TaskInstance: movie_pipeline.scrape_collection_page manual__2024-12-17T08:37:29.489787+00:00 [scheduled]>
[2024-12-17T14:07:34.765+0530] {logging_mixin.py:190} INFO - [2024-12-17T14:07:34.764+0530] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-12-17T14:07:34.833+0530] {logging_mixin.py:190} INFO - [2024-12-17 14:07:34,833] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T08:37:29.489787+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T08:37:29.489787+00:00'
[2024-12-17T14:07:34.834+0530] {logging_mixin.py:190} INFO - [2024-12-17T14:07:34.833+0530] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T08:37:29.489787+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T08:37:29.489787+00:00'
[2024-12-17T14:07:34.847+0530] {logging_mixin.py:190} INFO - Task instance is in running state
[2024-12-17T14:07:34.847+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: queued
[2024-12-17T14:07:34.848+0530] {logging_mixin.py:190} INFO - Current task name:scrape_collection_page state:scheduled start_date:None
[2024-12-17T14:07:34.848+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline and current dag run status:running
[2024-12-17T14:07:34.849+0530] {logging_mixin.py:190} INFO - [2024-12-17T14:07:34.848+0530] {taskinstance.py:731} INFO - ::endgroup::
[2024-12-17T14:07:34.851+0530] {logging_mixin.py:190} INFO - 2024-12-17
[2024-12-17T14:07:34.852+0530] {logging_mixin.py:190} INFO - [2024-12-17 14:07:34,852] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T14:07:34.852+0530] {logging_mixin.py:190} INFO - [2024-12-17T14:07:34.852+0530] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T14:07:34.857+0530] {logging_mixin.py:190} INFO - [2024-12-17T14:07:34.857+0530] {taskinstance.py:340} INFO - ::group::Post task execution logs
[2024-12-17T14:07:34.858+0530] {logging_mixin.py:190} INFO - [2024-12-17T14:07:34.858+0530] {taskinstance.py:352} INFO - Marking task as SUCCESS. dag_id=movie_pipeline, task_id=scrape_collection_page, run_id=manual__2024-12-17T08:37:29.489787+00:00, execution_date=20241217T083729, start_date=, end_date=20241217T083734
[2024-12-17T14:07:34.867+0530] {logging_mixin.py:190} INFO - Task instance in success state
[2024-12-17T14:07:34.868+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: running
[2024-12-17T14:07:34.868+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline queued_at:None
[2024-12-17T14:07:34.868+0530] {logging_mixin.py:190} INFO - Task hostname:Jayeshs-MacBook-Air.local operator:PythonOperator
[2024-12-17T14:07:34.869+0530] {logging_mixin.py:190} INFO - [2024-12-17T14:07:34.869+0530] {dag.py:4410} INFO - [DAG TEST] end task task_id=scrape_collection_page map_index=-1
[2024-12-17T14:07:34.873+0530] {logging_mixin.py:190} INFO - [2024-12-17T14:07:34.872+0530] {dagrun.py:854} INFO - Marking run <DagRun movie_pipeline @ 2024-12-17 08:37:29.489787+00:00: manual__2024-12-17T08:37:29.489787+00:00, state:running, queued_at: None. externally triggered: False> successful
[2024-12-17T14:07:34.873+0530] {logging_mixin.py:190} INFO - Dag run in success state
[2024-12-17T14:07:34.873+0530] {logging_mixin.py:190} INFO - Dag run start:2024-12-17 08:37:29.489787+00:00 end:2024-12-17 08:37:34.873476+00:00
[2024-12-17T14:07:34.874+0530] {logging_mixin.py:190} INFO - [2024-12-17T14:07:34.873+0530] {dagrun.py:905} INFO - DagRun Finished: dag_id=movie_pipeline, execution_date=2024-12-17 08:37:29.489787+00:00, run_id=manual__2024-12-17T08:37:29.489787+00:00, run_start_date=2024-12-17 08:37:29.489787+00:00, run_end_date=2024-12-17 08:37:34.873476+00:00, run_duration=5.383689, state=success, external_trigger=False, run_type=manual, data_interval_start=2024-12-16 08:37:29.489787+00:00, data_interval_end=2024-12-17 08:37:29.489787+00:00, dag_hash=None
[2024-12-17T14:07:34.883+0530] {processor.py:925} INFO - DAG(s) 'movie_pipeline' retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T14:07:34.895+0530] {logging_mixin.py:190} INFO - [2024-12-17T14:07:34.894+0530] {dag.py:3239} INFO - Sync 1 DAGs
[2024-12-17T14:07:34.916+0530] {logging_mixin.py:190} INFO - [2024-12-17T14:07:34.915+0530] {dag.py:4180} INFO - Setting next_dagrun for movie_pipeline to 2024-12-17 01:30:00+00:00, run_after=2024-12-18 01:30:00+00:00
[2024-12-17T14:07:34.936+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 5.498 seconds
[2024-12-17T14:10:26.668+0530] {processor.py:186} INFO - Started process (PID=65474) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T14:10:26.671+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-17T14:10:26.673+0530] {logging_mixin.py:190} INFO - [2024-12-17T14:10:26.672+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T14:10:26.855+0530] {logging_mixin.py:190} INFO - [2024-12-17T14:10:26.855+0530] {dag.py:4435} INFO - dagrun id: movie_pipeline
[2024-12-17T14:10:26.871+0530] {logging_mixin.py:190} INFO - [2024-12-17T14:10:26.870+0530] {dag.py:4451} INFO - created dagrun <DagRun movie_pipeline @ 2024-12-17 08:40:26.718102+00:00: manual__2024-12-17T08:40:26.718102+00:00, state:running, queued_at: None. externally triggered: False>
[2024-12-17T14:10:26.902+0530] {logging_mixin.py:190} INFO - [2024-12-17T14:10:26.901+0530] {dag.py:4396} INFO - [DAG TEST] starting task_id=scrape_collection_page map_index=-1
[2024-12-17T14:10:26.902+0530] {logging_mixin.py:190} INFO - [2024-12-17T14:10:26.902+0530] {dag.py:4399} INFO - [DAG TEST] running task <TaskInstance: movie_pipeline.scrape_collection_page manual__2024-12-17T08:40:26.718102+00:00 [scheduled]>
[2024-12-17T14:10:32.010+0530] {logging_mixin.py:190} INFO - [2024-12-17T14:10:32.009+0530] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-12-17T14:10:32.088+0530] {logging_mixin.py:190} INFO - [2024-12-17 14:10:32,088] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T08:40:26.718102+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T08:40:26.718102+00:00'
[2024-12-17T14:10:32.089+0530] {logging_mixin.py:190} INFO - [2024-12-17T14:10:32.088+0530] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T08:40:26.718102+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T08:40:26.718102+00:00'
[2024-12-17T14:10:32.103+0530] {logging_mixin.py:190} INFO - Task instance is in running state
[2024-12-17T14:10:32.103+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: queued
[2024-12-17T14:10:32.103+0530] {logging_mixin.py:190} INFO - Current task name:scrape_collection_page state:scheduled start_date:None
[2024-12-17T14:10:32.104+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline and current dag run status:running
[2024-12-17T14:10:32.104+0530] {logging_mixin.py:190} INFO - [2024-12-17T14:10:32.104+0530] {taskinstance.py:731} INFO - ::endgroup::
[2024-12-17T14:10:32.108+0530] {logging_mixin.py:190} INFO - 2024-12-17
[2024-12-17T14:10:32.109+0530] {logging_mixin.py:190} INFO - [2024-12-17 14:10:32,108] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T14:10:32.109+0530] {logging_mixin.py:190} INFO - [2024-12-17T14:10:32.108+0530] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T14:10:32.114+0530] {logging_mixin.py:190} INFO - [2024-12-17T14:10:32.114+0530] {taskinstance.py:340} INFO - ::group::Post task execution logs
[2024-12-17T14:10:32.115+0530] {logging_mixin.py:190} INFO - [2024-12-17T14:10:32.115+0530] {taskinstance.py:352} INFO - Marking task as SUCCESS. dag_id=movie_pipeline, task_id=scrape_collection_page, run_id=manual__2024-12-17T08:40:26.718102+00:00, execution_date=20241217T084026, start_date=, end_date=20241217T084032
[2024-12-17T14:10:32.125+0530] {logging_mixin.py:190} INFO - Task instance in success state
[2024-12-17T14:10:32.126+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: running
[2024-12-17T14:10:32.126+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline queued_at:None
[2024-12-17T14:10:32.126+0530] {logging_mixin.py:190} INFO - Task hostname:Jayeshs-MacBook-Air.local operator:PythonOperator
[2024-12-17T14:10:32.126+0530] {logging_mixin.py:190} INFO - [2024-12-17T14:10:32.126+0530] {dag.py:4410} INFO - [DAG TEST] end task task_id=scrape_collection_page map_index=-1
[2024-12-17T14:10:32.130+0530] {logging_mixin.py:190} INFO - [2024-12-17T14:10:32.130+0530] {dagrun.py:854} INFO - Marking run <DagRun movie_pipeline @ 2024-12-17 08:40:26.718102+00:00: manual__2024-12-17T08:40:26.718102+00:00, state:running, queued_at: None. externally triggered: False> successful
[2024-12-17T14:10:32.130+0530] {logging_mixin.py:190} INFO - Dag run in success state
[2024-12-17T14:10:32.131+0530] {logging_mixin.py:190} INFO - Dag run start:2024-12-17 08:40:26.718102+00:00 end:2024-12-17 08:40:32.130787+00:00
[2024-12-17T14:10:32.131+0530] {logging_mixin.py:190} INFO - [2024-12-17T14:10:32.131+0530] {dagrun.py:905} INFO - DagRun Finished: dag_id=movie_pipeline, execution_date=2024-12-17 08:40:26.718102+00:00, run_id=manual__2024-12-17T08:40:26.718102+00:00, run_start_date=2024-12-17 08:40:26.718102+00:00, run_end_date=2024-12-17 08:40:32.130787+00:00, run_duration=5.412685, state=success, external_trigger=False, run_type=manual, data_interval_start=2024-12-16 08:40:26.718102+00:00, data_interval_end=2024-12-17 08:40:26.718102+00:00, dag_hash=None
[2024-12-17T14:10:32.140+0530] {processor.py:925} INFO - DAG(s) 'movie_pipeline' retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T14:10:32.154+0530] {logging_mixin.py:190} INFO - [2024-12-17T14:10:32.153+0530] {dag.py:3239} INFO - Sync 1 DAGs
[2024-12-17T14:10:32.173+0530] {logging_mixin.py:190} INFO - [2024-12-17T14:10:32.173+0530] {dag.py:4180} INFO - Setting next_dagrun for movie_pipeline to 2024-12-17 01:30:00+00:00, run_after=2024-12-18 01:30:00+00:00
[2024-12-17T14:10:32.193+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 5.528 seconds
[2024-12-17T14:13:26.105+0530] {processor.py:186} INFO - Started process (PID=65677) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T14:13:26.108+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-17T14:13:26.110+0530] {logging_mixin.py:190} INFO - [2024-12-17T14:13:26.109+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T14:13:26.299+0530] {logging_mixin.py:190} INFO - [2024-12-17T14:13:26.298+0530] {dag.py:4435} INFO - dagrun id: movie_pipeline
[2024-12-17T14:13:26.314+0530] {logging_mixin.py:190} INFO - [2024-12-17T14:13:26.314+0530] {dag.py:4451} INFO - created dagrun <DagRun movie_pipeline @ 2024-12-17 08:43:26.158585+00:00: manual__2024-12-17T08:43:26.158585+00:00, state:running, queued_at: None. externally triggered: False>
[2024-12-17T14:13:26.345+0530] {logging_mixin.py:190} INFO - [2024-12-17T14:13:26.345+0530] {dag.py:4396} INFO - [DAG TEST] starting task_id=scrape_collection_page map_index=-1
[2024-12-17T14:13:26.346+0530] {logging_mixin.py:190} INFO - [2024-12-17T14:13:26.345+0530] {dag.py:4399} INFO - [DAG TEST] running task <TaskInstance: movie_pipeline.scrape_collection_page manual__2024-12-17T08:43:26.158585+00:00 [scheduled]>
[2024-12-17T14:13:31.448+0530] {logging_mixin.py:190} INFO - [2024-12-17T14:13:31.448+0530] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-12-17T14:13:31.519+0530] {logging_mixin.py:190} INFO - [2024-12-17 14:13:31,519] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T08:43:26.158585+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T08:43:26.158585+00:00'
[2024-12-17T14:13:31.520+0530] {logging_mixin.py:190} INFO - [2024-12-17T14:13:31.519+0530] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T08:43:26.158585+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T08:43:26.158585+00:00'
[2024-12-17T14:13:31.533+0530] {logging_mixin.py:190} INFO - Task instance is in running state
[2024-12-17T14:13:31.533+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: queued
[2024-12-17T14:13:31.534+0530] {logging_mixin.py:190} INFO - Current task name:scrape_collection_page state:scheduled start_date:None
[2024-12-17T14:13:31.534+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline and current dag run status:running
[2024-12-17T14:13:31.534+0530] {logging_mixin.py:190} INFO - [2024-12-17T14:13:31.534+0530] {taskinstance.py:731} INFO - ::endgroup::
[2024-12-17T14:13:31.539+0530] {logging_mixin.py:190} INFO - 2024-12-17
[2024-12-17T14:13:31.539+0530] {logging_mixin.py:190} INFO - [2024-12-17 14:13:31,539] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T14:13:31.540+0530] {logging_mixin.py:190} INFO - [2024-12-17T14:13:31.539+0530] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T14:13:31.545+0530] {logging_mixin.py:190} INFO - [2024-12-17T14:13:31.545+0530] {taskinstance.py:340} INFO - ::group::Post task execution logs
[2024-12-17T14:13:31.546+0530] {logging_mixin.py:190} INFO - [2024-12-17T14:13:31.545+0530] {taskinstance.py:352} INFO - Marking task as SUCCESS. dag_id=movie_pipeline, task_id=scrape_collection_page, run_id=manual__2024-12-17T08:43:26.158585+00:00, execution_date=20241217T084326, start_date=, end_date=20241217T084331
[2024-12-17T14:13:31.557+0530] {logging_mixin.py:190} INFO - Task instance in success state
[2024-12-17T14:13:31.558+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: running
[2024-12-17T14:13:31.558+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline queued_at:None
[2024-12-17T14:13:31.558+0530] {logging_mixin.py:190} INFO - Task hostname:Jayeshs-MacBook-Air.local operator:PythonOperator
[2024-12-17T14:13:31.559+0530] {logging_mixin.py:190} INFO - [2024-12-17T14:13:31.559+0530] {dag.py:4410} INFO - [DAG TEST] end task task_id=scrape_collection_page map_index=-1
[2024-12-17T14:13:31.564+0530] {logging_mixin.py:190} INFO - [2024-12-17T14:13:31.563+0530] {dagrun.py:854} INFO - Marking run <DagRun movie_pipeline @ 2024-12-17 08:43:26.158585+00:00: manual__2024-12-17T08:43:26.158585+00:00, state:running, queued_at: None. externally triggered: False> successful
[2024-12-17T14:13:31.564+0530] {logging_mixin.py:190} INFO - Dag run in success state
[2024-12-17T14:13:31.564+0530] {logging_mixin.py:190} INFO - Dag run start:2024-12-17 08:43:26.158585+00:00 end:2024-12-17 08:43:31.564276+00:00
[2024-12-17T14:13:31.565+0530] {logging_mixin.py:190} INFO - [2024-12-17T14:13:31.564+0530] {dagrun.py:905} INFO - DagRun Finished: dag_id=movie_pipeline, execution_date=2024-12-17 08:43:26.158585+00:00, run_id=manual__2024-12-17T08:43:26.158585+00:00, run_start_date=2024-12-17 08:43:26.158585+00:00, run_end_date=2024-12-17 08:43:31.564276+00:00, run_duration=5.405691, state=success, external_trigger=False, run_type=manual, data_interval_start=2024-12-16 08:43:26.158585+00:00, data_interval_end=2024-12-17 08:43:26.158585+00:00, dag_hash=None
[2024-12-17T14:13:31.573+0530] {processor.py:925} INFO - DAG(s) 'movie_pipeline' retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T14:13:31.586+0530] {logging_mixin.py:190} INFO - [2024-12-17T14:13:31.585+0530] {dag.py:3239} INFO - Sync 1 DAGs
[2024-12-17T14:13:31.602+0530] {logging_mixin.py:190} INFO - [2024-12-17T14:13:31.602+0530] {dag.py:4180} INFO - Setting next_dagrun for movie_pipeline to 2024-12-17 01:30:00+00:00, run_after=2024-12-18 01:30:00+00:00
[2024-12-17T14:13:31.623+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 5.521 seconds
[2024-12-17T14:16:24.501+0530] {processor.py:186} INFO - Started process (PID=65889) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T14:16:24.504+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-17T14:16:24.506+0530] {logging_mixin.py:190} INFO - [2024-12-17T14:16:24.505+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T14:16:24.682+0530] {logging_mixin.py:190} INFO - [2024-12-17T14:16:24.682+0530] {dag.py:4435} INFO - dagrun id: movie_pipeline
[2024-12-17T14:16:24.696+0530] {logging_mixin.py:190} INFO - [2024-12-17T14:16:24.696+0530] {dag.py:4451} INFO - created dagrun <DagRun movie_pipeline @ 2024-12-17 08:46:24.549332+00:00: manual__2024-12-17T08:46:24.549332+00:00, state:running, queued_at: None. externally triggered: False>
[2024-12-17T14:16:24.730+0530] {logging_mixin.py:190} INFO - [2024-12-17T14:16:24.729+0530] {dag.py:4396} INFO - [DAG TEST] starting task_id=scrape_collection_page map_index=-1
[2024-12-17T14:16:24.730+0530] {logging_mixin.py:190} INFO - [2024-12-17T14:16:24.730+0530] {dag.py:4399} INFO - [DAG TEST] running task <TaskInstance: movie_pipeline.scrape_collection_page manual__2024-12-17T08:46:24.549332+00:00 [scheduled]>
[2024-12-17T14:16:29.837+0530] {logging_mixin.py:190} INFO - [2024-12-17T14:16:29.836+0530] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-12-17T14:16:29.911+0530] {logging_mixin.py:190} INFO - [2024-12-17 14:16:29,911] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T08:46:24.549332+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T08:46:24.549332+00:00'
[2024-12-17T14:16:29.912+0530] {logging_mixin.py:190} INFO - [2024-12-17T14:16:29.911+0530] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T08:46:24.549332+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T08:46:24.549332+00:00'
[2024-12-17T14:16:29.926+0530] {logging_mixin.py:190} INFO - Task instance is in running state
[2024-12-17T14:16:29.927+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: queued
[2024-12-17T14:16:29.927+0530] {logging_mixin.py:190} INFO - Current task name:scrape_collection_page state:scheduled start_date:None
[2024-12-17T14:16:29.927+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline and current dag run status:running
[2024-12-17T14:16:29.928+0530] {logging_mixin.py:190} INFO - [2024-12-17T14:16:29.928+0530] {taskinstance.py:731} INFO - ::endgroup::
[2024-12-17T14:16:29.931+0530] {logging_mixin.py:190} INFO - 2024-12-17
[2024-12-17T14:16:29.932+0530] {logging_mixin.py:190} INFO - [2024-12-17 14:16:29,931] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T14:16:29.932+0530] {logging_mixin.py:190} INFO - [2024-12-17T14:16:29.931+0530] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T14:16:29.938+0530] {logging_mixin.py:190} INFO - [2024-12-17T14:16:29.938+0530] {taskinstance.py:340} INFO - ::group::Post task execution logs
[2024-12-17T14:16:29.939+0530] {logging_mixin.py:190} INFO - [2024-12-17T14:16:29.939+0530] {taskinstance.py:352} INFO - Marking task as SUCCESS. dag_id=movie_pipeline, task_id=scrape_collection_page, run_id=manual__2024-12-17T08:46:24.549332+00:00, execution_date=20241217T084624, start_date=, end_date=20241217T084629
[2024-12-17T14:16:29.948+0530] {logging_mixin.py:190} INFO - Task instance in success state
[2024-12-17T14:16:29.948+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: running
[2024-12-17T14:16:29.948+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline queued_at:None
[2024-12-17T14:16:29.949+0530] {logging_mixin.py:190} INFO - Task hostname:Jayeshs-MacBook-Air.local operator:PythonOperator
[2024-12-17T14:16:29.949+0530] {logging_mixin.py:190} INFO - [2024-12-17T14:16:29.949+0530] {dag.py:4410} INFO - [DAG TEST] end task task_id=scrape_collection_page map_index=-1
[2024-12-17T14:16:29.953+0530] {logging_mixin.py:190} INFO - [2024-12-17T14:16:29.953+0530] {dagrun.py:854} INFO - Marking run <DagRun movie_pipeline @ 2024-12-17 08:46:24.549332+00:00: manual__2024-12-17T08:46:24.549332+00:00, state:running, queued_at: None. externally triggered: False> successful
[2024-12-17T14:16:29.953+0530] {logging_mixin.py:190} INFO - Dag run in success state
[2024-12-17T14:16:29.954+0530] {logging_mixin.py:190} INFO - Dag run start:2024-12-17 08:46:24.549332+00:00 end:2024-12-17 08:46:29.953905+00:00
[2024-12-17T14:16:29.954+0530] {logging_mixin.py:190} INFO - [2024-12-17T14:16:29.954+0530] {dagrun.py:905} INFO - DagRun Finished: dag_id=movie_pipeline, execution_date=2024-12-17 08:46:24.549332+00:00, run_id=manual__2024-12-17T08:46:24.549332+00:00, run_start_date=2024-12-17 08:46:24.549332+00:00, run_end_date=2024-12-17 08:46:29.953905+00:00, run_duration=5.404573, state=success, external_trigger=False, run_type=manual, data_interval_start=2024-12-16 08:46:24.549332+00:00, data_interval_end=2024-12-17 08:46:24.549332+00:00, dag_hash=None
[2024-12-17T14:16:29.963+0530] {processor.py:925} INFO - DAG(s) 'movie_pipeline' retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T14:16:29.975+0530] {logging_mixin.py:190} INFO - [2024-12-17T14:16:29.974+0530] {dag.py:3239} INFO - Sync 1 DAGs
[2024-12-17T14:16:29.992+0530] {logging_mixin.py:190} INFO - [2024-12-17T14:16:29.992+0530] {dag.py:4180} INFO - Setting next_dagrun for movie_pipeline to 2024-12-17 01:30:00+00:00, run_after=2024-12-18 01:30:00+00:00
[2024-12-17T14:16:30.013+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 5.517 seconds
[2024-12-17T14:19:22.981+0530] {processor.py:186} INFO - Started process (PID=66086) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T14:19:22.984+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-17T14:19:22.985+0530] {logging_mixin.py:190} INFO - [2024-12-17T14:19:22.985+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T14:19:23.163+0530] {logging_mixin.py:190} INFO - [2024-12-17T14:19:23.163+0530] {dag.py:4435} INFO - dagrun id: movie_pipeline
[2024-12-17T14:19:23.178+0530] {logging_mixin.py:190} INFO - [2024-12-17T14:19:23.178+0530] {dag.py:4451} INFO - created dagrun <DagRun movie_pipeline @ 2024-12-17 08:49:23.029701+00:00: manual__2024-12-17T08:49:23.029701+00:00, state:running, queued_at: None. externally triggered: False>
[2024-12-17T14:19:23.208+0530] {logging_mixin.py:190} INFO - [2024-12-17T14:19:23.207+0530] {dag.py:4396} INFO - [DAG TEST] starting task_id=scrape_collection_page map_index=-1
[2024-12-17T14:19:23.208+0530] {logging_mixin.py:190} INFO - [2024-12-17T14:19:23.208+0530] {dag.py:4399} INFO - [DAG TEST] running task <TaskInstance: movie_pipeline.scrape_collection_page manual__2024-12-17T08:49:23.029701+00:00 [scheduled]>
[2024-12-17T14:19:28.313+0530] {logging_mixin.py:190} INFO - [2024-12-17T14:19:28.313+0530] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-12-17T14:19:28.385+0530] {logging_mixin.py:190} INFO - [2024-12-17 14:19:28,385] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T08:49:23.029701+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T08:49:23.029701+00:00'
[2024-12-17T14:19:28.386+0530] {logging_mixin.py:190} INFO - [2024-12-17T14:19:28.385+0530] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T08:49:23.029701+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T08:49:23.029701+00:00'
[2024-12-17T14:19:28.398+0530] {logging_mixin.py:190} INFO - Task instance is in running state
[2024-12-17T14:19:28.399+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: queued
[2024-12-17T14:19:28.399+0530] {logging_mixin.py:190} INFO - Current task name:scrape_collection_page state:scheduled start_date:None
[2024-12-17T14:19:28.400+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline and current dag run status:running
[2024-12-17T14:19:28.401+0530] {logging_mixin.py:190} INFO - [2024-12-17T14:19:28.400+0530] {taskinstance.py:731} INFO - ::endgroup::
[2024-12-17T14:19:28.403+0530] {logging_mixin.py:190} INFO - 2024-12-17
[2024-12-17T14:19:28.404+0530] {logging_mixin.py:190} INFO - [2024-12-17 14:19:28,404] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T14:19:28.405+0530] {logging_mixin.py:190} INFO - [2024-12-17T14:19:28.404+0530] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T14:19:28.410+0530] {logging_mixin.py:190} INFO - [2024-12-17T14:19:28.409+0530] {taskinstance.py:340} INFO - ::group::Post task execution logs
[2024-12-17T14:19:28.410+0530] {logging_mixin.py:190} INFO - [2024-12-17T14:19:28.410+0530] {taskinstance.py:352} INFO - Marking task as SUCCESS. dag_id=movie_pipeline, task_id=scrape_collection_page, run_id=manual__2024-12-17T08:49:23.029701+00:00, execution_date=20241217T084923, start_date=, end_date=20241217T084928
[2024-12-17T14:19:28.420+0530] {logging_mixin.py:190} INFO - Task instance in success state
[2024-12-17T14:19:28.420+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: running
[2024-12-17T14:19:28.420+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline queued_at:None
[2024-12-17T14:19:28.421+0530] {logging_mixin.py:190} INFO - Task hostname:Jayeshs-MacBook-Air.local operator:PythonOperator
[2024-12-17T14:19:28.421+0530] {logging_mixin.py:190} INFO - [2024-12-17T14:19:28.421+0530] {dag.py:4410} INFO - [DAG TEST] end task task_id=scrape_collection_page map_index=-1
[2024-12-17T14:19:28.425+0530] {logging_mixin.py:190} INFO - [2024-12-17T14:19:28.424+0530] {dagrun.py:854} INFO - Marking run <DagRun movie_pipeline @ 2024-12-17 08:49:23.029701+00:00: manual__2024-12-17T08:49:23.029701+00:00, state:running, queued_at: None. externally triggered: False> successful
[2024-12-17T14:19:28.425+0530] {logging_mixin.py:190} INFO - Dag run in success state
[2024-12-17T14:19:28.425+0530] {logging_mixin.py:190} INFO - Dag run start:2024-12-17 08:49:23.029701+00:00 end:2024-12-17 08:49:28.425330+00:00
[2024-12-17T14:19:28.426+0530] {logging_mixin.py:190} INFO - [2024-12-17T14:19:28.425+0530] {dagrun.py:905} INFO - DagRun Finished: dag_id=movie_pipeline, execution_date=2024-12-17 08:49:23.029701+00:00, run_id=manual__2024-12-17T08:49:23.029701+00:00, run_start_date=2024-12-17 08:49:23.029701+00:00, run_end_date=2024-12-17 08:49:28.425330+00:00, run_duration=5.395629, state=success, external_trigger=False, run_type=manual, data_interval_start=2024-12-16 08:49:23.029701+00:00, data_interval_end=2024-12-17 08:49:23.029701+00:00, dag_hash=None
[2024-12-17T14:19:28.435+0530] {processor.py:925} INFO - DAG(s) 'movie_pipeline' retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T14:19:28.447+0530] {logging_mixin.py:190} INFO - [2024-12-17T14:19:28.446+0530] {dag.py:3239} INFO - Sync 1 DAGs
[2024-12-17T14:19:28.467+0530] {logging_mixin.py:190} INFO - [2024-12-17T14:19:28.466+0530] {dag.py:4180} INFO - Setting next_dagrun for movie_pipeline to 2024-12-17 01:30:00+00:00, run_after=2024-12-18 01:30:00+00:00
[2024-12-17T14:19:28.488+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 5.510 seconds
[2024-12-17T14:22:19.907+0530] {processor.py:186} INFO - Started process (PID=66296) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T14:22:19.910+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-17T14:22:19.912+0530] {logging_mixin.py:190} INFO - [2024-12-17T14:22:19.911+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T14:22:20.090+0530] {logging_mixin.py:190} INFO - [2024-12-17T14:22:20.089+0530] {dag.py:4435} INFO - dagrun id: movie_pipeline
[2024-12-17T14:22:20.104+0530] {logging_mixin.py:190} INFO - [2024-12-17T14:22:20.104+0530] {dag.py:4451} INFO - created dagrun <DagRun movie_pipeline @ 2024-12-17 08:52:19.955494+00:00: manual__2024-12-17T08:52:19.955494+00:00, state:running, queued_at: None. externally triggered: False>
[2024-12-17T14:22:20.135+0530] {logging_mixin.py:190} INFO - [2024-12-17T14:22:20.135+0530] {dag.py:4396} INFO - [DAG TEST] starting task_id=scrape_collection_page map_index=-1
[2024-12-17T14:22:20.136+0530] {logging_mixin.py:190} INFO - [2024-12-17T14:22:20.135+0530] {dag.py:4399} INFO - [DAG TEST] running task <TaskInstance: movie_pipeline.scrape_collection_page manual__2024-12-17T08:52:19.955494+00:00 [scheduled]>
[2024-12-17T14:22:25.235+0530] {logging_mixin.py:190} INFO - [2024-12-17T14:22:25.235+0530] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-12-17T14:22:25.303+0530] {logging_mixin.py:190} INFO - [2024-12-17 14:22:25,303] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T08:52:19.955494+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T08:52:19.955494+00:00'
[2024-12-17T14:22:25.303+0530] {logging_mixin.py:190} INFO - [2024-12-17T14:22:25.303+0530] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T08:52:19.955494+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T08:52:19.955494+00:00'
[2024-12-17T14:22:25.315+0530] {logging_mixin.py:190} INFO - Task instance is in running state
[2024-12-17T14:22:25.316+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: queued
[2024-12-17T14:22:25.316+0530] {logging_mixin.py:190} INFO - Current task name:scrape_collection_page state:scheduled start_date:None
[2024-12-17T14:22:25.317+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline and current dag run status:running
[2024-12-17T14:22:25.317+0530] {logging_mixin.py:190} INFO - [2024-12-17T14:22:25.317+0530] {taskinstance.py:731} INFO - ::endgroup::
[2024-12-17T14:22:25.319+0530] {logging_mixin.py:190} INFO - 2024-12-17
[2024-12-17T14:22:25.320+0530] {logging_mixin.py:190} INFO - [2024-12-17 14:22:25,319] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T14:22:25.320+0530] {logging_mixin.py:190} INFO - [2024-12-17T14:22:25.319+0530] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T14:22:25.328+0530] {logging_mixin.py:190} INFO - [2024-12-17T14:22:25.327+0530] {taskinstance.py:340} INFO - ::group::Post task execution logs
[2024-12-17T14:22:25.328+0530] {logging_mixin.py:190} INFO - [2024-12-17T14:22:25.328+0530] {taskinstance.py:352} INFO - Marking task as SUCCESS. dag_id=movie_pipeline, task_id=scrape_collection_page, run_id=manual__2024-12-17T08:52:19.955494+00:00, execution_date=20241217T085219, start_date=, end_date=20241217T085225
[2024-12-17T14:22:25.338+0530] {logging_mixin.py:190} INFO - Task instance in success state
[2024-12-17T14:22:25.338+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: running
[2024-12-17T14:22:25.339+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline queued_at:None
[2024-12-17T14:22:25.339+0530] {logging_mixin.py:190} INFO - Task hostname:Jayeshs-MacBook-Air.local operator:PythonOperator
[2024-12-17T14:22:25.339+0530] {logging_mixin.py:190} INFO - [2024-12-17T14:22:25.339+0530] {dag.py:4410} INFO - [DAG TEST] end task task_id=scrape_collection_page map_index=-1
[2024-12-17T14:22:25.344+0530] {logging_mixin.py:190} INFO - [2024-12-17T14:22:25.343+0530] {dagrun.py:854} INFO - Marking run <DagRun movie_pipeline @ 2024-12-17 08:52:19.955494+00:00: manual__2024-12-17T08:52:19.955494+00:00, state:running, queued_at: None. externally triggered: False> successful
[2024-12-17T14:22:25.344+0530] {logging_mixin.py:190} INFO - Dag run in success state
[2024-12-17T14:22:25.344+0530] {logging_mixin.py:190} INFO - Dag run start:2024-12-17 08:52:19.955494+00:00 end:2024-12-17 08:52:25.344318+00:00
[2024-12-17T14:22:25.345+0530] {logging_mixin.py:190} INFO - [2024-12-17T14:22:25.345+0530] {dagrun.py:905} INFO - DagRun Finished: dag_id=movie_pipeline, execution_date=2024-12-17 08:52:19.955494+00:00, run_id=manual__2024-12-17T08:52:19.955494+00:00, run_start_date=2024-12-17 08:52:19.955494+00:00, run_end_date=2024-12-17 08:52:25.344318+00:00, run_duration=5.388824, state=success, external_trigger=False, run_type=manual, data_interval_start=2024-12-16 08:52:19.955494+00:00, data_interval_end=2024-12-17 08:52:19.955494+00:00, dag_hash=None
[2024-12-17T14:22:25.353+0530] {processor.py:925} INFO - DAG(s) 'movie_pipeline' retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T14:22:25.365+0530] {logging_mixin.py:190} INFO - [2024-12-17T14:22:25.365+0530] {dag.py:3239} INFO - Sync 1 DAGs
[2024-12-17T14:22:25.384+0530] {logging_mixin.py:190} INFO - [2024-12-17T14:22:25.384+0530] {dag.py:4180} INFO - Setting next_dagrun for movie_pipeline to 2024-12-17 01:30:00+00:00, run_after=2024-12-18 01:30:00+00:00
[2024-12-17T14:22:25.403+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 5.500 seconds
[2024-12-17T14:25:19.063+0530] {processor.py:186} INFO - Started process (PID=66501) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T14:25:19.066+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-17T14:25:19.068+0530] {logging_mixin.py:190} INFO - [2024-12-17T14:25:19.068+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T14:25:19.246+0530] {logging_mixin.py:190} INFO - [2024-12-17T14:25:19.246+0530] {dag.py:4435} INFO - dagrun id: movie_pipeline
[2024-12-17T14:25:19.261+0530] {logging_mixin.py:190} INFO - [2024-12-17T14:25:19.260+0530] {dag.py:4451} INFO - created dagrun <DagRun movie_pipeline @ 2024-12-17 08:55:19.114722+00:00: manual__2024-12-17T08:55:19.114722+00:00, state:running, queued_at: None. externally triggered: False>
[2024-12-17T14:25:19.292+0530] {logging_mixin.py:190} INFO - [2024-12-17T14:25:19.292+0530] {dag.py:4396} INFO - [DAG TEST] starting task_id=scrape_collection_page map_index=-1
[2024-12-17T14:25:19.293+0530] {logging_mixin.py:190} INFO - [2024-12-17T14:25:19.293+0530] {dag.py:4399} INFO - [DAG TEST] running task <TaskInstance: movie_pipeline.scrape_collection_page manual__2024-12-17T08:55:19.114722+00:00 [scheduled]>
[2024-12-17T14:25:24.391+0530] {logging_mixin.py:190} INFO - [2024-12-17T14:25:24.391+0530] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-12-17T14:25:24.462+0530] {logging_mixin.py:190} INFO - [2024-12-17 14:25:24,462] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T08:55:19.114722+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T08:55:19.114722+00:00'
[2024-12-17T14:25:24.463+0530] {logging_mixin.py:190} INFO - [2024-12-17T14:25:24.462+0530] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T08:55:19.114722+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T08:55:19.114722+00:00'
[2024-12-17T14:25:24.475+0530] {logging_mixin.py:190} INFO - Task instance is in running state
[2024-12-17T14:25:24.476+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: queued
[2024-12-17T14:25:24.476+0530] {logging_mixin.py:190} INFO - Current task name:scrape_collection_page state:scheduled start_date:None
[2024-12-17T14:25:24.476+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline and current dag run status:running
[2024-12-17T14:25:24.477+0530] {logging_mixin.py:190} INFO - [2024-12-17T14:25:24.476+0530] {taskinstance.py:731} INFO - ::endgroup::
[2024-12-17T14:25:24.479+0530] {logging_mixin.py:190} INFO - 2024-12-17
[2024-12-17T14:25:24.479+0530] {logging_mixin.py:190} INFO - [2024-12-17 14:25:24,479] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T14:25:24.480+0530] {logging_mixin.py:190} INFO - [2024-12-17T14:25:24.479+0530] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T14:25:24.487+0530] {logging_mixin.py:190} INFO - [2024-12-17T14:25:24.487+0530] {taskinstance.py:340} INFO - ::group::Post task execution logs
[2024-12-17T14:25:24.488+0530] {logging_mixin.py:190} INFO - [2024-12-17T14:25:24.488+0530] {taskinstance.py:352} INFO - Marking task as SUCCESS. dag_id=movie_pipeline, task_id=scrape_collection_page, run_id=manual__2024-12-17T08:55:19.114722+00:00, execution_date=20241217T085519, start_date=, end_date=20241217T085524
[2024-12-17T14:25:24.497+0530] {logging_mixin.py:190} INFO - Task instance in success state
[2024-12-17T14:25:24.498+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: running
[2024-12-17T14:25:24.498+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline queued_at:None
[2024-12-17T14:25:24.498+0530] {logging_mixin.py:190} INFO - Task hostname:Jayeshs-MacBook-Air.local operator:PythonOperator
[2024-12-17T14:25:24.500+0530] {logging_mixin.py:190} INFO - [2024-12-17T14:25:24.499+0530] {dag.py:4410} INFO - [DAG TEST] end task task_id=scrape_collection_page map_index=-1
[2024-12-17T14:25:24.504+0530] {logging_mixin.py:190} INFO - [2024-12-17T14:25:24.504+0530] {dagrun.py:854} INFO - Marking run <DagRun movie_pipeline @ 2024-12-17 08:55:19.114722+00:00: manual__2024-12-17T08:55:19.114722+00:00, state:running, queued_at: None. externally triggered: False> successful
[2024-12-17T14:25:24.504+0530] {logging_mixin.py:190} INFO - Dag run in success state
[2024-12-17T14:25:24.504+0530] {logging_mixin.py:190} INFO - Dag run start:2024-12-17 08:55:19.114722+00:00 end:2024-12-17 08:55:24.504544+00:00
[2024-12-17T14:25:24.505+0530] {logging_mixin.py:190} INFO - [2024-12-17T14:25:24.505+0530] {dagrun.py:905} INFO - DagRun Finished: dag_id=movie_pipeline, execution_date=2024-12-17 08:55:19.114722+00:00, run_id=manual__2024-12-17T08:55:19.114722+00:00, run_start_date=2024-12-17 08:55:19.114722+00:00, run_end_date=2024-12-17 08:55:24.504544+00:00, run_duration=5.389822, state=success, external_trigger=False, run_type=manual, data_interval_start=2024-12-16 08:55:19.114722+00:00, data_interval_end=2024-12-17 08:55:19.114722+00:00, dag_hash=None
[2024-12-17T14:25:24.514+0530] {processor.py:925} INFO - DAG(s) 'movie_pipeline' retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T14:25:24.526+0530] {logging_mixin.py:190} INFO - [2024-12-17T14:25:24.525+0530] {dag.py:3239} INFO - Sync 1 DAGs
[2024-12-17T14:25:24.544+0530] {logging_mixin.py:190} INFO - [2024-12-17T14:25:24.543+0530] {dag.py:4180} INFO - Setting next_dagrun for movie_pipeline to 2024-12-17 01:30:00+00:00, run_after=2024-12-18 01:30:00+00:00
[2024-12-17T14:25:24.566+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 5.507 seconds
[2024-12-17T14:28:15.979+0530] {processor.py:186} INFO - Started process (PID=66710) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T14:28:15.980+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-17T14:28:15.981+0530] {logging_mixin.py:190} INFO - [2024-12-17T14:28:15.981+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T14:28:16.158+0530] {logging_mixin.py:190} INFO - [2024-12-17T14:28:16.158+0530] {dag.py:4435} INFO - dagrun id: movie_pipeline
[2024-12-17T14:28:16.176+0530] {logging_mixin.py:190} INFO - [2024-12-17T14:28:16.175+0530] {dag.py:4451} INFO - created dagrun <DagRun movie_pipeline @ 2024-12-17 08:58:16.022908+00:00: manual__2024-12-17T08:58:16.022908+00:00, state:running, queued_at: None. externally triggered: False>
[2024-12-17T14:28:16.209+0530] {logging_mixin.py:190} INFO - [2024-12-17T14:28:16.208+0530] {dag.py:4396} INFO - [DAG TEST] starting task_id=scrape_collection_page map_index=-1
[2024-12-17T14:28:16.209+0530] {logging_mixin.py:190} INFO - [2024-12-17T14:28:16.209+0530] {dag.py:4399} INFO - [DAG TEST] running task <TaskInstance: movie_pipeline.scrape_collection_page manual__2024-12-17T08:58:16.022908+00:00 [scheduled]>
[2024-12-17T14:28:21.360+0530] {logging_mixin.py:190} INFO - [2024-12-17T14:28:21.360+0530] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-12-17T14:28:21.426+0530] {logging_mixin.py:190} INFO - [2024-12-17 14:28:21,426] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T08:58:16.022908+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T08:58:16.022908+00:00'
[2024-12-17T14:28:21.427+0530] {logging_mixin.py:190} INFO - [2024-12-17T14:28:21.426+0530] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T08:58:16.022908+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T08:58:16.022908+00:00'
[2024-12-17T14:28:21.439+0530] {logging_mixin.py:190} INFO - Task instance is in running state
[2024-12-17T14:28:21.440+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: queued
[2024-12-17T14:28:21.440+0530] {logging_mixin.py:190} INFO - Current task name:scrape_collection_page state:scheduled start_date:None
[2024-12-17T14:28:21.441+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline and current dag run status:running
[2024-12-17T14:28:21.441+0530] {logging_mixin.py:190} INFO - [2024-12-17T14:28:21.441+0530] {taskinstance.py:731} INFO - ::endgroup::
[2024-12-17T14:28:21.443+0530] {logging_mixin.py:190} INFO - 2024-12-17
[2024-12-17T14:28:21.444+0530] {logging_mixin.py:190} INFO - [2024-12-17 14:28:21,444] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T14:28:21.444+0530] {logging_mixin.py:190} INFO - [2024-12-17T14:28:21.444+0530] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T14:28:21.450+0530] {logging_mixin.py:190} INFO - [2024-12-17T14:28:21.449+0530] {taskinstance.py:340} INFO - ::group::Post task execution logs
[2024-12-17T14:28:21.450+0530] {logging_mixin.py:190} INFO - [2024-12-17T14:28:21.450+0530] {taskinstance.py:352} INFO - Marking task as SUCCESS. dag_id=movie_pipeline, task_id=scrape_collection_page, run_id=manual__2024-12-17T08:58:16.022908+00:00, execution_date=20241217T085816, start_date=, end_date=20241217T085821
[2024-12-17T14:28:21.460+0530] {logging_mixin.py:190} INFO - Task instance in success state
[2024-12-17T14:28:21.461+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: running
[2024-12-17T14:28:21.461+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline queued_at:None
[2024-12-17T14:28:21.462+0530] {logging_mixin.py:190} INFO - Task hostname:Jayeshs-MacBook-Air.local operator:PythonOperator
[2024-12-17T14:28:21.462+0530] {logging_mixin.py:190} INFO - [2024-12-17T14:28:21.462+0530] {dag.py:4410} INFO - [DAG TEST] end task task_id=scrape_collection_page map_index=-1
[2024-12-17T14:28:21.468+0530] {logging_mixin.py:190} INFO - [2024-12-17T14:28:21.467+0530] {dagrun.py:854} INFO - Marking run <DagRun movie_pipeline @ 2024-12-17 08:58:16.022908+00:00: manual__2024-12-17T08:58:16.022908+00:00, state:running, queued_at: None. externally triggered: False> successful
[2024-12-17T14:28:21.468+0530] {logging_mixin.py:190} INFO - Dag run in success state
[2024-12-17T14:28:21.468+0530] {logging_mixin.py:190} INFO - Dag run start:2024-12-17 08:58:16.022908+00:00 end:2024-12-17 08:58:21.468424+00:00
[2024-12-17T14:28:21.469+0530] {logging_mixin.py:190} INFO - [2024-12-17T14:28:21.468+0530] {dagrun.py:905} INFO - DagRun Finished: dag_id=movie_pipeline, execution_date=2024-12-17 08:58:16.022908+00:00, run_id=manual__2024-12-17T08:58:16.022908+00:00, run_start_date=2024-12-17 08:58:16.022908+00:00, run_end_date=2024-12-17 08:58:21.468424+00:00, run_duration=5.445516, state=success, external_trigger=False, run_type=manual, data_interval_start=2024-12-16 08:58:16.022908+00:00, data_interval_end=2024-12-17 08:58:16.022908+00:00, dag_hash=None
[2024-12-17T14:28:21.477+0530] {processor.py:925} INFO - DAG(s) 'movie_pipeline' retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T14:28:21.490+0530] {logging_mixin.py:190} INFO - [2024-12-17T14:28:21.490+0530] {dag.py:3239} INFO - Sync 1 DAGs
[2024-12-17T14:28:21.507+0530] {logging_mixin.py:190} INFO - [2024-12-17T14:28:21.507+0530] {dag.py:4180} INFO - Setting next_dagrun for movie_pipeline to 2024-12-17 01:30:00+00:00, run_after=2024-12-18 01:30:00+00:00
[2024-12-17T14:28:21.525+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 5.550 seconds
[2024-12-17T14:31:15.362+0530] {processor.py:186} INFO - Started process (PID=66912) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T14:31:15.365+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-17T14:31:15.366+0530] {logging_mixin.py:190} INFO - [2024-12-17T14:31:15.366+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T14:31:15.540+0530] {logging_mixin.py:190} INFO - [2024-12-17T14:31:15.539+0530] {dag.py:4435} INFO - dagrun id: movie_pipeline
[2024-12-17T14:31:15.556+0530] {logging_mixin.py:190} INFO - [2024-12-17T14:31:15.555+0530] {dag.py:4451} INFO - created dagrun <DagRun movie_pipeline @ 2024-12-17 09:01:15.408610+00:00: manual__2024-12-17T09:01:15.408610+00:00, state:running, queued_at: None. externally triggered: False>
[2024-12-17T14:31:15.588+0530] {logging_mixin.py:190} INFO - [2024-12-17T14:31:15.587+0530] {dag.py:4396} INFO - [DAG TEST] starting task_id=scrape_collection_page map_index=-1
[2024-12-17T14:31:15.588+0530] {logging_mixin.py:190} INFO - [2024-12-17T14:31:15.588+0530] {dag.py:4399} INFO - [DAG TEST] running task <TaskInstance: movie_pipeline.scrape_collection_page manual__2024-12-17T09:01:15.408610+00:00 [scheduled]>
[2024-12-17T14:31:20.685+0530] {logging_mixin.py:190} INFO - [2024-12-17T14:31:20.685+0530] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-12-17T14:31:20.755+0530] {logging_mixin.py:190} INFO - [2024-12-17 14:31:20,755] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T09:01:15.408610+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T09:01:15.408610+00:00'
[2024-12-17T14:31:20.755+0530] {logging_mixin.py:190} INFO - [2024-12-17T14:31:20.755+0530] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T09:01:15.408610+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T09:01:15.408610+00:00'
[2024-12-17T14:31:20.768+0530] {logging_mixin.py:190} INFO - Task instance is in running state
[2024-12-17T14:31:20.768+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: queued
[2024-12-17T14:31:20.769+0530] {logging_mixin.py:190} INFO - Current task name:scrape_collection_page state:scheduled start_date:None
[2024-12-17T14:31:20.769+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline and current dag run status:running
[2024-12-17T14:31:20.769+0530] {logging_mixin.py:190} INFO - [2024-12-17T14:31:20.769+0530] {taskinstance.py:731} INFO - ::endgroup::
[2024-12-17T14:31:20.772+0530] {logging_mixin.py:190} INFO - 2024-12-17
[2024-12-17T14:31:20.772+0530] {logging_mixin.py:190} INFO - [2024-12-17 14:31:20,772] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T14:31:20.772+0530] {logging_mixin.py:190} INFO - [2024-12-17T14:31:20.772+0530] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T14:31:20.780+0530] {logging_mixin.py:190} INFO - [2024-12-17T14:31:20.780+0530] {taskinstance.py:340} INFO - ::group::Post task execution logs
[2024-12-17T14:31:20.780+0530] {logging_mixin.py:190} INFO - [2024-12-17T14:31:20.780+0530] {taskinstance.py:352} INFO - Marking task as SUCCESS. dag_id=movie_pipeline, task_id=scrape_collection_page, run_id=manual__2024-12-17T09:01:15.408610+00:00, execution_date=20241217T090115, start_date=, end_date=20241217T090120
[2024-12-17T14:31:20.790+0530] {logging_mixin.py:190} INFO - Task instance in success state
[2024-12-17T14:31:20.791+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: running
[2024-12-17T14:31:20.791+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline queued_at:None
[2024-12-17T14:31:20.791+0530] {logging_mixin.py:190} INFO - Task hostname:Jayeshs-MacBook-Air.local operator:PythonOperator
[2024-12-17T14:31:20.792+0530] {logging_mixin.py:190} INFO - [2024-12-17T14:31:20.792+0530] {dag.py:4410} INFO - [DAG TEST] end task task_id=scrape_collection_page map_index=-1
[2024-12-17T14:31:20.796+0530] {logging_mixin.py:190} INFO - [2024-12-17T14:31:20.795+0530] {dagrun.py:854} INFO - Marking run <DagRun movie_pipeline @ 2024-12-17 09:01:15.408610+00:00: manual__2024-12-17T09:01:15.408610+00:00, state:running, queued_at: None. externally triggered: False> successful
[2024-12-17T14:31:20.796+0530] {logging_mixin.py:190} INFO - Dag run in success state
[2024-12-17T14:31:20.796+0530] {logging_mixin.py:190} INFO - Dag run start:2024-12-17 09:01:15.408610+00:00 end:2024-12-17 09:01:20.796413+00:00
[2024-12-17T14:31:20.797+0530] {logging_mixin.py:190} INFO - [2024-12-17T14:31:20.796+0530] {dagrun.py:905} INFO - DagRun Finished: dag_id=movie_pipeline, execution_date=2024-12-17 09:01:15.408610+00:00, run_id=manual__2024-12-17T09:01:15.408610+00:00, run_start_date=2024-12-17 09:01:15.408610+00:00, run_end_date=2024-12-17 09:01:20.796413+00:00, run_duration=5.387803, state=success, external_trigger=False, run_type=manual, data_interval_start=2024-12-16 09:01:15.408610+00:00, data_interval_end=2024-12-17 09:01:15.408610+00:00, dag_hash=None
[2024-12-17T14:31:20.807+0530] {processor.py:925} INFO - DAG(s) 'movie_pipeline' retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T14:31:20.820+0530] {logging_mixin.py:190} INFO - [2024-12-17T14:31:20.819+0530] {dag.py:3239} INFO - Sync 1 DAGs
[2024-12-17T14:31:20.837+0530] {logging_mixin.py:190} INFO - [2024-12-17T14:31:20.837+0530] {dag.py:4180} INFO - Setting next_dagrun for movie_pipeline to 2024-12-17 01:30:00+00:00, run_after=2024-12-18 01:30:00+00:00
[2024-12-17T14:31:20.859+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 5.501 seconds
[2024-12-17T14:34:12.199+0530] {processor.py:186} INFO - Started process (PID=67128) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T14:34:12.202+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-17T14:34:12.203+0530] {logging_mixin.py:190} INFO - [2024-12-17T14:34:12.203+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T14:34:12.387+0530] {logging_mixin.py:190} INFO - [2024-12-17T14:34:12.387+0530] {dag.py:4435} INFO - dagrun id: movie_pipeline
[2024-12-17T14:34:12.404+0530] {logging_mixin.py:190} INFO - [2024-12-17T14:34:12.403+0530] {dag.py:4451} INFO - created dagrun <DagRun movie_pipeline @ 2024-12-17 09:04:12.249338+00:00: manual__2024-12-17T09:04:12.249338+00:00, state:running, queued_at: None. externally triggered: False>
[2024-12-17T14:34:12.434+0530] {logging_mixin.py:190} INFO - [2024-12-17T14:34:12.434+0530] {dag.py:4396} INFO - [DAG TEST] starting task_id=scrape_collection_page map_index=-1
[2024-12-17T14:34:12.434+0530] {logging_mixin.py:190} INFO - [2024-12-17T14:34:12.434+0530] {dag.py:4399} INFO - [DAG TEST] running task <TaskInstance: movie_pipeline.scrape_collection_page manual__2024-12-17T09:04:12.249338+00:00 [scheduled]>
[2024-12-17T14:34:17.544+0530] {logging_mixin.py:190} INFO - [2024-12-17T14:34:17.543+0530] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-12-17T14:34:17.617+0530] {logging_mixin.py:190} INFO - [2024-12-17 14:34:17,617] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T09:04:12.249338+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T09:04:12.249338+00:00'
[2024-12-17T14:34:17.617+0530] {logging_mixin.py:190} INFO - [2024-12-17T14:34:17.617+0530] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T09:04:12.249338+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T09:04:12.249338+00:00'
[2024-12-17T14:34:17.632+0530] {logging_mixin.py:190} INFO - Task instance is in running state
[2024-12-17T14:34:17.632+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: queued
[2024-12-17T14:34:17.632+0530] {logging_mixin.py:190} INFO - Current task name:scrape_collection_page state:scheduled start_date:None
[2024-12-17T14:34:17.633+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline and current dag run status:running
[2024-12-17T14:34:17.633+0530] {logging_mixin.py:190} INFO - [2024-12-17T14:34:17.633+0530] {taskinstance.py:731} INFO - ::endgroup::
[2024-12-17T14:34:17.637+0530] {logging_mixin.py:190} INFO - 2024-12-17
[2024-12-17T14:34:17.638+0530] {logging_mixin.py:190} INFO - [2024-12-17 14:34:17,637] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T14:34:17.638+0530] {logging_mixin.py:190} INFO - [2024-12-17T14:34:17.637+0530] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T14:34:17.644+0530] {logging_mixin.py:190} INFO - [2024-12-17T14:34:17.643+0530] {taskinstance.py:340} INFO - ::group::Post task execution logs
[2024-12-17T14:34:17.644+0530] {logging_mixin.py:190} INFO - [2024-12-17T14:34:17.644+0530] {taskinstance.py:352} INFO - Marking task as SUCCESS. dag_id=movie_pipeline, task_id=scrape_collection_page, run_id=manual__2024-12-17T09:04:12.249338+00:00, execution_date=20241217T090412, start_date=, end_date=20241217T090417
[2024-12-17T14:34:17.655+0530] {logging_mixin.py:190} INFO - Task instance in success state
[2024-12-17T14:34:17.655+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: running
[2024-12-17T14:34:17.655+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline queued_at:None
[2024-12-17T14:34:17.655+0530] {logging_mixin.py:190} INFO - Task hostname:Jayeshs-MacBook-Air.local operator:PythonOperator
[2024-12-17T14:34:17.656+0530] {logging_mixin.py:190} INFO - [2024-12-17T14:34:17.656+0530] {dag.py:4410} INFO - [DAG TEST] end task task_id=scrape_collection_page map_index=-1
[2024-12-17T14:34:17.660+0530] {logging_mixin.py:190} INFO - [2024-12-17T14:34:17.659+0530] {dagrun.py:854} INFO - Marking run <DagRun movie_pipeline @ 2024-12-17 09:04:12.249338+00:00: manual__2024-12-17T09:04:12.249338+00:00, state:running, queued_at: None. externally triggered: False> successful
[2024-12-17T14:34:17.660+0530] {logging_mixin.py:190} INFO - Dag run in success state
[2024-12-17T14:34:17.660+0530] {logging_mixin.py:190} INFO - Dag run start:2024-12-17 09:04:12.249338+00:00 end:2024-12-17 09:04:17.660454+00:00
[2024-12-17T14:34:17.661+0530] {logging_mixin.py:190} INFO - [2024-12-17T14:34:17.661+0530] {dagrun.py:905} INFO - DagRun Finished: dag_id=movie_pipeline, execution_date=2024-12-17 09:04:12.249338+00:00, run_id=manual__2024-12-17T09:04:12.249338+00:00, run_start_date=2024-12-17 09:04:12.249338+00:00, run_end_date=2024-12-17 09:04:17.660454+00:00, run_duration=5.411116, state=success, external_trigger=False, run_type=manual, data_interval_start=2024-12-16 09:04:12.249338+00:00, data_interval_end=2024-12-17 09:04:12.249338+00:00, dag_hash=None
[2024-12-17T14:34:17.671+0530] {processor.py:925} INFO - DAG(s) 'movie_pipeline' retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T14:34:17.682+0530] {logging_mixin.py:190} INFO - [2024-12-17T14:34:17.682+0530] {dag.py:3239} INFO - Sync 1 DAGs
[2024-12-17T14:34:17.701+0530] {logging_mixin.py:190} INFO - [2024-12-17T14:34:17.701+0530] {dag.py:4180} INFO - Setting next_dagrun for movie_pipeline to 2024-12-17 01:30:00+00:00, run_after=2024-12-18 01:30:00+00:00
[2024-12-17T14:34:17.723+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 5.528 seconds
[2024-12-17T14:37:11.579+0530] {processor.py:186} INFO - Started process (PID=67323) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T14:37:11.581+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-17T14:37:11.582+0530] {logging_mixin.py:190} INFO - [2024-12-17T14:37:11.582+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T14:37:11.757+0530] {logging_mixin.py:190} INFO - [2024-12-17T14:37:11.757+0530] {dag.py:4435} INFO - dagrun id: movie_pipeline
[2024-12-17T14:37:11.771+0530] {logging_mixin.py:190} INFO - [2024-12-17T14:37:11.771+0530] {dag.py:4451} INFO - created dagrun <DagRun movie_pipeline @ 2024-12-17 09:07:11.621980+00:00: manual__2024-12-17T09:07:11.621980+00:00, state:running, queued_at: None. externally triggered: False>
[2024-12-17T14:37:11.803+0530] {logging_mixin.py:190} INFO - [2024-12-17T14:37:11.803+0530] {dag.py:4396} INFO - [DAG TEST] starting task_id=scrape_collection_page map_index=-1
[2024-12-17T14:37:11.804+0530] {logging_mixin.py:190} INFO - [2024-12-17T14:37:11.803+0530] {dag.py:4399} INFO - [DAG TEST] running task <TaskInstance: movie_pipeline.scrape_collection_page manual__2024-12-17T09:07:11.621980+00:00 [scheduled]>
[2024-12-17T14:37:16.902+0530] {logging_mixin.py:190} INFO - [2024-12-17T14:37:16.902+0530] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-12-17T14:37:16.964+0530] {logging_mixin.py:190} INFO - [2024-12-17 14:37:16,964] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T09:07:11.621980+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T09:07:11.621980+00:00'
[2024-12-17T14:37:16.965+0530] {logging_mixin.py:190} INFO - [2024-12-17T14:37:16.964+0530] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T09:07:11.621980+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T09:07:11.621980+00:00'
[2024-12-17T14:37:16.968+0530] {logging_mixin.py:190} INFO - Task instance is in running state
[2024-12-17T14:37:16.969+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: queued
[2024-12-17T14:37:16.969+0530] {logging_mixin.py:190} INFO - Current task name:scrape_collection_page state:scheduled start_date:None
[2024-12-17T14:37:16.970+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline and current dag run status:running
[2024-12-17T14:37:16.970+0530] {logging_mixin.py:190} INFO - [2024-12-17T14:37:16.970+0530] {taskinstance.py:731} INFO - ::endgroup::
[2024-12-17T14:37:16.972+0530] {logging_mixin.py:190} INFO - 2024-12-17
[2024-12-17T14:37:16.973+0530] {logging_mixin.py:190} INFO - [2024-12-17 14:37:16,973] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T14:37:16.973+0530] {logging_mixin.py:190} INFO - [2024-12-17T14:37:16.973+0530] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T14:37:16.979+0530] {logging_mixin.py:190} INFO - [2024-12-17T14:37:16.978+0530] {taskinstance.py:340} INFO - ::group::Post task execution logs
[2024-12-17T14:37:16.979+0530] {logging_mixin.py:190} INFO - [2024-12-17T14:37:16.979+0530] {taskinstance.py:352} INFO - Marking task as SUCCESS. dag_id=movie_pipeline, task_id=scrape_collection_page, run_id=manual__2024-12-17T09:07:11.621980+00:00, execution_date=20241217T090711, start_date=, end_date=20241217T090716
[2024-12-17T14:37:16.989+0530] {logging_mixin.py:190} INFO - Task instance in success state
[2024-12-17T14:37:16.989+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: running
[2024-12-17T14:37:16.990+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline queued_at:None
[2024-12-17T14:37:16.990+0530] {logging_mixin.py:190} INFO - Task hostname:Jayeshs-MacBook-Air.local operator:PythonOperator
[2024-12-17T14:37:16.990+0530] {logging_mixin.py:190} INFO - [2024-12-17T14:37:16.990+0530] {dag.py:4410} INFO - [DAG TEST] end task task_id=scrape_collection_page map_index=-1
[2024-12-17T14:37:16.994+0530] {logging_mixin.py:190} INFO - [2024-12-17T14:37:16.993+0530] {dagrun.py:854} INFO - Marking run <DagRun movie_pipeline @ 2024-12-17 09:07:11.621980+00:00: manual__2024-12-17T09:07:11.621980+00:00, state:running, queued_at: None. externally triggered: False> successful
[2024-12-17T14:37:16.994+0530] {logging_mixin.py:190} INFO - Dag run in success state
[2024-12-17T14:37:16.994+0530] {logging_mixin.py:190} INFO - Dag run start:2024-12-17 09:07:11.621980+00:00 end:2024-12-17 09:07:16.994436+00:00
[2024-12-17T14:37:16.995+0530] {logging_mixin.py:190} INFO - [2024-12-17T14:37:16.994+0530] {dagrun.py:905} INFO - DagRun Finished: dag_id=movie_pipeline, execution_date=2024-12-17 09:07:11.621980+00:00, run_id=manual__2024-12-17T09:07:11.621980+00:00, run_start_date=2024-12-17 09:07:11.621980+00:00, run_end_date=2024-12-17 09:07:16.994436+00:00, run_duration=5.372456, state=success, external_trigger=False, run_type=manual, data_interval_start=2024-12-16 09:07:11.621980+00:00, data_interval_end=2024-12-17 09:07:11.621980+00:00, dag_hash=None
[2024-12-17T14:37:17.004+0530] {processor.py:925} INFO - DAG(s) 'movie_pipeline' retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T14:37:17.018+0530] {logging_mixin.py:190} INFO - [2024-12-17T14:37:17.018+0530] {dag.py:3239} INFO - Sync 1 DAGs
[2024-12-17T14:37:17.037+0530] {logging_mixin.py:190} INFO - [2024-12-17T14:37:17.037+0530] {dag.py:4180} INFO - Setting next_dagrun for movie_pipeline to 2024-12-17 01:30:00+00:00, run_after=2024-12-18 01:30:00+00:00
[2024-12-17T14:37:17.056+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 5.480 seconds
[2024-12-17T14:40:07.347+0530] {processor.py:186} INFO - Started process (PID=67528) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T14:40:07.348+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-17T14:40:07.349+0530] {logging_mixin.py:190} INFO - [2024-12-17T14:40:07.349+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T14:40:07.525+0530] {logging_mixin.py:190} INFO - [2024-12-17T14:40:07.524+0530] {dag.py:4435} INFO - dagrun id: movie_pipeline
[2024-12-17T14:40:07.539+0530] {logging_mixin.py:190} INFO - [2024-12-17T14:40:07.538+0530] {dag.py:4451} INFO - created dagrun <DagRun movie_pipeline @ 2024-12-17 09:10:07.390216+00:00: manual__2024-12-17T09:10:07.390216+00:00, state:running, queued_at: None. externally triggered: False>
[2024-12-17T14:40:07.565+0530] {logging_mixin.py:190} INFO - [2024-12-17T14:40:07.565+0530] {dag.py:4396} INFO - [DAG TEST] starting task_id=scrape_collection_page map_index=-1
[2024-12-17T14:40:07.566+0530] {logging_mixin.py:190} INFO - [2024-12-17T14:40:07.566+0530] {dag.py:4399} INFO - [DAG TEST] running task <TaskInstance: movie_pipeline.scrape_collection_page manual__2024-12-17T09:10:07.390216+00:00 [scheduled]>
[2024-12-17T14:40:12.664+0530] {logging_mixin.py:190} INFO - [2024-12-17T14:40:12.664+0530] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-12-17T14:40:12.726+0530] {logging_mixin.py:190} INFO - [2024-12-17 14:40:12,725] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T09:10:07.390216+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T09:10:07.390216+00:00'
[2024-12-17T14:40:12.726+0530] {logging_mixin.py:190} INFO - [2024-12-17T14:40:12.725+0530] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T09:10:07.390216+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T09:10:07.390216+00:00'
[2024-12-17T14:40:12.729+0530] {logging_mixin.py:190} INFO - Task instance is in running state
[2024-12-17T14:40:12.729+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: queued
[2024-12-17T14:40:12.729+0530] {logging_mixin.py:190} INFO - Current task name:scrape_collection_page state:scheduled start_date:None
[2024-12-17T14:40:12.730+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline and current dag run status:running
[2024-12-17T14:40:12.730+0530] {logging_mixin.py:190} INFO - [2024-12-17T14:40:12.730+0530] {taskinstance.py:731} INFO - ::endgroup::
[2024-12-17T14:40:12.732+0530] {logging_mixin.py:190} INFO - 2024-12-17
[2024-12-17T14:40:12.732+0530] {logging_mixin.py:190} INFO - [2024-12-17 14:40:12,732] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T14:40:12.732+0530] {logging_mixin.py:190} INFO - [2024-12-17T14:40:12.732+0530] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T14:40:12.738+0530] {logging_mixin.py:190} INFO - [2024-12-17T14:40:12.738+0530] {taskinstance.py:340} INFO - ::group::Post task execution logs
[2024-12-17T14:40:12.739+0530] {logging_mixin.py:190} INFO - [2024-12-17T14:40:12.739+0530] {taskinstance.py:352} INFO - Marking task as SUCCESS. dag_id=movie_pipeline, task_id=scrape_collection_page, run_id=manual__2024-12-17T09:10:07.390216+00:00, execution_date=20241217T091007, start_date=, end_date=20241217T091012
[2024-12-17T14:40:12.748+0530] {logging_mixin.py:190} INFO - Task instance in success state
[2024-12-17T14:40:12.748+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: running
[2024-12-17T14:40:12.749+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline queued_at:None
[2024-12-17T14:40:12.749+0530] {logging_mixin.py:190} INFO - Task hostname:Jayeshs-MacBook-Air.local operator:PythonOperator
[2024-12-17T14:40:12.749+0530] {logging_mixin.py:190} INFO - [2024-12-17T14:40:12.749+0530] {dag.py:4410} INFO - [DAG TEST] end task task_id=scrape_collection_page map_index=-1
[2024-12-17T14:40:12.754+0530] {logging_mixin.py:190} INFO - [2024-12-17T14:40:12.753+0530] {dagrun.py:854} INFO - Marking run <DagRun movie_pipeline @ 2024-12-17 09:10:07.390216+00:00: manual__2024-12-17T09:10:07.390216+00:00, state:running, queued_at: None. externally triggered: False> successful
[2024-12-17T14:40:12.754+0530] {logging_mixin.py:190} INFO - Dag run in success state
[2024-12-17T14:40:12.754+0530] {logging_mixin.py:190} INFO - Dag run start:2024-12-17 09:10:07.390216+00:00 end:2024-12-17 09:10:12.754380+00:00
[2024-12-17T14:40:12.755+0530] {logging_mixin.py:190} INFO - [2024-12-17T14:40:12.754+0530] {dagrun.py:905} INFO - DagRun Finished: dag_id=movie_pipeline, execution_date=2024-12-17 09:10:07.390216+00:00, run_id=manual__2024-12-17T09:10:07.390216+00:00, run_start_date=2024-12-17 09:10:07.390216+00:00, run_end_date=2024-12-17 09:10:12.754380+00:00, run_duration=5.364164, state=success, external_trigger=False, run_type=manual, data_interval_start=2024-12-16 09:10:07.390216+00:00, data_interval_end=2024-12-17 09:10:07.390216+00:00, dag_hash=None
[2024-12-17T14:40:12.764+0530] {processor.py:925} INFO - DAG(s) 'movie_pipeline' retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T14:40:12.776+0530] {logging_mixin.py:190} INFO - [2024-12-17T14:40:12.776+0530] {dag.py:3239} INFO - Sync 1 DAGs
[2024-12-17T14:40:12.795+0530] {logging_mixin.py:190} INFO - [2024-12-17T14:40:12.795+0530] {dag.py:4180} INFO - Setting next_dagrun for movie_pipeline to 2024-12-17 01:30:00+00:00, run_after=2024-12-18 01:30:00+00:00
[2024-12-17T14:40:12.815+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 5.471 seconds
[2024-12-17T14:43:03.098+0530] {processor.py:186} INFO - Started process (PID=67723) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T14:43:03.100+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-17T14:43:03.101+0530] {logging_mixin.py:190} INFO - [2024-12-17T14:43:03.101+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T14:43:03.277+0530] {logging_mixin.py:190} INFO - [2024-12-17T14:43:03.277+0530] {dag.py:4435} INFO - dagrun id: movie_pipeline
[2024-12-17T14:43:03.292+0530] {logging_mixin.py:190} INFO - [2024-12-17T14:43:03.291+0530] {dag.py:4451} INFO - created dagrun <DagRun movie_pipeline @ 2024-12-17 09:13:03.143164+00:00: manual__2024-12-17T09:13:03.143164+00:00, state:running, queued_at: None. externally triggered: False>
[2024-12-17T14:43:03.317+0530] {logging_mixin.py:190} INFO - [2024-12-17T14:43:03.317+0530] {dag.py:4396} INFO - [DAG TEST] starting task_id=scrape_collection_page map_index=-1
[2024-12-17T14:43:03.318+0530] {logging_mixin.py:190} INFO - [2024-12-17T14:43:03.317+0530] {dag.py:4399} INFO - [DAG TEST] running task <TaskInstance: movie_pipeline.scrape_collection_page manual__2024-12-17T09:13:03.143164+00:00 [scheduled]>
[2024-12-17T14:43:08.422+0530] {logging_mixin.py:190} INFO - [2024-12-17T14:43:08.421+0530] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-12-17T14:43:08.484+0530] {logging_mixin.py:190} INFO - [2024-12-17 14:43:08,484] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T09:13:03.143164+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T09:13:03.143164+00:00'
[2024-12-17T14:43:08.485+0530] {logging_mixin.py:190} INFO - [2024-12-17T14:43:08.484+0530] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T09:13:03.143164+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T09:13:03.143164+00:00'
[2024-12-17T14:43:08.487+0530] {logging_mixin.py:190} INFO - Task instance is in running state
[2024-12-17T14:43:08.488+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: queued
[2024-12-17T14:43:08.488+0530] {logging_mixin.py:190} INFO - Current task name:scrape_collection_page state:scheduled start_date:None
[2024-12-17T14:43:08.489+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline and current dag run status:running
[2024-12-17T14:43:08.489+0530] {logging_mixin.py:190} INFO - [2024-12-17T14:43:08.489+0530] {taskinstance.py:731} INFO - ::endgroup::
[2024-12-17T14:43:08.491+0530] {logging_mixin.py:190} INFO - 2024-12-17
[2024-12-17T14:43:08.492+0530] {logging_mixin.py:190} INFO - [2024-12-17 14:43:08,491] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T14:43:08.492+0530] {logging_mixin.py:190} INFO - [2024-12-17T14:43:08.491+0530] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T14:43:08.497+0530] {logging_mixin.py:190} INFO - [2024-12-17T14:43:08.497+0530] {taskinstance.py:340} INFO - ::group::Post task execution logs
[2024-12-17T14:43:08.498+0530] {logging_mixin.py:190} INFO - [2024-12-17T14:43:08.498+0530] {taskinstance.py:352} INFO - Marking task as SUCCESS. dag_id=movie_pipeline, task_id=scrape_collection_page, run_id=manual__2024-12-17T09:13:03.143164+00:00, execution_date=20241217T091303, start_date=, end_date=20241217T091308
[2024-12-17T14:43:08.508+0530] {logging_mixin.py:190} INFO - Task instance in success state
[2024-12-17T14:43:08.509+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: running
[2024-12-17T14:43:08.509+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline queued_at:None
[2024-12-17T14:43:08.509+0530] {logging_mixin.py:190} INFO - Task hostname:Jayeshs-MacBook-Air.local operator:PythonOperator
[2024-12-17T14:43:08.510+0530] {logging_mixin.py:190} INFO - [2024-12-17T14:43:08.509+0530] {dag.py:4410} INFO - [DAG TEST] end task task_id=scrape_collection_page map_index=-1
[2024-12-17T14:43:08.514+0530] {logging_mixin.py:190} INFO - [2024-12-17T14:43:08.514+0530] {dagrun.py:854} INFO - Marking run <DagRun movie_pipeline @ 2024-12-17 09:13:03.143164+00:00: manual__2024-12-17T09:13:03.143164+00:00, state:running, queued_at: None. externally triggered: False> successful
[2024-12-17T14:43:08.514+0530] {logging_mixin.py:190} INFO - Dag run in success state
[2024-12-17T14:43:08.515+0530] {logging_mixin.py:190} INFO - Dag run start:2024-12-17 09:13:03.143164+00:00 end:2024-12-17 09:13:08.514804+00:00
[2024-12-17T14:43:08.515+0530] {logging_mixin.py:190} INFO - [2024-12-17T14:43:08.515+0530] {dagrun.py:905} INFO - DagRun Finished: dag_id=movie_pipeline, execution_date=2024-12-17 09:13:03.143164+00:00, run_id=manual__2024-12-17T09:13:03.143164+00:00, run_start_date=2024-12-17 09:13:03.143164+00:00, run_end_date=2024-12-17 09:13:08.514804+00:00, run_duration=5.37164, state=success, external_trigger=False, run_type=manual, data_interval_start=2024-12-16 09:13:03.143164+00:00, data_interval_end=2024-12-17 09:13:03.143164+00:00, dag_hash=None
[2024-12-17T14:43:08.525+0530] {processor.py:925} INFO - DAG(s) 'movie_pipeline' retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T14:43:08.537+0530] {logging_mixin.py:190} INFO - [2024-12-17T14:43:08.537+0530] {dag.py:3239} INFO - Sync 1 DAGs
[2024-12-17T14:43:08.555+0530] {logging_mixin.py:190} INFO - [2024-12-17T14:43:08.555+0530] {dag.py:4180} INFO - Setting next_dagrun for movie_pipeline to 2024-12-17 01:30:00+00:00, run_after=2024-12-18 01:30:00+00:00
[2024-12-17T14:43:08.576+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 5.482 seconds
[2024-12-17T14:45:57.899+0530] {processor.py:186} INFO - Started process (PID=67929) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T14:45:57.902+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-17T14:45:57.903+0530] {logging_mixin.py:190} INFO - [2024-12-17T14:45:57.903+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T14:45:58.076+0530] {logging_mixin.py:190} INFO - [2024-12-17T14:45:58.076+0530] {dag.py:4435} INFO - dagrun id: movie_pipeline
[2024-12-17T14:45:58.090+0530] {logging_mixin.py:190} INFO - [2024-12-17T14:45:58.090+0530] {dag.py:4451} INFO - created dagrun <DagRun movie_pipeline @ 2024-12-17 09:15:57.943025+00:00: manual__2024-12-17T09:15:57.943025+00:00, state:running, queued_at: None. externally triggered: False>
[2024-12-17T14:45:58.118+0530] {logging_mixin.py:190} INFO - [2024-12-17T14:45:58.117+0530] {dag.py:4396} INFO - [DAG TEST] starting task_id=scrape_collection_page map_index=-1
[2024-12-17T14:45:58.118+0530] {logging_mixin.py:190} INFO - [2024-12-17T14:45:58.118+0530] {dag.py:4399} INFO - [DAG TEST] running task <TaskInstance: movie_pipeline.scrape_collection_page manual__2024-12-17T09:15:57.943025+00:00 [scheduled]>
[2024-12-17T14:46:03.217+0530] {logging_mixin.py:190} INFO - [2024-12-17T14:46:03.217+0530] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-12-17T14:46:03.279+0530] {logging_mixin.py:190} INFO - [2024-12-17 14:46:03,279] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T09:15:57.943025+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T09:15:57.943025+00:00'
[2024-12-17T14:46:03.280+0530] {logging_mixin.py:190} INFO - [2024-12-17T14:46:03.279+0530] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T09:15:57.943025+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T09:15:57.943025+00:00'
[2024-12-17T14:46:03.283+0530] {logging_mixin.py:190} INFO - Task instance is in running state
[2024-12-17T14:46:03.284+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: queued
[2024-12-17T14:46:03.284+0530] {logging_mixin.py:190} INFO - Current task name:scrape_collection_page state:scheduled start_date:None
[2024-12-17T14:46:03.284+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline and current dag run status:running
[2024-12-17T14:46:03.285+0530] {logging_mixin.py:190} INFO - [2024-12-17T14:46:03.285+0530] {taskinstance.py:731} INFO - ::endgroup::
[2024-12-17T14:46:03.286+0530] {logging_mixin.py:190} INFO - 2024-12-17
[2024-12-17T14:46:03.287+0530] {logging_mixin.py:190} INFO - [2024-12-17 14:46:03,287] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T14:46:03.287+0530] {logging_mixin.py:190} INFO - [2024-12-17T14:46:03.287+0530] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T14:46:03.293+0530] {logging_mixin.py:190} INFO - [2024-12-17T14:46:03.292+0530] {taskinstance.py:340} INFO - ::group::Post task execution logs
[2024-12-17T14:46:03.293+0530] {logging_mixin.py:190} INFO - [2024-12-17T14:46:03.293+0530] {taskinstance.py:352} INFO - Marking task as SUCCESS. dag_id=movie_pipeline, task_id=scrape_collection_page, run_id=manual__2024-12-17T09:15:57.943025+00:00, execution_date=20241217T091557, start_date=, end_date=20241217T091603
[2024-12-17T14:46:03.303+0530] {logging_mixin.py:190} INFO - Task instance in success state
[2024-12-17T14:46:03.303+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: running
[2024-12-17T14:46:03.303+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline queued_at:None
[2024-12-17T14:46:03.304+0530] {logging_mixin.py:190} INFO - Task hostname:Jayeshs-MacBook-Air.local operator:PythonOperator
[2024-12-17T14:46:03.304+0530] {logging_mixin.py:190} INFO - [2024-12-17T14:46:03.304+0530] {dag.py:4410} INFO - [DAG TEST] end task task_id=scrape_collection_page map_index=-1
[2024-12-17T14:46:03.308+0530] {logging_mixin.py:190} INFO - [2024-12-17T14:46:03.307+0530] {dagrun.py:854} INFO - Marking run <DagRun movie_pipeline @ 2024-12-17 09:15:57.943025+00:00: manual__2024-12-17T09:15:57.943025+00:00, state:running, queued_at: None. externally triggered: False> successful
[2024-12-17T14:46:03.308+0530] {logging_mixin.py:190} INFO - Dag run in success state
[2024-12-17T14:46:03.308+0530] {logging_mixin.py:190} INFO - Dag run start:2024-12-17 09:15:57.943025+00:00 end:2024-12-17 09:16:03.308457+00:00
[2024-12-17T14:46:03.309+0530] {logging_mixin.py:190} INFO - [2024-12-17T14:46:03.308+0530] {dagrun.py:905} INFO - DagRun Finished: dag_id=movie_pipeline, execution_date=2024-12-17 09:15:57.943025+00:00, run_id=manual__2024-12-17T09:15:57.943025+00:00, run_start_date=2024-12-17 09:15:57.943025+00:00, run_end_date=2024-12-17 09:16:03.308457+00:00, run_duration=5.365432, state=success, external_trigger=False, run_type=manual, data_interval_start=2024-12-16 09:15:57.943025+00:00, data_interval_end=2024-12-17 09:15:57.943025+00:00, dag_hash=None
[2024-12-17T14:46:03.317+0530] {processor.py:925} INFO - DAG(s) 'movie_pipeline' retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T14:46:03.329+0530] {logging_mixin.py:190} INFO - [2024-12-17T14:46:03.329+0530] {dag.py:3239} INFO - Sync 1 DAGs
[2024-12-17T14:46:03.348+0530] {logging_mixin.py:190} INFO - [2024-12-17T14:46:03.348+0530] {dag.py:4180} INFO - Setting next_dagrun for movie_pipeline to 2024-12-17 01:30:00+00:00, run_after=2024-12-18 01:30:00+00:00
[2024-12-17T14:46:03.368+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 5.473 seconds
[2024-12-17T14:48:53.560+0530] {processor.py:186} INFO - Started process (PID=68133) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T14:48:53.561+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-17T14:48:53.563+0530] {logging_mixin.py:190} INFO - [2024-12-17T14:48:53.562+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T14:48:53.756+0530] {logging_mixin.py:190} INFO - [2024-12-17T14:48:53.756+0530] {dag.py:4435} INFO - dagrun id: movie_pipeline
[2024-12-17T14:48:53.777+0530] {logging_mixin.py:190} INFO - [2024-12-17T14:48:53.776+0530] {dag.py:4451} INFO - created dagrun <DagRun movie_pipeline @ 2024-12-17 09:18:53.604557+00:00: manual__2024-12-17T09:18:53.604557+00:00, state:running, queued_at: None. externally triggered: False>
[2024-12-17T14:48:53.813+0530] {logging_mixin.py:190} INFO - [2024-12-17T14:48:53.812+0530] {dag.py:4396} INFO - [DAG TEST] starting task_id=scrape_collection_page map_index=-1
[2024-12-17T14:48:53.813+0530] {logging_mixin.py:190} INFO - [2024-12-17T14:48:53.813+0530] {dag.py:4399} INFO - [DAG TEST] running task <TaskInstance: movie_pipeline.scrape_collection_page manual__2024-12-17T09:18:53.604557+00:00 [scheduled]>
[2024-12-17T14:48:58.932+0530] {logging_mixin.py:190} INFO - [2024-12-17T14:48:58.931+0530] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-12-17T14:48:59.005+0530] {logging_mixin.py:190} INFO - [2024-12-17 14:48:59,005] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T09:18:53.604557+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T09:18:53.604557+00:00'
[2024-12-17T14:48:59.006+0530] {logging_mixin.py:190} INFO - [2024-12-17T14:48:59.005+0530] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T09:18:53.604557+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T09:18:53.604557+00:00'
[2024-12-17T14:48:59.018+0530] {logging_mixin.py:190} INFO - Task instance is in running state
[2024-12-17T14:48:59.019+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: queued
[2024-12-17T14:48:59.019+0530] {logging_mixin.py:190} INFO - Current task name:scrape_collection_page state:scheduled start_date:None
[2024-12-17T14:48:59.019+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline and current dag run status:running
[2024-12-17T14:48:59.020+0530] {logging_mixin.py:190} INFO - [2024-12-17T14:48:59.020+0530] {taskinstance.py:731} INFO - ::endgroup::
[2024-12-17T14:48:59.025+0530] {logging_mixin.py:190} INFO - 2024-12-17
[2024-12-17T14:48:59.026+0530] {logging_mixin.py:190} INFO - [2024-12-17 14:48:59,025] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T14:48:59.026+0530] {logging_mixin.py:190} INFO - [2024-12-17T14:48:59.025+0530] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T14:48:59.034+0530] {logging_mixin.py:190} INFO - [2024-12-17T14:48:59.033+0530] {taskinstance.py:340} INFO - ::group::Post task execution logs
[2024-12-17T14:48:59.034+0530] {logging_mixin.py:190} INFO - [2024-12-17T14:48:59.034+0530] {taskinstance.py:352} INFO - Marking task as SUCCESS. dag_id=movie_pipeline, task_id=scrape_collection_page, run_id=manual__2024-12-17T09:18:53.604557+00:00, execution_date=20241217T091853, start_date=, end_date=20241217T091859
[2024-12-17T14:48:59.043+0530] {logging_mixin.py:190} INFO - Task instance in success state
[2024-12-17T14:48:59.044+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: running
[2024-12-17T14:48:59.044+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline queued_at:None
[2024-12-17T14:48:59.044+0530] {logging_mixin.py:190} INFO - Task hostname:Jayeshs-MacBook-Air.local operator:PythonOperator
[2024-12-17T14:48:59.045+0530] {logging_mixin.py:190} INFO - [2024-12-17T14:48:59.044+0530] {dag.py:4410} INFO - [DAG TEST] end task task_id=scrape_collection_page map_index=-1
[2024-12-17T14:48:59.048+0530] {logging_mixin.py:190} INFO - [2024-12-17T14:48:59.048+0530] {dagrun.py:854} INFO - Marking run <DagRun movie_pipeline @ 2024-12-17 09:18:53.604557+00:00: manual__2024-12-17T09:18:53.604557+00:00, state:running, queued_at: None. externally triggered: False> successful
[2024-12-17T14:48:59.049+0530] {logging_mixin.py:190} INFO - Dag run in success state
[2024-12-17T14:48:59.049+0530] {logging_mixin.py:190} INFO - Dag run start:2024-12-17 09:18:53.604557+00:00 end:2024-12-17 09:18:59.049187+00:00
[2024-12-17T14:48:59.050+0530] {logging_mixin.py:190} INFO - [2024-12-17T14:48:59.050+0530] {dagrun.py:905} INFO - DagRun Finished: dag_id=movie_pipeline, execution_date=2024-12-17 09:18:53.604557+00:00, run_id=manual__2024-12-17T09:18:53.604557+00:00, run_start_date=2024-12-17 09:18:53.604557+00:00, run_end_date=2024-12-17 09:18:59.049187+00:00, run_duration=5.44463, state=success, external_trigger=False, run_type=manual, data_interval_start=2024-12-16 09:18:53.604557+00:00, data_interval_end=2024-12-17 09:18:53.604557+00:00, dag_hash=None
[2024-12-17T14:48:59.060+0530] {processor.py:925} INFO - DAG(s) 'movie_pipeline' retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T14:48:59.074+0530] {logging_mixin.py:190} INFO - [2024-12-17T14:48:59.073+0530] {dag.py:3239} INFO - Sync 1 DAGs
[2024-12-17T14:48:59.092+0530] {logging_mixin.py:190} INFO - [2024-12-17T14:48:59.091+0530] {dag.py:4180} INFO - Setting next_dagrun for movie_pipeline to 2024-12-17 01:30:00+00:00, run_after=2024-12-18 01:30:00+00:00
[2024-12-17T14:48:59.110+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 5.553 seconds
[2024-12-17T14:51:49.750+0530] {processor.py:186} INFO - Started process (PID=68333) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T14:51:49.751+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-17T14:51:49.752+0530] {logging_mixin.py:190} INFO - [2024-12-17T14:51:49.752+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T14:51:49.935+0530] {logging_mixin.py:190} INFO - [2024-12-17T14:51:49.934+0530] {dag.py:4435} INFO - dagrun id: movie_pipeline
[2024-12-17T14:51:49.949+0530] {logging_mixin.py:190} INFO - [2024-12-17T14:51:49.948+0530] {dag.py:4451} INFO - created dagrun <DagRun movie_pipeline @ 2024-12-17 09:21:49.795140+00:00: manual__2024-12-17T09:21:49.795140+00:00, state:running, queued_at: None. externally triggered: False>
[2024-12-17T14:51:49.975+0530] {logging_mixin.py:190} INFO - [2024-12-17T14:51:49.974+0530] {dag.py:4396} INFO - [DAG TEST] starting task_id=scrape_collection_page map_index=-1
[2024-12-17T14:51:49.975+0530] {logging_mixin.py:190} INFO - [2024-12-17T14:51:49.975+0530] {dag.py:4399} INFO - [DAG TEST] running task <TaskInstance: movie_pipeline.scrape_collection_page manual__2024-12-17T09:21:49.795140+00:00 [scheduled]>
[2024-12-17T14:51:55.074+0530] {logging_mixin.py:190} INFO - [2024-12-17T14:51:55.073+0530] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-12-17T14:51:55.136+0530] {logging_mixin.py:190} INFO - [2024-12-17 14:51:55,135] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T09:21:49.795140+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T09:21:49.795140+00:00'
[2024-12-17T14:51:55.136+0530] {logging_mixin.py:190} INFO - [2024-12-17T14:51:55.135+0530] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T09:21:49.795140+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T09:21:49.795140+00:00'
[2024-12-17T14:51:55.140+0530] {logging_mixin.py:190} INFO - Task instance is in running state
[2024-12-17T14:51:55.140+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: queued
[2024-12-17T14:51:55.140+0530] {logging_mixin.py:190} INFO - Current task name:scrape_collection_page state:scheduled start_date:None
[2024-12-17T14:51:55.140+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline and current dag run status:running
[2024-12-17T14:51:55.141+0530] {logging_mixin.py:190} INFO - [2024-12-17T14:51:55.141+0530] {taskinstance.py:731} INFO - ::endgroup::
[2024-12-17T14:51:55.142+0530] {logging_mixin.py:190} INFO - 2024-12-17
[2024-12-17T14:51:55.143+0530] {logging_mixin.py:190} INFO - [2024-12-17 14:51:55,143] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T14:51:55.144+0530] {logging_mixin.py:190} INFO - [2024-12-17T14:51:55.143+0530] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T14:51:55.149+0530] {logging_mixin.py:190} INFO - [2024-12-17T14:51:55.149+0530] {taskinstance.py:340} INFO - ::group::Post task execution logs
[2024-12-17T14:51:55.149+0530] {logging_mixin.py:190} INFO - [2024-12-17T14:51:55.149+0530] {taskinstance.py:352} INFO - Marking task as SUCCESS. dag_id=movie_pipeline, task_id=scrape_collection_page, run_id=manual__2024-12-17T09:21:49.795140+00:00, execution_date=20241217T092149, start_date=, end_date=20241217T092155
[2024-12-17T14:51:55.160+0530] {logging_mixin.py:190} INFO - Task instance in success state
[2024-12-17T14:51:55.160+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: running
[2024-12-17T14:51:55.161+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline queued_at:None
[2024-12-17T14:51:55.161+0530] {logging_mixin.py:190} INFO - Task hostname:Jayeshs-MacBook-Air.local operator:PythonOperator
[2024-12-17T14:51:55.161+0530] {logging_mixin.py:190} INFO - [2024-12-17T14:51:55.161+0530] {dag.py:4410} INFO - [DAG TEST] end task task_id=scrape_collection_page map_index=-1
[2024-12-17T14:51:55.166+0530] {logging_mixin.py:190} INFO - [2024-12-17T14:51:55.165+0530] {dagrun.py:854} INFO - Marking run <DagRun movie_pipeline @ 2024-12-17 09:21:49.795140+00:00: manual__2024-12-17T09:21:49.795140+00:00, state:running, queued_at: None. externally triggered: False> successful
[2024-12-17T14:51:55.167+0530] {logging_mixin.py:190} INFO - Dag run in success state
[2024-12-17T14:51:55.167+0530] {logging_mixin.py:190} INFO - Dag run start:2024-12-17 09:21:49.795140+00:00 end:2024-12-17 09:21:55.167442+00:00
[2024-12-17T14:51:55.168+0530] {logging_mixin.py:190} INFO - [2024-12-17T14:51:55.168+0530] {dagrun.py:905} INFO - DagRun Finished: dag_id=movie_pipeline, execution_date=2024-12-17 09:21:49.795140+00:00, run_id=manual__2024-12-17T09:21:49.795140+00:00, run_start_date=2024-12-17 09:21:49.795140+00:00, run_end_date=2024-12-17 09:21:55.167442+00:00, run_duration=5.372302, state=success, external_trigger=False, run_type=manual, data_interval_start=2024-12-16 09:21:49.795140+00:00, data_interval_end=2024-12-17 09:21:49.795140+00:00, dag_hash=None
[2024-12-17T14:51:55.177+0530] {processor.py:925} INFO - DAG(s) 'movie_pipeline' retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T14:51:55.189+0530] {logging_mixin.py:190} INFO - [2024-12-17T14:51:55.189+0530] {dag.py:3239} INFO - Sync 1 DAGs
[2024-12-17T14:51:55.206+0530] {logging_mixin.py:190} INFO - [2024-12-17T14:51:55.206+0530] {dag.py:4180} INFO - Setting next_dagrun for movie_pipeline to 2024-12-17 01:30:00+00:00, run_after=2024-12-18 01:30:00+00:00
[2024-12-17T14:51:55.227+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 5.482 seconds
[2024-12-17T14:54:47.572+0530] {processor.py:186} INFO - Started process (PID=68567) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T14:54:47.573+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-17T14:54:47.574+0530] {logging_mixin.py:190} INFO - [2024-12-17T14:54:47.574+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T14:54:47.751+0530] {logging_mixin.py:190} INFO - [2024-12-17T14:54:47.750+0530] {dag.py:4435} INFO - dagrun id: movie_pipeline
[2024-12-17T14:54:47.766+0530] {logging_mixin.py:190} INFO - [2024-12-17T14:54:47.765+0530] {dag.py:4451} INFO - created dagrun <DagRun movie_pipeline @ 2024-12-17 09:24:47.617051+00:00: manual__2024-12-17T09:24:47.617051+00:00, state:running, queued_at: None. externally triggered: False>
[2024-12-17T14:54:47.792+0530] {logging_mixin.py:190} INFO - [2024-12-17T14:54:47.791+0530] {dag.py:4396} INFO - [DAG TEST] starting task_id=scrape_collection_page map_index=-1
[2024-12-17T14:54:47.792+0530] {logging_mixin.py:190} INFO - [2024-12-17T14:54:47.792+0530] {dag.py:4399} INFO - [DAG TEST] running task <TaskInstance: movie_pipeline.scrape_collection_page manual__2024-12-17T09:24:47.617051+00:00 [scheduled]>
[2024-12-17T14:54:52.890+0530] {logging_mixin.py:190} INFO - [2024-12-17T14:54:52.889+0530] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-12-17T14:54:52.954+0530] {logging_mixin.py:190} INFO - [2024-12-17 14:54:52,954] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T09:24:47.617051+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T09:24:47.617051+00:00'
[2024-12-17T14:54:52.955+0530] {logging_mixin.py:190} INFO - [2024-12-17T14:54:52.954+0530] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T09:24:47.617051+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T09:24:47.617051+00:00'
[2024-12-17T14:54:52.957+0530] {logging_mixin.py:190} INFO - Task instance is in running state
[2024-12-17T14:54:52.958+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: queued
[2024-12-17T14:54:52.958+0530] {logging_mixin.py:190} INFO - Current task name:scrape_collection_page state:scheduled start_date:None
[2024-12-17T14:54:52.958+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline and current dag run status:running
[2024-12-17T14:54:52.959+0530] {logging_mixin.py:190} INFO - [2024-12-17T14:54:52.958+0530] {taskinstance.py:731} INFO - ::endgroup::
[2024-12-17T14:54:52.960+0530] {logging_mixin.py:190} INFO - 2024-12-17
[2024-12-17T14:54:52.961+0530] {logging_mixin.py:190} INFO - [2024-12-17 14:54:52,961] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T14:54:52.961+0530] {logging_mixin.py:190} INFO - [2024-12-17T14:54:52.961+0530] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T14:54:52.966+0530] {logging_mixin.py:190} INFO - [2024-12-17T14:54:52.966+0530] {taskinstance.py:340} INFO - ::group::Post task execution logs
[2024-12-17T14:54:52.967+0530] {logging_mixin.py:190} INFO - [2024-12-17T14:54:52.967+0530] {taskinstance.py:352} INFO - Marking task as SUCCESS. dag_id=movie_pipeline, task_id=scrape_collection_page, run_id=manual__2024-12-17T09:24:47.617051+00:00, execution_date=20241217T092447, start_date=, end_date=20241217T092452
[2024-12-17T14:54:52.976+0530] {logging_mixin.py:190} INFO - Task instance in success state
[2024-12-17T14:54:52.976+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: running
[2024-12-17T14:54:52.977+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline queued_at:None
[2024-12-17T14:54:52.977+0530] {logging_mixin.py:190} INFO - Task hostname:Jayeshs-MacBook-Air.local operator:PythonOperator
[2024-12-17T14:54:52.978+0530] {logging_mixin.py:190} INFO - [2024-12-17T14:54:52.977+0530] {dag.py:4410} INFO - [DAG TEST] end task task_id=scrape_collection_page map_index=-1
[2024-12-17T14:54:52.984+0530] {logging_mixin.py:190} INFO - [2024-12-17T14:54:52.984+0530] {dagrun.py:854} INFO - Marking run <DagRun movie_pipeline @ 2024-12-17 09:24:47.617051+00:00: manual__2024-12-17T09:24:47.617051+00:00, state:running, queued_at: None. externally triggered: False> successful
[2024-12-17T14:54:52.985+0530] {logging_mixin.py:190} INFO - Dag run in success state
[2024-12-17T14:54:52.985+0530] {logging_mixin.py:190} INFO - Dag run start:2024-12-17 09:24:47.617051+00:00 end:2024-12-17 09:24:52.984956+00:00
[2024-12-17T14:54:52.985+0530] {logging_mixin.py:190} INFO - [2024-12-17T14:54:52.985+0530] {dagrun.py:905} INFO - DagRun Finished: dag_id=movie_pipeline, execution_date=2024-12-17 09:24:47.617051+00:00, run_id=manual__2024-12-17T09:24:47.617051+00:00, run_start_date=2024-12-17 09:24:47.617051+00:00, run_end_date=2024-12-17 09:24:52.984956+00:00, run_duration=5.367905, state=success, external_trigger=False, run_type=manual, data_interval_start=2024-12-16 09:24:47.617051+00:00, data_interval_end=2024-12-17 09:24:47.617051+00:00, dag_hash=None
[2024-12-17T14:54:52.994+0530] {processor.py:925} INFO - DAG(s) 'movie_pipeline' retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T14:54:53.007+0530] {logging_mixin.py:190} INFO - [2024-12-17T14:54:53.007+0530] {dag.py:3239} INFO - Sync 1 DAGs
[2024-12-17T14:54:53.026+0530] {logging_mixin.py:190} INFO - [2024-12-17T14:54:53.025+0530] {dag.py:4180} INFO - Setting next_dagrun for movie_pipeline to 2024-12-17 01:30:00+00:00, run_after=2024-12-18 01:30:00+00:00
[2024-12-17T14:54:53.043+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 5.475 seconds
[2024-12-17T14:57:42.018+0530] {processor.py:186} INFO - Started process (PID=68761) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T14:57:42.019+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-17T14:57:42.021+0530] {logging_mixin.py:190} INFO - [2024-12-17T14:57:42.020+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T14:57:42.196+0530] {logging_mixin.py:190} INFO - [2024-12-17T14:57:42.196+0530] {dag.py:4435} INFO - dagrun id: movie_pipeline
[2024-12-17T14:57:42.210+0530] {logging_mixin.py:190} INFO - [2024-12-17T14:57:42.210+0530] {dag.py:4451} INFO - created dagrun <DagRun movie_pipeline @ 2024-12-17 09:27:42.061142+00:00: manual__2024-12-17T09:27:42.061142+00:00, state:running, queued_at: None. externally triggered: False>
[2024-12-17T14:57:42.238+0530] {logging_mixin.py:190} INFO - [2024-12-17T14:57:42.238+0530] {dag.py:4396} INFO - [DAG TEST] starting task_id=scrape_collection_page map_index=-1
[2024-12-17T14:57:42.239+0530] {logging_mixin.py:190} INFO - [2024-12-17T14:57:42.239+0530] {dag.py:4399} INFO - [DAG TEST] running task <TaskInstance: movie_pipeline.scrape_collection_page manual__2024-12-17T09:27:42.061142+00:00 [scheduled]>
[2024-12-17T14:57:47.336+0530] {logging_mixin.py:190} INFO - [2024-12-17T14:57:47.336+0530] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-12-17T14:57:47.411+0530] {logging_mixin.py:190} INFO - [2024-12-17 14:57:47,410] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T09:27:42.061142+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T09:27:42.061142+00:00'
[2024-12-17T14:57:47.411+0530] {logging_mixin.py:190} INFO - [2024-12-17T14:57:47.410+0530] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T09:27:42.061142+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T09:27:42.061142+00:00'
[2024-12-17T14:57:47.414+0530] {logging_mixin.py:190} INFO - Task instance is in running state
[2024-12-17T14:57:47.414+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: queued
[2024-12-17T14:57:47.414+0530] {logging_mixin.py:190} INFO - Current task name:scrape_collection_page state:scheduled start_date:None
[2024-12-17T14:57:47.415+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline and current dag run status:running
[2024-12-17T14:57:47.415+0530] {logging_mixin.py:190} INFO - [2024-12-17T14:57:47.415+0530] {taskinstance.py:731} INFO - ::endgroup::
[2024-12-17T14:57:47.417+0530] {logging_mixin.py:190} INFO - 2024-12-17
[2024-12-17T14:57:47.418+0530] {logging_mixin.py:190} INFO - [2024-12-17 14:57:47,417] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T14:57:47.418+0530] {logging_mixin.py:190} INFO - [2024-12-17T14:57:47.417+0530] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T14:57:47.432+0530] {logging_mixin.py:190} INFO - [2024-12-17T14:57:47.432+0530] {taskinstance.py:340} INFO - ::group::Post task execution logs
[2024-12-17T14:57:47.434+0530] {logging_mixin.py:190} INFO - [2024-12-17T14:57:47.434+0530] {taskinstance.py:352} INFO - Marking task as SUCCESS. dag_id=movie_pipeline, task_id=scrape_collection_page, run_id=manual__2024-12-17T09:27:42.061142+00:00, execution_date=20241217T092742, start_date=, end_date=20241217T092747
[2024-12-17T14:57:47.450+0530] {logging_mixin.py:190} INFO - Task instance in success state
[2024-12-17T14:57:47.451+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: running
[2024-12-17T14:57:47.452+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline queued_at:None
[2024-12-17T14:57:47.452+0530] {logging_mixin.py:190} INFO - Task hostname:Jayeshs-MacBook-Air.local operator:PythonOperator
[2024-12-17T14:57:47.452+0530] {logging_mixin.py:190} INFO - [2024-12-17T14:57:47.452+0530] {dag.py:4410} INFO - [DAG TEST] end task task_id=scrape_collection_page map_index=-1
[2024-12-17T14:57:47.462+0530] {logging_mixin.py:190} INFO - [2024-12-17T14:57:47.461+0530] {dagrun.py:854} INFO - Marking run <DagRun movie_pipeline @ 2024-12-17 09:27:42.061142+00:00: manual__2024-12-17T09:27:42.061142+00:00, state:running, queued_at: None. externally triggered: False> successful
[2024-12-17T14:57:47.462+0530] {logging_mixin.py:190} INFO - Dag run in success state
[2024-12-17T14:57:47.463+0530] {logging_mixin.py:190} INFO - Dag run start:2024-12-17 09:27:42.061142+00:00 end:2024-12-17 09:27:47.462600+00:00
[2024-12-17T14:57:47.464+0530] {logging_mixin.py:190} INFO - [2024-12-17T14:57:47.463+0530] {dagrun.py:905} INFO - DagRun Finished: dag_id=movie_pipeline, execution_date=2024-12-17 09:27:42.061142+00:00, run_id=manual__2024-12-17T09:27:42.061142+00:00, run_start_date=2024-12-17 09:27:42.061142+00:00, run_end_date=2024-12-17 09:27:47.462600+00:00, run_duration=5.401458, state=success, external_trigger=False, run_type=manual, data_interval_start=2024-12-16 09:27:42.061142+00:00, data_interval_end=2024-12-17 09:27:42.061142+00:00, dag_hash=None
[2024-12-17T14:57:47.477+0530] {processor.py:925} INFO - DAG(s) 'movie_pipeline' retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T14:57:47.493+0530] {logging_mixin.py:190} INFO - [2024-12-17T14:57:47.493+0530] {dag.py:3239} INFO - Sync 1 DAGs
[2024-12-17T14:57:47.510+0530] {logging_mixin.py:190} INFO - [2024-12-17T14:57:47.510+0530] {dag.py:4180} INFO - Setting next_dagrun for movie_pipeline to 2024-12-17 01:30:00+00:00, run_after=2024-12-18 01:30:00+00:00
[2024-12-17T14:57:47.536+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 5.522 seconds
[2024-12-17T15:00:38.766+0530] {processor.py:186} INFO - Started process (PID=68969) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T15:00:38.768+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-17T15:00:38.769+0530] {logging_mixin.py:190} INFO - [2024-12-17T15:00:38.769+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T15:00:38.941+0530] {logging_mixin.py:190} INFO - [2024-12-17T15:00:38.941+0530] {dag.py:4435} INFO - dagrun id: movie_pipeline
[2024-12-17T15:00:38.956+0530] {logging_mixin.py:190} INFO - [2024-12-17T15:00:38.955+0530] {dag.py:4451} INFO - created dagrun <DagRun movie_pipeline @ 2024-12-17 09:30:38.809722+00:00: manual__2024-12-17T09:30:38.809722+00:00, state:running, queued_at: None. externally triggered: False>
[2024-12-17T15:00:38.981+0530] {logging_mixin.py:190} INFO - [2024-12-17T15:00:38.981+0530] {dag.py:4396} INFO - [DAG TEST] starting task_id=scrape_collection_page map_index=-1
[2024-12-17T15:00:38.982+0530] {logging_mixin.py:190} INFO - [2024-12-17T15:00:38.981+0530] {dag.py:4399} INFO - [DAG TEST] running task <TaskInstance: movie_pipeline.scrape_collection_page manual__2024-12-17T09:30:38.809722+00:00 [scheduled]>
[2024-12-17T15:00:44.080+0530] {logging_mixin.py:190} INFO - [2024-12-17T15:00:44.079+0530] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-12-17T15:00:44.141+0530] {logging_mixin.py:190} INFO - [2024-12-17 15:00:44,141] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T09:30:38.809722+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T09:30:38.809722+00:00'
[2024-12-17T15:00:44.142+0530] {logging_mixin.py:190} INFO - [2024-12-17T15:00:44.141+0530] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T09:30:38.809722+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T09:30:38.809722+00:00'
[2024-12-17T15:00:44.145+0530] {logging_mixin.py:190} INFO - Task instance is in running state
[2024-12-17T15:00:44.146+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: queued
[2024-12-17T15:00:44.146+0530] {logging_mixin.py:190} INFO - Current task name:scrape_collection_page state:scheduled start_date:None
[2024-12-17T15:00:44.146+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline and current dag run status:running
[2024-12-17T15:00:44.146+0530] {logging_mixin.py:190} INFO - [2024-12-17T15:00:44.146+0530] {taskinstance.py:731} INFO - ::endgroup::
[2024-12-17T15:00:44.148+0530] {logging_mixin.py:190} INFO - 2024-12-17
[2024-12-17T15:00:44.149+0530] {logging_mixin.py:190} INFO - [2024-12-17 15:00:44,148] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T15:00:44.149+0530] {logging_mixin.py:190} INFO - [2024-12-17T15:00:44.148+0530] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T15:00:44.154+0530] {logging_mixin.py:190} INFO - [2024-12-17T15:00:44.154+0530] {taskinstance.py:340} INFO - ::group::Post task execution logs
[2024-12-17T15:00:44.155+0530] {logging_mixin.py:190} INFO - [2024-12-17T15:00:44.155+0530] {taskinstance.py:352} INFO - Marking task as SUCCESS. dag_id=movie_pipeline, task_id=scrape_collection_page, run_id=manual__2024-12-17T09:30:38.809722+00:00, execution_date=20241217T093038, start_date=, end_date=20241217T093044
[2024-12-17T15:00:44.170+0530] {logging_mixin.py:190} INFO - Task instance in success state
[2024-12-17T15:00:44.170+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: running
[2024-12-17T15:00:44.172+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline queued_at:None
[2024-12-17T15:00:44.172+0530] {logging_mixin.py:190} INFO - Task hostname:Jayeshs-MacBook-Air.local operator:PythonOperator
[2024-12-17T15:00:44.173+0530] {logging_mixin.py:190} INFO - [2024-12-17T15:00:44.172+0530] {dag.py:4410} INFO - [DAG TEST] end task task_id=scrape_collection_page map_index=-1
[2024-12-17T15:00:44.177+0530] {logging_mixin.py:190} INFO - [2024-12-17T15:00:44.176+0530] {dagrun.py:854} INFO - Marking run <DagRun movie_pipeline @ 2024-12-17 09:30:38.809722+00:00: manual__2024-12-17T09:30:38.809722+00:00, state:running, queued_at: None. externally triggered: False> successful
[2024-12-17T15:00:44.177+0530] {logging_mixin.py:190} INFO - Dag run in success state
[2024-12-17T15:00:44.177+0530] {logging_mixin.py:190} INFO - Dag run start:2024-12-17 09:30:38.809722+00:00 end:2024-12-17 09:30:44.177245+00:00
[2024-12-17T15:00:44.177+0530] {logging_mixin.py:190} INFO - [2024-12-17T15:00:44.177+0530] {dagrun.py:905} INFO - DagRun Finished: dag_id=movie_pipeline, execution_date=2024-12-17 09:30:38.809722+00:00, run_id=manual__2024-12-17T09:30:38.809722+00:00, run_start_date=2024-12-17 09:30:38.809722+00:00, run_end_date=2024-12-17 09:30:44.177245+00:00, run_duration=5.367523, state=success, external_trigger=False, run_type=manual, data_interval_start=2024-12-16 09:30:38.809722+00:00, data_interval_end=2024-12-17 09:30:38.809722+00:00, dag_hash=None
[2024-12-17T15:00:44.187+0530] {processor.py:925} INFO - DAG(s) 'movie_pipeline' retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T15:00:44.199+0530] {logging_mixin.py:190} INFO - [2024-12-17T15:00:44.198+0530] {dag.py:3239} INFO - Sync 1 DAGs
[2024-12-17T15:00:44.217+0530] {logging_mixin.py:190} INFO - [2024-12-17T15:00:44.217+0530] {dag.py:4180} INFO - Setting next_dagrun for movie_pipeline to 2024-12-17 01:30:00+00:00, run_after=2024-12-18 01:30:00+00:00
[2024-12-17T15:00:44.237+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 5.474 seconds
[2024-12-17T15:03:34.494+0530] {processor.py:186} INFO - Started process (PID=69163) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T15:03:34.495+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-17T15:03:34.497+0530] {logging_mixin.py:190} INFO - [2024-12-17T15:03:34.496+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T15:03:34.679+0530] {logging_mixin.py:190} INFO - [2024-12-17T15:03:34.679+0530] {dag.py:4435} INFO - dagrun id: movie_pipeline
[2024-12-17T15:03:34.694+0530] {logging_mixin.py:190} INFO - [2024-12-17T15:03:34.694+0530] {dag.py:4451} INFO - created dagrun <DagRun movie_pipeline @ 2024-12-17 09:33:34.540245+00:00: manual__2024-12-17T09:33:34.540245+00:00, state:running, queued_at: None. externally triggered: False>
[2024-12-17T15:03:34.719+0530] {logging_mixin.py:190} INFO - [2024-12-17T15:03:34.719+0530] {dag.py:4396} INFO - [DAG TEST] starting task_id=scrape_collection_page map_index=-1
[2024-12-17T15:03:34.720+0530] {logging_mixin.py:190} INFO - [2024-12-17T15:03:34.719+0530] {dag.py:4399} INFO - [DAG TEST] running task <TaskInstance: movie_pipeline.scrape_collection_page manual__2024-12-17T09:33:34.540245+00:00 [scheduled]>
[2024-12-17T15:03:39.813+0530] {logging_mixin.py:190} INFO - [2024-12-17T15:03:39.812+0530] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-12-17T15:03:39.875+0530] {logging_mixin.py:190} INFO - [2024-12-17 15:03:39,875] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T09:33:34.540245+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T09:33:34.540245+00:00'
[2024-12-17T15:03:39.876+0530] {logging_mixin.py:190} INFO - [2024-12-17T15:03:39.875+0530] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T09:33:34.540245+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T09:33:34.540245+00:00'
[2024-12-17T15:03:39.878+0530] {logging_mixin.py:190} INFO - Task instance is in running state
[2024-12-17T15:03:39.878+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: queued
[2024-12-17T15:03:39.879+0530] {logging_mixin.py:190} INFO - Current task name:scrape_collection_page state:scheduled start_date:None
[2024-12-17T15:03:39.879+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline and current dag run status:running
[2024-12-17T15:03:39.880+0530] {logging_mixin.py:190} INFO - [2024-12-17T15:03:39.880+0530] {taskinstance.py:731} INFO - ::endgroup::
[2024-12-17T15:03:39.882+0530] {logging_mixin.py:190} INFO - 2024-12-17
[2024-12-17T15:03:39.883+0530] {logging_mixin.py:190} INFO - [2024-12-17 15:03:39,882] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T15:03:39.883+0530] {logging_mixin.py:190} INFO - [2024-12-17T15:03:39.882+0530] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T15:03:39.890+0530] {logging_mixin.py:190} INFO - [2024-12-17T15:03:39.890+0530] {taskinstance.py:340} INFO - ::group::Post task execution logs
[2024-12-17T15:03:39.890+0530] {logging_mixin.py:190} INFO - [2024-12-17T15:03:39.890+0530] {taskinstance.py:352} INFO - Marking task as SUCCESS. dag_id=movie_pipeline, task_id=scrape_collection_page, run_id=manual__2024-12-17T09:33:34.540245+00:00, execution_date=20241217T093334, start_date=, end_date=20241217T093339
[2024-12-17T15:03:39.900+0530] {logging_mixin.py:190} INFO - Task instance in success state
[2024-12-17T15:03:39.900+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: running
[2024-12-17T15:03:39.901+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline queued_at:None
[2024-12-17T15:03:39.901+0530] {logging_mixin.py:190} INFO - Task hostname:Jayeshs-MacBook-Air.local operator:PythonOperator
[2024-12-17T15:03:39.901+0530] {logging_mixin.py:190} INFO - [2024-12-17T15:03:39.901+0530] {dag.py:4410} INFO - [DAG TEST] end task task_id=scrape_collection_page map_index=-1
[2024-12-17T15:03:39.906+0530] {logging_mixin.py:190} INFO - [2024-12-17T15:03:39.906+0530] {dagrun.py:854} INFO - Marking run <DagRun movie_pipeline @ 2024-12-17 09:33:34.540245+00:00: manual__2024-12-17T09:33:34.540245+00:00, state:running, queued_at: None. externally triggered: False> successful
[2024-12-17T15:03:39.906+0530] {logging_mixin.py:190} INFO - Dag run in success state
[2024-12-17T15:03:39.906+0530] {logging_mixin.py:190} INFO - Dag run start:2024-12-17 09:33:34.540245+00:00 end:2024-12-17 09:33:39.906648+00:00
[2024-12-17T15:03:39.907+0530] {logging_mixin.py:190} INFO - [2024-12-17T15:03:39.907+0530] {dagrun.py:905} INFO - DagRun Finished: dag_id=movie_pipeline, execution_date=2024-12-17 09:33:34.540245+00:00, run_id=manual__2024-12-17T09:33:34.540245+00:00, run_start_date=2024-12-17 09:33:34.540245+00:00, run_end_date=2024-12-17 09:33:39.906648+00:00, run_duration=5.366403, state=success, external_trigger=False, run_type=manual, data_interval_start=2024-12-16 09:33:34.540245+00:00, data_interval_end=2024-12-17 09:33:34.540245+00:00, dag_hash=None
[2024-12-17T15:03:39.916+0530] {processor.py:925} INFO - DAG(s) 'movie_pipeline' retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T15:03:39.929+0530] {logging_mixin.py:190} INFO - [2024-12-17T15:03:39.929+0530] {dag.py:3239} INFO - Sync 1 DAGs
[2024-12-17T15:03:39.946+0530] {logging_mixin.py:190} INFO - [2024-12-17T15:03:39.946+0530] {dag.py:4180} INFO - Setting next_dagrun for movie_pipeline to 2024-12-17 01:30:00+00:00, run_after=2024-12-18 01:30:00+00:00
[2024-12-17T15:03:39.966+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 5.476 seconds
[2024-12-17T15:06:40.964+0530] {processor.py:186} INFO - Started process (PID=69386) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T15:06:40.973+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-17T15:06:40.977+0530] {logging_mixin.py:190} INFO - [2024-12-17T15:06:40.976+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T15:06:41.462+0530] {logging_mixin.py:190} INFO - [2024-12-17T15:06:41.462+0530] {dag.py:4435} INFO - dagrun id: movie_pipeline
[2024-12-17T15:06:41.491+0530] {logging_mixin.py:190} INFO - [2024-12-17T15:06:41.490+0530] {dag.py:4451} INFO - created dagrun <DagRun movie_pipeline @ 2024-12-17 09:36:41.170187+00:00: manual__2024-12-17T09:36:41.170187+00:00, state:running, queued_at: None. externally triggered: False>
[2024-12-17T15:06:41.560+0530] {logging_mixin.py:190} INFO - [2024-12-17T15:06:41.559+0530] {dag.py:4396} INFO - [DAG TEST] starting task_id=scrape_collection_page map_index=-1
[2024-12-17T15:06:41.562+0530] {logging_mixin.py:190} INFO - [2024-12-17T15:06:41.561+0530] {dag.py:4399} INFO - [DAG TEST] running task <TaskInstance: movie_pipeline.scrape_collection_page manual__2024-12-17T09:36:41.170187+00:00 [scheduled]>
[2024-12-17T15:06:46.743+0530] {logging_mixin.py:190} INFO - [2024-12-17T15:06:46.742+0530] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-12-17T15:06:46.825+0530] {logging_mixin.py:190} INFO - [2024-12-17 15:06:46,825] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T09:36:41.170187+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T09:36:41.170187+00:00'
[2024-12-17T15:06:46.826+0530] {logging_mixin.py:190} INFO - [2024-12-17T15:06:46.825+0530] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T09:36:41.170187+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T09:36:41.170187+00:00'
[2024-12-17T15:06:46.838+0530] {logging_mixin.py:190} INFO - Task instance is in running state
[2024-12-17T15:06:46.839+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: queued
[2024-12-17T15:06:46.839+0530] {logging_mixin.py:190} INFO - Current task name:scrape_collection_page state:scheduled start_date:None
[2024-12-17T15:06:46.839+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline and current dag run status:running
[2024-12-17T15:06:46.840+0530] {logging_mixin.py:190} INFO - [2024-12-17T15:06:46.839+0530] {taskinstance.py:731} INFO - ::endgroup::
[2024-12-17T15:06:46.845+0530] {logging_mixin.py:190} INFO - 2024-12-17
[2024-12-17T15:06:46.846+0530] {logging_mixin.py:190} INFO - [2024-12-17 15:06:46,846] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T15:06:46.847+0530] {logging_mixin.py:190} INFO - [2024-12-17T15:06:46.846+0530] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T15:06:46.852+0530] {logging_mixin.py:190} INFO - [2024-12-17T15:06:46.852+0530] {taskinstance.py:340} INFO - ::group::Post task execution logs
[2024-12-17T15:06:46.853+0530] {logging_mixin.py:190} INFO - [2024-12-17T15:06:46.853+0530] {taskinstance.py:352} INFO - Marking task as SUCCESS. dag_id=movie_pipeline, task_id=scrape_collection_page, run_id=manual__2024-12-17T09:36:41.170187+00:00, execution_date=20241217T093641, start_date=, end_date=20241217T093646
[2024-12-17T15:06:46.864+0530] {logging_mixin.py:190} INFO - Task instance in success state
[2024-12-17T15:06:46.864+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: running
[2024-12-17T15:06:46.865+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline queued_at:None
[2024-12-17T15:06:46.866+0530] {logging_mixin.py:190} INFO - Task hostname:Jayeshs-MacBook-Air.local operator:PythonOperator
[2024-12-17T15:06:46.866+0530] {logging_mixin.py:190} INFO - [2024-12-17T15:06:46.866+0530] {dag.py:4410} INFO - [DAG TEST] end task task_id=scrape_collection_page map_index=-1
[2024-12-17T15:06:46.870+0530] {logging_mixin.py:190} INFO - [2024-12-17T15:06:46.870+0530] {dagrun.py:854} INFO - Marking run <DagRun movie_pipeline @ 2024-12-17 09:36:41.170187+00:00: manual__2024-12-17T09:36:41.170187+00:00, state:running, queued_at: None. externally triggered: False> successful
[2024-12-17T15:06:46.870+0530] {logging_mixin.py:190} INFO - Dag run in success state
[2024-12-17T15:06:46.871+0530] {logging_mixin.py:190} INFO - Dag run start:2024-12-17 09:36:41.170187+00:00 end:2024-12-17 09:36:46.870811+00:00
[2024-12-17T15:06:46.871+0530] {logging_mixin.py:190} INFO - [2024-12-17T15:06:46.871+0530] {dagrun.py:905} INFO - DagRun Finished: dag_id=movie_pipeline, execution_date=2024-12-17 09:36:41.170187+00:00, run_id=manual__2024-12-17T09:36:41.170187+00:00, run_start_date=2024-12-17 09:36:41.170187+00:00, run_end_date=2024-12-17 09:36:46.870811+00:00, run_duration=5.700624, state=success, external_trigger=False, run_type=manual, data_interval_start=2024-12-16 09:36:41.170187+00:00, data_interval_end=2024-12-17 09:36:41.170187+00:00, dag_hash=None
[2024-12-17T15:06:46.880+0530] {processor.py:925} INFO - DAG(s) 'movie_pipeline' retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T15:06:46.903+0530] {logging_mixin.py:190} INFO - [2024-12-17T15:06:46.902+0530] {dag.py:3239} INFO - Sync 1 DAGs
[2024-12-17T15:06:46.924+0530] {logging_mixin.py:190} INFO - [2024-12-17T15:06:46.924+0530] {dag.py:4180} INFO - Setting next_dagrun for movie_pipeline to 2024-12-17 01:30:00+00:00, run_after=2024-12-18 01:30:00+00:00
[2024-12-17T15:06:46.947+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 5.997 seconds
[2024-12-17T15:12:07.395+0530] {processor.py:186} INFO - Started process (PID=69774) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T15:12:07.398+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-17T15:12:07.403+0530] {logging_mixin.py:190} INFO - [2024-12-17T15:12:07.400+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T15:12:07.686+0530] {logging_mixin.py:190} INFO - [2024-12-17T15:12:07.686+0530] {dag.py:4435} INFO - dagrun id: movie_pipeline
[2024-12-17T15:12:07.709+0530] {logging_mixin.py:190} INFO - [2024-12-17T15:12:07.708+0530] {dag.py:4451} INFO - created dagrun <DagRun movie_pipeline @ 2024-12-17 09:42:07.519882+00:00: manual__2024-12-17T09:42:07.519882+00:00, state:running, queued_at: None. externally triggered: False>
[2024-12-17T15:12:07.762+0530] {logging_mixin.py:190} INFO - [2024-12-17T15:12:07.762+0530] {dag.py:4396} INFO - [DAG TEST] starting task_id=scrape_collection_page map_index=-1
[2024-12-17T15:12:07.763+0530] {logging_mixin.py:190} INFO - [2024-12-17T15:12:07.762+0530] {dag.py:4399} INFO - [DAG TEST] running task <TaskInstance: movie_pipeline.scrape_collection_page manual__2024-12-17T09:42:07.519882+00:00 [scheduled]>
[2024-12-17T15:12:12.970+0530] {logging_mixin.py:190} INFO - [2024-12-17T15:12:12.970+0530] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-12-17T15:12:13.124+0530] {logging_mixin.py:190} INFO - [2024-12-17 15:12:13,124] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T09:42:07.519882+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T09:42:07.519882+00:00'
[2024-12-17T15:12:13.125+0530] {logging_mixin.py:190} INFO - [2024-12-17T15:12:13.124+0530] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T09:42:07.519882+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T09:42:07.519882+00:00'
[2024-12-17T15:12:13.140+0530] {logging_mixin.py:190} INFO - Task instance is in running state
[2024-12-17T15:12:13.141+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: queued
[2024-12-17T15:12:13.141+0530] {logging_mixin.py:190} INFO - Current task name:scrape_collection_page state:scheduled start_date:None
[2024-12-17T15:12:13.141+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline and current dag run status:running
[2024-12-17T15:12:13.142+0530] {logging_mixin.py:190} INFO - [2024-12-17T15:12:13.142+0530] {taskinstance.py:731} INFO - ::endgroup::
[2024-12-17T15:12:13.145+0530] {logging_mixin.py:190} INFO - 2024-12-17
[2024-12-17T15:12:13.145+0530] {logging_mixin.py:190} INFO - [2024-12-17 15:12:13,145] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T15:12:13.146+0530] {logging_mixin.py:190} INFO - [2024-12-17T15:12:13.145+0530] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T15:12:13.152+0530] {logging_mixin.py:190} INFO - [2024-12-17T15:12:13.151+0530] {taskinstance.py:340} INFO - ::group::Post task execution logs
[2024-12-17T15:12:13.152+0530] {logging_mixin.py:190} INFO - [2024-12-17T15:12:13.152+0530] {taskinstance.py:352} INFO - Marking task as SUCCESS. dag_id=movie_pipeline, task_id=scrape_collection_page, run_id=manual__2024-12-17T09:42:07.519882+00:00, execution_date=20241217T094207, start_date=, end_date=20241217T094213
[2024-12-17T15:12:13.164+0530] {logging_mixin.py:190} INFO - Task instance in success state
[2024-12-17T15:12:13.164+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: running
[2024-12-17T15:12:13.164+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline queued_at:None
[2024-12-17T15:12:13.165+0530] {logging_mixin.py:190} INFO - Task hostname:Jayeshs-MacBook-Air.local operator:PythonOperator
[2024-12-17T15:12:13.165+0530] {logging_mixin.py:190} INFO - [2024-12-17T15:12:13.165+0530] {dag.py:4410} INFO - [DAG TEST] end task task_id=scrape_collection_page map_index=-1
[2024-12-17T15:12:13.170+0530] {logging_mixin.py:190} INFO - [2024-12-17T15:12:13.170+0530] {dagrun.py:854} INFO - Marking run <DagRun movie_pipeline @ 2024-12-17 09:42:07.519882+00:00: manual__2024-12-17T09:42:07.519882+00:00, state:running, queued_at: None. externally triggered: False> successful
[2024-12-17T15:12:13.170+0530] {logging_mixin.py:190} INFO - Dag run in success state
[2024-12-17T15:12:13.171+0530] {logging_mixin.py:190} INFO - Dag run start:2024-12-17 09:42:07.519882+00:00 end:2024-12-17 09:42:13.170771+00:00
[2024-12-17T15:12:13.171+0530] {logging_mixin.py:190} INFO - [2024-12-17T15:12:13.171+0530] {dagrun.py:905} INFO - DagRun Finished: dag_id=movie_pipeline, execution_date=2024-12-17 09:42:07.519882+00:00, run_id=manual__2024-12-17T09:42:07.519882+00:00, run_start_date=2024-12-17 09:42:07.519882+00:00, run_end_date=2024-12-17 09:42:13.170771+00:00, run_duration=5.650889, state=success, external_trigger=False, run_type=manual, data_interval_start=2024-12-16 09:42:07.519882+00:00, data_interval_end=2024-12-17 09:42:07.519882+00:00, dag_hash=None
[2024-12-17T15:12:13.182+0530] {processor.py:925} INFO - DAG(s) 'movie_pipeline' retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T15:12:13.201+0530] {logging_mixin.py:190} INFO - [2024-12-17T15:12:13.200+0530] {dag.py:3239} INFO - Sync 1 DAGs
[2024-12-17T15:12:13.222+0530] {logging_mixin.py:190} INFO - [2024-12-17T15:12:13.221+0530] {dag.py:4180} INFO - Setting next_dagrun for movie_pipeline to 2024-12-17 01:30:00+00:00, run_after=2024-12-18 01:30:00+00:00
[2024-12-17T15:12:13.244+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 5.854 seconds
[2024-12-17T15:16:25.133+0530] {processor.py:186} INFO - Started process (PID=70037) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T15:16:25.135+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-17T15:16:25.137+0530] {logging_mixin.py:190} INFO - [2024-12-17T15:16:25.137+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T15:16:25.312+0530] {logging_mixin.py:190} INFO - [2024-12-17T15:16:25.311+0530] {dag.py:4435} INFO - dagrun id: movie_pipeline
[2024-12-17T15:16:25.325+0530] {logging_mixin.py:190} INFO - [2024-12-17T15:16:25.325+0530] {dag.py:4451} INFO - created dagrun <DagRun movie_pipeline @ 2024-12-17 09:46:25.178132+00:00: manual__2024-12-17T09:46:25.178132+00:00, state:running, queued_at: None. externally triggered: False>
[2024-12-17T15:16:25.356+0530] {logging_mixin.py:190} INFO - [2024-12-17T15:16:25.356+0530] {dag.py:4396} INFO - [DAG TEST] starting task_id=scrape_collection_page map_index=-1
[2024-12-17T15:16:25.357+0530] {logging_mixin.py:190} INFO - [2024-12-17T15:16:25.356+0530] {dag.py:4399} INFO - [DAG TEST] running task <TaskInstance: movie_pipeline.scrape_collection_page manual__2024-12-17T09:46:25.178132+00:00 [scheduled]>
[2024-12-17T15:16:30.452+0530] {logging_mixin.py:190} INFO - [2024-12-17T15:16:30.452+0530] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-12-17T15:16:30.528+0530] {logging_mixin.py:190} INFO - [2024-12-17 15:16:30,528] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T09:46:25.178132+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T09:46:25.178132+00:00'
[2024-12-17T15:16:30.529+0530] {logging_mixin.py:190} INFO - [2024-12-17T15:16:30.528+0530] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T09:46:25.178132+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T09:46:25.178132+00:00'
[2024-12-17T15:16:30.541+0530] {logging_mixin.py:190} INFO - Task instance is in running state
[2024-12-17T15:16:30.542+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: queued
[2024-12-17T15:16:30.542+0530] {logging_mixin.py:190} INFO - Current task name:scrape_collection_page state:scheduled start_date:None
[2024-12-17T15:16:30.542+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline and current dag run status:running
[2024-12-17T15:16:30.543+0530] {logging_mixin.py:190} INFO - [2024-12-17T15:16:30.543+0530] {taskinstance.py:731} INFO - ::endgroup::
[2024-12-17T15:16:30.546+0530] {logging_mixin.py:190} INFO - 2024-12-17
[2024-12-17T15:16:30.547+0530] {logging_mixin.py:190} INFO - [2024-12-17 15:16:30,547] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T15:16:30.547+0530] {logging_mixin.py:190} INFO - [2024-12-17T15:16:30.547+0530] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T15:16:30.553+0530] {logging_mixin.py:190} INFO - [2024-12-17T15:16:30.552+0530] {taskinstance.py:340} INFO - ::group::Post task execution logs
[2024-12-17T15:16:30.553+0530] {logging_mixin.py:190} INFO - [2024-12-17T15:16:30.553+0530] {taskinstance.py:352} INFO - Marking task as SUCCESS. dag_id=movie_pipeline, task_id=scrape_collection_page, run_id=manual__2024-12-17T09:46:25.178132+00:00, execution_date=20241217T094625, start_date=, end_date=20241217T094630
[2024-12-17T15:16:30.562+0530] {logging_mixin.py:190} INFO - Task instance in success state
[2024-12-17T15:16:30.563+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: running
[2024-12-17T15:16:30.563+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline queued_at:None
[2024-12-17T15:16:30.563+0530] {logging_mixin.py:190} INFO - Task hostname:Jayeshs-MacBook-Air.local operator:PythonOperator
[2024-12-17T15:16:30.564+0530] {logging_mixin.py:190} INFO - [2024-12-17T15:16:30.563+0530] {dag.py:4410} INFO - [DAG TEST] end task task_id=scrape_collection_page map_index=-1
[2024-12-17T15:16:30.567+0530] {logging_mixin.py:190} INFO - [2024-12-17T15:16:30.567+0530] {dagrun.py:854} INFO - Marking run <DagRun movie_pipeline @ 2024-12-17 09:46:25.178132+00:00: manual__2024-12-17T09:46:25.178132+00:00, state:running, queued_at: None. externally triggered: False> successful
[2024-12-17T15:16:30.568+0530] {logging_mixin.py:190} INFO - Dag run in success state
[2024-12-17T15:16:30.568+0530] {logging_mixin.py:190} INFO - Dag run start:2024-12-17 09:46:25.178132+00:00 end:2024-12-17 09:46:30.567987+00:00
[2024-12-17T15:16:30.568+0530] {logging_mixin.py:190} INFO - [2024-12-17T15:16:30.568+0530] {dagrun.py:905} INFO - DagRun Finished: dag_id=movie_pipeline, execution_date=2024-12-17 09:46:25.178132+00:00, run_id=manual__2024-12-17T09:46:25.178132+00:00, run_start_date=2024-12-17 09:46:25.178132+00:00, run_end_date=2024-12-17 09:46:30.567987+00:00, run_duration=5.389855, state=success, external_trigger=False, run_type=manual, data_interval_start=2024-12-16 09:46:25.178132+00:00, data_interval_end=2024-12-17 09:46:25.178132+00:00, dag_hash=None
[2024-12-17T15:16:30.577+0530] {processor.py:925} INFO - DAG(s) 'movie_pipeline' retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T15:16:30.589+0530] {logging_mixin.py:190} INFO - [2024-12-17T15:16:30.589+0530] {dag.py:3239} INFO - Sync 1 DAGs
[2024-12-17T15:16:30.606+0530] {logging_mixin.py:190} INFO - [2024-12-17T15:16:30.605+0530] {dag.py:4180} INFO - Setting next_dagrun for movie_pipeline to 2024-12-17 01:30:00+00:00, run_after=2024-12-18 01:30:00+00:00
[2024-12-17T15:16:30.624+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 5.494 seconds
[2024-12-17T15:20:06.029+0530] {processor.py:186} INFO - Started process (PID=70279) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T15:20:06.032+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-17T15:20:06.036+0530] {logging_mixin.py:190} INFO - [2024-12-17T15:20:06.035+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T15:20:06.432+0530] {logging_mixin.py:190} INFO - [2024-12-17T15:20:06.431+0530] {dag.py:4435} INFO - dagrun id: movie_pipeline
[2024-12-17T15:20:06.455+0530] {logging_mixin.py:190} INFO - [2024-12-17T15:20:06.454+0530] {dag.py:4451} INFO - created dagrun <DagRun movie_pipeline @ 2024-12-17 09:50:06.092972+00:00: manual__2024-12-17T09:50:06.092972+00:00, state:running, queued_at: None. externally triggered: False>
[2024-12-17T15:20:06.494+0530] {logging_mixin.py:190} INFO - [2024-12-17T15:20:06.494+0530] {dag.py:4396} INFO - [DAG TEST] starting task_id=scrape_collection_page map_index=-1
[2024-12-17T15:20:06.494+0530] {logging_mixin.py:190} INFO - [2024-12-17T15:20:06.494+0530] {dag.py:4399} INFO - [DAG TEST] running task <TaskInstance: movie_pipeline.scrape_collection_page manual__2024-12-17T09:50:06.092972+00:00 [scheduled]>
[2024-12-17T15:20:11.620+0530] {logging_mixin.py:190} INFO - [2024-12-17T15:20:11.620+0530] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-12-17T15:20:11.708+0530] {logging_mixin.py:190} INFO - [2024-12-17 15:20:11,708] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T09:50:06.092972+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T09:50:06.092972+00:00'
[2024-12-17T15:20:11.708+0530] {logging_mixin.py:190} INFO - [2024-12-17T15:20:11.708+0530] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T09:50:06.092972+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T09:50:06.092972+00:00'
[2024-12-17T15:20:11.717+0530] {logging_mixin.py:190} INFO - Task instance is in running state
[2024-12-17T15:20:11.718+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: queued
[2024-12-17T15:20:11.718+0530] {logging_mixin.py:190} INFO - Current task name:scrape_collection_page state:scheduled start_date:None
[2024-12-17T15:20:11.719+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline and current dag run status:running
[2024-12-17T15:20:11.719+0530] {logging_mixin.py:190} INFO - [2024-12-17T15:20:11.719+0530] {taskinstance.py:731} INFO - ::endgroup::
[2024-12-17T15:20:11.733+0530] {logging_mixin.py:190} INFO - 2024-12-17
[2024-12-17T15:20:11.734+0530] {logging_mixin.py:190} INFO - [2024-12-17 15:20:11,734] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T15:20:11.734+0530] {logging_mixin.py:190} INFO - [2024-12-17T15:20:11.734+0530] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T15:20:11.769+0530] {logging_mixin.py:190} INFO - [2024-12-17T15:20:11.768+0530] {taskinstance.py:340} INFO - ::group::Post task execution logs
[2024-12-17T15:20:11.770+0530] {logging_mixin.py:190} INFO - [2024-12-17T15:20:11.770+0530] {taskinstance.py:352} INFO - Marking task as SUCCESS. dag_id=movie_pipeline, task_id=scrape_collection_page, run_id=manual__2024-12-17T09:50:06.092972+00:00, execution_date=20241217T095006, start_date=, end_date=20241217T095011
[2024-12-17T15:20:11.799+0530] {logging_mixin.py:190} INFO - Task instance in success state
[2024-12-17T15:20:11.799+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: running
[2024-12-17T15:20:11.800+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline queued_at:None
[2024-12-17T15:20:11.800+0530] {logging_mixin.py:190} INFO - Task hostname:Jayeshs-MacBook-Air.local operator:PythonOperator
[2024-12-17T15:20:11.805+0530] {logging_mixin.py:190} INFO - [2024-12-17T15:20:11.803+0530] {dag.py:4410} INFO - [DAG TEST] end task task_id=scrape_collection_page map_index=-1
[2024-12-17T15:20:11.818+0530] {logging_mixin.py:190} INFO - [2024-12-17T15:20:11.818+0530] {dagrun.py:854} INFO - Marking run <DagRun movie_pipeline @ 2024-12-17 09:50:06.092972+00:00: manual__2024-12-17T09:50:06.092972+00:00, state:running, queued_at: None. externally triggered: False> successful
[2024-12-17T15:20:11.819+0530] {logging_mixin.py:190} INFO - Dag run in success state
[2024-12-17T15:20:11.819+0530] {logging_mixin.py:190} INFO - Dag run start:2024-12-17 09:50:06.092972+00:00 end:2024-12-17 09:50:11.819273+00:00
[2024-12-17T15:20:11.820+0530] {logging_mixin.py:190} INFO - [2024-12-17T15:20:11.819+0530] {dagrun.py:905} INFO - DagRun Finished: dag_id=movie_pipeline, execution_date=2024-12-17 09:50:06.092972+00:00, run_id=manual__2024-12-17T09:50:06.092972+00:00, run_start_date=2024-12-17 09:50:06.092972+00:00, run_end_date=2024-12-17 09:50:11.819273+00:00, run_duration=5.726301, state=success, external_trigger=False, run_type=manual, data_interval_start=2024-12-16 09:50:06.092972+00:00, data_interval_end=2024-12-17 09:50:06.092972+00:00, dag_hash=None
[2024-12-17T15:20:11.832+0530] {processor.py:925} INFO - DAG(s) 'movie_pipeline' retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T15:20:11.856+0530] {logging_mixin.py:190} INFO - [2024-12-17T15:20:11.855+0530] {dag.py:3239} INFO - Sync 1 DAGs
[2024-12-17T15:20:11.882+0530] {logging_mixin.py:190} INFO - [2024-12-17T15:20:11.881+0530] {dag.py:4180} INFO - Setting next_dagrun for movie_pipeline to 2024-12-17 01:30:00+00:00, run_after=2024-12-18 01:30:00+00:00
[2024-12-17T15:20:11.902+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 5.878 seconds
[2024-12-17T15:23:45.948+0530] {processor.py:186} INFO - Started process (PID=70506) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T15:23:45.951+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-17T15:23:45.952+0530] {logging_mixin.py:190} INFO - [2024-12-17T15:23:45.952+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T15:23:46.114+0530] {logging_mixin.py:190} INFO - [2024-12-17T15:23:46.114+0530] {dag.py:4435} INFO - dagrun id: movie_pipeline
[2024-12-17T15:23:46.127+0530] {logging_mixin.py:190} INFO - [2024-12-17T15:23:46.127+0530] {dag.py:4451} INFO - created dagrun <DagRun movie_pipeline @ 2024-12-17 09:53:45.991495+00:00: manual__2024-12-17T09:53:45.991495+00:00, state:running, queued_at: None. externally triggered: False>
[2024-12-17T15:23:46.157+0530] {logging_mixin.py:190} INFO - [2024-12-17T15:23:46.157+0530] {dag.py:4396} INFO - [DAG TEST] starting task_id=scrape_collection_page map_index=-1
[2024-12-17T15:23:46.158+0530] {logging_mixin.py:190} INFO - [2024-12-17T15:23:46.157+0530] {dag.py:4399} INFO - [DAG TEST] running task <TaskInstance: movie_pipeline.scrape_collection_page manual__2024-12-17T09:53:45.991495+00:00 [scheduled]>
[2024-12-17T15:23:51.250+0530] {logging_mixin.py:190} INFO - [2024-12-17T15:23:51.250+0530] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-12-17T15:23:51.312+0530] {logging_mixin.py:190} INFO - [2024-12-17 15:23:51,312] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T09:53:45.991495+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T09:53:45.991495+00:00'
[2024-12-17T15:23:51.313+0530] {logging_mixin.py:190} INFO - [2024-12-17T15:23:51.312+0530] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T09:53:45.991495+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T09:53:45.991495+00:00'
[2024-12-17T15:23:51.325+0530] {logging_mixin.py:190} INFO - Task instance is in running state
[2024-12-17T15:23:51.326+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: queued
[2024-12-17T15:23:51.326+0530] {logging_mixin.py:190} INFO - Current task name:scrape_collection_page state:scheduled start_date:None
[2024-12-17T15:23:51.326+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline and current dag run status:running
[2024-12-17T15:23:51.327+0530] {logging_mixin.py:190} INFO - [2024-12-17T15:23:51.327+0530] {taskinstance.py:731} INFO - ::endgroup::
[2024-12-17T15:23:51.329+0530] {logging_mixin.py:190} INFO - 2024-12-17
[2024-12-17T15:23:51.330+0530] {logging_mixin.py:190} INFO - [2024-12-17 15:23:51,329] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T15:23:51.330+0530] {logging_mixin.py:190} INFO - [2024-12-17T15:23:51.329+0530] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T15:23:51.335+0530] {logging_mixin.py:190} INFO - [2024-12-17T15:23:51.335+0530] {taskinstance.py:340} INFO - ::group::Post task execution logs
[2024-12-17T15:23:51.335+0530] {logging_mixin.py:190} INFO - [2024-12-17T15:23:51.335+0530] {taskinstance.py:352} INFO - Marking task as SUCCESS. dag_id=movie_pipeline, task_id=scrape_collection_page, run_id=manual__2024-12-17T09:53:45.991495+00:00, execution_date=20241217T095345, start_date=, end_date=20241217T095351
[2024-12-17T15:23:51.346+0530] {logging_mixin.py:190} INFO - Task instance in success state
[2024-12-17T15:23:51.346+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: running
[2024-12-17T15:23:51.346+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline queued_at:None
[2024-12-17T15:23:51.347+0530] {logging_mixin.py:190} INFO - Task hostname:Jayeshs-MacBook-Air.local operator:PythonOperator
[2024-12-17T15:23:51.347+0530] {logging_mixin.py:190} INFO - [2024-12-17T15:23:51.347+0530] {dag.py:4410} INFO - [DAG TEST] end task task_id=scrape_collection_page map_index=-1
[2024-12-17T15:23:51.351+0530] {logging_mixin.py:190} INFO - [2024-12-17T15:23:51.350+0530] {dagrun.py:854} INFO - Marking run <DagRun movie_pipeline @ 2024-12-17 09:53:45.991495+00:00: manual__2024-12-17T09:53:45.991495+00:00, state:running, queued_at: None. externally triggered: False> successful
[2024-12-17T15:23:51.351+0530] {logging_mixin.py:190} INFO - Dag run in success state
[2024-12-17T15:23:51.351+0530] {logging_mixin.py:190} INFO - Dag run start:2024-12-17 09:53:45.991495+00:00 end:2024-12-17 09:53:51.351376+00:00
[2024-12-17T15:23:51.352+0530] {logging_mixin.py:190} INFO - [2024-12-17T15:23:51.351+0530] {dagrun.py:905} INFO - DagRun Finished: dag_id=movie_pipeline, execution_date=2024-12-17 09:53:45.991495+00:00, run_id=manual__2024-12-17T09:53:45.991495+00:00, run_start_date=2024-12-17 09:53:45.991495+00:00, run_end_date=2024-12-17 09:53:51.351376+00:00, run_duration=5.359881, state=success, external_trigger=False, run_type=manual, data_interval_start=2024-12-16 09:53:45.991495+00:00, data_interval_end=2024-12-17 09:53:45.991495+00:00, dag_hash=None
[2024-12-17T15:23:51.361+0530] {processor.py:925} INFO - DAG(s) 'movie_pipeline' retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T15:23:51.372+0530] {logging_mixin.py:190} INFO - [2024-12-17T15:23:51.372+0530] {dag.py:3239} INFO - Sync 1 DAGs
[2024-12-17T15:23:51.389+0530] {logging_mixin.py:190} INFO - [2024-12-17T15:23:51.389+0530] {dag.py:4180} INFO - Setting next_dagrun for movie_pipeline to 2024-12-17 01:30:00+00:00, run_after=2024-12-18 01:30:00+00:00
[2024-12-17T15:23:51.407+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 5.462 seconds
[2024-12-17T15:26:27.767+0530] {processor.py:186} INFO - Started process (PID=70719) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T15:26:27.768+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-17T15:26:27.769+0530] {logging_mixin.py:190} INFO - [2024-12-17T15:26:27.769+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T15:26:27.929+0530] {logging_mixin.py:190} INFO - [2024-12-17T15:26:27.929+0530] {dag.py:4435} INFO - dagrun id: movie_pipeline
[2024-12-17T15:26:27.943+0530] {logging_mixin.py:190} INFO - [2024-12-17T15:26:27.942+0530] {dag.py:4451} INFO - created dagrun <DagRun movie_pipeline @ 2024-12-17 09:56:27.808130+00:00: manual__2024-12-17T09:56:27.808130+00:00, state:running, queued_at: None. externally triggered: False>
[2024-12-17T15:26:27.968+0530] {logging_mixin.py:190} INFO - [2024-12-17T15:26:27.968+0530] {dag.py:4396} INFO - [DAG TEST] starting task_id=scrape_collection_page map_index=-1
[2024-12-17T15:26:27.969+0530] {logging_mixin.py:190} INFO - [2024-12-17T15:26:27.968+0530] {dag.py:4399} INFO - [DAG TEST] running task <TaskInstance: movie_pipeline.scrape_collection_page manual__2024-12-17T09:56:27.808130+00:00 [scheduled]>
[2024-12-17T15:26:33.065+0530] {logging_mixin.py:190} INFO - [2024-12-17T15:26:33.065+0530] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-12-17T15:26:33.125+0530] {logging_mixin.py:190} INFO - [2024-12-17 15:26:33,125] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T09:56:27.808130+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T09:56:27.808130+00:00'
[2024-12-17T15:26:33.125+0530] {logging_mixin.py:190} INFO - [2024-12-17T15:26:33.125+0530] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T09:56:27.808130+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T09:56:27.808130+00:00'
[2024-12-17T15:26:33.128+0530] {logging_mixin.py:190} INFO - Task instance is in running state
[2024-12-17T15:26:33.129+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: queued
[2024-12-17T15:26:33.129+0530] {logging_mixin.py:190} INFO - Current task name:scrape_collection_page state:scheduled start_date:None
[2024-12-17T15:26:33.129+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline and current dag run status:running
[2024-12-17T15:26:33.130+0530] {logging_mixin.py:190} INFO - [2024-12-17T15:26:33.129+0530] {taskinstance.py:731} INFO - ::endgroup::
[2024-12-17T15:26:33.131+0530] {logging_mixin.py:190} INFO - 2024-12-17
[2024-12-17T15:26:33.132+0530] {logging_mixin.py:190} INFO - [2024-12-17 15:26:33,131] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T15:26:33.132+0530] {logging_mixin.py:190} INFO - [2024-12-17T15:26:33.131+0530] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T15:26:33.137+0530] {logging_mixin.py:190} INFO - [2024-12-17T15:26:33.137+0530] {taskinstance.py:340} INFO - ::group::Post task execution logs
[2024-12-17T15:26:33.137+0530] {logging_mixin.py:190} INFO - [2024-12-17T15:26:33.137+0530] {taskinstance.py:352} INFO - Marking task as SUCCESS. dag_id=movie_pipeline, task_id=scrape_collection_page, run_id=manual__2024-12-17T09:56:27.808130+00:00, execution_date=20241217T095627, start_date=, end_date=20241217T095633
[2024-12-17T15:26:33.146+0530] {logging_mixin.py:190} INFO - Task instance in success state
[2024-12-17T15:26:33.147+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: running
[2024-12-17T15:26:33.147+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline queued_at:None
[2024-12-17T15:26:33.148+0530] {logging_mixin.py:190} INFO - Task hostname:Jayeshs-MacBook-Air.local operator:PythonOperator
[2024-12-17T15:26:33.149+0530] {logging_mixin.py:190} INFO - [2024-12-17T15:26:33.148+0530] {dag.py:4410} INFO - [DAG TEST] end task task_id=scrape_collection_page map_index=-1
[2024-12-17T15:26:33.153+0530] {logging_mixin.py:190} INFO - [2024-12-17T15:26:33.152+0530] {dagrun.py:854} INFO - Marking run <DagRun movie_pipeline @ 2024-12-17 09:56:27.808130+00:00: manual__2024-12-17T09:56:27.808130+00:00, state:running, queued_at: None. externally triggered: False> successful
[2024-12-17T15:26:33.153+0530] {logging_mixin.py:190} INFO - Dag run in success state
[2024-12-17T15:26:33.153+0530] {logging_mixin.py:190} INFO - Dag run start:2024-12-17 09:56:27.808130+00:00 end:2024-12-17 09:56:33.153593+00:00
[2024-12-17T15:26:33.154+0530] {logging_mixin.py:190} INFO - [2024-12-17T15:26:33.154+0530] {dagrun.py:905} INFO - DagRun Finished: dag_id=movie_pipeline, execution_date=2024-12-17 09:56:27.808130+00:00, run_id=manual__2024-12-17T09:56:27.808130+00:00, run_start_date=2024-12-17 09:56:27.808130+00:00, run_end_date=2024-12-17 09:56:33.153593+00:00, run_duration=5.345463, state=success, external_trigger=False, run_type=manual, data_interval_start=2024-12-16 09:56:27.808130+00:00, data_interval_end=2024-12-17 09:56:27.808130+00:00, dag_hash=None
[2024-12-17T15:26:33.163+0530] {processor.py:925} INFO - DAG(s) 'movie_pipeline' retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T15:26:33.174+0530] {logging_mixin.py:190} INFO - [2024-12-17T15:26:33.174+0530] {dag.py:3239} INFO - Sync 1 DAGs
[2024-12-17T15:26:33.191+0530] {logging_mixin.py:190} INFO - [2024-12-17T15:26:33.191+0530] {dag.py:4180} INFO - Setting next_dagrun for movie_pipeline to 2024-12-17 01:30:00+00:00, run_after=2024-12-18 01:30:00+00:00
[2024-12-17T15:26:33.211+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 5.448 seconds
[2024-12-17T15:29:51.548+0530] {processor.py:186} INFO - Started process (PID=70934) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T15:29:51.551+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-17T15:29:51.553+0530] {logging_mixin.py:190} INFO - [2024-12-17T15:29:51.552+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T15:29:51.718+0530] {logging_mixin.py:190} INFO - [2024-12-17T15:29:51.717+0530] {dag.py:4435} INFO - dagrun id: movie_pipeline
[2024-12-17T15:29:51.733+0530] {logging_mixin.py:190} INFO - [2024-12-17T15:29:51.732+0530] {dag.py:4451} INFO - created dagrun <DagRun movie_pipeline @ 2024-12-17 09:59:51.593606+00:00: manual__2024-12-17T09:59:51.593606+00:00, state:running, queued_at: None. externally triggered: False>
[2024-12-17T15:29:51.764+0530] {logging_mixin.py:190} INFO - [2024-12-17T15:29:51.764+0530] {dag.py:4396} INFO - [DAG TEST] starting task_id=scrape_collection_page map_index=-1
[2024-12-17T15:29:51.765+0530] {logging_mixin.py:190} INFO - [2024-12-17T15:29:51.764+0530] {dag.py:4399} INFO - [DAG TEST] running task <TaskInstance: movie_pipeline.scrape_collection_page manual__2024-12-17T09:59:51.593606+00:00 [scheduled]>
[2024-12-17T15:29:56.868+0530] {logging_mixin.py:190} INFO - [2024-12-17T15:29:56.868+0530] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-12-17T15:29:56.940+0530] {logging_mixin.py:190} INFO - [2024-12-17 15:29:56,940] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T09:59:51.593606+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T09:59:51.593606+00:00'
[2024-12-17T15:29:56.941+0530] {logging_mixin.py:190} INFO - [2024-12-17T15:29:56.940+0530] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T09:59:51.593606+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T09:59:51.593606+00:00'
[2024-12-17T15:29:56.954+0530] {logging_mixin.py:190} INFO - Task instance is in running state
[2024-12-17T15:29:56.954+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: queued
[2024-12-17T15:29:56.955+0530] {logging_mixin.py:190} INFO - Current task name:scrape_collection_page state:scheduled start_date:None
[2024-12-17T15:29:56.955+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline and current dag run status:running
[2024-12-17T15:29:56.956+0530] {logging_mixin.py:190} INFO - [2024-12-17T15:29:56.955+0530] {taskinstance.py:731} INFO - ::endgroup::
[2024-12-17T15:29:56.959+0530] {logging_mixin.py:190} INFO - 2024-12-17
[2024-12-17T15:29:56.960+0530] {logging_mixin.py:190} INFO - [2024-12-17 15:29:56,959] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T15:29:56.960+0530] {logging_mixin.py:190} INFO - [2024-12-17T15:29:56.959+0530] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T15:29:56.965+0530] {logging_mixin.py:190} INFO - [2024-12-17T15:29:56.965+0530] {taskinstance.py:340} INFO - ::group::Post task execution logs
[2024-12-17T15:29:56.965+0530] {logging_mixin.py:190} INFO - [2024-12-17T15:29:56.965+0530] {taskinstance.py:352} INFO - Marking task as SUCCESS. dag_id=movie_pipeline, task_id=scrape_collection_page, run_id=manual__2024-12-17T09:59:51.593606+00:00, execution_date=20241217T095951, start_date=, end_date=20241217T095956
[2024-12-17T15:29:56.975+0530] {logging_mixin.py:190} INFO - Task instance in success state
[2024-12-17T15:29:56.975+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: running
[2024-12-17T15:29:56.976+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline queued_at:None
[2024-12-17T15:29:56.976+0530] {logging_mixin.py:190} INFO - Task hostname:Jayeshs-MacBook-Air.local operator:PythonOperator
[2024-12-17T15:29:56.977+0530] {logging_mixin.py:190} INFO - [2024-12-17T15:29:56.976+0530] {dag.py:4410} INFO - [DAG TEST] end task task_id=scrape_collection_page map_index=-1
[2024-12-17T15:29:56.981+0530] {logging_mixin.py:190} INFO - [2024-12-17T15:29:56.981+0530] {dagrun.py:854} INFO - Marking run <DagRun movie_pipeline @ 2024-12-17 09:59:51.593606+00:00: manual__2024-12-17T09:59:51.593606+00:00, state:running, queued_at: None. externally triggered: False> successful
[2024-12-17T15:29:56.981+0530] {logging_mixin.py:190} INFO - Dag run in success state
[2024-12-17T15:29:56.982+0530] {logging_mixin.py:190} INFO - Dag run start:2024-12-17 09:59:51.593606+00:00 end:2024-12-17 09:59:56.981742+00:00
[2024-12-17T15:29:56.982+0530] {logging_mixin.py:190} INFO - [2024-12-17T15:29:56.982+0530] {dagrun.py:905} INFO - DagRun Finished: dag_id=movie_pipeline, execution_date=2024-12-17 09:59:51.593606+00:00, run_id=manual__2024-12-17T09:59:51.593606+00:00, run_start_date=2024-12-17 09:59:51.593606+00:00, run_end_date=2024-12-17 09:59:56.981742+00:00, run_duration=5.388136, state=success, external_trigger=False, run_type=manual, data_interval_start=2024-12-16 09:59:51.593606+00:00, data_interval_end=2024-12-17 09:59:51.593606+00:00, dag_hash=None
[2024-12-17T15:29:56.990+0530] {processor.py:925} INFO - DAG(s) 'movie_pipeline' retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T15:29:57.002+0530] {logging_mixin.py:190} INFO - [2024-12-17T15:29:57.002+0530] {dag.py:3239} INFO - Sync 1 DAGs
[2024-12-17T15:29:57.019+0530] {logging_mixin.py:190} INFO - [2024-12-17T15:29:57.019+0530] {dag.py:4180} INFO - Setting next_dagrun for movie_pipeline to 2024-12-17 01:30:00+00:00, run_after=2024-12-18 01:30:00+00:00
[2024-12-17T15:29:57.040+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 5.495 seconds
[2024-12-17T15:33:23.777+0530] {processor.py:186} INFO - Started process (PID=71185) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T15:33:23.780+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-17T15:33:23.782+0530] {logging_mixin.py:190} INFO - [2024-12-17T15:33:23.782+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T15:33:23.976+0530] {logging_mixin.py:190} INFO - [2024-12-17T15:33:23.976+0530] {dag.py:4435} INFO - dagrun id: movie_pipeline
[2024-12-17T15:33:23.991+0530] {logging_mixin.py:190} INFO - [2024-12-17T15:33:23.990+0530] {dag.py:4451} INFO - created dagrun <DagRun movie_pipeline @ 2024-12-17 10:03:23.828592+00:00: manual__2024-12-17T10:03:23.828592+00:00, state:running, queued_at: None. externally triggered: False>
[2024-12-17T15:33:24.019+0530] {logging_mixin.py:190} INFO - [2024-12-17T15:33:24.019+0530] {dag.py:4396} INFO - [DAG TEST] starting task_id=scrape_collection_page map_index=-1
[2024-12-17T15:33:24.020+0530] {logging_mixin.py:190} INFO - [2024-12-17T15:33:24.020+0530] {dag.py:4399} INFO - [DAG TEST] running task <TaskInstance: movie_pipeline.scrape_collection_page manual__2024-12-17T10:03:23.828592+00:00 [scheduled]>
[2024-12-17T15:33:29.114+0530] {logging_mixin.py:190} INFO - [2024-12-17T15:33:29.113+0530] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-12-17T15:33:29.176+0530] {logging_mixin.py:190} INFO - [2024-12-17 15:33:29,176] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T10:03:23.828592+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T10:03:23.828592+00:00'
[2024-12-17T15:33:29.177+0530] {logging_mixin.py:190} INFO - [2024-12-17T15:33:29.176+0530] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T10:03:23.828592+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T10:03:23.828592+00:00'
[2024-12-17T15:33:29.189+0530] {logging_mixin.py:190} INFO - Task instance is in running state
[2024-12-17T15:33:29.189+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: queued
[2024-12-17T15:33:29.190+0530] {logging_mixin.py:190} INFO - Current task name:scrape_collection_page state:scheduled start_date:None
[2024-12-17T15:33:29.190+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline and current dag run status:running
[2024-12-17T15:33:29.190+0530] {logging_mixin.py:190} INFO - [2024-12-17T15:33:29.190+0530] {taskinstance.py:731} INFO - ::endgroup::
[2024-12-17T15:33:29.192+0530] {logging_mixin.py:190} INFO - 2024-12-17
[2024-12-17T15:33:29.193+0530] {logging_mixin.py:190} INFO - [2024-12-17 15:33:29,193] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T15:33:29.193+0530] {logging_mixin.py:190} INFO - [2024-12-17T15:33:29.193+0530] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T15:33:29.199+0530] {logging_mixin.py:190} INFO - [2024-12-17T15:33:29.198+0530] {taskinstance.py:340} INFO - ::group::Post task execution logs
[2024-12-17T15:33:29.199+0530] {logging_mixin.py:190} INFO - [2024-12-17T15:33:29.199+0530] {taskinstance.py:352} INFO - Marking task as SUCCESS. dag_id=movie_pipeline, task_id=scrape_collection_page, run_id=manual__2024-12-17T10:03:23.828592+00:00, execution_date=20241217T100323, start_date=, end_date=20241217T100329
[2024-12-17T15:33:29.211+0530] {logging_mixin.py:190} INFO - Task instance in success state
[2024-12-17T15:33:29.211+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: running
[2024-12-17T15:33:29.212+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline queued_at:None
[2024-12-17T15:33:29.212+0530] {logging_mixin.py:190} INFO - Task hostname:Jayeshs-MacBook-Air.local operator:PythonOperator
[2024-12-17T15:33:29.212+0530] {logging_mixin.py:190} INFO - [2024-12-17T15:33:29.212+0530] {dag.py:4410} INFO - [DAG TEST] end task task_id=scrape_collection_page map_index=-1
[2024-12-17T15:33:29.216+0530] {logging_mixin.py:190} INFO - [2024-12-17T15:33:29.216+0530] {dagrun.py:854} INFO - Marking run <DagRun movie_pipeline @ 2024-12-17 10:03:23.828592+00:00: manual__2024-12-17T10:03:23.828592+00:00, state:running, queued_at: None. externally triggered: False> successful
[2024-12-17T15:33:29.216+0530] {logging_mixin.py:190} INFO - Dag run in success state
[2024-12-17T15:33:29.217+0530] {logging_mixin.py:190} INFO - Dag run start:2024-12-17 10:03:23.828592+00:00 end:2024-12-17 10:03:29.216791+00:00
[2024-12-17T15:33:29.217+0530] {logging_mixin.py:190} INFO - [2024-12-17T15:33:29.217+0530] {dagrun.py:905} INFO - DagRun Finished: dag_id=movie_pipeline, execution_date=2024-12-17 10:03:23.828592+00:00, run_id=manual__2024-12-17T10:03:23.828592+00:00, run_start_date=2024-12-17 10:03:23.828592+00:00, run_end_date=2024-12-17 10:03:29.216791+00:00, run_duration=5.388199, state=success, external_trigger=False, run_type=manual, data_interval_start=2024-12-16 10:03:23.828592+00:00, data_interval_end=2024-12-17 10:03:23.828592+00:00, dag_hash=None
[2024-12-17T15:33:29.226+0530] {processor.py:925} INFO - DAG(s) 'movie_pipeline' retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T15:33:29.237+0530] {logging_mixin.py:190} INFO - [2024-12-17T15:33:29.237+0530] {dag.py:3239} INFO - Sync 1 DAGs
[2024-12-17T15:33:29.254+0530] {logging_mixin.py:190} INFO - [2024-12-17T15:33:29.254+0530] {dag.py:4180} INFO - Setting next_dagrun for movie_pipeline to 2024-12-17 01:30:00+00:00, run_after=2024-12-18 01:30:00+00:00
[2024-12-17T15:33:29.272+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 5.498 seconds
[2024-12-17T15:36:32.639+0530] {processor.py:186} INFO - Started process (PID=71405) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T15:36:32.642+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-17T15:36:32.643+0530] {logging_mixin.py:190} INFO - [2024-12-17T15:36:32.643+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T15:36:32.824+0530] {logging_mixin.py:190} INFO - [2024-12-17T15:36:32.824+0530] {dag.py:4435} INFO - dagrun id: movie_pipeline
[2024-12-17T15:36:32.838+0530] {logging_mixin.py:190} INFO - [2024-12-17T15:36:32.838+0530] {dag.py:4451} INFO - created dagrun <DagRun movie_pipeline @ 2024-12-17 10:06:32.687131+00:00: manual__2024-12-17T10:06:32.687131+00:00, state:running, queued_at: None. externally triggered: False>
[2024-12-17T15:36:32.871+0530] {logging_mixin.py:190} INFO - [2024-12-17T15:36:32.870+0530] {dag.py:4396} INFO - [DAG TEST] starting task_id=scrape_collection_page map_index=-1
[2024-12-17T15:36:32.871+0530] {logging_mixin.py:190} INFO - [2024-12-17T15:36:32.871+0530] {dag.py:4399} INFO - [DAG TEST] running task <TaskInstance: movie_pipeline.scrape_collection_page manual__2024-12-17T10:06:32.687131+00:00 [scheduled]>
[2024-12-17T15:36:37.968+0530] {logging_mixin.py:190} INFO - [2024-12-17T15:36:37.968+0530] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-12-17T15:36:38.034+0530] {logging_mixin.py:190} INFO - [2024-12-17 15:36:38,034] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T10:06:32.687131+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T10:06:32.687131+00:00'
[2024-12-17T15:36:38.035+0530] {logging_mixin.py:190} INFO - [2024-12-17T15:36:38.034+0530] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T10:06:32.687131+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T10:06:32.687131+00:00'
[2024-12-17T15:36:38.048+0530] {logging_mixin.py:190} INFO - Task instance is in running state
[2024-12-17T15:36:38.049+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: queued
[2024-12-17T15:36:38.049+0530] {logging_mixin.py:190} INFO - Current task name:scrape_collection_page state:scheduled start_date:None
[2024-12-17T15:36:38.049+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline and current dag run status:running
[2024-12-17T15:36:38.050+0530] {logging_mixin.py:190} INFO - [2024-12-17T15:36:38.050+0530] {taskinstance.py:731} INFO - ::endgroup::
[2024-12-17T15:36:38.052+0530] {logging_mixin.py:190} INFO - 2024-12-17
[2024-12-17T15:36:38.052+0530] {logging_mixin.py:190} INFO - [2024-12-17 15:36:38,052] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T15:36:38.053+0530] {logging_mixin.py:190} INFO - [2024-12-17T15:36:38.052+0530] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T15:36:38.059+0530] {logging_mixin.py:190} INFO - [2024-12-17T15:36:38.059+0530] {taskinstance.py:340} INFO - ::group::Post task execution logs
[2024-12-17T15:36:38.060+0530] {logging_mixin.py:190} INFO - [2024-12-17T15:36:38.060+0530] {taskinstance.py:352} INFO - Marking task as SUCCESS. dag_id=movie_pipeline, task_id=scrape_collection_page, run_id=manual__2024-12-17T10:06:32.687131+00:00, execution_date=20241217T100632, start_date=, end_date=20241217T100638
[2024-12-17T15:36:38.070+0530] {logging_mixin.py:190} INFO - Task instance in success state
[2024-12-17T15:36:38.070+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: running
[2024-12-17T15:36:38.070+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline queued_at:None
[2024-12-17T15:36:38.071+0530] {logging_mixin.py:190} INFO - Task hostname:Jayeshs-MacBook-Air.local operator:PythonOperator
[2024-12-17T15:36:38.071+0530] {logging_mixin.py:190} INFO - [2024-12-17T15:36:38.071+0530] {dag.py:4410} INFO - [DAG TEST] end task task_id=scrape_collection_page map_index=-1
[2024-12-17T15:36:38.075+0530] {logging_mixin.py:190} INFO - [2024-12-17T15:36:38.074+0530] {dagrun.py:854} INFO - Marking run <DagRun movie_pipeline @ 2024-12-17 10:06:32.687131+00:00: manual__2024-12-17T10:06:32.687131+00:00, state:running, queued_at: None. externally triggered: False> successful
[2024-12-17T15:36:38.075+0530] {logging_mixin.py:190} INFO - Dag run in success state
[2024-12-17T15:36:38.075+0530] {logging_mixin.py:190} INFO - Dag run start:2024-12-17 10:06:32.687131+00:00 end:2024-12-17 10:06:38.075476+00:00
[2024-12-17T15:36:38.076+0530] {logging_mixin.py:190} INFO - [2024-12-17T15:36:38.075+0530] {dagrun.py:905} INFO - DagRun Finished: dag_id=movie_pipeline, execution_date=2024-12-17 10:06:32.687131+00:00, run_id=manual__2024-12-17T10:06:32.687131+00:00, run_start_date=2024-12-17 10:06:32.687131+00:00, run_end_date=2024-12-17 10:06:38.075476+00:00, run_duration=5.388345, state=success, external_trigger=False, run_type=manual, data_interval_start=2024-12-16 10:06:32.687131+00:00, data_interval_end=2024-12-17 10:06:32.687131+00:00, dag_hash=None
[2024-12-17T15:36:38.085+0530] {processor.py:925} INFO - DAG(s) 'movie_pipeline' retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T15:36:38.097+0530] {logging_mixin.py:190} INFO - [2024-12-17T15:36:38.096+0530] {dag.py:3239} INFO - Sync 1 DAGs
[2024-12-17T15:36:38.118+0530] {logging_mixin.py:190} INFO - [2024-12-17T15:36:38.118+0530] {dag.py:4180} INFO - Setting next_dagrun for movie_pipeline to 2024-12-17 01:30:00+00:00, run_after=2024-12-18 01:30:00+00:00
[2024-12-17T15:36:38.137+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 5.502 seconds
[2024-12-17T15:39:21.155+0530] {processor.py:186} INFO - Started process (PID=71594) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T15:39:21.156+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-17T15:39:21.158+0530] {logging_mixin.py:190} INFO - [2024-12-17T15:39:21.157+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T15:39:21.338+0530] {logging_mixin.py:190} INFO - [2024-12-17T15:39:21.338+0530] {dag.py:4435} INFO - dagrun id: movie_pipeline
[2024-12-17T15:39:21.352+0530] {logging_mixin.py:190} INFO - [2024-12-17T15:39:21.352+0530] {dag.py:4451} INFO - created dagrun <DagRun movie_pipeline @ 2024-12-17 10:09:21.199014+00:00: manual__2024-12-17T10:09:21.199014+00:00, state:running, queued_at: None. externally triggered: False>
[2024-12-17T15:39:21.379+0530] {logging_mixin.py:190} INFO - [2024-12-17T15:39:21.378+0530] {dag.py:4396} INFO - [DAG TEST] starting task_id=scrape_collection_page map_index=-1
[2024-12-17T15:39:21.379+0530] {logging_mixin.py:190} INFO - [2024-12-17T15:39:21.379+0530] {dag.py:4399} INFO - [DAG TEST] running task <TaskInstance: movie_pipeline.scrape_collection_page manual__2024-12-17T10:09:21.199014+00:00 [scheduled]>
[2024-12-17T15:39:26.473+0530] {logging_mixin.py:190} INFO - [2024-12-17T15:39:26.473+0530] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-12-17T15:39:26.532+0530] {logging_mixin.py:190} INFO - [2024-12-17 15:39:26,532] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T10:09:21.199014+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T10:09:21.199014+00:00'
[2024-12-17T15:39:26.533+0530] {logging_mixin.py:190} INFO - [2024-12-17T15:39:26.532+0530] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T10:09:21.199014+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T10:09:21.199014+00:00'
[2024-12-17T15:39:26.536+0530] {logging_mixin.py:190} INFO - Task instance is in running state
[2024-12-17T15:39:26.536+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: queued
[2024-12-17T15:39:26.536+0530] {logging_mixin.py:190} INFO - Current task name:scrape_collection_page state:scheduled start_date:None
[2024-12-17T15:39:26.537+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline and current dag run status:running
[2024-12-17T15:39:26.537+0530] {logging_mixin.py:190} INFO - [2024-12-17T15:39:26.537+0530] {taskinstance.py:731} INFO - ::endgroup::
[2024-12-17T15:39:26.539+0530] {logging_mixin.py:190} INFO - 2024-12-17
[2024-12-17T15:39:26.539+0530] {logging_mixin.py:190} INFO - [2024-12-17 15:39:26,539] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T15:39:26.539+0530] {logging_mixin.py:190} INFO - [2024-12-17T15:39:26.539+0530] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T15:39:26.544+0530] {logging_mixin.py:190} INFO - [2024-12-17T15:39:26.544+0530] {taskinstance.py:340} INFO - ::group::Post task execution logs
[2024-12-17T15:39:26.545+0530] {logging_mixin.py:190} INFO - [2024-12-17T15:39:26.545+0530] {taskinstance.py:352} INFO - Marking task as SUCCESS. dag_id=movie_pipeline, task_id=scrape_collection_page, run_id=manual__2024-12-17T10:09:21.199014+00:00, execution_date=20241217T100921, start_date=, end_date=20241217T100926
[2024-12-17T15:39:26.554+0530] {logging_mixin.py:190} INFO - Task instance in success state
[2024-12-17T15:39:26.554+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: running
[2024-12-17T15:39:26.554+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline queued_at:None
[2024-12-17T15:39:26.555+0530] {logging_mixin.py:190} INFO - Task hostname:Jayeshs-MacBook-Air.local operator:PythonOperator
[2024-12-17T15:39:26.555+0530] {logging_mixin.py:190} INFO - [2024-12-17T15:39:26.555+0530] {dag.py:4410} INFO - [DAG TEST] end task task_id=scrape_collection_page map_index=-1
[2024-12-17T15:39:26.560+0530] {logging_mixin.py:190} INFO - [2024-12-17T15:39:26.560+0530] {dagrun.py:854} INFO - Marking run <DagRun movie_pipeline @ 2024-12-17 10:09:21.199014+00:00: manual__2024-12-17T10:09:21.199014+00:00, state:running, queued_at: None. externally triggered: False> successful
[2024-12-17T15:39:26.560+0530] {logging_mixin.py:190} INFO - Dag run in success state
[2024-12-17T15:39:26.561+0530] {logging_mixin.py:190} INFO - Dag run start:2024-12-17 10:09:21.199014+00:00 end:2024-12-17 10:09:26.560889+00:00
[2024-12-17T15:39:26.561+0530] {logging_mixin.py:190} INFO - [2024-12-17T15:39:26.561+0530] {dagrun.py:905} INFO - DagRun Finished: dag_id=movie_pipeline, execution_date=2024-12-17 10:09:21.199014+00:00, run_id=manual__2024-12-17T10:09:21.199014+00:00, run_start_date=2024-12-17 10:09:21.199014+00:00, run_end_date=2024-12-17 10:09:26.560889+00:00, run_duration=5.361875, state=success, external_trigger=False, run_type=manual, data_interval_start=2024-12-16 10:09:21.199014+00:00, data_interval_end=2024-12-17 10:09:21.199014+00:00, dag_hash=None
[2024-12-17T15:39:26.570+0530] {processor.py:925} INFO - DAG(s) 'movie_pipeline' retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T15:39:26.582+0530] {logging_mixin.py:190} INFO - [2024-12-17T15:39:26.581+0530] {dag.py:3239} INFO - Sync 1 DAGs
[2024-12-17T15:39:26.599+0530] {logging_mixin.py:190} INFO - [2024-12-17T15:39:26.599+0530] {dag.py:4180} INFO - Setting next_dagrun for movie_pipeline to 2024-12-17 01:30:00+00:00, run_after=2024-12-18 01:30:00+00:00
[2024-12-17T15:39:26.618+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 5.467 seconds
[2024-12-17T15:42:11.633+0530] {processor.py:186} INFO - Started process (PID=71798) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T15:42:11.634+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-17T15:42:11.635+0530] {logging_mixin.py:190} INFO - [2024-12-17T15:42:11.635+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T15:42:11.801+0530] {logging_mixin.py:190} INFO - [2024-12-17T15:42:11.801+0530] {dag.py:4435} INFO - dagrun id: movie_pipeline
[2024-12-17T15:42:11.815+0530] {logging_mixin.py:190} INFO - [2024-12-17T15:42:11.815+0530] {dag.py:4451} INFO - created dagrun <DagRun movie_pipeline @ 2024-12-17 10:12:11.674344+00:00: manual__2024-12-17T10:12:11.674344+00:00, state:running, queued_at: None. externally triggered: False>
[2024-12-17T15:42:11.841+0530] {logging_mixin.py:190} INFO - [2024-12-17T15:42:11.841+0530] {dag.py:4396} INFO - [DAG TEST] starting task_id=scrape_collection_page map_index=-1
[2024-12-17T15:42:11.842+0530] {logging_mixin.py:190} INFO - [2024-12-17T15:42:11.842+0530] {dag.py:4399} INFO - [DAG TEST] running task <TaskInstance: movie_pipeline.scrape_collection_page manual__2024-12-17T10:12:11.674344+00:00 [scheduled]>
[2024-12-17T15:42:16.938+0530] {logging_mixin.py:190} INFO - [2024-12-17T15:42:16.938+0530] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-12-17T15:42:16.999+0530] {logging_mixin.py:190} INFO - [2024-12-17 15:42:16,998] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T10:12:11.674344+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T10:12:11.674344+00:00'
[2024-12-17T15:42:16.999+0530] {logging_mixin.py:190} INFO - [2024-12-17T15:42:16.998+0530] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T10:12:11.674344+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T10:12:11.674344+00:00'
[2024-12-17T15:42:17.002+0530] {logging_mixin.py:190} INFO - Task instance is in running state
[2024-12-17T15:42:17.002+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: queued
[2024-12-17T15:42:17.002+0530] {logging_mixin.py:190} INFO - Current task name:scrape_collection_page state:scheduled start_date:None
[2024-12-17T15:42:17.002+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline and current dag run status:running
[2024-12-17T15:42:17.003+0530] {logging_mixin.py:190} INFO - [2024-12-17T15:42:17.003+0530] {taskinstance.py:731} INFO - ::endgroup::
[2024-12-17T15:42:17.005+0530] {logging_mixin.py:190} INFO - 2024-12-17
[2024-12-17T15:42:17.006+0530] {logging_mixin.py:190} INFO - [2024-12-17 15:42:17,006] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T15:42:17.006+0530] {logging_mixin.py:190} INFO - [2024-12-17T15:42:17.006+0530] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T15:42:17.011+0530] {logging_mixin.py:190} INFO - [2024-12-17T15:42:17.011+0530] {taskinstance.py:340} INFO - ::group::Post task execution logs
[2024-12-17T15:42:17.012+0530] {logging_mixin.py:190} INFO - [2024-12-17T15:42:17.011+0530] {taskinstance.py:352} INFO - Marking task as SUCCESS. dag_id=movie_pipeline, task_id=scrape_collection_page, run_id=manual__2024-12-17T10:12:11.674344+00:00, execution_date=20241217T101211, start_date=, end_date=20241217T101217
[2024-12-17T15:42:17.021+0530] {logging_mixin.py:190} INFO - Task instance in success state
[2024-12-17T15:42:17.021+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: running
[2024-12-17T15:42:17.021+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline queued_at:None
[2024-12-17T15:42:17.021+0530] {logging_mixin.py:190} INFO - Task hostname:Jayeshs-MacBook-Air.local operator:PythonOperator
[2024-12-17T15:42:17.022+0530] {logging_mixin.py:190} INFO - [2024-12-17T15:42:17.022+0530] {dag.py:4410} INFO - [DAG TEST] end task task_id=scrape_collection_page map_index=-1
[2024-12-17T15:42:17.026+0530] {logging_mixin.py:190} INFO - [2024-12-17T15:42:17.025+0530] {dagrun.py:854} INFO - Marking run <DagRun movie_pipeline @ 2024-12-17 10:12:11.674344+00:00: manual__2024-12-17T10:12:11.674344+00:00, state:running, queued_at: None. externally triggered: False> successful
[2024-12-17T15:42:17.026+0530] {logging_mixin.py:190} INFO - Dag run in success state
[2024-12-17T15:42:17.026+0530] {logging_mixin.py:190} INFO - Dag run start:2024-12-17 10:12:11.674344+00:00 end:2024-12-17 10:12:17.026472+00:00
[2024-12-17T15:42:17.027+0530] {logging_mixin.py:190} INFO - [2024-12-17T15:42:17.026+0530] {dagrun.py:905} INFO - DagRun Finished: dag_id=movie_pipeline, execution_date=2024-12-17 10:12:11.674344+00:00, run_id=manual__2024-12-17T10:12:11.674344+00:00, run_start_date=2024-12-17 10:12:11.674344+00:00, run_end_date=2024-12-17 10:12:17.026472+00:00, run_duration=5.352128, state=success, external_trigger=False, run_type=manual, data_interval_start=2024-12-16 10:12:11.674344+00:00, data_interval_end=2024-12-17 10:12:11.674344+00:00, dag_hash=None
[2024-12-17T15:42:17.036+0530] {processor.py:925} INFO - DAG(s) 'movie_pipeline' retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T15:42:17.049+0530] {logging_mixin.py:190} INFO - [2024-12-17T15:42:17.048+0530] {dag.py:3239} INFO - Sync 1 DAGs
[2024-12-17T15:42:17.066+0530] {logging_mixin.py:190} INFO - [2024-12-17T15:42:17.066+0530] {dag.py:4180} INFO - Setting next_dagrun for movie_pipeline to 2024-12-17 01:30:00+00:00, run_after=2024-12-18 01:30:00+00:00
[2024-12-17T15:42:17.084+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 5.455 seconds
[2024-12-17T15:45:03.076+0530] {processor.py:186} INFO - Started process (PID=72002) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T15:45:03.077+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-17T15:45:03.078+0530] {logging_mixin.py:190} INFO - [2024-12-17T15:45:03.078+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T15:45:03.244+0530] {logging_mixin.py:190} INFO - [2024-12-17T15:45:03.244+0530] {dag.py:4435} INFO - dagrun id: movie_pipeline
[2024-12-17T15:45:03.258+0530] {logging_mixin.py:190} INFO - [2024-12-17T15:45:03.257+0530] {dag.py:4451} INFO - created dagrun <DagRun movie_pipeline @ 2024-12-17 10:15:03.117598+00:00: manual__2024-12-17T10:15:03.117598+00:00, state:running, queued_at: None. externally triggered: False>
[2024-12-17T15:45:03.283+0530] {logging_mixin.py:190} INFO - [2024-12-17T15:45:03.283+0530] {dag.py:4396} INFO - [DAG TEST] starting task_id=scrape_collection_page map_index=-1
[2024-12-17T15:45:03.284+0530] {logging_mixin.py:190} INFO - [2024-12-17T15:45:03.283+0530] {dag.py:4399} INFO - [DAG TEST] running task <TaskInstance: movie_pipeline.scrape_collection_page manual__2024-12-17T10:15:03.117598+00:00 [scheduled]>
[2024-12-17T15:45:08.380+0530] {logging_mixin.py:190} INFO - [2024-12-17T15:45:08.380+0530] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-12-17T15:45:08.440+0530] {logging_mixin.py:190} INFO - [2024-12-17 15:45:08,440] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T10:15:03.117598+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T10:15:03.117598+00:00'
[2024-12-17T15:45:08.440+0530] {logging_mixin.py:190} INFO - [2024-12-17T15:45:08.440+0530] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T10:15:03.117598+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T10:15:03.117598+00:00'
[2024-12-17T15:45:08.443+0530] {logging_mixin.py:190} INFO - Task instance is in running state
[2024-12-17T15:45:08.444+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: queued
[2024-12-17T15:45:08.444+0530] {logging_mixin.py:190} INFO - Current task name:scrape_collection_page state:scheduled start_date:None
[2024-12-17T15:45:08.444+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline and current dag run status:running
[2024-12-17T15:45:08.445+0530] {logging_mixin.py:190} INFO - [2024-12-17T15:45:08.445+0530] {taskinstance.py:731} INFO - ::endgroup::
[2024-12-17T15:45:08.446+0530] {logging_mixin.py:190} INFO - 2024-12-17
[2024-12-17T15:45:08.447+0530] {logging_mixin.py:190} INFO - [2024-12-17 15:45:08,447] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T15:45:08.447+0530] {logging_mixin.py:190} INFO - [2024-12-17T15:45:08.447+0530] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T15:45:08.452+0530] {logging_mixin.py:190} INFO - [2024-12-17T15:45:08.452+0530] {taskinstance.py:340} INFO - ::group::Post task execution logs
[2024-12-17T15:45:08.453+0530] {logging_mixin.py:190} INFO - [2024-12-17T15:45:08.452+0530] {taskinstance.py:352} INFO - Marking task as SUCCESS. dag_id=movie_pipeline, task_id=scrape_collection_page, run_id=manual__2024-12-17T10:15:03.117598+00:00, execution_date=20241217T101503, start_date=, end_date=20241217T101508
[2024-12-17T15:45:08.462+0530] {logging_mixin.py:190} INFO - Task instance in success state
[2024-12-17T15:45:08.462+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: running
[2024-12-17T15:45:08.462+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline queued_at:None
[2024-12-17T15:45:08.462+0530] {logging_mixin.py:190} INFO - Task hostname:Jayeshs-MacBook-Air.local operator:PythonOperator
[2024-12-17T15:45:08.463+0530] {logging_mixin.py:190} INFO - [2024-12-17T15:45:08.463+0530] {dag.py:4410} INFO - [DAG TEST] end task task_id=scrape_collection_page map_index=-1
[2024-12-17T15:45:08.468+0530] {logging_mixin.py:190} INFO - [2024-12-17T15:45:08.467+0530] {dagrun.py:854} INFO - Marking run <DagRun movie_pipeline @ 2024-12-17 10:15:03.117598+00:00: manual__2024-12-17T10:15:03.117598+00:00, state:running, queued_at: None. externally triggered: False> successful
[2024-12-17T15:45:08.468+0530] {logging_mixin.py:190} INFO - Dag run in success state
[2024-12-17T15:45:08.468+0530] {logging_mixin.py:190} INFO - Dag run start:2024-12-17 10:15:03.117598+00:00 end:2024-12-17 10:15:08.468470+00:00
[2024-12-17T15:45:08.469+0530] {logging_mixin.py:190} INFO - [2024-12-17T15:45:08.468+0530] {dagrun.py:905} INFO - DagRun Finished: dag_id=movie_pipeline, execution_date=2024-12-17 10:15:03.117598+00:00, run_id=manual__2024-12-17T10:15:03.117598+00:00, run_start_date=2024-12-17 10:15:03.117598+00:00, run_end_date=2024-12-17 10:15:08.468470+00:00, run_duration=5.350872, state=success, external_trigger=False, run_type=manual, data_interval_start=2024-12-16 10:15:03.117598+00:00, data_interval_end=2024-12-17 10:15:03.117598+00:00, dag_hash=None
[2024-12-17T15:45:08.478+0530] {processor.py:925} INFO - DAG(s) 'movie_pipeline' retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T15:45:08.490+0530] {logging_mixin.py:190} INFO - [2024-12-17T15:45:08.489+0530] {dag.py:3239} INFO - Sync 1 DAGs
[2024-12-17T15:45:08.507+0530] {logging_mixin.py:190} INFO - [2024-12-17T15:45:08.507+0530] {dag.py:4180} INFO - Setting next_dagrun for movie_pipeline to 2024-12-17 01:30:00+00:00, run_after=2024-12-18 01:30:00+00:00
[2024-12-17T15:45:08.526+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 5.453 seconds
[2024-12-17T15:47:54.853+0530] {processor.py:186} INFO - Started process (PID=72192) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T15:47:54.855+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-17T15:47:54.857+0530] {logging_mixin.py:190} INFO - [2024-12-17T15:47:54.856+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T15:47:55.020+0530] {logging_mixin.py:190} INFO - [2024-12-17T15:47:55.020+0530] {dag.py:4435} INFO - dagrun id: movie_pipeline
[2024-12-17T15:47:55.034+0530] {logging_mixin.py:190} INFO - [2024-12-17T15:47:55.033+0530] {dag.py:4451} INFO - created dagrun <DagRun movie_pipeline @ 2024-12-17 10:17:54.895762+00:00: manual__2024-12-17T10:17:54.895762+00:00, state:running, queued_at: None. externally triggered: False>
[2024-12-17T15:47:55.060+0530] {logging_mixin.py:190} INFO - [2024-12-17T15:47:55.060+0530] {dag.py:4396} INFO - [DAG TEST] starting task_id=scrape_collection_page map_index=-1
[2024-12-17T15:47:55.061+0530] {logging_mixin.py:190} INFO - [2024-12-17T15:47:55.060+0530] {dag.py:4399} INFO - [DAG TEST] running task <TaskInstance: movie_pipeline.scrape_collection_page manual__2024-12-17T10:17:54.895762+00:00 [scheduled]>
[2024-12-17T15:48:00.158+0530] {logging_mixin.py:190} INFO - [2024-12-17T15:48:00.158+0530] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-12-17T15:48:00.220+0530] {logging_mixin.py:190} INFO - [2024-12-17 15:48:00,220] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T10:17:54.895762+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T10:17:54.895762+00:00'
[2024-12-17T15:48:00.220+0530] {logging_mixin.py:190} INFO - [2024-12-17T15:48:00.220+0530] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T10:17:54.895762+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T10:17:54.895762+00:00'
[2024-12-17T15:48:00.224+0530] {logging_mixin.py:190} INFO - Task instance is in running state
[2024-12-17T15:48:00.224+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: queued
[2024-12-17T15:48:00.224+0530] {logging_mixin.py:190} INFO - Current task name:scrape_collection_page state:scheduled start_date:None
[2024-12-17T15:48:00.224+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline and current dag run status:running
[2024-12-17T15:48:00.225+0530] {logging_mixin.py:190} INFO - [2024-12-17T15:48:00.225+0530] {taskinstance.py:731} INFO - ::endgroup::
[2024-12-17T15:48:00.226+0530] {logging_mixin.py:190} INFO - 2024-12-17
[2024-12-17T15:48:00.227+0530] {logging_mixin.py:190} INFO - [2024-12-17 15:48:00,227] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T15:48:00.227+0530] {logging_mixin.py:190} INFO - [2024-12-17T15:48:00.227+0530] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T15:48:00.232+0530] {logging_mixin.py:190} INFO - [2024-12-17T15:48:00.232+0530] {taskinstance.py:340} INFO - ::group::Post task execution logs
[2024-12-17T15:48:00.232+0530] {logging_mixin.py:190} INFO - [2024-12-17T15:48:00.232+0530] {taskinstance.py:352} INFO - Marking task as SUCCESS. dag_id=movie_pipeline, task_id=scrape_collection_page, run_id=manual__2024-12-17T10:17:54.895762+00:00, execution_date=20241217T101754, start_date=, end_date=20241217T101800
[2024-12-17T15:48:00.243+0530] {logging_mixin.py:190} INFO - Task instance in success state
[2024-12-17T15:48:00.243+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: running
[2024-12-17T15:48:00.243+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline queued_at:None
[2024-12-17T15:48:00.243+0530] {logging_mixin.py:190} INFO - Task hostname:Jayeshs-MacBook-Air.local operator:PythonOperator
[2024-12-17T15:48:00.244+0530] {logging_mixin.py:190} INFO - [2024-12-17T15:48:00.243+0530] {dag.py:4410} INFO - [DAG TEST] end task task_id=scrape_collection_page map_index=-1
[2024-12-17T15:48:00.248+0530] {logging_mixin.py:190} INFO - [2024-12-17T15:48:00.247+0530] {dagrun.py:854} INFO - Marking run <DagRun movie_pipeline @ 2024-12-17 10:17:54.895762+00:00: manual__2024-12-17T10:17:54.895762+00:00, state:running, queued_at: None. externally triggered: False> successful
[2024-12-17T15:48:00.248+0530] {logging_mixin.py:190} INFO - Dag run in success state
[2024-12-17T15:48:00.248+0530] {logging_mixin.py:190} INFO - Dag run start:2024-12-17 10:17:54.895762+00:00 end:2024-12-17 10:18:00.248591+00:00
[2024-12-17T15:48:00.249+0530] {logging_mixin.py:190} INFO - [2024-12-17T15:48:00.249+0530] {dagrun.py:905} INFO - DagRun Finished: dag_id=movie_pipeline, execution_date=2024-12-17 10:17:54.895762+00:00, run_id=manual__2024-12-17T10:17:54.895762+00:00, run_start_date=2024-12-17 10:17:54.895762+00:00, run_end_date=2024-12-17 10:18:00.248591+00:00, run_duration=5.352829, state=success, external_trigger=False, run_type=manual, data_interval_start=2024-12-16 10:17:54.895762+00:00, data_interval_end=2024-12-17 10:17:54.895762+00:00, dag_hash=None
[2024-12-17T15:48:00.260+0530] {processor.py:925} INFO - DAG(s) 'movie_pipeline' retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T15:48:00.272+0530] {logging_mixin.py:190} INFO - [2024-12-17T15:48:00.271+0530] {dag.py:3239} INFO - Sync 1 DAGs
[2024-12-17T15:48:00.289+0530] {logging_mixin.py:190} INFO - [2024-12-17T15:48:00.289+0530] {dag.py:4180} INFO - Setting next_dagrun for movie_pipeline to 2024-12-17 01:30:00+00:00, run_after=2024-12-18 01:30:00+00:00
[2024-12-17T15:48:00.308+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 5.458 seconds
[2024-12-17T15:50:45.586+0530] {processor.py:186} INFO - Started process (PID=72396) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T15:50:45.587+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-17T15:50:45.588+0530] {logging_mixin.py:190} INFO - [2024-12-17T15:50:45.588+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T15:50:45.754+0530] {logging_mixin.py:190} INFO - [2024-12-17T15:50:45.753+0530] {dag.py:4435} INFO - dagrun id: movie_pipeline
[2024-12-17T15:50:45.768+0530] {logging_mixin.py:190} INFO - [2024-12-17T15:50:45.767+0530] {dag.py:4451} INFO - created dagrun <DagRun movie_pipeline @ 2024-12-17 10:20:45.629028+00:00: manual__2024-12-17T10:20:45.629028+00:00, state:running, queued_at: None. externally triggered: False>
[2024-12-17T15:50:45.793+0530] {logging_mixin.py:190} INFO - [2024-12-17T15:50:45.793+0530] {dag.py:4396} INFO - [DAG TEST] starting task_id=scrape_collection_page map_index=-1
[2024-12-17T15:50:45.794+0530] {logging_mixin.py:190} INFO - [2024-12-17T15:50:45.793+0530] {dag.py:4399} INFO - [DAG TEST] running task <TaskInstance: movie_pipeline.scrape_collection_page manual__2024-12-17T10:20:45.629028+00:00 [scheduled]>
[2024-12-17T15:50:50.887+0530] {logging_mixin.py:190} INFO - [2024-12-17T15:50:50.887+0530] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-12-17T15:50:50.956+0530] {logging_mixin.py:190} INFO - [2024-12-17 15:50:50,956] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T10:20:45.629028+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T10:20:45.629028+00:00'
[2024-12-17T15:50:50.957+0530] {logging_mixin.py:190} INFO - [2024-12-17T15:50:50.956+0530] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T10:20:45.629028+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T10:20:45.629028+00:00'
[2024-12-17T15:50:50.960+0530] {logging_mixin.py:190} INFO - Task instance is in running state
[2024-12-17T15:50:50.960+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: queued
[2024-12-17T15:50:50.960+0530] {logging_mixin.py:190} INFO - Current task name:scrape_collection_page state:scheduled start_date:None
[2024-12-17T15:50:50.961+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline and current dag run status:running
[2024-12-17T15:50:50.961+0530] {logging_mixin.py:190} INFO - [2024-12-17T15:50:50.961+0530] {taskinstance.py:731} INFO - ::endgroup::
[2024-12-17T15:50:50.964+0530] {logging_mixin.py:190} INFO - 2024-12-17
[2024-12-17T15:50:50.964+0530] {logging_mixin.py:190} INFO - [2024-12-17 15:50:50,964] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T15:50:50.965+0530] {logging_mixin.py:190} INFO - [2024-12-17T15:50:50.964+0530] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T15:50:50.970+0530] {logging_mixin.py:190} INFO - [2024-12-17T15:50:50.970+0530] {taskinstance.py:340} INFO - ::group::Post task execution logs
[2024-12-17T15:50:50.971+0530] {logging_mixin.py:190} INFO - [2024-12-17T15:50:50.971+0530] {taskinstance.py:352} INFO - Marking task as SUCCESS. dag_id=movie_pipeline, task_id=scrape_collection_page, run_id=manual__2024-12-17T10:20:45.629028+00:00, execution_date=20241217T102045, start_date=, end_date=20241217T102050
[2024-12-17T15:50:50.983+0530] {logging_mixin.py:190} INFO - Task instance in success state
[2024-12-17T15:50:50.983+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: running
[2024-12-17T15:50:50.983+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline queued_at:None
[2024-12-17T15:50:50.983+0530] {logging_mixin.py:190} INFO - Task hostname:Jayeshs-MacBook-Air.local operator:PythonOperator
[2024-12-17T15:50:50.984+0530] {logging_mixin.py:190} INFO - [2024-12-17T15:50:50.983+0530] {dag.py:4410} INFO - [DAG TEST] end task task_id=scrape_collection_page map_index=-1
[2024-12-17T15:50:50.988+0530] {logging_mixin.py:190} INFO - [2024-12-17T15:50:50.988+0530] {dagrun.py:854} INFO - Marking run <DagRun movie_pipeline @ 2024-12-17 10:20:45.629028+00:00: manual__2024-12-17T10:20:45.629028+00:00, state:running, queued_at: None. externally triggered: False> successful
[2024-12-17T15:50:50.989+0530] {logging_mixin.py:190} INFO - Dag run in success state
[2024-12-17T15:50:50.989+0530] {logging_mixin.py:190} INFO - Dag run start:2024-12-17 10:20:45.629028+00:00 end:2024-12-17 10:20:50.989178+00:00
[2024-12-17T15:50:50.989+0530] {logging_mixin.py:190} INFO - [2024-12-17T15:50:50.989+0530] {dagrun.py:905} INFO - DagRun Finished: dag_id=movie_pipeline, execution_date=2024-12-17 10:20:45.629028+00:00, run_id=manual__2024-12-17T10:20:45.629028+00:00, run_start_date=2024-12-17 10:20:45.629028+00:00, run_end_date=2024-12-17 10:20:50.989178+00:00, run_duration=5.36015, state=success, external_trigger=False, run_type=manual, data_interval_start=2024-12-16 10:20:45.629028+00:00, data_interval_end=2024-12-17 10:20:45.629028+00:00, dag_hash=None
[2024-12-17T15:50:50.999+0530] {processor.py:925} INFO - DAG(s) 'movie_pipeline' retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T15:50:51.012+0530] {logging_mixin.py:190} INFO - [2024-12-17T15:50:51.011+0530] {dag.py:3239} INFO - Sync 1 DAGs
[2024-12-17T15:50:51.031+0530] {logging_mixin.py:190} INFO - [2024-12-17T15:50:51.031+0530] {dag.py:4180} INFO - Setting next_dagrun for movie_pipeline to 2024-12-17 01:30:00+00:00, run_after=2024-12-18 01:30:00+00:00
[2024-12-17T15:50:51.050+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 5.468 seconds
[2024-12-17T15:53:55.276+0530] {processor.py:186} INFO - Started process (PID=72636) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T15:53:55.280+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-17T15:53:55.282+0530] {logging_mixin.py:190} INFO - [2024-12-17T15:53:55.281+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T15:53:55.954+0530] {logging_mixin.py:190} INFO - [2024-12-17T15:53:55.954+0530] {dag.py:4435} INFO - dagrun id: movie_pipeline
[2024-12-17T15:53:55.974+0530] {logging_mixin.py:190} INFO - [2024-12-17T15:53:55.973+0530] {dag.py:4451} INFO - created dagrun <DagRun movie_pipeline @ 2024-12-17 10:23:55.404967+00:00: manual__2024-12-17T10:23:55.404967+00:00, state:running, queued_at: None. externally triggered: False>
[2024-12-17T15:53:56.015+0530] {logging_mixin.py:190} INFO - [2024-12-17T15:53:56.014+0530] {dag.py:4396} INFO - [DAG TEST] starting task_id=scrape_collection_page map_index=-1
[2024-12-17T15:53:56.015+0530] {logging_mixin.py:190} INFO - [2024-12-17T15:53:56.015+0530] {dag.py:4399} INFO - [DAG TEST] running task <TaskInstance: movie_pipeline.scrape_collection_page manual__2024-12-17T10:23:55.404967+00:00 [scheduled]>
[2024-12-17T15:54:01.149+0530] {logging_mixin.py:190} INFO - [2024-12-17T15:54:01.149+0530] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-12-17T15:54:01.245+0530] {logging_mixin.py:190} INFO - [2024-12-17 15:54:01,245] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T10:23:55.404967+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T10:23:55.404967+00:00'
[2024-12-17T15:54:01.246+0530] {logging_mixin.py:190} INFO - [2024-12-17T15:54:01.245+0530] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T10:23:55.404967+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T10:23:55.404967+00:00'
[2024-12-17T15:54:01.259+0530] {logging_mixin.py:190} INFO - Task instance is in running state
[2024-12-17T15:54:01.259+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: queued
[2024-12-17T15:54:01.259+0530] {logging_mixin.py:190} INFO - Current task name:scrape_collection_page state:scheduled start_date:None
[2024-12-17T15:54:01.260+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline and current dag run status:running
[2024-12-17T15:54:01.260+0530] {logging_mixin.py:190} INFO - [2024-12-17T15:54:01.260+0530] {taskinstance.py:731} INFO - ::endgroup::
[2024-12-17T15:54:01.263+0530] {logging_mixin.py:190} INFO - 2024-12-17
[2024-12-17T15:54:01.264+0530] {logging_mixin.py:190} INFO - [2024-12-17 15:54:01,264] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T15:54:01.264+0530] {logging_mixin.py:190} INFO - [2024-12-17T15:54:01.264+0530] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T15:54:01.271+0530] {logging_mixin.py:190} INFO - [2024-12-17T15:54:01.271+0530] {taskinstance.py:340} INFO - ::group::Post task execution logs
[2024-12-17T15:54:01.272+0530] {logging_mixin.py:190} INFO - [2024-12-17T15:54:01.272+0530] {taskinstance.py:352} INFO - Marking task as SUCCESS. dag_id=movie_pipeline, task_id=scrape_collection_page, run_id=manual__2024-12-17T10:23:55.404967+00:00, execution_date=20241217T102355, start_date=, end_date=20241217T102401
[2024-12-17T15:54:01.286+0530] {logging_mixin.py:190} INFO - Task instance in success state
[2024-12-17T15:54:01.286+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: running
[2024-12-17T15:54:01.287+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline queued_at:None
[2024-12-17T15:54:01.287+0530] {logging_mixin.py:190} INFO - Task hostname:Jayeshs-MacBook-Air.local operator:PythonOperator
[2024-12-17T15:54:01.287+0530] {logging_mixin.py:190} INFO - [2024-12-17T15:54:01.287+0530] {dag.py:4410} INFO - [DAG TEST] end task task_id=scrape_collection_page map_index=-1
[2024-12-17T15:54:01.292+0530] {logging_mixin.py:190} INFO - [2024-12-17T15:54:01.292+0530] {dagrun.py:854} INFO - Marking run <DagRun movie_pipeline @ 2024-12-17 10:23:55.404967+00:00: manual__2024-12-17T10:23:55.404967+00:00, state:running, queued_at: None. externally triggered: False> successful
[2024-12-17T15:54:01.292+0530] {logging_mixin.py:190} INFO - Dag run in success state
[2024-12-17T15:54:01.293+0530] {logging_mixin.py:190} INFO - Dag run start:2024-12-17 10:23:55.404967+00:00 end:2024-12-17 10:24:01.292840+00:00
[2024-12-17T15:54:01.293+0530] {logging_mixin.py:190} INFO - [2024-12-17T15:54:01.293+0530] {dagrun.py:905} INFO - DagRun Finished: dag_id=movie_pipeline, execution_date=2024-12-17 10:23:55.404967+00:00, run_id=manual__2024-12-17T10:23:55.404967+00:00, run_start_date=2024-12-17 10:23:55.404967+00:00, run_end_date=2024-12-17 10:24:01.292840+00:00, run_duration=5.887873, state=success, external_trigger=False, run_type=manual, data_interval_start=2024-12-16 10:23:55.404967+00:00, data_interval_end=2024-12-17 10:23:55.404967+00:00, dag_hash=None
[2024-12-17T15:54:01.308+0530] {processor.py:925} INFO - DAG(s) 'movie_pipeline' retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T15:54:01.327+0530] {logging_mixin.py:190} INFO - [2024-12-17T15:54:01.327+0530] {dag.py:3239} INFO - Sync 1 DAGs
[2024-12-17T15:54:01.348+0530] {logging_mixin.py:190} INFO - [2024-12-17T15:54:01.347+0530] {dag.py:4180} INFO - Setting next_dagrun for movie_pipeline to 2024-12-17 01:30:00+00:00, run_after=2024-12-18 01:30:00+00:00
[2024-12-17T15:54:01.370+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 6.098 seconds
[2024-12-17T15:57:43.381+0530] {processor.py:186} INFO - Started process (PID=72893) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T15:57:43.383+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-17T15:57:43.385+0530] {logging_mixin.py:190} INFO - [2024-12-17T15:57:43.385+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T15:57:43.575+0530] {logging_mixin.py:190} INFO - [2024-12-17T15:57:43.575+0530] {dag.py:4435} INFO - dagrun id: movie_pipeline
[2024-12-17T15:57:43.592+0530] {logging_mixin.py:190} INFO - [2024-12-17T15:57:43.591+0530] {dag.py:4451} INFO - created dagrun <DagRun movie_pipeline @ 2024-12-17 10:27:43.430758+00:00: manual__2024-12-17T10:27:43.430758+00:00, state:running, queued_at: None. externally triggered: False>
[2024-12-17T15:57:43.622+0530] {logging_mixin.py:190} INFO - [2024-12-17T15:57:43.622+0530] {dag.py:4396} INFO - [DAG TEST] starting task_id=scrape_collection_page map_index=-1
[2024-12-17T15:57:43.623+0530] {logging_mixin.py:190} INFO - [2024-12-17T15:57:43.622+0530] {dag.py:4399} INFO - [DAG TEST] running task <TaskInstance: movie_pipeline.scrape_collection_page manual__2024-12-17T10:27:43.430758+00:00 [scheduled]>
[2024-12-17T15:57:48.727+0530] {logging_mixin.py:190} INFO - [2024-12-17T15:57:48.727+0530] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-12-17T15:57:48.797+0530] {logging_mixin.py:190} INFO - [2024-12-17 15:57:48,797] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T10:27:43.430758+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T10:27:43.430758+00:00'
[2024-12-17T15:57:48.798+0530] {logging_mixin.py:190} INFO - [2024-12-17T15:57:48.797+0530] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T10:27:43.430758+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T10:27:43.430758+00:00'
[2024-12-17T15:57:48.811+0530] {logging_mixin.py:190} INFO - Task instance is in running state
[2024-12-17T15:57:48.812+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: queued
[2024-12-17T15:57:48.812+0530] {logging_mixin.py:190} INFO - Current task name:scrape_collection_page state:scheduled start_date:None
[2024-12-17T15:57:48.812+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline and current dag run status:running
[2024-12-17T15:57:48.813+0530] {logging_mixin.py:190} INFO - [2024-12-17T15:57:48.813+0530] {taskinstance.py:731} INFO - ::endgroup::
[2024-12-17T15:57:48.816+0530] {logging_mixin.py:190} INFO - 2024-12-17
[2024-12-17T15:57:48.816+0530] {logging_mixin.py:190} INFO - [2024-12-17 15:57:48,816] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T15:57:48.817+0530] {logging_mixin.py:190} INFO - [2024-12-17T15:57:48.816+0530] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T15:57:48.822+0530] {logging_mixin.py:190} INFO - [2024-12-17T15:57:48.822+0530] {taskinstance.py:340} INFO - ::group::Post task execution logs
[2024-12-17T15:57:48.822+0530] {logging_mixin.py:190} INFO - [2024-12-17T15:57:48.822+0530] {taskinstance.py:352} INFO - Marking task as SUCCESS. dag_id=movie_pipeline, task_id=scrape_collection_page, run_id=manual__2024-12-17T10:27:43.430758+00:00, execution_date=20241217T102743, start_date=, end_date=20241217T102748
[2024-12-17T15:57:48.834+0530] {logging_mixin.py:190} INFO - Task instance in success state
[2024-12-17T15:57:48.834+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: running
[2024-12-17T15:57:48.834+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline queued_at:None
[2024-12-17T15:57:48.834+0530] {logging_mixin.py:190} INFO - Task hostname:Jayeshs-MacBook-Air.local operator:PythonOperator
[2024-12-17T15:57:48.835+0530] {logging_mixin.py:190} INFO - [2024-12-17T15:57:48.835+0530] {dag.py:4410} INFO - [DAG TEST] end task task_id=scrape_collection_page map_index=-1
[2024-12-17T15:57:48.839+0530] {logging_mixin.py:190} INFO - [2024-12-17T15:57:48.838+0530] {dagrun.py:854} INFO - Marking run <DagRun movie_pipeline @ 2024-12-17 10:27:43.430758+00:00: manual__2024-12-17T10:27:43.430758+00:00, state:running, queued_at: None. externally triggered: False> successful
[2024-12-17T15:57:48.839+0530] {logging_mixin.py:190} INFO - Dag run in success state
[2024-12-17T15:57:48.840+0530] {logging_mixin.py:190} INFO - Dag run start:2024-12-17 10:27:43.430758+00:00 end:2024-12-17 10:27:48.839457+00:00
[2024-12-17T15:57:48.841+0530] {logging_mixin.py:190} INFO - [2024-12-17T15:57:48.841+0530] {dagrun.py:905} INFO - DagRun Finished: dag_id=movie_pipeline, execution_date=2024-12-17 10:27:43.430758+00:00, run_id=manual__2024-12-17T10:27:43.430758+00:00, run_start_date=2024-12-17 10:27:43.430758+00:00, run_end_date=2024-12-17 10:27:48.839457+00:00, run_duration=5.408699, state=success, external_trigger=False, run_type=manual, data_interval_start=2024-12-16 10:27:43.430758+00:00, data_interval_end=2024-12-17 10:27:43.430758+00:00, dag_hash=None
[2024-12-17T15:57:48.851+0530] {processor.py:925} INFO - DAG(s) 'movie_pipeline' retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T15:57:48.865+0530] {logging_mixin.py:190} INFO - [2024-12-17T15:57:48.864+0530] {dag.py:3239} INFO - Sync 1 DAGs
[2024-12-17T15:57:48.884+0530] {logging_mixin.py:190} INFO - [2024-12-17T15:57:48.884+0530] {dag.py:4180} INFO - Setting next_dagrun for movie_pipeline to 2024-12-17 01:30:00+00:00, run_after=2024-12-18 01:30:00+00:00
[2024-12-17T15:57:48.904+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 5.528 seconds
[2024-12-17T16:00:57.465+0530] {processor.py:186} INFO - Started process (PID=73112) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T16:00:57.466+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-17T16:00:57.468+0530] {logging_mixin.py:190} INFO - [2024-12-17T16:00:57.468+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T16:00:57.656+0530] {logging_mixin.py:190} INFO - [2024-12-17T16:00:57.656+0530] {dag.py:4435} INFO - dagrun id: movie_pipeline
[2024-12-17T16:00:57.672+0530] {logging_mixin.py:190} INFO - [2024-12-17T16:00:57.672+0530] {dag.py:4451} INFO - created dagrun <DagRun movie_pipeline @ 2024-12-17 10:30:57.511796+00:00: manual__2024-12-17T10:30:57.511796+00:00, state:running, queued_at: None. externally triggered: False>
[2024-12-17T16:00:57.702+0530] {logging_mixin.py:190} INFO - [2024-12-17T16:00:57.702+0530] {dag.py:4396} INFO - [DAG TEST] starting task_id=scrape_collection_page map_index=-1
[2024-12-17T16:00:57.703+0530] {logging_mixin.py:190} INFO - [2024-12-17T16:00:57.703+0530] {dag.py:4399} INFO - [DAG TEST] running task <TaskInstance: movie_pipeline.scrape_collection_page manual__2024-12-17T10:30:57.511796+00:00 [scheduled]>
[2024-12-17T16:01:02.807+0530] {logging_mixin.py:190} INFO - [2024-12-17T16:01:02.806+0530] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-12-17T16:01:02.874+0530] {logging_mixin.py:190} INFO - [2024-12-17 16:01:02,874] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T10:30:57.511796+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T10:30:57.511796+00:00'
[2024-12-17T16:01:02.875+0530] {logging_mixin.py:190} INFO - [2024-12-17T16:01:02.874+0530] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T10:30:57.511796+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T10:30:57.511796+00:00'
[2024-12-17T16:01:02.878+0530] {logging_mixin.py:190} INFO - Task instance is in running state
[2024-12-17T16:01:02.878+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: queued
[2024-12-17T16:01:02.878+0530] {logging_mixin.py:190} INFO - Current task name:scrape_collection_page state:scheduled start_date:None
[2024-12-17T16:01:02.879+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline and current dag run status:running
[2024-12-17T16:01:02.879+0530] {logging_mixin.py:190} INFO - [2024-12-17T16:01:02.879+0530] {taskinstance.py:731} INFO - ::endgroup::
[2024-12-17T16:01:02.882+0530] {logging_mixin.py:190} INFO - 2024-12-17
[2024-12-17T16:01:02.882+0530] {logging_mixin.py:190} INFO - [2024-12-17 16:01:02,882] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T16:01:02.883+0530] {logging_mixin.py:190} INFO - [2024-12-17T16:01:02.882+0530] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T16:01:02.889+0530] {logging_mixin.py:190} INFO - [2024-12-17T16:01:02.888+0530] {taskinstance.py:340} INFO - ::group::Post task execution logs
[2024-12-17T16:01:02.889+0530] {logging_mixin.py:190} INFO - [2024-12-17T16:01:02.889+0530] {taskinstance.py:352} INFO - Marking task as SUCCESS. dag_id=movie_pipeline, task_id=scrape_collection_page, run_id=manual__2024-12-17T10:30:57.511796+00:00, execution_date=20241217T103057, start_date=, end_date=20241217T103102
[2024-12-17T16:01:02.900+0530] {logging_mixin.py:190} INFO - Task instance in success state
[2024-12-17T16:01:02.900+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: running
[2024-12-17T16:01:02.900+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline queued_at:None
[2024-12-17T16:01:02.901+0530] {logging_mixin.py:190} INFO - Task hostname:Jayeshs-MacBook-Air.local operator:PythonOperator
[2024-12-17T16:01:02.901+0530] {logging_mixin.py:190} INFO - [2024-12-17T16:01:02.901+0530] {dag.py:4410} INFO - [DAG TEST] end task task_id=scrape_collection_page map_index=-1
[2024-12-17T16:01:02.907+0530] {logging_mixin.py:190} INFO - [2024-12-17T16:01:02.906+0530] {dagrun.py:854} INFO - Marking run <DagRun movie_pipeline @ 2024-12-17 10:30:57.511796+00:00: manual__2024-12-17T10:30:57.511796+00:00, state:running, queued_at: None. externally triggered: False> successful
[2024-12-17T16:01:02.907+0530] {logging_mixin.py:190} INFO - Dag run in success state
[2024-12-17T16:01:02.907+0530] {logging_mixin.py:190} INFO - Dag run start:2024-12-17 10:30:57.511796+00:00 end:2024-12-17 10:31:02.907290+00:00
[2024-12-17T16:01:02.908+0530] {logging_mixin.py:190} INFO - [2024-12-17T16:01:02.907+0530] {dagrun.py:905} INFO - DagRun Finished: dag_id=movie_pipeline, execution_date=2024-12-17 10:30:57.511796+00:00, run_id=manual__2024-12-17T10:30:57.511796+00:00, run_start_date=2024-12-17 10:30:57.511796+00:00, run_end_date=2024-12-17 10:31:02.907290+00:00, run_duration=5.395494, state=success, external_trigger=False, run_type=manual, data_interval_start=2024-12-16 10:30:57.511796+00:00, data_interval_end=2024-12-17 10:30:57.511796+00:00, dag_hash=None
[2024-12-17T16:01:02.918+0530] {processor.py:925} INFO - DAG(s) 'movie_pipeline' retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T16:01:02.931+0530] {logging_mixin.py:190} INFO - [2024-12-17T16:01:02.931+0530] {dag.py:3239} INFO - Sync 1 DAGs
[2024-12-17T16:01:02.951+0530] {logging_mixin.py:190} INFO - [2024-12-17T16:01:02.950+0530] {dag.py:4180} INFO - Setting next_dagrun for movie_pipeline to 2024-12-17 01:30:00+00:00, run_after=2024-12-18 01:30:00+00:00
[2024-12-17T16:01:02.972+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 5.512 seconds
[2024-12-17T16:04:09.711+0530] {processor.py:186} INFO - Started process (PID=73333) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T16:04:09.714+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-17T16:04:09.716+0530] {logging_mixin.py:190} INFO - [2024-12-17T16:04:09.716+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T16:04:09.910+0530] {logging_mixin.py:190} INFO - [2024-12-17T16:04:09.910+0530] {dag.py:4435} INFO - dagrun id: movie_pipeline
[2024-12-17T16:04:09.926+0530] {logging_mixin.py:190} INFO - [2024-12-17T16:04:09.925+0530] {dag.py:4451} INFO - created dagrun <DagRun movie_pipeline @ 2024-12-17 10:34:09.763145+00:00: manual__2024-12-17T10:34:09.763145+00:00, state:running, queued_at: None. externally triggered: False>
[2024-12-17T16:04:09.958+0530] {logging_mixin.py:190} INFO - [2024-12-17T16:04:09.958+0530] {dag.py:4396} INFO - [DAG TEST] starting task_id=scrape_collection_page map_index=-1
[2024-12-17T16:04:09.959+0530] {logging_mixin.py:190} INFO - [2024-12-17T16:04:09.959+0530] {dag.py:4399} INFO - [DAG TEST] running task <TaskInstance: movie_pipeline.scrape_collection_page manual__2024-12-17T10:34:09.763145+00:00 [scheduled]>
[2024-12-17T16:04:15.066+0530] {logging_mixin.py:190} INFO - [2024-12-17T16:04:15.065+0530] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-12-17T16:04:15.146+0530] {logging_mixin.py:190} INFO - [2024-12-17 16:04:15,146] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T10:34:09.763145+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T10:34:09.763145+00:00'
[2024-12-17T16:04:15.146+0530] {logging_mixin.py:190} INFO - [2024-12-17T16:04:15.146+0530] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T10:34:09.763145+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T10:34:09.763145+00:00'
[2024-12-17T16:04:15.160+0530] {logging_mixin.py:190} INFO - Task instance is in running state
[2024-12-17T16:04:15.161+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: queued
[2024-12-17T16:04:15.161+0530] {logging_mixin.py:190} INFO - Current task name:scrape_collection_page state:scheduled start_date:None
[2024-12-17T16:04:15.161+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline and current dag run status:running
[2024-12-17T16:04:15.162+0530] {logging_mixin.py:190} INFO - [2024-12-17T16:04:15.162+0530] {taskinstance.py:731} INFO - ::endgroup::
[2024-12-17T16:04:15.166+0530] {logging_mixin.py:190} INFO - 2024-12-17
[2024-12-17T16:04:15.166+0530] {logging_mixin.py:190} INFO - [2024-12-17 16:04:15,166] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T16:04:15.167+0530] {logging_mixin.py:190} INFO - [2024-12-17T16:04:15.166+0530] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T16:04:15.171+0530] {logging_mixin.py:190} INFO - [2024-12-17T16:04:15.171+0530] {taskinstance.py:340} INFO - ::group::Post task execution logs
[2024-12-17T16:04:15.172+0530] {logging_mixin.py:190} INFO - [2024-12-17T16:04:15.172+0530] {taskinstance.py:352} INFO - Marking task as SUCCESS. dag_id=movie_pipeline, task_id=scrape_collection_page, run_id=manual__2024-12-17T10:34:09.763145+00:00, execution_date=20241217T103409, start_date=, end_date=20241217T103415
[2024-12-17T16:04:15.183+0530] {logging_mixin.py:190} INFO - Task instance in success state
[2024-12-17T16:04:15.184+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: running
[2024-12-17T16:04:15.184+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline queued_at:None
[2024-12-17T16:04:15.184+0530] {logging_mixin.py:190} INFO - Task hostname:Jayeshs-MacBook-Air.local operator:PythonOperator
[2024-12-17T16:04:15.185+0530] {logging_mixin.py:190} INFO - [2024-12-17T16:04:15.185+0530] {dag.py:4410} INFO - [DAG TEST] end task task_id=scrape_collection_page map_index=-1
[2024-12-17T16:04:15.188+0530] {logging_mixin.py:190} INFO - [2024-12-17T16:04:15.188+0530] {dagrun.py:854} INFO - Marking run <DagRun movie_pipeline @ 2024-12-17 10:34:09.763145+00:00: manual__2024-12-17T10:34:09.763145+00:00, state:running, queued_at: None. externally triggered: False> successful
[2024-12-17T16:04:15.189+0530] {logging_mixin.py:190} INFO - Dag run in success state
[2024-12-17T16:04:15.191+0530] {logging_mixin.py:190} INFO - Dag run start:2024-12-17 10:34:09.763145+00:00 end:2024-12-17 10:34:15.189164+00:00
[2024-12-17T16:04:15.191+0530] {logging_mixin.py:190} INFO - [2024-12-17T16:04:15.191+0530] {dagrun.py:905} INFO - DagRun Finished: dag_id=movie_pipeline, execution_date=2024-12-17 10:34:09.763145+00:00, run_id=manual__2024-12-17T10:34:09.763145+00:00, run_start_date=2024-12-17 10:34:09.763145+00:00, run_end_date=2024-12-17 10:34:15.189164+00:00, run_duration=5.426019, state=success, external_trigger=False, run_type=manual, data_interval_start=2024-12-16 10:34:09.763145+00:00, data_interval_end=2024-12-17 10:34:09.763145+00:00, dag_hash=None
[2024-12-17T16:04:15.200+0530] {processor.py:925} INFO - DAG(s) 'movie_pipeline' retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T16:04:15.216+0530] {logging_mixin.py:190} INFO - [2024-12-17T16:04:15.215+0530] {dag.py:3239} INFO - Sync 1 DAGs
[2024-12-17T16:04:15.234+0530] {logging_mixin.py:190} INFO - [2024-12-17T16:04:15.234+0530] {dag.py:4180} INFO - Setting next_dagrun for movie_pipeline to 2024-12-17 01:30:00+00:00, run_after=2024-12-18 01:30:00+00:00
[2024-12-17T16:04:15.255+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 5.547 seconds
[2024-12-17T16:07:47.384+0530] {processor.py:186} INFO - Started process (PID=73560) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T16:07:47.386+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-17T16:07:47.388+0530] {logging_mixin.py:190} INFO - [2024-12-17T16:07:47.388+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T16:07:47.565+0530] {logging_mixin.py:190} INFO - [2024-12-17T16:07:47.565+0530] {dag.py:4435} INFO - dagrun id: movie_pipeline
[2024-12-17T16:07:47.587+0530] {logging_mixin.py:190} INFO - [2024-12-17T16:07:47.587+0530] {dag.py:4451} INFO - created dagrun <DagRun movie_pipeline @ 2024-12-17 10:37:47.432433+00:00: manual__2024-12-17T10:37:47.432433+00:00, state:running, queued_at: None. externally triggered: False>
[2024-12-17T16:07:47.618+0530] {logging_mixin.py:190} INFO - [2024-12-17T16:07:47.617+0530] {dag.py:4396} INFO - [DAG TEST] starting task_id=scrape_collection_page map_index=-1
[2024-12-17T16:07:47.618+0530] {logging_mixin.py:190} INFO - [2024-12-17T16:07:47.618+0530] {dag.py:4399} INFO - [DAG TEST] running task <TaskInstance: movie_pipeline.scrape_collection_page manual__2024-12-17T10:37:47.432433+00:00 [scheduled]>
[2024-12-17T16:07:52.725+0530] {logging_mixin.py:190} INFO - [2024-12-17T16:07:52.724+0530] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-12-17T16:07:52.798+0530] {logging_mixin.py:190} INFO - [2024-12-17 16:07:52,798] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T10:37:47.432433+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T10:37:47.432433+00:00'
[2024-12-17T16:07:52.798+0530] {logging_mixin.py:190} INFO - [2024-12-17T16:07:52.798+0530] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T10:37:47.432433+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T10:37:47.432433+00:00'
[2024-12-17T16:07:52.811+0530] {logging_mixin.py:190} INFO - Task instance is in running state
[2024-12-17T16:07:52.812+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: queued
[2024-12-17T16:07:52.812+0530] {logging_mixin.py:190} INFO - Current task name:scrape_collection_page state:scheduled start_date:None
[2024-12-17T16:07:52.812+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline and current dag run status:running
[2024-12-17T16:07:52.813+0530] {logging_mixin.py:190} INFO - [2024-12-17T16:07:52.812+0530] {taskinstance.py:731} INFO - ::endgroup::
[2024-12-17T16:07:52.817+0530] {logging_mixin.py:190} INFO - 2024-12-17
[2024-12-17T16:07:52.818+0530] {logging_mixin.py:190} INFO - [2024-12-17 16:07:52,818] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T16:07:52.819+0530] {logging_mixin.py:190} INFO - [2024-12-17T16:07:52.818+0530] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T16:07:52.826+0530] {logging_mixin.py:190} INFO - [2024-12-17T16:07:52.826+0530] {taskinstance.py:340} INFO - ::group::Post task execution logs
[2024-12-17T16:07:52.827+0530] {logging_mixin.py:190} INFO - [2024-12-17T16:07:52.827+0530] {taskinstance.py:352} INFO - Marking task as SUCCESS. dag_id=movie_pipeline, task_id=scrape_collection_page, run_id=manual__2024-12-17T10:37:47.432433+00:00, execution_date=20241217T103747, start_date=, end_date=20241217T103752
[2024-12-17T16:07:52.837+0530] {logging_mixin.py:190} INFO - Task instance in success state
[2024-12-17T16:07:52.837+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: running
[2024-12-17T16:07:52.837+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline queued_at:None
[2024-12-17T16:07:52.837+0530] {logging_mixin.py:190} INFO - Task hostname:Jayeshs-MacBook-Air.local operator:PythonOperator
[2024-12-17T16:07:52.838+0530] {logging_mixin.py:190} INFO - [2024-12-17T16:07:52.838+0530] {dag.py:4410} INFO - [DAG TEST] end task task_id=scrape_collection_page map_index=-1
[2024-12-17T16:07:52.841+0530] {logging_mixin.py:190} INFO - [2024-12-17T16:07:52.841+0530] {dagrun.py:854} INFO - Marking run <DagRun movie_pipeline @ 2024-12-17 10:37:47.432433+00:00: manual__2024-12-17T10:37:47.432433+00:00, state:running, queued_at: None. externally triggered: False> successful
[2024-12-17T16:07:52.842+0530] {logging_mixin.py:190} INFO - Dag run in success state
[2024-12-17T16:07:52.842+0530] {logging_mixin.py:190} INFO - Dag run start:2024-12-17 10:37:47.432433+00:00 end:2024-12-17 10:37:52.842102+00:00
[2024-12-17T16:07:52.842+0530] {logging_mixin.py:190} INFO - [2024-12-17T16:07:52.842+0530] {dagrun.py:905} INFO - DagRun Finished: dag_id=movie_pipeline, execution_date=2024-12-17 10:37:47.432433+00:00, run_id=manual__2024-12-17T10:37:47.432433+00:00, run_start_date=2024-12-17 10:37:47.432433+00:00, run_end_date=2024-12-17 10:37:52.842102+00:00, run_duration=5.409669, state=success, external_trigger=False, run_type=manual, data_interval_start=2024-12-16 10:37:47.432433+00:00, data_interval_end=2024-12-17 10:37:47.432433+00:00, dag_hash=None
[2024-12-17T16:07:52.851+0530] {processor.py:925} INFO - DAG(s) 'movie_pipeline' retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T16:07:52.863+0530] {logging_mixin.py:190} INFO - [2024-12-17T16:07:52.863+0530] {dag.py:3239} INFO - Sync 1 DAGs
[2024-12-17T16:07:52.880+0530] {logging_mixin.py:190} INFO - [2024-12-17T16:07:52.880+0530] {dag.py:4180} INFO - Setting next_dagrun for movie_pipeline to 2024-12-17 01:30:00+00:00, run_after=2024-12-18 01:30:00+00:00
[2024-12-17T16:07:52.898+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 5.517 seconds
[2024-12-17T16:10:54.275+0530] {processor.py:186} INFO - Started process (PID=73781) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T16:10:54.277+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-17T16:10:54.278+0530] {logging_mixin.py:190} INFO - [2024-12-17T16:10:54.278+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T16:10:54.447+0530] {logging_mixin.py:190} INFO - [2024-12-17T16:10:54.446+0530] {dag.py:4435} INFO - dagrun id: movie_pipeline
[2024-12-17T16:10:54.461+0530] {logging_mixin.py:190} INFO - [2024-12-17T16:10:54.460+0530] {dag.py:4451} INFO - created dagrun <DagRun movie_pipeline @ 2024-12-17 10:40:54.321750+00:00: manual__2024-12-17T10:40:54.321750+00:00, state:running, queued_at: None. externally triggered: False>
[2024-12-17T16:10:54.487+0530] {logging_mixin.py:190} INFO - [2024-12-17T16:10:54.486+0530] {dag.py:4396} INFO - [DAG TEST] starting task_id=scrape_collection_page map_index=-1
[2024-12-17T16:10:54.487+0530] {logging_mixin.py:190} INFO - [2024-12-17T16:10:54.487+0530] {dag.py:4399} INFO - [DAG TEST] running task <TaskInstance: movie_pipeline.scrape_collection_page manual__2024-12-17T10:40:54.321750+00:00 [scheduled]>
[2024-12-17T16:10:59.582+0530] {logging_mixin.py:190} INFO - [2024-12-17T16:10:59.582+0530] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-12-17T16:10:59.643+0530] {logging_mixin.py:190} INFO - [2024-12-17 16:10:59,643] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T10:40:54.321750+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T10:40:54.321750+00:00'
[2024-12-17T16:10:59.644+0530] {logging_mixin.py:190} INFO - [2024-12-17T16:10:59.643+0530] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T10:40:54.321750+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T10:40:54.321750+00:00'
[2024-12-17T16:10:59.646+0530] {logging_mixin.py:190} INFO - Task instance is in running state
[2024-12-17T16:10:59.647+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: queued
[2024-12-17T16:10:59.647+0530] {logging_mixin.py:190} INFO - Current task name:scrape_collection_page state:scheduled start_date:None
[2024-12-17T16:10:59.647+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline and current dag run status:running
[2024-12-17T16:10:59.648+0530] {logging_mixin.py:190} INFO - [2024-12-17T16:10:59.648+0530] {taskinstance.py:731} INFO - ::endgroup::
[2024-12-17T16:10:59.649+0530] {logging_mixin.py:190} INFO - 2024-12-17
[2024-12-17T16:10:59.650+0530] {logging_mixin.py:190} INFO - [2024-12-17 16:10:59,650] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T16:10:59.651+0530] {logging_mixin.py:190} INFO - [2024-12-17T16:10:59.650+0530] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T16:10:59.656+0530] {logging_mixin.py:190} INFO - [2024-12-17T16:10:59.656+0530] {taskinstance.py:340} INFO - ::group::Post task execution logs
[2024-12-17T16:10:59.656+0530] {logging_mixin.py:190} INFO - [2024-12-17T16:10:59.656+0530] {taskinstance.py:352} INFO - Marking task as SUCCESS. dag_id=movie_pipeline, task_id=scrape_collection_page, run_id=manual__2024-12-17T10:40:54.321750+00:00, execution_date=20241217T104054, start_date=, end_date=20241217T104059
[2024-12-17T16:10:59.665+0530] {logging_mixin.py:190} INFO - Task instance in success state
[2024-12-17T16:10:59.666+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: running
[2024-12-17T16:10:59.666+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline queued_at:None
[2024-12-17T16:10:59.666+0530] {logging_mixin.py:190} INFO - Task hostname:Jayeshs-MacBook-Air.local operator:PythonOperator
[2024-12-17T16:10:59.666+0530] {logging_mixin.py:190} INFO - [2024-12-17T16:10:59.666+0530] {dag.py:4410} INFO - [DAG TEST] end task task_id=scrape_collection_page map_index=-1
[2024-12-17T16:10:59.672+0530] {logging_mixin.py:190} INFO - [2024-12-17T16:10:59.671+0530] {dagrun.py:854} INFO - Marking run <DagRun movie_pipeline @ 2024-12-17 10:40:54.321750+00:00: manual__2024-12-17T10:40:54.321750+00:00, state:running, queued_at: None. externally triggered: False> successful
[2024-12-17T16:10:59.672+0530] {logging_mixin.py:190} INFO - Dag run in success state
[2024-12-17T16:10:59.672+0530] {logging_mixin.py:190} INFO - Dag run start:2024-12-17 10:40:54.321750+00:00 end:2024-12-17 10:40:59.672345+00:00
[2024-12-17T16:10:59.673+0530] {logging_mixin.py:190} INFO - [2024-12-17T16:10:59.672+0530] {dagrun.py:905} INFO - DagRun Finished: dag_id=movie_pipeline, execution_date=2024-12-17 10:40:54.321750+00:00, run_id=manual__2024-12-17T10:40:54.321750+00:00, run_start_date=2024-12-17 10:40:54.321750+00:00, run_end_date=2024-12-17 10:40:59.672345+00:00, run_duration=5.350595, state=success, external_trigger=False, run_type=manual, data_interval_start=2024-12-16 10:40:54.321750+00:00, data_interval_end=2024-12-17 10:40:54.321750+00:00, dag_hash=None
[2024-12-17T16:10:59.681+0530] {processor.py:925} INFO - DAG(s) 'movie_pipeline' retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T16:10:59.693+0530] {logging_mixin.py:190} INFO - [2024-12-17T16:10:59.692+0530] {dag.py:3239} INFO - Sync 1 DAGs
[2024-12-17T16:10:59.710+0530] {logging_mixin.py:190} INFO - [2024-12-17T16:10:59.710+0530] {dag.py:4180} INFO - Setting next_dagrun for movie_pipeline to 2024-12-17 01:30:00+00:00, run_after=2024-12-18 01:30:00+00:00
[2024-12-17T16:10:59.728+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 5.457 seconds
[2024-12-17T16:14:07.650+0530] {processor.py:186} INFO - Started process (PID=74000) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T16:14:07.651+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-17T16:14:07.653+0530] {logging_mixin.py:190} INFO - [2024-12-17T16:14:07.652+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T16:14:07.888+0530] {logging_mixin.py:190} INFO - [2024-12-17T16:14:07.888+0530] {dag.py:4435} INFO - dagrun id: movie_pipeline
[2024-12-17T16:14:07.907+0530] {logging_mixin.py:190} INFO - [2024-12-17T16:14:07.906+0530] {dag.py:4451} INFO - created dagrun <DagRun movie_pipeline @ 2024-12-17 10:44:07.708573+00:00: manual__2024-12-17T10:44:07.708573+00:00, state:running, queued_at: None. externally triggered: False>
[2024-12-17T16:14:07.943+0530] {logging_mixin.py:190} INFO - [2024-12-17T16:14:07.943+0530] {dag.py:4396} INFO - [DAG TEST] starting task_id=scrape_collection_page map_index=-1
[2024-12-17T16:14:07.944+0530] {logging_mixin.py:190} INFO - [2024-12-17T16:14:07.944+0530] {dag.py:4399} INFO - [DAG TEST] running task <TaskInstance: movie_pipeline.scrape_collection_page manual__2024-12-17T10:44:07.708573+00:00 [scheduled]>
[2024-12-17T16:14:13.074+0530] {logging_mixin.py:190} INFO - [2024-12-17T16:14:13.073+0530] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-12-17T16:14:13.164+0530] {logging_mixin.py:190} INFO - [2024-12-17 16:14:13,164] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T10:44:07.708573+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T10:44:07.708573+00:00'
[2024-12-17T16:14:13.164+0530] {logging_mixin.py:190} INFO - [2024-12-17T16:14:13.164+0530] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T10:44:07.708573+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T10:44:07.708573+00:00'
[2024-12-17T16:14:13.167+0530] {logging_mixin.py:190} INFO - Task instance is in running state
[2024-12-17T16:14:13.167+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: queued
[2024-12-17T16:14:13.168+0530] {logging_mixin.py:190} INFO - Current task name:scrape_collection_page state:scheduled start_date:None
[2024-12-17T16:14:13.168+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline and current dag run status:running
[2024-12-17T16:14:13.168+0530] {logging_mixin.py:190} INFO - [2024-12-17T16:14:13.168+0530] {taskinstance.py:731} INFO - ::endgroup::
[2024-12-17T16:14:13.171+0530] {logging_mixin.py:190} INFO - 2024-12-17
[2024-12-17T16:14:13.171+0530] {logging_mixin.py:190} INFO - [2024-12-17 16:14:13,171] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T16:14:13.172+0530] {logging_mixin.py:190} INFO - [2024-12-17T16:14:13.171+0530] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T16:14:13.183+0530] {logging_mixin.py:190} INFO - [2024-12-17T16:14:13.183+0530] {taskinstance.py:340} INFO - ::group::Post task execution logs
[2024-12-17T16:14:13.184+0530] {logging_mixin.py:190} INFO - [2024-12-17T16:14:13.184+0530] {taskinstance.py:352} INFO - Marking task as SUCCESS. dag_id=movie_pipeline, task_id=scrape_collection_page, run_id=manual__2024-12-17T10:44:07.708573+00:00, execution_date=20241217T104407, start_date=, end_date=20241217T104413
[2024-12-17T16:14:13.198+0530] {logging_mixin.py:190} INFO - Task instance in success state
[2024-12-17T16:14:13.198+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: running
[2024-12-17T16:14:13.198+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline queued_at:None
[2024-12-17T16:14:13.198+0530] {logging_mixin.py:190} INFO - Task hostname:Jayeshs-MacBook-Air.local operator:PythonOperator
[2024-12-17T16:14:13.199+0530] {logging_mixin.py:190} INFO - [2024-12-17T16:14:13.199+0530] {dag.py:4410} INFO - [DAG TEST] end task task_id=scrape_collection_page map_index=-1
[2024-12-17T16:14:13.203+0530] {logging_mixin.py:190} INFO - [2024-12-17T16:14:13.203+0530] {dagrun.py:854} INFO - Marking run <DagRun movie_pipeline @ 2024-12-17 10:44:07.708573+00:00: manual__2024-12-17T10:44:07.708573+00:00, state:running, queued_at: None. externally triggered: False> successful
[2024-12-17T16:14:13.203+0530] {logging_mixin.py:190} INFO - Dag run in success state
[2024-12-17T16:14:13.203+0530] {logging_mixin.py:190} INFO - Dag run start:2024-12-17 10:44:07.708573+00:00 end:2024-12-17 10:44:13.203505+00:00
[2024-12-17T16:14:13.204+0530] {logging_mixin.py:190} INFO - [2024-12-17T16:14:13.203+0530] {dagrun.py:905} INFO - DagRun Finished: dag_id=movie_pipeline, execution_date=2024-12-17 10:44:07.708573+00:00, run_id=manual__2024-12-17T10:44:07.708573+00:00, run_start_date=2024-12-17 10:44:07.708573+00:00, run_end_date=2024-12-17 10:44:13.203505+00:00, run_duration=5.494932, state=success, external_trigger=False, run_type=manual, data_interval_start=2024-12-16 10:44:07.708573+00:00, data_interval_end=2024-12-17 10:44:07.708573+00:00, dag_hash=None
[2024-12-17T16:14:13.212+0530] {processor.py:925} INFO - DAG(s) 'movie_pipeline' retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T16:14:13.233+0530] {logging_mixin.py:190} INFO - [2024-12-17T16:14:13.232+0530] {dag.py:3239} INFO - Sync 1 DAGs
[2024-12-17T16:14:13.253+0530] {logging_mixin.py:190} INFO - [2024-12-17T16:14:13.253+0530] {dag.py:4180} INFO - Setting next_dagrun for movie_pipeline to 2024-12-17 01:30:00+00:00, run_after=2024-12-18 01:30:00+00:00
[2024-12-17T16:14:13.278+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 5.633 seconds
[2024-12-17T16:18:00.442+0530] {processor.py:186} INFO - Started process (PID=74254) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T16:18:00.447+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-17T16:18:00.449+0530] {logging_mixin.py:190} INFO - [2024-12-17T16:18:00.448+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T16:18:00.671+0530] {logging_mixin.py:190} INFO - [2024-12-17T16:18:00.671+0530] {dag.py:4435} INFO - dagrun id: movie_pipeline
[2024-12-17T16:18:00.687+0530] {logging_mixin.py:190} INFO - [2024-12-17T16:18:00.686+0530] {dag.py:4451} INFO - created dagrun <DagRun movie_pipeline @ 2024-12-17 10:48:00.502968+00:00: manual__2024-12-17T10:48:00.502968+00:00, state:running, queued_at: None. externally triggered: False>
[2024-12-17T16:18:00.719+0530] {logging_mixin.py:190} INFO - [2024-12-17T16:18:00.719+0530] {dag.py:4396} INFO - [DAG TEST] starting task_id=scrape_collection_page map_index=-1
[2024-12-17T16:18:00.720+0530] {logging_mixin.py:190} INFO - [2024-12-17T16:18:00.719+0530] {dag.py:4399} INFO - [DAG TEST] running task <TaskInstance: movie_pipeline.scrape_collection_page manual__2024-12-17T10:48:00.502968+00:00 [scheduled]>
[2024-12-17T16:18:05.821+0530] {logging_mixin.py:190} INFO - [2024-12-17T16:18:05.821+0530] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-12-17T16:18:05.896+0530] {logging_mixin.py:190} INFO - [2024-12-17 16:18:05,896] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T10:48:00.502968+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T10:48:00.502968+00:00'
[2024-12-17T16:18:05.896+0530] {logging_mixin.py:190} INFO - [2024-12-17T16:18:05.896+0530] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T10:48:00.502968+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T10:48:00.502968+00:00'
[2024-12-17T16:18:05.912+0530] {logging_mixin.py:190} INFO - Task instance is in running state
[2024-12-17T16:18:05.912+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: queued
[2024-12-17T16:18:05.912+0530] {logging_mixin.py:190} INFO - Current task name:scrape_collection_page state:scheduled start_date:None
[2024-12-17T16:18:05.913+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline and current dag run status:running
[2024-12-17T16:18:05.913+0530] {logging_mixin.py:190} INFO - [2024-12-17T16:18:05.913+0530] {taskinstance.py:731} INFO - ::endgroup::
[2024-12-17T16:18:05.916+0530] {logging_mixin.py:190} INFO - 2024-12-17
[2024-12-17T16:18:05.917+0530] {logging_mixin.py:190} INFO - [2024-12-17 16:18:05,917] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T16:18:05.917+0530] {logging_mixin.py:190} INFO - [2024-12-17T16:18:05.917+0530] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T16:18:05.924+0530] {logging_mixin.py:190} INFO - [2024-12-17T16:18:05.923+0530] {taskinstance.py:340} INFO - ::group::Post task execution logs
[2024-12-17T16:18:05.924+0530] {logging_mixin.py:190} INFO - [2024-12-17T16:18:05.924+0530] {taskinstance.py:352} INFO - Marking task as SUCCESS. dag_id=movie_pipeline, task_id=scrape_collection_page, run_id=manual__2024-12-17T10:48:00.502968+00:00, execution_date=20241217T104800, start_date=, end_date=20241217T104805
[2024-12-17T16:18:05.934+0530] {logging_mixin.py:190} INFO - Task instance in success state
[2024-12-17T16:18:05.934+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: running
[2024-12-17T16:18:05.934+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline queued_at:None
[2024-12-17T16:18:05.934+0530] {logging_mixin.py:190} INFO - Task hostname:Jayeshs-MacBook-Air.local operator:PythonOperator
[2024-12-17T16:18:05.935+0530] {logging_mixin.py:190} INFO - [2024-12-17T16:18:05.935+0530] {dag.py:4410} INFO - [DAG TEST] end task task_id=scrape_collection_page map_index=-1
[2024-12-17T16:18:05.940+0530] {logging_mixin.py:190} INFO - [2024-12-17T16:18:05.940+0530] {dagrun.py:854} INFO - Marking run <DagRun movie_pipeline @ 2024-12-17 10:48:00.502968+00:00: manual__2024-12-17T10:48:00.502968+00:00, state:running, queued_at: None. externally triggered: False> successful
[2024-12-17T16:18:05.940+0530] {logging_mixin.py:190} INFO - Dag run in success state
[2024-12-17T16:18:05.941+0530] {logging_mixin.py:190} INFO - Dag run start:2024-12-17 10:48:00.502968+00:00 end:2024-12-17 10:48:05.940705+00:00
[2024-12-17T16:18:05.941+0530] {logging_mixin.py:190} INFO - [2024-12-17T16:18:05.941+0530] {dagrun.py:905} INFO - DagRun Finished: dag_id=movie_pipeline, execution_date=2024-12-17 10:48:00.502968+00:00, run_id=manual__2024-12-17T10:48:00.502968+00:00, run_start_date=2024-12-17 10:48:00.502968+00:00, run_end_date=2024-12-17 10:48:05.940705+00:00, run_duration=5.437737, state=success, external_trigger=False, run_type=manual, data_interval_start=2024-12-16 10:48:00.502968+00:00, data_interval_end=2024-12-17 10:48:00.502968+00:00, dag_hash=None
[2024-12-17T16:18:05.950+0530] {processor.py:925} INFO - DAG(s) 'movie_pipeline' retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T16:18:05.964+0530] {logging_mixin.py:190} INFO - [2024-12-17T16:18:05.963+0530] {dag.py:3239} INFO - Sync 1 DAGs
[2024-12-17T16:18:05.983+0530] {logging_mixin.py:190} INFO - [2024-12-17T16:18:05.983+0530] {dag.py:4180} INFO - Setting next_dagrun for movie_pipeline to 2024-12-17 01:30:00+00:00, run_after=2024-12-18 01:30:00+00:00
[2024-12-17T16:18:06.004+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 5.565 seconds
[2024-12-17T16:21:15.854+0530] {processor.py:186} INFO - Started process (PID=74474) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T16:21:15.856+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-17T16:21:15.860+0530] {logging_mixin.py:190} INFO - [2024-12-17T16:21:15.860+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T16:21:16.070+0530] {logging_mixin.py:190} INFO - [2024-12-17T16:21:16.070+0530] {dag.py:4435} INFO - dagrun id: movie_pipeline
[2024-12-17T16:21:16.087+0530] {logging_mixin.py:190} INFO - [2024-12-17T16:21:16.086+0530] {dag.py:4451} INFO - created dagrun <DagRun movie_pipeline @ 2024-12-17 10:51:15.910104+00:00: manual__2024-12-17T10:51:15.910104+00:00, state:running, queued_at: None. externally triggered: False>
[2024-12-17T16:21:16.120+0530] {logging_mixin.py:190} INFO - [2024-12-17T16:21:16.120+0530] {dag.py:4396} INFO - [DAG TEST] starting task_id=scrape_collection_page map_index=-1
[2024-12-17T16:21:16.121+0530] {logging_mixin.py:190} INFO - [2024-12-17T16:21:16.120+0530] {dag.py:4399} INFO - [DAG TEST] running task <TaskInstance: movie_pipeline.scrape_collection_page manual__2024-12-17T10:51:15.910104+00:00 [scheduled]>
[2024-12-17T16:21:21.234+0530] {logging_mixin.py:190} INFO - [2024-12-17T16:21:21.234+0530] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-12-17T16:21:21.300+0530] {logging_mixin.py:190} INFO - [2024-12-17 16:21:21,300] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T10:51:15.910104+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T10:51:15.910104+00:00'
[2024-12-17T16:21:21.300+0530] {logging_mixin.py:190} INFO - [2024-12-17T16:21:21.300+0530] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T10:51:15.910104+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T10:51:15.910104+00:00'
[2024-12-17T16:21:21.304+0530] {logging_mixin.py:190} INFO - Task instance is in running state
[2024-12-17T16:21:21.304+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: queued
[2024-12-17T16:21:21.304+0530] {logging_mixin.py:190} INFO - Current task name:scrape_collection_page state:scheduled start_date:None
[2024-12-17T16:21:21.304+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline and current dag run status:running
[2024-12-17T16:21:21.305+0530] {logging_mixin.py:190} INFO - [2024-12-17T16:21:21.304+0530] {taskinstance.py:731} INFO - ::endgroup::
[2024-12-17T16:21:21.306+0530] {logging_mixin.py:190} INFO - 2024-12-17
[2024-12-17T16:21:21.307+0530] {logging_mixin.py:190} INFO - [2024-12-17 16:21:21,307] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T16:21:21.307+0530] {logging_mixin.py:190} INFO - [2024-12-17T16:21:21.307+0530] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T16:21:21.313+0530] {logging_mixin.py:190} INFO - [2024-12-17T16:21:21.313+0530] {taskinstance.py:340} INFO - ::group::Post task execution logs
[2024-12-17T16:21:21.314+0530] {logging_mixin.py:190} INFO - [2024-12-17T16:21:21.314+0530] {taskinstance.py:352} INFO - Marking task as SUCCESS. dag_id=movie_pipeline, task_id=scrape_collection_page, run_id=manual__2024-12-17T10:51:15.910104+00:00, execution_date=20241217T105115, start_date=, end_date=20241217T105121
[2024-12-17T16:21:21.325+0530] {logging_mixin.py:190} INFO - Task instance in success state
[2024-12-17T16:21:21.325+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: running
[2024-12-17T16:21:21.325+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline queued_at:None
[2024-12-17T16:21:21.326+0530] {logging_mixin.py:190} INFO - Task hostname:Jayeshs-MacBook-Air.local operator:PythonOperator
[2024-12-17T16:21:21.327+0530] {logging_mixin.py:190} INFO - [2024-12-17T16:21:21.327+0530] {dag.py:4410} INFO - [DAG TEST] end task task_id=scrape_collection_page map_index=-1
[2024-12-17T16:21:21.332+0530] {logging_mixin.py:190} INFO - [2024-12-17T16:21:21.331+0530] {dagrun.py:854} INFO - Marking run <DagRun movie_pipeline @ 2024-12-17 10:51:15.910104+00:00: manual__2024-12-17T10:51:15.910104+00:00, state:running, queued_at: None. externally triggered: False> successful
[2024-12-17T16:21:21.332+0530] {logging_mixin.py:190} INFO - Dag run in success state
[2024-12-17T16:21:21.333+0530] {logging_mixin.py:190} INFO - Dag run start:2024-12-17 10:51:15.910104+00:00 end:2024-12-17 10:51:21.332712+00:00
[2024-12-17T16:21:21.333+0530] {logging_mixin.py:190} INFO - [2024-12-17T16:21:21.333+0530] {dagrun.py:905} INFO - DagRun Finished: dag_id=movie_pipeline, execution_date=2024-12-17 10:51:15.910104+00:00, run_id=manual__2024-12-17T10:51:15.910104+00:00, run_start_date=2024-12-17 10:51:15.910104+00:00, run_end_date=2024-12-17 10:51:21.332712+00:00, run_duration=5.422608, state=success, external_trigger=False, run_type=manual, data_interval_start=2024-12-16 10:51:15.910104+00:00, data_interval_end=2024-12-17 10:51:15.910104+00:00, dag_hash=None
[2024-12-17T16:21:21.343+0530] {processor.py:925} INFO - DAG(s) 'movie_pipeline' retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T16:21:21.356+0530] {logging_mixin.py:190} INFO - [2024-12-17T16:21:21.355+0530] {dag.py:3239} INFO - Sync 1 DAGs
[2024-12-17T16:21:21.377+0530] {logging_mixin.py:190} INFO - [2024-12-17T16:21:21.377+0530] {dag.py:4180} INFO - Setting next_dagrun for movie_pipeline to 2024-12-17 01:30:00+00:00, run_after=2024-12-18 01:30:00+00:00
[2024-12-17T16:21:21.398+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 5.552 seconds
[2024-12-17T16:24:25.143+0530] {processor.py:186} INFO - Started process (PID=74696) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T16:24:25.144+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-17T16:24:25.148+0530] {logging_mixin.py:190} INFO - [2024-12-17T16:24:25.147+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T16:24:25.348+0530] {logging_mixin.py:190} INFO - [2024-12-17T16:24:25.346+0530] {dag.py:4435} INFO - dagrun id: movie_pipeline
[2024-12-17T16:24:25.365+0530] {logging_mixin.py:190} INFO - [2024-12-17T16:24:25.365+0530] {dag.py:4451} INFO - created dagrun <DagRun movie_pipeline @ 2024-12-17 10:54:25.199699+00:00: manual__2024-12-17T10:54:25.199699+00:00, state:running, queued_at: None. externally triggered: False>
[2024-12-17T16:24:25.392+0530] {logging_mixin.py:190} INFO - [2024-12-17T16:24:25.391+0530] {dag.py:4396} INFO - [DAG TEST] starting task_id=scrape_collection_page map_index=-1
[2024-12-17T16:24:25.392+0530] {logging_mixin.py:190} INFO - [2024-12-17T16:24:25.392+0530] {dag.py:4399} INFO - [DAG TEST] running task <TaskInstance: movie_pipeline.scrape_collection_page manual__2024-12-17T10:54:25.199699+00:00 [scheduled]>
[2024-12-17T16:24:30.502+0530] {logging_mixin.py:190} INFO - [2024-12-17T16:24:30.502+0530] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-12-17T16:24:30.569+0530] {logging_mixin.py:190} INFO - [2024-12-17 16:24:30,569] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T10:54:25.199699+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T10:54:25.199699+00:00'
[2024-12-17T16:24:30.570+0530] {logging_mixin.py:190} INFO - [2024-12-17T16:24:30.569+0530] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T10:54:25.199699+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T10:54:25.199699+00:00'
[2024-12-17T16:24:30.572+0530] {logging_mixin.py:190} INFO - Task instance is in running state
[2024-12-17T16:24:30.573+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: queued
[2024-12-17T16:24:30.573+0530] {logging_mixin.py:190} INFO - Current task name:scrape_collection_page state:scheduled start_date:None
[2024-12-17T16:24:30.573+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline and current dag run status:running
[2024-12-17T16:24:30.574+0530] {logging_mixin.py:190} INFO - [2024-12-17T16:24:30.573+0530] {taskinstance.py:731} INFO - ::endgroup::
[2024-12-17T16:24:30.575+0530] {logging_mixin.py:190} INFO - 2024-12-17
[2024-12-17T16:24:30.576+0530] {logging_mixin.py:190} INFO - [2024-12-17 16:24:30,575] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T16:24:30.576+0530] {logging_mixin.py:190} INFO - [2024-12-17T16:24:30.575+0530] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T16:24:30.583+0530] {logging_mixin.py:190} INFO - [2024-12-17T16:24:30.582+0530] {taskinstance.py:340} INFO - ::group::Post task execution logs
[2024-12-17T16:24:30.584+0530] {logging_mixin.py:190} INFO - [2024-12-17T16:24:30.583+0530] {taskinstance.py:352} INFO - Marking task as SUCCESS. dag_id=movie_pipeline, task_id=scrape_collection_page, run_id=manual__2024-12-17T10:54:25.199699+00:00, execution_date=20241217T105425, start_date=, end_date=20241217T105430
[2024-12-17T16:24:30.594+0530] {logging_mixin.py:190} INFO - Task instance in success state
[2024-12-17T16:24:30.595+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: running
[2024-12-17T16:24:30.596+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline queued_at:None
[2024-12-17T16:24:30.596+0530] {logging_mixin.py:190} INFO - Task hostname:Jayeshs-MacBook-Air.local operator:PythonOperator
[2024-12-17T16:24:30.597+0530] {logging_mixin.py:190} INFO - [2024-12-17T16:24:30.596+0530] {dag.py:4410} INFO - [DAG TEST] end task task_id=scrape_collection_page map_index=-1
[2024-12-17T16:24:30.601+0530] {logging_mixin.py:190} INFO - [2024-12-17T16:24:30.601+0530] {dagrun.py:854} INFO - Marking run <DagRun movie_pipeline @ 2024-12-17 10:54:25.199699+00:00: manual__2024-12-17T10:54:25.199699+00:00, state:running, queued_at: None. externally triggered: False> successful
[2024-12-17T16:24:30.602+0530] {logging_mixin.py:190} INFO - Dag run in success state
[2024-12-17T16:24:30.602+0530] {logging_mixin.py:190} INFO - Dag run start:2024-12-17 10:54:25.199699+00:00 end:2024-12-17 10:54:30.602165+00:00
[2024-12-17T16:24:30.603+0530] {logging_mixin.py:190} INFO - [2024-12-17T16:24:30.603+0530] {dagrun.py:905} INFO - DagRun Finished: dag_id=movie_pipeline, execution_date=2024-12-17 10:54:25.199699+00:00, run_id=manual__2024-12-17T10:54:25.199699+00:00, run_start_date=2024-12-17 10:54:25.199699+00:00, run_end_date=2024-12-17 10:54:30.602165+00:00, run_duration=5.402466, state=success, external_trigger=False, run_type=manual, data_interval_start=2024-12-16 10:54:25.199699+00:00, data_interval_end=2024-12-17 10:54:25.199699+00:00, dag_hash=None
[2024-12-17T16:24:30.611+0530] {processor.py:925} INFO - DAG(s) 'movie_pipeline' retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T16:24:30.626+0530] {logging_mixin.py:190} INFO - [2024-12-17T16:24:30.625+0530] {dag.py:3239} INFO - Sync 1 DAGs
[2024-12-17T16:24:30.646+0530] {logging_mixin.py:190} INFO - [2024-12-17T16:24:30.646+0530] {dag.py:4180} INFO - Setting next_dagrun for movie_pipeline to 2024-12-17 01:30:00+00:00, run_after=2024-12-18 01:30:00+00:00
[2024-12-17T16:24:30.668+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 5.529 seconds
[2024-12-17T16:27:36.327+0530] {processor.py:186} INFO - Started process (PID=74908) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T16:27:36.328+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-17T16:27:36.330+0530] {logging_mixin.py:190} INFO - [2024-12-17T16:27:36.330+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T16:27:36.561+0530] {logging_mixin.py:190} INFO - [2024-12-17T16:27:36.561+0530] {dag.py:4435} INFO - dagrun id: movie_pipeline
[2024-12-17T16:27:36.584+0530] {logging_mixin.py:190} INFO - [2024-12-17T16:27:36.584+0530] {dag.py:4451} INFO - created dagrun <DagRun movie_pipeline @ 2024-12-17 10:57:36.391766+00:00: manual__2024-12-17T10:57:36.391766+00:00, state:running, queued_at: None. externally triggered: False>
[2024-12-17T16:27:36.614+0530] {logging_mixin.py:190} INFO - [2024-12-17T16:27:36.614+0530] {dag.py:4396} INFO - [DAG TEST] starting task_id=scrape_collection_page map_index=-1
[2024-12-17T16:27:36.615+0530] {logging_mixin.py:190} INFO - [2024-12-17T16:27:36.614+0530] {dag.py:4399} INFO - [DAG TEST] running task <TaskInstance: movie_pipeline.scrape_collection_page manual__2024-12-17T10:57:36.391766+00:00 [scheduled]>
[2024-12-17T16:27:41.737+0530] {logging_mixin.py:190} INFO - [2024-12-17T16:27:41.737+0530] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-12-17T16:27:41.818+0530] {logging_mixin.py:190} INFO - [2024-12-17 16:27:41,818] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T10:57:36.391766+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T10:57:36.391766+00:00'
[2024-12-17T16:27:41.818+0530] {logging_mixin.py:190} INFO - [2024-12-17T16:27:41.818+0530] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T10:57:36.391766+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T10:57:36.391766+00:00'
[2024-12-17T16:27:41.824+0530] {logging_mixin.py:190} INFO - Task instance is in running state
[2024-12-17T16:27:41.825+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: queued
[2024-12-17T16:27:41.825+0530] {logging_mixin.py:190} INFO - Current task name:scrape_collection_page state:scheduled start_date:None
[2024-12-17T16:27:41.826+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline and current dag run status:running
[2024-12-17T16:27:41.826+0530] {logging_mixin.py:190} INFO - [2024-12-17T16:27:41.826+0530] {taskinstance.py:731} INFO - ::endgroup::
[2024-12-17T16:27:41.829+0530] {logging_mixin.py:190} INFO - 2024-12-17
[2024-12-17T16:27:41.829+0530] {logging_mixin.py:190} INFO - [2024-12-17 16:27:41,829] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T16:27:41.830+0530] {logging_mixin.py:190} INFO - [2024-12-17T16:27:41.829+0530] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T16:27:41.839+0530] {logging_mixin.py:190} INFO - [2024-12-17T16:27:41.839+0530] {taskinstance.py:340} INFO - ::group::Post task execution logs
[2024-12-17T16:27:41.839+0530] {logging_mixin.py:190} INFO - [2024-12-17T16:27:41.839+0530] {taskinstance.py:352} INFO - Marking task as SUCCESS. dag_id=movie_pipeline, task_id=scrape_collection_page, run_id=manual__2024-12-17T10:57:36.391766+00:00, execution_date=20241217T105736, start_date=, end_date=20241217T105741
[2024-12-17T16:27:41.849+0530] {logging_mixin.py:190} INFO - Task instance in success state
[2024-12-17T16:27:41.850+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: running
[2024-12-17T16:27:41.850+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline queued_at:None
[2024-12-17T16:27:41.850+0530] {logging_mixin.py:190} INFO - Task hostname:Jayeshs-MacBook-Air.local operator:PythonOperator
[2024-12-17T16:27:41.851+0530] {logging_mixin.py:190} INFO - [2024-12-17T16:27:41.850+0530] {dag.py:4410} INFO - [DAG TEST] end task task_id=scrape_collection_page map_index=-1
[2024-12-17T16:27:41.856+0530] {logging_mixin.py:190} INFO - [2024-12-17T16:27:41.855+0530] {dagrun.py:854} INFO - Marking run <DagRun movie_pipeline @ 2024-12-17 10:57:36.391766+00:00: manual__2024-12-17T10:57:36.391766+00:00, state:running, queued_at: None. externally triggered: False> successful
[2024-12-17T16:27:41.857+0530] {logging_mixin.py:190} INFO - Dag run in success state
[2024-12-17T16:27:41.857+0530] {logging_mixin.py:190} INFO - Dag run start:2024-12-17 10:57:36.391766+00:00 end:2024-12-17 10:57:41.856967+00:00
[2024-12-17T16:27:41.857+0530] {logging_mixin.py:190} INFO - [2024-12-17T16:27:41.857+0530] {dagrun.py:905} INFO - DagRun Finished: dag_id=movie_pipeline, execution_date=2024-12-17 10:57:36.391766+00:00, run_id=manual__2024-12-17T10:57:36.391766+00:00, run_start_date=2024-12-17 10:57:36.391766+00:00, run_end_date=2024-12-17 10:57:41.856967+00:00, run_duration=5.465201, state=success, external_trigger=False, run_type=manual, data_interval_start=2024-12-16 10:57:36.391766+00:00, data_interval_end=2024-12-17 10:57:36.391766+00:00, dag_hash=None
[2024-12-17T16:27:41.880+0530] {processor.py:925} INFO - DAG(s) 'movie_pipeline' retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T16:27:41.907+0530] {logging_mixin.py:190} INFO - [2024-12-17T16:27:41.906+0530] {dag.py:3239} INFO - Sync 1 DAGs
[2024-12-17T16:27:41.945+0530] {logging_mixin.py:190} INFO - [2024-12-17T16:27:41.945+0530] {dag.py:4180} INFO - Setting next_dagrun for movie_pipeline to 2024-12-17 01:30:00+00:00, run_after=2024-12-18 01:30:00+00:00
[2024-12-17T16:27:41.978+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 5.658 seconds
[2024-12-17T16:30:50.237+0530] {processor.py:186} INFO - Started process (PID=75130) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T16:30:50.238+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-17T16:30:50.240+0530] {logging_mixin.py:190} INFO - [2024-12-17T16:30:50.240+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T16:30:50.413+0530] {logging_mixin.py:190} INFO - [2024-12-17T16:30:50.412+0530] {dag.py:4435} INFO - dagrun id: movie_pipeline
[2024-12-17T16:30:50.429+0530] {logging_mixin.py:190} INFO - [2024-12-17T16:30:50.428+0530] {dag.py:4451} INFO - created dagrun <DagRun movie_pipeline @ 2024-12-17 11:00:50.282921+00:00: manual__2024-12-17T11:00:50.282921+00:00, state:running, queued_at: None. externally triggered: False>
[2024-12-17T16:30:50.454+0530] {logging_mixin.py:190} INFO - [2024-12-17T16:30:50.453+0530] {dag.py:4396} INFO - [DAG TEST] starting task_id=scrape_collection_page map_index=-1
[2024-12-17T16:30:50.454+0530] {logging_mixin.py:190} INFO - [2024-12-17T16:30:50.454+0530] {dag.py:4399} INFO - [DAG TEST] running task <TaskInstance: movie_pipeline.scrape_collection_page manual__2024-12-17T11:00:50.282921+00:00 [scheduled]>
[2024-12-17T16:30:55.555+0530] {logging_mixin.py:190} INFO - [2024-12-17T16:30:55.555+0530] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-12-17T16:30:55.615+0530] {logging_mixin.py:190} INFO - [2024-12-17 16:30:55,615] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T11:00:50.282921+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T11:00:50.282921+00:00'
[2024-12-17T16:30:55.616+0530] {logging_mixin.py:190} INFO - [2024-12-17T16:30:55.615+0530] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T11:00:50.282921+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T11:00:50.282921+00:00'
[2024-12-17T16:30:55.619+0530] {logging_mixin.py:190} INFO - Task instance is in running state
[2024-12-17T16:30:55.620+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: queued
[2024-12-17T16:30:55.620+0530] {logging_mixin.py:190} INFO - Current task name:scrape_collection_page state:scheduled start_date:None
[2024-12-17T16:30:55.620+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline and current dag run status:running
[2024-12-17T16:30:55.621+0530] {logging_mixin.py:190} INFO - [2024-12-17T16:30:55.621+0530] {taskinstance.py:731} INFO - ::endgroup::
[2024-12-17T16:30:55.622+0530] {logging_mixin.py:190} INFO - 2024-12-17
[2024-12-17T16:30:55.623+0530] {logging_mixin.py:190} INFO - [2024-12-17 16:30:55,623] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T16:30:55.623+0530] {logging_mixin.py:190} INFO - [2024-12-17T16:30:55.623+0530] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T16:30:55.628+0530] {logging_mixin.py:190} INFO - [2024-12-17T16:30:55.628+0530] {taskinstance.py:340} INFO - ::group::Post task execution logs
[2024-12-17T16:30:55.628+0530] {logging_mixin.py:190} INFO - [2024-12-17T16:30:55.628+0530] {taskinstance.py:352} INFO - Marking task as SUCCESS. dag_id=movie_pipeline, task_id=scrape_collection_page, run_id=manual__2024-12-17T11:00:50.282921+00:00, execution_date=20241217T110050, start_date=, end_date=20241217T110055
[2024-12-17T16:30:55.637+0530] {logging_mixin.py:190} INFO - Task instance in success state
[2024-12-17T16:30:55.638+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: running
[2024-12-17T16:30:55.638+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline queued_at:None
[2024-12-17T16:30:55.638+0530] {logging_mixin.py:190} INFO - Task hostname:Jayeshs-MacBook-Air.local operator:PythonOperator
[2024-12-17T16:30:55.638+0530] {logging_mixin.py:190} INFO - [2024-12-17T16:30:55.638+0530] {dag.py:4410} INFO - [DAG TEST] end task task_id=scrape_collection_page map_index=-1
[2024-12-17T16:30:55.642+0530] {logging_mixin.py:190} INFO - [2024-12-17T16:30:55.642+0530] {dagrun.py:854} INFO - Marking run <DagRun movie_pipeline @ 2024-12-17 11:00:50.282921+00:00: manual__2024-12-17T11:00:50.282921+00:00, state:running, queued_at: None. externally triggered: False> successful
[2024-12-17T16:30:55.642+0530] {logging_mixin.py:190} INFO - Dag run in success state
[2024-12-17T16:30:55.642+0530] {logging_mixin.py:190} INFO - Dag run start:2024-12-17 11:00:50.282921+00:00 end:2024-12-17 11:00:55.642712+00:00
[2024-12-17T16:30:55.643+0530] {logging_mixin.py:190} INFO - [2024-12-17T16:30:55.643+0530] {dagrun.py:905} INFO - DagRun Finished: dag_id=movie_pipeline, execution_date=2024-12-17 11:00:50.282921+00:00, run_id=manual__2024-12-17T11:00:50.282921+00:00, run_start_date=2024-12-17 11:00:50.282921+00:00, run_end_date=2024-12-17 11:00:55.642712+00:00, run_duration=5.359791, state=success, external_trigger=False, run_type=manual, data_interval_start=2024-12-16 11:00:50.282921+00:00, data_interval_end=2024-12-17 11:00:50.282921+00:00, dag_hash=None
[2024-12-17T16:30:55.652+0530] {processor.py:925} INFO - DAG(s) 'movie_pipeline' retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T16:30:55.663+0530] {logging_mixin.py:190} INFO - [2024-12-17T16:30:55.663+0530] {dag.py:3239} INFO - Sync 1 DAGs
[2024-12-17T16:30:55.681+0530] {logging_mixin.py:190} INFO - [2024-12-17T16:30:55.681+0530] {dag.py:4180} INFO - Setting next_dagrun for movie_pipeline to 2024-12-17 01:30:00+00:00, run_after=2024-12-18 01:30:00+00:00
[2024-12-17T16:30:55.699+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 5.470 seconds
[2024-12-17T16:33:36.582+0530] {processor.py:186} INFO - Started process (PID=75320) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T16:33:36.584+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-17T16:33:36.585+0530] {logging_mixin.py:190} INFO - [2024-12-17T16:33:36.584+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T16:33:36.746+0530] {logging_mixin.py:190} INFO - [2024-12-17T16:33:36.745+0530] {dag.py:4435} INFO - dagrun id: movie_pipeline
[2024-12-17T16:33:36.761+0530] {logging_mixin.py:190} INFO - [2024-12-17T16:33:36.760+0530] {dag.py:4451} INFO - created dagrun <DagRun movie_pipeline @ 2024-12-17 11:03:36.623196+00:00: manual__2024-12-17T11:03:36.623196+00:00, state:running, queued_at: None. externally triggered: False>
[2024-12-17T16:33:36.786+0530] {logging_mixin.py:190} INFO - [2024-12-17T16:33:36.786+0530] {dag.py:4396} INFO - [DAG TEST] starting task_id=scrape_collection_page map_index=-1
[2024-12-17T16:33:36.787+0530] {logging_mixin.py:190} INFO - [2024-12-17T16:33:36.787+0530] {dag.py:4399} INFO - [DAG TEST] running task <TaskInstance: movie_pipeline.scrape_collection_page manual__2024-12-17T11:03:36.623196+00:00 [scheduled]>
[2024-12-17T16:33:41.880+0530] {logging_mixin.py:190} INFO - [2024-12-17T16:33:41.880+0530] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-12-17T16:33:41.938+0530] {logging_mixin.py:190} INFO - [2024-12-17 16:33:41,938] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T11:03:36.623196+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T11:03:36.623196+00:00'
[2024-12-17T16:33:41.939+0530] {logging_mixin.py:190} INFO - [2024-12-17T16:33:41.938+0530] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T11:03:36.623196+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T11:03:36.623196+00:00'
[2024-12-17T16:33:41.942+0530] {logging_mixin.py:190} INFO - Task instance is in running state
[2024-12-17T16:33:41.942+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: queued
[2024-12-17T16:33:41.942+0530] {logging_mixin.py:190} INFO - Current task name:scrape_collection_page state:scheduled start_date:None
[2024-12-17T16:33:41.943+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline and current dag run status:running
[2024-12-17T16:33:41.943+0530] {logging_mixin.py:190} INFO - [2024-12-17T16:33:41.943+0530] {taskinstance.py:731} INFO - ::endgroup::
[2024-12-17T16:33:41.945+0530] {logging_mixin.py:190} INFO - 2024-12-17
[2024-12-17T16:33:41.945+0530] {logging_mixin.py:190} INFO - [2024-12-17 16:33:41,945] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T16:33:41.946+0530] {logging_mixin.py:190} INFO - [2024-12-17T16:33:41.945+0530] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T16:33:41.951+0530] {logging_mixin.py:190} INFO - [2024-12-17T16:33:41.950+0530] {taskinstance.py:340} INFO - ::group::Post task execution logs
[2024-12-17T16:33:41.951+0530] {logging_mixin.py:190} INFO - [2024-12-17T16:33:41.951+0530] {taskinstance.py:352} INFO - Marking task as SUCCESS. dag_id=movie_pipeline, task_id=scrape_collection_page, run_id=manual__2024-12-17T11:03:36.623196+00:00, execution_date=20241217T110336, start_date=, end_date=20241217T110341
[2024-12-17T16:33:41.960+0530] {logging_mixin.py:190} INFO - Task instance in success state
[2024-12-17T16:33:41.961+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: running
[2024-12-17T16:33:41.961+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline queued_at:None
[2024-12-17T16:33:41.961+0530] {logging_mixin.py:190} INFO - Task hostname:Jayeshs-MacBook-Air.local operator:PythonOperator
[2024-12-17T16:33:41.961+0530] {logging_mixin.py:190} INFO - [2024-12-17T16:33:41.961+0530] {dag.py:4410} INFO - [DAG TEST] end task task_id=scrape_collection_page map_index=-1
[2024-12-17T16:33:41.965+0530] {logging_mixin.py:190} INFO - [2024-12-17T16:33:41.965+0530] {dagrun.py:854} INFO - Marking run <DagRun movie_pipeline @ 2024-12-17 11:03:36.623196+00:00: manual__2024-12-17T11:03:36.623196+00:00, state:running, queued_at: None. externally triggered: False> successful
[2024-12-17T16:33:41.965+0530] {logging_mixin.py:190} INFO - Dag run in success state
[2024-12-17T16:33:41.966+0530] {logging_mixin.py:190} INFO - Dag run start:2024-12-17 11:03:36.623196+00:00 end:2024-12-17 11:03:41.965878+00:00
[2024-12-17T16:33:41.966+0530] {logging_mixin.py:190} INFO - [2024-12-17T16:33:41.966+0530] {dagrun.py:905} INFO - DagRun Finished: dag_id=movie_pipeline, execution_date=2024-12-17 11:03:36.623196+00:00, run_id=manual__2024-12-17T11:03:36.623196+00:00, run_start_date=2024-12-17 11:03:36.623196+00:00, run_end_date=2024-12-17 11:03:41.965878+00:00, run_duration=5.342682, state=success, external_trigger=False, run_type=manual, data_interval_start=2024-12-16 11:03:36.623196+00:00, data_interval_end=2024-12-17 11:03:36.623196+00:00, dag_hash=None
[2024-12-17T16:33:41.976+0530] {processor.py:925} INFO - DAG(s) 'movie_pipeline' retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T16:33:41.988+0530] {logging_mixin.py:190} INFO - [2024-12-17T16:33:41.987+0530] {dag.py:3239} INFO - Sync 1 DAGs
[2024-12-17T16:33:42.005+0530] {logging_mixin.py:190} INFO - [2024-12-17T16:33:42.004+0530] {dag.py:4180} INFO - Setting next_dagrun for movie_pipeline to 2024-12-17 01:30:00+00:00, run_after=2024-12-18 01:30:00+00:00
[2024-12-17T16:33:42.028+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 5.449 seconds
[2024-12-17T16:36:22.509+0530] {processor.py:186} INFO - Started process (PID=75522) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T16:36:22.510+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-17T16:36:22.512+0530] {logging_mixin.py:190} INFO - [2024-12-17T16:36:22.512+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T16:36:22.676+0530] {logging_mixin.py:190} INFO - [2024-12-17T16:36:22.675+0530] {dag.py:4435} INFO - dagrun id: movie_pipeline
[2024-12-17T16:36:22.689+0530] {logging_mixin.py:190} INFO - [2024-12-17T16:36:22.689+0530] {dag.py:4451} INFO - created dagrun <DagRun movie_pipeline @ 2024-12-17 11:06:22.552390+00:00: manual__2024-12-17T11:06:22.552390+00:00, state:running, queued_at: None. externally triggered: False>
[2024-12-17T16:36:22.714+0530] {logging_mixin.py:190} INFO - [2024-12-17T16:36:22.713+0530] {dag.py:4396} INFO - [DAG TEST] starting task_id=scrape_collection_page map_index=-1
[2024-12-17T16:36:22.714+0530] {logging_mixin.py:190} INFO - [2024-12-17T16:36:22.714+0530] {dag.py:4399} INFO - [DAG TEST] running task <TaskInstance: movie_pipeline.scrape_collection_page manual__2024-12-17T11:06:22.552390+00:00 [scheduled]>
[2024-12-17T16:36:27.818+0530] {logging_mixin.py:190} INFO - [2024-12-17T16:36:27.818+0530] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-12-17T16:36:27.878+0530] {logging_mixin.py:190} INFO - [2024-12-17 16:36:27,878] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T11:06:22.552390+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T11:06:22.552390+00:00'
[2024-12-17T16:36:27.878+0530] {logging_mixin.py:190} INFO - [2024-12-17T16:36:27.878+0530] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T11:06:22.552390+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T11:06:22.552390+00:00'
[2024-12-17T16:36:27.881+0530] {logging_mixin.py:190} INFO - Task instance is in running state
[2024-12-17T16:36:27.881+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: queued
[2024-12-17T16:36:27.881+0530] {logging_mixin.py:190} INFO - Current task name:scrape_collection_page state:scheduled start_date:None
[2024-12-17T16:36:27.882+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline and current dag run status:running
[2024-12-17T16:36:27.882+0530] {logging_mixin.py:190} INFO - [2024-12-17T16:36:27.882+0530] {taskinstance.py:731} INFO - ::endgroup::
[2024-12-17T16:36:27.884+0530] {logging_mixin.py:190} INFO - 2024-12-17
[2024-12-17T16:36:27.884+0530] {logging_mixin.py:190} INFO - [2024-12-17 16:36:27,884] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T16:36:27.885+0530] {logging_mixin.py:190} INFO - [2024-12-17T16:36:27.884+0530] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T16:36:27.890+0530] {logging_mixin.py:190} INFO - [2024-12-17T16:36:27.889+0530] {taskinstance.py:340} INFO - ::group::Post task execution logs
[2024-12-17T16:36:27.890+0530] {logging_mixin.py:190} INFO - [2024-12-17T16:36:27.890+0530] {taskinstance.py:352} INFO - Marking task as SUCCESS. dag_id=movie_pipeline, task_id=scrape_collection_page, run_id=manual__2024-12-17T11:06:22.552390+00:00, execution_date=20241217T110622, start_date=, end_date=20241217T110627
[2024-12-17T16:36:27.900+0530] {logging_mixin.py:190} INFO - Task instance in success state
[2024-12-17T16:36:27.900+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: running
[2024-12-17T16:36:27.900+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline queued_at:None
[2024-12-17T16:36:27.900+0530] {logging_mixin.py:190} INFO - Task hostname:Jayeshs-MacBook-Air.local operator:PythonOperator
[2024-12-17T16:36:27.901+0530] {logging_mixin.py:190} INFO - [2024-12-17T16:36:27.901+0530] {dag.py:4410} INFO - [DAG TEST] end task task_id=scrape_collection_page map_index=-1
[2024-12-17T16:36:27.905+0530] {logging_mixin.py:190} INFO - [2024-12-17T16:36:27.904+0530] {dagrun.py:854} INFO - Marking run <DagRun movie_pipeline @ 2024-12-17 11:06:22.552390+00:00: manual__2024-12-17T11:06:22.552390+00:00, state:running, queued_at: None. externally triggered: False> successful
[2024-12-17T16:36:27.905+0530] {logging_mixin.py:190} INFO - Dag run in success state
[2024-12-17T16:36:27.905+0530] {logging_mixin.py:190} INFO - Dag run start:2024-12-17 11:06:22.552390+00:00 end:2024-12-17 11:06:27.905272+00:00
[2024-12-17T16:36:27.905+0530] {logging_mixin.py:190} INFO - [2024-12-17T16:36:27.905+0530] {dagrun.py:905} INFO - DagRun Finished: dag_id=movie_pipeline, execution_date=2024-12-17 11:06:22.552390+00:00, run_id=manual__2024-12-17T11:06:22.552390+00:00, run_start_date=2024-12-17 11:06:22.552390+00:00, run_end_date=2024-12-17 11:06:27.905272+00:00, run_duration=5.352882, state=success, external_trigger=False, run_type=manual, data_interval_start=2024-12-16 11:06:22.552390+00:00, data_interval_end=2024-12-17 11:06:22.552390+00:00, dag_hash=None
[2024-12-17T16:36:27.915+0530] {processor.py:925} INFO - DAG(s) 'movie_pipeline' retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T16:36:27.926+0530] {logging_mixin.py:190} INFO - [2024-12-17T16:36:27.926+0530] {dag.py:3239} INFO - Sync 1 DAGs
[2024-12-17T16:36:27.943+0530] {logging_mixin.py:190} INFO - [2024-12-17T16:36:27.943+0530] {dag.py:4180} INFO - Setting next_dagrun for movie_pipeline to 2024-12-17 01:30:00+00:00, run_after=2024-12-18 01:30:00+00:00
[2024-12-17T16:36:27.961+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 5.455 seconds
[2024-12-17T16:39:08.065+0530] {processor.py:186} INFO - Started process (PID=75712) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T16:39:08.066+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-17T16:39:08.067+0530] {logging_mixin.py:190} INFO - [2024-12-17T16:39:08.067+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T16:39:08.231+0530] {logging_mixin.py:190} INFO - [2024-12-17T16:39:08.230+0530] {dag.py:4435} INFO - dagrun id: movie_pipeline
[2024-12-17T16:39:08.244+0530] {logging_mixin.py:190} INFO - [2024-12-17T16:39:08.243+0530] {dag.py:4451} INFO - created dagrun <DagRun movie_pipeline @ 2024-12-17 11:09:08.106339+00:00: manual__2024-12-17T11:09:08.106339+00:00, state:running, queued_at: None. externally triggered: False>
[2024-12-17T16:39:08.269+0530] {logging_mixin.py:190} INFO - [2024-12-17T16:39:08.268+0530] {dag.py:4396} INFO - [DAG TEST] starting task_id=scrape_collection_page map_index=-1
[2024-12-17T16:39:08.269+0530] {logging_mixin.py:190} INFO - [2024-12-17T16:39:08.269+0530] {dag.py:4399} INFO - [DAG TEST] running task <TaskInstance: movie_pipeline.scrape_collection_page manual__2024-12-17T11:09:08.106339+00:00 [scheduled]>
[2024-12-17T16:39:13.365+0530] {logging_mixin.py:190} INFO - [2024-12-17T16:39:13.365+0530] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-12-17T16:39:13.422+0530] {logging_mixin.py:190} INFO - [2024-12-17 16:39:13,422] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T11:09:08.106339+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T11:09:08.106339+00:00'
[2024-12-17T16:39:13.423+0530] {logging_mixin.py:190} INFO - [2024-12-17T16:39:13.422+0530] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T11:09:08.106339+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T11:09:08.106339+00:00'
[2024-12-17T16:39:13.425+0530] {logging_mixin.py:190} INFO - Task instance is in running state
[2024-12-17T16:39:13.426+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: queued
[2024-12-17T16:39:13.426+0530] {logging_mixin.py:190} INFO - Current task name:scrape_collection_page state:scheduled start_date:None
[2024-12-17T16:39:13.426+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline and current dag run status:running
[2024-12-17T16:39:13.426+0530] {logging_mixin.py:190} INFO - [2024-12-17T16:39:13.426+0530] {taskinstance.py:731} INFO - ::endgroup::
[2024-12-17T16:39:13.428+0530] {logging_mixin.py:190} INFO - 2024-12-17
[2024-12-17T16:39:13.429+0530] {logging_mixin.py:190} INFO - [2024-12-17 16:39:13,429] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T16:39:13.429+0530] {logging_mixin.py:190} INFO - [2024-12-17T16:39:13.429+0530] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T16:39:13.434+0530] {logging_mixin.py:190} INFO - [2024-12-17T16:39:13.434+0530] {taskinstance.py:340} INFO - ::group::Post task execution logs
[2024-12-17T16:39:13.435+0530] {logging_mixin.py:190} INFO - [2024-12-17T16:39:13.435+0530] {taskinstance.py:352} INFO - Marking task as SUCCESS. dag_id=movie_pipeline, task_id=scrape_collection_page, run_id=manual__2024-12-17T11:09:08.106339+00:00, execution_date=20241217T110908, start_date=, end_date=20241217T110913
[2024-12-17T16:39:13.444+0530] {logging_mixin.py:190} INFO - Task instance in success state
[2024-12-17T16:39:13.445+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: running
[2024-12-17T16:39:13.445+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline queued_at:None
[2024-12-17T16:39:13.445+0530] {logging_mixin.py:190} INFO - Task hostname:Jayeshs-MacBook-Air.local operator:PythonOperator
[2024-12-17T16:39:13.446+0530] {logging_mixin.py:190} INFO - [2024-12-17T16:39:13.445+0530] {dag.py:4410} INFO - [DAG TEST] end task task_id=scrape_collection_page map_index=-1
[2024-12-17T16:39:13.449+0530] {logging_mixin.py:190} INFO - [2024-12-17T16:39:13.449+0530] {dagrun.py:854} INFO - Marking run <DagRun movie_pipeline @ 2024-12-17 11:09:08.106339+00:00: manual__2024-12-17T11:09:08.106339+00:00, state:running, queued_at: None. externally triggered: False> successful
[2024-12-17T16:39:13.450+0530] {logging_mixin.py:190} INFO - Dag run in success state
[2024-12-17T16:39:13.450+0530] {logging_mixin.py:190} INFO - Dag run start:2024-12-17 11:09:08.106339+00:00 end:2024-12-17 11:09:13.449969+00:00
[2024-12-17T16:39:13.450+0530] {logging_mixin.py:190} INFO - [2024-12-17T16:39:13.450+0530] {dagrun.py:905} INFO - DagRun Finished: dag_id=movie_pipeline, execution_date=2024-12-17 11:09:08.106339+00:00, run_id=manual__2024-12-17T11:09:08.106339+00:00, run_start_date=2024-12-17 11:09:08.106339+00:00, run_end_date=2024-12-17 11:09:13.449969+00:00, run_duration=5.34363, state=success, external_trigger=False, run_type=manual, data_interval_start=2024-12-16 11:09:08.106339+00:00, data_interval_end=2024-12-17 11:09:08.106339+00:00, dag_hash=None
[2024-12-17T16:39:13.460+0530] {processor.py:925} INFO - DAG(s) 'movie_pipeline' retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T16:39:13.471+0530] {logging_mixin.py:190} INFO - [2024-12-17T16:39:13.471+0530] {dag.py:3239} INFO - Sync 1 DAGs
[2024-12-17T16:39:13.489+0530] {logging_mixin.py:190} INFO - [2024-12-17T16:39:13.489+0530] {dag.py:4180} INFO - Setting next_dagrun for movie_pipeline to 2024-12-17 01:30:00+00:00, run_after=2024-12-18 01:30:00+00:00
[2024-12-17T16:39:13.508+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 5.446 seconds
[2024-12-17T16:41:53.815+0530] {processor.py:186} INFO - Started process (PID=75901) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T16:41:53.816+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-17T16:41:53.817+0530] {logging_mixin.py:190} INFO - [2024-12-17T16:41:53.817+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T16:41:53.980+0530] {logging_mixin.py:190} INFO - [2024-12-17T16:41:53.979+0530] {dag.py:4435} INFO - dagrun id: movie_pipeline
[2024-12-17T16:41:53.993+0530] {logging_mixin.py:190} INFO - [2024-12-17T16:41:53.993+0530] {dag.py:4451} INFO - created dagrun <DagRun movie_pipeline @ 2024-12-17 11:11:53.855940+00:00: manual__2024-12-17T11:11:53.855940+00:00, state:running, queued_at: None. externally triggered: False>
[2024-12-17T16:41:54.018+0530] {logging_mixin.py:190} INFO - [2024-12-17T16:41:54.018+0530] {dag.py:4396} INFO - [DAG TEST] starting task_id=scrape_collection_page map_index=-1
[2024-12-17T16:41:54.019+0530] {logging_mixin.py:190} INFO - [2024-12-17T16:41:54.019+0530] {dag.py:4399} INFO - [DAG TEST] running task <TaskInstance: movie_pipeline.scrape_collection_page manual__2024-12-17T11:11:53.855940+00:00 [scheduled]>
[2024-12-17T16:41:59.107+0530] {logging_mixin.py:190} INFO - [2024-12-17T16:41:59.106+0530] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-12-17T16:41:59.172+0530] {logging_mixin.py:190} INFO - [2024-12-17 16:41:59,172] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T11:11:53.855940+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T11:11:53.855940+00:00'
[2024-12-17T16:41:59.172+0530] {logging_mixin.py:190} INFO - [2024-12-17T16:41:59.172+0530] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T11:11:53.855940+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T11:11:53.855940+00:00'
[2024-12-17T16:41:59.176+0530] {logging_mixin.py:190} INFO - Task instance is in running state
[2024-12-17T16:41:59.176+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: queued
[2024-12-17T16:41:59.176+0530] {logging_mixin.py:190} INFO - Current task name:scrape_collection_page state:scheduled start_date:None
[2024-12-17T16:41:59.177+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline and current dag run status:running
[2024-12-17T16:41:59.177+0530] {logging_mixin.py:190} INFO - [2024-12-17T16:41:59.177+0530] {taskinstance.py:731} INFO - ::endgroup::
[2024-12-17T16:41:59.179+0530] {logging_mixin.py:190} INFO - 2024-12-17
[2024-12-17T16:41:59.179+0530] {logging_mixin.py:190} INFO - [2024-12-17 16:41:59,179] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T16:41:59.179+0530] {logging_mixin.py:190} INFO - [2024-12-17T16:41:59.179+0530] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T16:41:59.184+0530] {logging_mixin.py:190} INFO - [2024-12-17T16:41:59.184+0530] {taskinstance.py:340} INFO - ::group::Post task execution logs
[2024-12-17T16:41:59.185+0530] {logging_mixin.py:190} INFO - [2024-12-17T16:41:59.185+0530] {taskinstance.py:352} INFO - Marking task as SUCCESS. dag_id=movie_pipeline, task_id=scrape_collection_page, run_id=manual__2024-12-17T11:11:53.855940+00:00, execution_date=20241217T111153, start_date=, end_date=20241217T111159
[2024-12-17T16:41:59.194+0530] {logging_mixin.py:190} INFO - Task instance in success state
[2024-12-17T16:41:59.194+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: running
[2024-12-17T16:41:59.195+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline queued_at:None
[2024-12-17T16:41:59.195+0530] {logging_mixin.py:190} INFO - Task hostname:Jayeshs-MacBook-Air.local operator:PythonOperator
[2024-12-17T16:41:59.195+0530] {logging_mixin.py:190} INFO - [2024-12-17T16:41:59.195+0530] {dag.py:4410} INFO - [DAG TEST] end task task_id=scrape_collection_page map_index=-1
[2024-12-17T16:41:59.200+0530] {logging_mixin.py:190} INFO - [2024-12-17T16:41:59.200+0530] {dagrun.py:854} INFO - Marking run <DagRun movie_pipeline @ 2024-12-17 11:11:53.855940+00:00: manual__2024-12-17T11:11:53.855940+00:00, state:running, queued_at: None. externally triggered: False> successful
[2024-12-17T16:41:59.200+0530] {logging_mixin.py:190} INFO - Dag run in success state
[2024-12-17T16:41:59.201+0530] {logging_mixin.py:190} INFO - Dag run start:2024-12-17 11:11:53.855940+00:00 end:2024-12-17 11:11:59.200779+00:00
[2024-12-17T16:41:59.201+0530] {logging_mixin.py:190} INFO - [2024-12-17T16:41:59.201+0530] {dagrun.py:905} INFO - DagRun Finished: dag_id=movie_pipeline, execution_date=2024-12-17 11:11:53.855940+00:00, run_id=manual__2024-12-17T11:11:53.855940+00:00, run_start_date=2024-12-17 11:11:53.855940+00:00, run_end_date=2024-12-17 11:11:59.200779+00:00, run_duration=5.344839, state=success, external_trigger=False, run_type=manual, data_interval_start=2024-12-16 11:11:53.855940+00:00, data_interval_end=2024-12-17 11:11:53.855940+00:00, dag_hash=None
[2024-12-17T16:41:59.210+0530] {processor.py:925} INFO - DAG(s) 'movie_pipeline' retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T16:41:59.222+0530] {logging_mixin.py:190} INFO - [2024-12-17T16:41:59.221+0530] {dag.py:3239} INFO - Sync 1 DAGs
[2024-12-17T16:41:59.239+0530] {logging_mixin.py:190} INFO - [2024-12-17T16:41:59.239+0530] {dag.py:4180} INFO - Setting next_dagrun for movie_pipeline to 2024-12-17 01:30:00+00:00, run_after=2024-12-18 01:30:00+00:00
[2024-12-17T16:41:59.256+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 5.445 seconds
[2024-12-17T16:44:41.613+0530] {processor.py:186} INFO - Started process (PID=76115) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T16:44:41.614+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-17T16:44:41.615+0530] {logging_mixin.py:190} INFO - [2024-12-17T16:44:41.615+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T16:44:41.777+0530] {logging_mixin.py:190} INFO - [2024-12-17T16:44:41.777+0530] {dag.py:4435} INFO - dagrun id: movie_pipeline
[2024-12-17T16:44:41.791+0530] {logging_mixin.py:190} INFO - [2024-12-17T16:44:41.791+0530] {dag.py:4451} INFO - created dagrun <DagRun movie_pipeline @ 2024-12-17 11:14:41.654106+00:00: manual__2024-12-17T11:14:41.654106+00:00, state:running, queued_at: None. externally triggered: False>
[2024-12-17T16:44:41.816+0530] {logging_mixin.py:190} INFO - [2024-12-17T16:44:41.816+0530] {dag.py:4396} INFO - [DAG TEST] starting task_id=scrape_collection_page map_index=-1
[2024-12-17T16:44:41.817+0530] {logging_mixin.py:190} INFO - [2024-12-17T16:44:41.816+0530] {dag.py:4399} INFO - [DAG TEST] running task <TaskInstance: movie_pipeline.scrape_collection_page manual__2024-12-17T11:14:41.654106+00:00 [scheduled]>
[2024-12-17T16:44:46.911+0530] {logging_mixin.py:190} INFO - [2024-12-17T16:44:46.910+0530] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-12-17T16:44:46.976+0530] {logging_mixin.py:190} INFO - [2024-12-17 16:44:46,976] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T11:14:41.654106+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T11:14:41.654106+00:00'
[2024-12-17T16:44:46.976+0530] {logging_mixin.py:190} INFO - [2024-12-17T16:44:46.976+0530] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T11:14:41.654106+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T11:14:41.654106+00:00'
[2024-12-17T16:44:46.979+0530] {logging_mixin.py:190} INFO - Task instance is in running state
[2024-12-17T16:44:46.979+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: queued
[2024-12-17T16:44:46.979+0530] {logging_mixin.py:190} INFO - Current task name:scrape_collection_page state:scheduled start_date:None
[2024-12-17T16:44:46.979+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline and current dag run status:running
[2024-12-17T16:44:46.980+0530] {logging_mixin.py:190} INFO - [2024-12-17T16:44:46.980+0530] {taskinstance.py:731} INFO - ::endgroup::
[2024-12-17T16:44:46.982+0530] {logging_mixin.py:190} INFO - 2024-12-17
[2024-12-17T16:44:46.983+0530] {logging_mixin.py:190} INFO - [2024-12-17 16:44:46,982] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T16:44:46.983+0530] {logging_mixin.py:190} INFO - [2024-12-17T16:44:46.982+0530] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T16:44:46.988+0530] {logging_mixin.py:190} INFO - [2024-12-17T16:44:46.988+0530] {taskinstance.py:340} INFO - ::group::Post task execution logs
[2024-12-17T16:44:46.988+0530] {logging_mixin.py:190} INFO - [2024-12-17T16:44:46.988+0530] {taskinstance.py:352} INFO - Marking task as SUCCESS. dag_id=movie_pipeline, task_id=scrape_collection_page, run_id=manual__2024-12-17T11:14:41.654106+00:00, execution_date=20241217T111441, start_date=, end_date=20241217T111446
[2024-12-17T16:44:46.997+0530] {logging_mixin.py:190} INFO - Task instance in success state
[2024-12-17T16:44:46.997+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: running
[2024-12-17T16:44:46.998+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline queued_at:None
[2024-12-17T16:44:46.998+0530] {logging_mixin.py:190} INFO - Task hostname:Jayeshs-MacBook-Air.local operator:PythonOperator
[2024-12-17T16:44:46.998+0530] {logging_mixin.py:190} INFO - [2024-12-17T16:44:46.998+0530] {dag.py:4410} INFO - [DAG TEST] end task task_id=scrape_collection_page map_index=-1
[2024-12-17T16:44:47.002+0530] {logging_mixin.py:190} INFO - [2024-12-17T16:44:47.002+0530] {dagrun.py:854} INFO - Marking run <DagRun movie_pipeline @ 2024-12-17 11:14:41.654106+00:00: manual__2024-12-17T11:14:41.654106+00:00, state:running, queued_at: None. externally triggered: False> successful
[2024-12-17T16:44:47.002+0530] {logging_mixin.py:190} INFO - Dag run in success state
[2024-12-17T16:44:47.003+0530] {logging_mixin.py:190} INFO - Dag run start:2024-12-17 11:14:41.654106+00:00 end:2024-12-17 11:14:47.002739+00:00
[2024-12-17T16:44:47.004+0530] {logging_mixin.py:190} INFO - [2024-12-17T16:44:47.003+0530] {dagrun.py:905} INFO - DagRun Finished: dag_id=movie_pipeline, execution_date=2024-12-17 11:14:41.654106+00:00, run_id=manual__2024-12-17T11:14:41.654106+00:00, run_start_date=2024-12-17 11:14:41.654106+00:00, run_end_date=2024-12-17 11:14:47.002739+00:00, run_duration=5.348633, state=success, external_trigger=False, run_type=manual, data_interval_start=2024-12-16 11:14:41.654106+00:00, data_interval_end=2024-12-17 11:14:41.654106+00:00, dag_hash=None
[2024-12-17T16:44:47.013+0530] {processor.py:925} INFO - DAG(s) 'movie_pipeline' retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T16:44:47.027+0530] {logging_mixin.py:190} INFO - [2024-12-17T16:44:47.026+0530] {dag.py:3239} INFO - Sync 1 DAGs
[2024-12-17T16:44:47.047+0530] {logging_mixin.py:190} INFO - [2024-12-17T16:44:47.047+0530] {dag.py:4180} INFO - Setting next_dagrun for movie_pipeline to 2024-12-17 01:30:00+00:00, run_after=2024-12-18 01:30:00+00:00
[2024-12-17T16:44:47.066+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 5.457 seconds
[2024-12-17T16:47:30.942+0530] {processor.py:186} INFO - Started process (PID=76305) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T16:47:30.943+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-17T16:47:30.944+0530] {logging_mixin.py:190} INFO - [2024-12-17T16:47:30.944+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T16:47:31.105+0530] {logging_mixin.py:190} INFO - [2024-12-17T16:47:31.105+0530] {dag.py:4435} INFO - dagrun id: movie_pipeline
[2024-12-17T16:47:31.118+0530] {logging_mixin.py:190} INFO - [2024-12-17T16:47:31.118+0530] {dag.py:4451} INFO - created dagrun <DagRun movie_pipeline @ 2024-12-17 11:17:30.983621+00:00: manual__2024-12-17T11:17:30.983621+00:00, state:running, queued_at: None. externally triggered: False>
[2024-12-17T16:47:31.143+0530] {logging_mixin.py:190} INFO - [2024-12-17T16:47:31.143+0530] {dag.py:4396} INFO - [DAG TEST] starting task_id=scrape_collection_page map_index=-1
[2024-12-17T16:47:31.143+0530] {logging_mixin.py:190} INFO - [2024-12-17T16:47:31.143+0530] {dag.py:4399} INFO - [DAG TEST] running task <TaskInstance: movie_pipeline.scrape_collection_page manual__2024-12-17T11:17:30.983621+00:00 [scheduled]>
[2024-12-17T16:47:36.239+0530] {logging_mixin.py:190} INFO - [2024-12-17T16:47:36.239+0530] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-12-17T16:47:36.297+0530] {logging_mixin.py:190} INFO - [2024-12-17 16:47:36,297] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T11:17:30.983621+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T11:17:30.983621+00:00'
[2024-12-17T16:47:36.298+0530] {logging_mixin.py:190} INFO - [2024-12-17T16:47:36.297+0530] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T11:17:30.983621+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T11:17:30.983621+00:00'
[2024-12-17T16:47:36.300+0530] {logging_mixin.py:190} INFO - Task instance is in running state
[2024-12-17T16:47:36.300+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: queued
[2024-12-17T16:47:36.301+0530] {logging_mixin.py:190} INFO - Current task name:scrape_collection_page state:scheduled start_date:None
[2024-12-17T16:47:36.301+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline and current dag run status:running
[2024-12-17T16:47:36.302+0530] {logging_mixin.py:190} INFO - [2024-12-17T16:47:36.302+0530] {taskinstance.py:731} INFO - ::endgroup::
[2024-12-17T16:47:36.304+0530] {logging_mixin.py:190} INFO - 2024-12-17
[2024-12-17T16:47:36.304+0530] {logging_mixin.py:190} INFO - [2024-12-17 16:47:36,304] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T16:47:36.304+0530] {logging_mixin.py:190} INFO - [2024-12-17T16:47:36.304+0530] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T16:47:36.309+0530] {logging_mixin.py:190} INFO - [2024-12-17T16:47:36.309+0530] {taskinstance.py:340} INFO - ::group::Post task execution logs
[2024-12-17T16:47:36.310+0530] {logging_mixin.py:190} INFO - [2024-12-17T16:47:36.309+0530] {taskinstance.py:352} INFO - Marking task as SUCCESS. dag_id=movie_pipeline, task_id=scrape_collection_page, run_id=manual__2024-12-17T11:17:30.983621+00:00, execution_date=20241217T111730, start_date=, end_date=20241217T111736
[2024-12-17T16:47:36.319+0530] {logging_mixin.py:190} INFO - Task instance in success state
[2024-12-17T16:47:36.320+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: running
[2024-12-17T16:47:36.320+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline queued_at:None
[2024-12-17T16:47:36.320+0530] {logging_mixin.py:190} INFO - Task hostname:Jayeshs-MacBook-Air.local operator:PythonOperator
[2024-12-17T16:47:36.320+0530] {logging_mixin.py:190} INFO - [2024-12-17T16:47:36.320+0530] {dag.py:4410} INFO - [DAG TEST] end task task_id=scrape_collection_page map_index=-1
[2024-12-17T16:47:36.324+0530] {logging_mixin.py:190} INFO - [2024-12-17T16:47:36.324+0530] {dagrun.py:854} INFO - Marking run <DagRun movie_pipeline @ 2024-12-17 11:17:30.983621+00:00: manual__2024-12-17T11:17:30.983621+00:00, state:running, queued_at: None. externally triggered: False> successful
[2024-12-17T16:47:36.324+0530] {logging_mixin.py:190} INFO - Dag run in success state
[2024-12-17T16:47:36.325+0530] {logging_mixin.py:190} INFO - Dag run start:2024-12-17 11:17:30.983621+00:00 end:2024-12-17 11:17:36.324807+00:00
[2024-12-17T16:47:36.325+0530] {logging_mixin.py:190} INFO - [2024-12-17T16:47:36.325+0530] {dagrun.py:905} INFO - DagRun Finished: dag_id=movie_pipeline, execution_date=2024-12-17 11:17:30.983621+00:00, run_id=manual__2024-12-17T11:17:30.983621+00:00, run_start_date=2024-12-17 11:17:30.983621+00:00, run_end_date=2024-12-17 11:17:36.324807+00:00, run_duration=5.341186, state=success, external_trigger=False, run_type=manual, data_interval_start=2024-12-16 11:17:30.983621+00:00, data_interval_end=2024-12-17 11:17:30.983621+00:00, dag_hash=None
[2024-12-17T16:47:36.334+0530] {processor.py:925} INFO - DAG(s) 'movie_pipeline' retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T16:47:36.347+0530] {logging_mixin.py:190} INFO - [2024-12-17T16:47:36.346+0530] {dag.py:3239} INFO - Sync 1 DAGs
[2024-12-17T16:47:36.364+0530] {logging_mixin.py:190} INFO - [2024-12-17T16:47:36.364+0530] {dag.py:4180} INFO - Setting next_dagrun for movie_pipeline to 2024-12-17 01:30:00+00:00, run_after=2024-12-18 01:30:00+00:00
[2024-12-17T16:47:36.382+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 5.444 seconds
[2024-12-17T16:50:18.300+0530] {processor.py:186} INFO - Started process (PID=76506) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T16:50:18.301+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-17T16:50:18.302+0530] {logging_mixin.py:190} INFO - [2024-12-17T16:50:18.302+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T16:50:18.466+0530] {logging_mixin.py:190} INFO - [2024-12-17T16:50:18.466+0530] {dag.py:4435} INFO - dagrun id: movie_pipeline
[2024-12-17T16:50:18.481+0530] {logging_mixin.py:190} INFO - [2024-12-17T16:50:18.480+0530] {dag.py:4451} INFO - created dagrun <DagRun movie_pipeline @ 2024-12-17 11:20:18.341526+00:00: manual__2024-12-17T11:20:18.341526+00:00, state:running, queued_at: None. externally triggered: False>
[2024-12-17T16:50:18.506+0530] {logging_mixin.py:190} INFO - [2024-12-17T16:50:18.506+0530] {dag.py:4396} INFO - [DAG TEST] starting task_id=scrape_collection_page map_index=-1
[2024-12-17T16:50:18.507+0530] {logging_mixin.py:190} INFO - [2024-12-17T16:50:18.506+0530] {dag.py:4399} INFO - [DAG TEST] running task <TaskInstance: movie_pipeline.scrape_collection_page manual__2024-12-17T11:20:18.341526+00:00 [scheduled]>
[2024-12-17T16:50:23.599+0530] {logging_mixin.py:190} INFO - [2024-12-17T16:50:23.599+0530] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-12-17T16:50:23.658+0530] {logging_mixin.py:190} INFO - [2024-12-17 16:50:23,658] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T11:20:18.341526+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T11:20:18.341526+00:00'
[2024-12-17T16:50:23.658+0530] {logging_mixin.py:190} INFO - [2024-12-17T16:50:23.658+0530] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T11:20:18.341526+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T11:20:18.341526+00:00'
[2024-12-17T16:50:23.661+0530] {logging_mixin.py:190} INFO - Task instance is in running state
[2024-12-17T16:50:23.661+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: queued
[2024-12-17T16:50:23.661+0530] {logging_mixin.py:190} INFO - Current task name:scrape_collection_page state:scheduled start_date:None
[2024-12-17T16:50:23.661+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline and current dag run status:running
[2024-12-17T16:50:23.662+0530] {logging_mixin.py:190} INFO - [2024-12-17T16:50:23.662+0530] {taskinstance.py:731} INFO - ::endgroup::
[2024-12-17T16:50:23.664+0530] {logging_mixin.py:190} INFO - 2024-12-17
[2024-12-17T16:50:23.664+0530] {logging_mixin.py:190} INFO - [2024-12-17 16:50:23,664] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T16:50:23.665+0530] {logging_mixin.py:190} INFO - [2024-12-17T16:50:23.664+0530] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T16:50:23.670+0530] {logging_mixin.py:190} INFO - [2024-12-17T16:50:23.670+0530] {taskinstance.py:340} INFO - ::group::Post task execution logs
[2024-12-17T16:50:23.670+0530] {logging_mixin.py:190} INFO - [2024-12-17T16:50:23.670+0530] {taskinstance.py:352} INFO - Marking task as SUCCESS. dag_id=movie_pipeline, task_id=scrape_collection_page, run_id=manual__2024-12-17T11:20:18.341526+00:00, execution_date=20241217T112018, start_date=, end_date=20241217T112023
[2024-12-17T16:50:23.679+0530] {logging_mixin.py:190} INFO - Task instance in success state
[2024-12-17T16:50:23.680+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: running
[2024-12-17T16:50:23.680+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline queued_at:None
[2024-12-17T16:50:23.680+0530] {logging_mixin.py:190} INFO - Task hostname:Jayeshs-MacBook-Air.local operator:PythonOperator
[2024-12-17T16:50:23.681+0530] {logging_mixin.py:190} INFO - [2024-12-17T16:50:23.680+0530] {dag.py:4410} INFO - [DAG TEST] end task task_id=scrape_collection_page map_index=-1
[2024-12-17T16:50:23.684+0530] {logging_mixin.py:190} INFO - [2024-12-17T16:50:23.684+0530] {dagrun.py:854} INFO - Marking run <DagRun movie_pipeline @ 2024-12-17 11:20:18.341526+00:00: manual__2024-12-17T11:20:18.341526+00:00, state:running, queued_at: None. externally triggered: False> successful
[2024-12-17T16:50:23.685+0530] {logging_mixin.py:190} INFO - Dag run in success state
[2024-12-17T16:50:23.685+0530] {logging_mixin.py:190} INFO - Dag run start:2024-12-17 11:20:18.341526+00:00 end:2024-12-17 11:20:23.684976+00:00
[2024-12-17T16:50:23.685+0530] {logging_mixin.py:190} INFO - [2024-12-17T16:50:23.685+0530] {dagrun.py:905} INFO - DagRun Finished: dag_id=movie_pipeline, execution_date=2024-12-17 11:20:18.341526+00:00, run_id=manual__2024-12-17T11:20:18.341526+00:00, run_start_date=2024-12-17 11:20:18.341526+00:00, run_end_date=2024-12-17 11:20:23.684976+00:00, run_duration=5.34345, state=success, external_trigger=False, run_type=manual, data_interval_start=2024-12-16 11:20:18.341526+00:00, data_interval_end=2024-12-17 11:20:18.341526+00:00, dag_hash=None
[2024-12-17T16:50:23.694+0530] {processor.py:925} INFO - DAG(s) 'movie_pipeline' retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T16:50:23.706+0530] {logging_mixin.py:190} INFO - [2024-12-17T16:50:23.706+0530] {dag.py:3239} INFO - Sync 1 DAGs
[2024-12-17T16:50:23.723+0530] {logging_mixin.py:190} INFO - [2024-12-17T16:50:23.723+0530] {dag.py:4180} INFO - Setting next_dagrun for movie_pipeline to 2024-12-17 01:30:00+00:00, run_after=2024-12-18 01:30:00+00:00
[2024-12-17T16:50:23.741+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 5.444 seconds
[2024-12-17T16:53:03.629+0530] {processor.py:186} INFO - Started process (PID=76695) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T16:53:03.630+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-17T16:53:03.631+0530] {logging_mixin.py:190} INFO - [2024-12-17T16:53:03.631+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T16:53:03.793+0530] {logging_mixin.py:190} INFO - [2024-12-17T16:53:03.793+0530] {dag.py:4435} INFO - dagrun id: movie_pipeline
[2024-12-17T16:53:03.807+0530] {logging_mixin.py:190} INFO - [2024-12-17T16:53:03.806+0530] {dag.py:4451} INFO - created dagrun <DagRun movie_pipeline @ 2024-12-17 11:23:03.670021+00:00: manual__2024-12-17T11:23:03.670021+00:00, state:running, queued_at: None. externally triggered: False>
[2024-12-17T16:53:03.832+0530] {logging_mixin.py:190} INFO - [2024-12-17T16:53:03.832+0530] {dag.py:4396} INFO - [DAG TEST] starting task_id=scrape_collection_page map_index=-1
[2024-12-17T16:53:03.833+0530] {logging_mixin.py:190} INFO - [2024-12-17T16:53:03.832+0530] {dag.py:4399} INFO - [DAG TEST] running task <TaskInstance: movie_pipeline.scrape_collection_page manual__2024-12-17T11:23:03.670021+00:00 [scheduled]>
[2024-12-17T16:53:08.928+0530] {logging_mixin.py:190} INFO - [2024-12-17T16:53:08.927+0530] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-12-17T16:53:08.986+0530] {logging_mixin.py:190} INFO - [2024-12-17 16:53:08,986] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T11:23:03.670021+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T11:23:03.670021+00:00'
[2024-12-17T16:53:08.987+0530] {logging_mixin.py:190} INFO - [2024-12-17T16:53:08.986+0530] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T11:23:03.670021+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T11:23:03.670021+00:00'
[2024-12-17T16:53:08.989+0530] {logging_mixin.py:190} INFO - Task instance is in running state
[2024-12-17T16:53:08.990+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: queued
[2024-12-17T16:53:08.990+0530] {logging_mixin.py:190} INFO - Current task name:scrape_collection_page state:scheduled start_date:None
[2024-12-17T16:53:08.990+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline and current dag run status:running
[2024-12-17T16:53:08.991+0530] {logging_mixin.py:190} INFO - [2024-12-17T16:53:08.990+0530] {taskinstance.py:731} INFO - ::endgroup::
[2024-12-17T16:53:08.992+0530] {logging_mixin.py:190} INFO - 2024-12-17
[2024-12-17T16:53:08.993+0530] {logging_mixin.py:190} INFO - [2024-12-17 16:53:08,993] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T16:53:08.993+0530] {logging_mixin.py:190} INFO - [2024-12-17T16:53:08.993+0530] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T16:53:08.998+0530] {logging_mixin.py:190} INFO - [2024-12-17T16:53:08.998+0530] {taskinstance.py:340} INFO - ::group::Post task execution logs
[2024-12-17T16:53:08.999+0530] {logging_mixin.py:190} INFO - [2024-12-17T16:53:08.998+0530] {taskinstance.py:352} INFO - Marking task as SUCCESS. dag_id=movie_pipeline, task_id=scrape_collection_page, run_id=manual__2024-12-17T11:23:03.670021+00:00, execution_date=20241217T112303, start_date=, end_date=20241217T112308
[2024-12-17T16:53:09.008+0530] {logging_mixin.py:190} INFO - Task instance in success state
[2024-12-17T16:53:09.008+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: running
[2024-12-17T16:53:09.009+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline queued_at:None
[2024-12-17T16:53:09.009+0530] {logging_mixin.py:190} INFO - Task hostname:Jayeshs-MacBook-Air.local operator:PythonOperator
[2024-12-17T16:53:09.009+0530] {logging_mixin.py:190} INFO - [2024-12-17T16:53:09.009+0530] {dag.py:4410} INFO - [DAG TEST] end task task_id=scrape_collection_page map_index=-1
[2024-12-17T16:53:09.013+0530] {logging_mixin.py:190} INFO - [2024-12-17T16:53:09.013+0530] {dagrun.py:854} INFO - Marking run <DagRun movie_pipeline @ 2024-12-17 11:23:03.670021+00:00: manual__2024-12-17T11:23:03.670021+00:00, state:running, queued_at: None. externally triggered: False> successful
[2024-12-17T16:53:09.013+0530] {logging_mixin.py:190} INFO - Dag run in success state
[2024-12-17T16:53:09.013+0530] {logging_mixin.py:190} INFO - Dag run start:2024-12-17 11:23:03.670021+00:00 end:2024-12-17 11:23:09.013718+00:00
[2024-12-17T16:53:09.014+0530] {logging_mixin.py:190} INFO - [2024-12-17T16:53:09.014+0530] {dagrun.py:905} INFO - DagRun Finished: dag_id=movie_pipeline, execution_date=2024-12-17 11:23:03.670021+00:00, run_id=manual__2024-12-17T11:23:03.670021+00:00, run_start_date=2024-12-17 11:23:03.670021+00:00, run_end_date=2024-12-17 11:23:09.013718+00:00, run_duration=5.343697, state=success, external_trigger=False, run_type=manual, data_interval_start=2024-12-16 11:23:03.670021+00:00, data_interval_end=2024-12-17 11:23:03.670021+00:00, dag_hash=None
[2024-12-17T16:53:09.022+0530] {processor.py:925} INFO - DAG(s) 'movie_pipeline' retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T16:53:09.034+0530] {logging_mixin.py:190} INFO - [2024-12-17T16:53:09.033+0530] {dag.py:3239} INFO - Sync 1 DAGs
[2024-12-17T16:53:09.050+0530] {logging_mixin.py:190} INFO - [2024-12-17T16:53:09.050+0530] {dag.py:4180} INFO - Setting next_dagrun for movie_pipeline to 2024-12-17 01:30:00+00:00, run_after=2024-12-18 01:30:00+00:00
[2024-12-17T16:53:09.068+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 5.442 seconds
[2024-12-17T16:55:54.399+0530] {processor.py:186} INFO - Started process (PID=76916) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T16:55:54.400+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-17T16:55:54.401+0530] {logging_mixin.py:190} INFO - [2024-12-17T16:55:54.401+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T16:55:54.562+0530] {logging_mixin.py:190} INFO - [2024-12-17T16:55:54.562+0530] {dag.py:4435} INFO - dagrun id: movie_pipeline
[2024-12-17T16:55:54.576+0530] {logging_mixin.py:190} INFO - [2024-12-17T16:55:54.576+0530] {dag.py:4451} INFO - created dagrun <DagRun movie_pipeline @ 2024-12-17 11:25:54.439560+00:00: manual__2024-12-17T11:25:54.439560+00:00, state:running, queued_at: None. externally triggered: False>
[2024-12-17T16:55:54.601+0530] {logging_mixin.py:190} INFO - [2024-12-17T16:55:54.601+0530] {dag.py:4396} INFO - [DAG TEST] starting task_id=scrape_collection_page map_index=-1
[2024-12-17T16:55:54.602+0530] {logging_mixin.py:190} INFO - [2024-12-17T16:55:54.602+0530] {dag.py:4399} INFO - [DAG TEST] running task <TaskInstance: movie_pipeline.scrape_collection_page manual__2024-12-17T11:25:54.439560+00:00 [scheduled]>
[2024-12-17T16:55:59.696+0530] {logging_mixin.py:190} INFO - [2024-12-17T16:55:59.695+0530] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-12-17T16:55:59.754+0530] {logging_mixin.py:190} INFO - [2024-12-17 16:55:59,753] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T11:25:54.439560+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T11:25:54.439560+00:00'
[2024-12-17T16:55:59.754+0530] {logging_mixin.py:190} INFO - [2024-12-17T16:55:59.753+0530] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T11:25:54.439560+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T11:25:54.439560+00:00'
[2024-12-17T16:55:59.757+0530] {logging_mixin.py:190} INFO - Task instance is in running state
[2024-12-17T16:55:59.757+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: queued
[2024-12-17T16:55:59.757+0530] {logging_mixin.py:190} INFO - Current task name:scrape_collection_page state:scheduled start_date:None
[2024-12-17T16:55:59.758+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline and current dag run status:running
[2024-12-17T16:55:59.758+0530] {logging_mixin.py:190} INFO - [2024-12-17T16:55:59.758+0530] {taskinstance.py:731} INFO - ::endgroup::
[2024-12-17T16:55:59.760+0530] {logging_mixin.py:190} INFO - 2024-12-17
[2024-12-17T16:55:59.761+0530] {logging_mixin.py:190} INFO - [2024-12-17 16:55:59,760] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T16:55:59.761+0530] {logging_mixin.py:190} INFO - [2024-12-17T16:55:59.760+0530] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T16:55:59.766+0530] {logging_mixin.py:190} INFO - [2024-12-17T16:55:59.766+0530] {taskinstance.py:340} INFO - ::group::Post task execution logs
[2024-12-17T16:55:59.767+0530] {logging_mixin.py:190} INFO - [2024-12-17T16:55:59.766+0530] {taskinstance.py:352} INFO - Marking task as SUCCESS. dag_id=movie_pipeline, task_id=scrape_collection_page, run_id=manual__2024-12-17T11:25:54.439560+00:00, execution_date=20241217T112554, start_date=, end_date=20241217T112559
[2024-12-17T16:55:59.776+0530] {logging_mixin.py:190} INFO - Task instance in success state
[2024-12-17T16:55:59.776+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: running
[2024-12-17T16:55:59.777+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline queued_at:None
[2024-12-17T16:55:59.777+0530] {logging_mixin.py:190} INFO - Task hostname:Jayeshs-MacBook-Air.local operator:PythonOperator
[2024-12-17T16:55:59.777+0530] {logging_mixin.py:190} INFO - [2024-12-17T16:55:59.777+0530] {dag.py:4410} INFO - [DAG TEST] end task task_id=scrape_collection_page map_index=-1
[2024-12-17T16:55:59.781+0530] {logging_mixin.py:190} INFO - [2024-12-17T16:55:59.781+0530] {dagrun.py:854} INFO - Marking run <DagRun movie_pipeline @ 2024-12-17 11:25:54.439560+00:00: manual__2024-12-17T11:25:54.439560+00:00, state:running, queued_at: None. externally triggered: False> successful
[2024-12-17T16:55:59.781+0530] {logging_mixin.py:190} INFO - Dag run in success state
[2024-12-17T16:55:59.782+0530] {logging_mixin.py:190} INFO - Dag run start:2024-12-17 11:25:54.439560+00:00 end:2024-12-17 11:25:59.781867+00:00
[2024-12-17T16:55:59.782+0530] {logging_mixin.py:190} INFO - [2024-12-17T16:55:59.782+0530] {dagrun.py:905} INFO - DagRun Finished: dag_id=movie_pipeline, execution_date=2024-12-17 11:25:54.439560+00:00, run_id=manual__2024-12-17T11:25:54.439560+00:00, run_start_date=2024-12-17 11:25:54.439560+00:00, run_end_date=2024-12-17 11:25:59.781867+00:00, run_duration=5.342307, state=success, external_trigger=False, run_type=manual, data_interval_start=2024-12-16 11:25:54.439560+00:00, data_interval_end=2024-12-17 11:25:54.439560+00:00, dag_hash=None
[2024-12-17T16:55:59.791+0530] {processor.py:925} INFO - DAG(s) 'movie_pipeline' retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T16:55:59.802+0530] {logging_mixin.py:190} INFO - [2024-12-17T16:55:59.802+0530] {dag.py:3239} INFO - Sync 1 DAGs
[2024-12-17T16:55:59.821+0530] {logging_mixin.py:190} INFO - [2024-12-17T16:55:59.820+0530] {dag.py:4180} INFO - Setting next_dagrun for movie_pipeline to 2024-12-17 01:30:00+00:00, run_after=2024-12-18 01:30:00+00:00
[2024-12-17T16:55:59.839+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 5.444 seconds
[2024-12-17T16:58:40.162+0530] {processor.py:186} INFO - Started process (PID=77118) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T16:58:40.165+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-17T16:58:40.166+0530] {logging_mixin.py:190} INFO - [2024-12-17T16:58:40.166+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T16:58:40.330+0530] {logging_mixin.py:190} INFO - [2024-12-17T16:58:40.329+0530] {dag.py:4435} INFO - dagrun id: movie_pipeline
[2024-12-17T16:58:40.343+0530] {logging_mixin.py:190} INFO - [2024-12-17T16:58:40.343+0530] {dag.py:4451} INFO - created dagrun <DagRun movie_pipeline @ 2024-12-17 11:28:40.204461+00:00: manual__2024-12-17T11:28:40.204461+00:00, state:running, queued_at: None. externally triggered: False>
[2024-12-17T16:58:40.368+0530] {logging_mixin.py:190} INFO - [2024-12-17T16:58:40.368+0530] {dag.py:4396} INFO - [DAG TEST] starting task_id=scrape_collection_page map_index=-1
[2024-12-17T16:58:40.369+0530] {logging_mixin.py:190} INFO - [2024-12-17T16:58:40.368+0530] {dag.py:4399} INFO - [DAG TEST] running task <TaskInstance: movie_pipeline.scrape_collection_page manual__2024-12-17T11:28:40.204461+00:00 [scheduled]>
[2024-12-17T16:58:45.465+0530] {logging_mixin.py:190} INFO - [2024-12-17T16:58:45.465+0530] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-12-17T16:58:45.523+0530] {logging_mixin.py:190} INFO - [2024-12-17 16:58:45,522] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T11:28:40.204461+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T11:28:40.204461+00:00'
[2024-12-17T16:58:45.523+0530] {logging_mixin.py:190} INFO - [2024-12-17T16:58:45.522+0530] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T11:28:40.204461+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T11:28:40.204461+00:00'
[2024-12-17T16:58:45.526+0530] {logging_mixin.py:190} INFO - Task instance is in running state
[2024-12-17T16:58:45.526+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: queued
[2024-12-17T16:58:45.526+0530] {logging_mixin.py:190} INFO - Current task name:scrape_collection_page state:scheduled start_date:None
[2024-12-17T16:58:45.526+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline and current dag run status:running
[2024-12-17T16:58:45.527+0530] {logging_mixin.py:190} INFO - [2024-12-17T16:58:45.527+0530] {taskinstance.py:731} INFO - ::endgroup::
[2024-12-17T16:58:45.529+0530] {logging_mixin.py:190} INFO - 2024-12-17
[2024-12-17T16:58:45.529+0530] {logging_mixin.py:190} INFO - [2024-12-17 16:58:45,529] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T16:58:45.530+0530] {logging_mixin.py:190} INFO - [2024-12-17T16:58:45.529+0530] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T16:58:45.535+0530] {logging_mixin.py:190} INFO - [2024-12-17T16:58:45.534+0530] {taskinstance.py:340} INFO - ::group::Post task execution logs
[2024-12-17T16:58:45.535+0530] {logging_mixin.py:190} INFO - [2024-12-17T16:58:45.535+0530] {taskinstance.py:352} INFO - Marking task as SUCCESS. dag_id=movie_pipeline, task_id=scrape_collection_page, run_id=manual__2024-12-17T11:28:40.204461+00:00, execution_date=20241217T112840, start_date=, end_date=20241217T112845
[2024-12-17T16:58:45.544+0530] {logging_mixin.py:190} INFO - Task instance in success state
[2024-12-17T16:58:45.544+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: running
[2024-12-17T16:58:45.544+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline queued_at:None
[2024-12-17T16:58:45.544+0530] {logging_mixin.py:190} INFO - Task hostname:Jayeshs-MacBook-Air.local operator:PythonOperator
[2024-12-17T16:58:45.545+0530] {logging_mixin.py:190} INFO - [2024-12-17T16:58:45.545+0530] {dag.py:4410} INFO - [DAG TEST] end task task_id=scrape_collection_page map_index=-1
[2024-12-17T16:58:45.549+0530] {logging_mixin.py:190} INFO - [2024-12-17T16:58:45.548+0530] {dagrun.py:854} INFO - Marking run <DagRun movie_pipeline @ 2024-12-17 11:28:40.204461+00:00: manual__2024-12-17T11:28:40.204461+00:00, state:running, queued_at: None. externally triggered: False> successful
[2024-12-17T16:58:45.549+0530] {logging_mixin.py:190} INFO - Dag run in success state
[2024-12-17T16:58:45.549+0530] {logging_mixin.py:190} INFO - Dag run start:2024-12-17 11:28:40.204461+00:00 end:2024-12-17 11:28:45.549345+00:00
[2024-12-17T16:58:45.550+0530] {logging_mixin.py:190} INFO - [2024-12-17T16:58:45.549+0530] {dagrun.py:905} INFO - DagRun Finished: dag_id=movie_pipeline, execution_date=2024-12-17 11:28:40.204461+00:00, run_id=manual__2024-12-17T11:28:40.204461+00:00, run_start_date=2024-12-17 11:28:40.204461+00:00, run_end_date=2024-12-17 11:28:45.549345+00:00, run_duration=5.344884, state=success, external_trigger=False, run_type=manual, data_interval_start=2024-12-16 11:28:40.204461+00:00, data_interval_end=2024-12-17 11:28:40.204461+00:00, dag_hash=None
[2024-12-17T16:58:45.559+0530] {processor.py:925} INFO - DAG(s) 'movie_pipeline' retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T16:58:45.570+0530] {logging_mixin.py:190} INFO - [2024-12-17T16:58:45.570+0530] {dag.py:3239} INFO - Sync 1 DAGs
[2024-12-17T16:58:45.587+0530] {logging_mixin.py:190} INFO - [2024-12-17T16:58:45.587+0530] {dag.py:4180} INFO - Setting next_dagrun for movie_pipeline to 2024-12-17 01:30:00+00:00, run_after=2024-12-18 01:30:00+00:00
[2024-12-17T16:58:45.606+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 5.447 seconds
[2024-12-17T17:01:27.811+0530] {processor.py:186} INFO - Started process (PID=77306) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T17:01:27.812+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-17T17:01:27.814+0530] {logging_mixin.py:190} INFO - [2024-12-17T17:01:27.813+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T17:01:27.979+0530] {logging_mixin.py:190} INFO - [2024-12-17T17:01:27.979+0530] {dag.py:4435} INFO - dagrun id: movie_pipeline
[2024-12-17T17:01:27.993+0530] {logging_mixin.py:190} INFO - [2024-12-17T17:01:27.993+0530] {dag.py:4451} INFO - created dagrun <DagRun movie_pipeline @ 2024-12-17 11:31:27.852441+00:00: manual__2024-12-17T11:31:27.852441+00:00, state:running, queued_at: None. externally triggered: False>
[2024-12-17T17:01:28.019+0530] {logging_mixin.py:190} INFO - [2024-12-17T17:01:28.019+0530] {dag.py:4396} INFO - [DAG TEST] starting task_id=scrape_collection_page map_index=-1
[2024-12-17T17:01:28.020+0530] {logging_mixin.py:190} INFO - [2024-12-17T17:01:28.019+0530] {dag.py:4399} INFO - [DAG TEST] running task <TaskInstance: movie_pipeline.scrape_collection_page manual__2024-12-17T11:31:27.852441+00:00 [scheduled]>
[2024-12-17T17:01:33.115+0530] {logging_mixin.py:190} INFO - [2024-12-17T17:01:33.115+0530] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-12-17T17:01:33.175+0530] {logging_mixin.py:190} INFO - [2024-12-17 17:01:33,174] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T11:31:27.852441+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T11:31:27.852441+00:00'
[2024-12-17T17:01:33.175+0530] {logging_mixin.py:190} INFO - [2024-12-17T17:01:33.174+0530] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T11:31:27.852441+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T11:31:27.852441+00:00'
[2024-12-17T17:01:33.178+0530] {logging_mixin.py:190} INFO - Task instance is in running state
[2024-12-17T17:01:33.178+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: queued
[2024-12-17T17:01:33.178+0530] {logging_mixin.py:190} INFO - Current task name:scrape_collection_page state:scheduled start_date:None
[2024-12-17T17:01:33.178+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline and current dag run status:running
[2024-12-17T17:01:33.179+0530] {logging_mixin.py:190} INFO - [2024-12-17T17:01:33.179+0530] {taskinstance.py:731} INFO - ::endgroup::
[2024-12-17T17:01:33.181+0530] {logging_mixin.py:190} INFO - 2024-12-17
[2024-12-17T17:01:33.182+0530] {logging_mixin.py:190} INFO - [2024-12-17 17:01:33,182] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T17:01:33.182+0530] {logging_mixin.py:190} INFO - [2024-12-17T17:01:33.182+0530] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T17:01:33.188+0530] {logging_mixin.py:190} INFO - [2024-12-17T17:01:33.187+0530] {taskinstance.py:340} INFO - ::group::Post task execution logs
[2024-12-17T17:01:33.188+0530] {logging_mixin.py:190} INFO - [2024-12-17T17:01:33.188+0530] {taskinstance.py:352} INFO - Marking task as SUCCESS. dag_id=movie_pipeline, task_id=scrape_collection_page, run_id=manual__2024-12-17T11:31:27.852441+00:00, execution_date=20241217T113127, start_date=, end_date=20241217T113133
[2024-12-17T17:01:33.197+0530] {logging_mixin.py:190} INFO - Task instance in success state
[2024-12-17T17:01:33.197+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: running
[2024-12-17T17:01:33.198+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline queued_at:None
[2024-12-17T17:01:33.198+0530] {logging_mixin.py:190} INFO - Task hostname:Jayeshs-MacBook-Air.local operator:PythonOperator
[2024-12-17T17:01:33.198+0530] {logging_mixin.py:190} INFO - [2024-12-17T17:01:33.198+0530] {dag.py:4410} INFO - [DAG TEST] end task task_id=scrape_collection_page map_index=-1
[2024-12-17T17:01:33.202+0530] {logging_mixin.py:190} INFO - [2024-12-17T17:01:33.202+0530] {dagrun.py:854} INFO - Marking run <DagRun movie_pipeline @ 2024-12-17 11:31:27.852441+00:00: manual__2024-12-17T11:31:27.852441+00:00, state:running, queued_at: None. externally triggered: False> successful
[2024-12-17T17:01:33.202+0530] {logging_mixin.py:190} INFO - Dag run in success state
[2024-12-17T17:01:33.203+0530] {logging_mixin.py:190} INFO - Dag run start:2024-12-17 11:31:27.852441+00:00 end:2024-12-17 11:31:33.202661+00:00
[2024-12-17T17:01:33.203+0530] {logging_mixin.py:190} INFO - [2024-12-17T17:01:33.203+0530] {dagrun.py:905} INFO - DagRun Finished: dag_id=movie_pipeline, execution_date=2024-12-17 11:31:27.852441+00:00, run_id=manual__2024-12-17T11:31:27.852441+00:00, run_start_date=2024-12-17 11:31:27.852441+00:00, run_end_date=2024-12-17 11:31:33.202661+00:00, run_duration=5.35022, state=success, external_trigger=False, run_type=manual, data_interval_start=2024-12-16 11:31:27.852441+00:00, data_interval_end=2024-12-17 11:31:27.852441+00:00, dag_hash=None
[2024-12-17T17:01:33.213+0530] {processor.py:925} INFO - DAG(s) 'movie_pipeline' retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T17:01:33.224+0530] {logging_mixin.py:190} INFO - [2024-12-17T17:01:33.224+0530] {dag.py:3239} INFO - Sync 1 DAGs
[2024-12-17T17:01:33.242+0530] {logging_mixin.py:190} INFO - [2024-12-17T17:01:33.242+0530] {dag.py:4180} INFO - Setting next_dagrun for movie_pipeline to 2024-12-17 01:30:00+00:00, run_after=2024-12-18 01:30:00+00:00
[2024-12-17T17:01:33.261+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 5.453 seconds
[2024-12-17T17:04:11.614+0530] {processor.py:186} INFO - Started process (PID=77504) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T17:04:11.615+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-17T17:04:11.617+0530] {logging_mixin.py:190} INFO - [2024-12-17T17:04:11.616+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T17:04:11.794+0530] {logging_mixin.py:190} INFO - [2024-12-17T17:04:11.794+0530] {dag.py:4435} INFO - dagrun id: movie_pipeline
[2024-12-17T17:04:11.810+0530] {logging_mixin.py:190} INFO - [2024-12-17T17:04:11.809+0530] {dag.py:4451} INFO - created dagrun <DagRun movie_pipeline @ 2024-12-17 11:34:11.658947+00:00: manual__2024-12-17T11:34:11.658947+00:00, state:running, queued_at: None. externally triggered: False>
[2024-12-17T17:04:11.841+0530] {logging_mixin.py:190} INFO - [2024-12-17T17:04:11.841+0530] {dag.py:4396} INFO - [DAG TEST] starting task_id=scrape_collection_page map_index=-1
[2024-12-17T17:04:11.842+0530] {logging_mixin.py:190} INFO - [2024-12-17T17:04:11.842+0530] {dag.py:4399} INFO - [DAG TEST] running task <TaskInstance: movie_pipeline.scrape_collection_page manual__2024-12-17T11:34:11.658947+00:00 [scheduled]>
[2024-12-17T17:04:16.958+0530] {logging_mixin.py:190} INFO - [2024-12-17T17:04:16.957+0530] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-12-17T17:04:17.015+0530] {logging_mixin.py:190} INFO - [2024-12-17 17:04:17,015] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T11:34:11.658947+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T11:34:11.658947+00:00'
[2024-12-17T17:04:17.016+0530] {logging_mixin.py:190} INFO - [2024-12-17T17:04:17.015+0530] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T11:34:11.658947+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T11:34:11.658947+00:00'
[2024-12-17T17:04:17.019+0530] {logging_mixin.py:190} INFO - Task instance is in running state
[2024-12-17T17:04:17.019+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: queued
[2024-12-17T17:04:17.019+0530] {logging_mixin.py:190} INFO - Current task name:scrape_collection_page state:scheduled start_date:None
[2024-12-17T17:04:17.019+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline and current dag run status:running
[2024-12-17T17:04:17.020+0530] {logging_mixin.py:190} INFO - [2024-12-17T17:04:17.020+0530] {taskinstance.py:731} INFO - ::endgroup::
[2024-12-17T17:04:17.021+0530] {logging_mixin.py:190} INFO - 2024-12-17
[2024-12-17T17:04:17.022+0530] {logging_mixin.py:190} INFO - [2024-12-17 17:04:17,022] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T17:04:17.022+0530] {logging_mixin.py:190} INFO - [2024-12-17T17:04:17.022+0530] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T17:04:17.027+0530] {logging_mixin.py:190} INFO - [2024-12-17T17:04:17.027+0530] {taskinstance.py:340} INFO - ::group::Post task execution logs
[2024-12-17T17:04:17.028+0530] {logging_mixin.py:190} INFO - [2024-12-17T17:04:17.027+0530] {taskinstance.py:352} INFO - Marking task as SUCCESS. dag_id=movie_pipeline, task_id=scrape_collection_page, run_id=manual__2024-12-17T11:34:11.658947+00:00, execution_date=20241217T113411, start_date=, end_date=20241217T113417
[2024-12-17T17:04:17.037+0530] {logging_mixin.py:190} INFO - Task instance in success state
[2024-12-17T17:04:17.037+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: running
[2024-12-17T17:04:17.037+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline queued_at:None
[2024-12-17T17:04:17.037+0530] {logging_mixin.py:190} INFO - Task hostname:Jayeshs-MacBook-Air.local operator:PythonOperator
[2024-12-17T17:04:17.038+0530] {logging_mixin.py:190} INFO - [2024-12-17T17:04:17.038+0530] {dag.py:4410} INFO - [DAG TEST] end task task_id=scrape_collection_page map_index=-1
[2024-12-17T17:04:17.042+0530] {logging_mixin.py:190} INFO - [2024-12-17T17:04:17.041+0530] {dagrun.py:854} INFO - Marking run <DagRun movie_pipeline @ 2024-12-17 11:34:11.658947+00:00: manual__2024-12-17T11:34:11.658947+00:00, state:running, queued_at: None. externally triggered: False> successful
[2024-12-17T17:04:17.042+0530] {logging_mixin.py:190} INFO - Dag run in success state
[2024-12-17T17:04:17.042+0530] {logging_mixin.py:190} INFO - Dag run start:2024-12-17 11:34:11.658947+00:00 end:2024-12-17 11:34:17.042271+00:00
[2024-12-17T17:04:17.042+0530] {logging_mixin.py:190} INFO - [2024-12-17T17:04:17.042+0530] {dagrun.py:905} INFO - DagRun Finished: dag_id=movie_pipeline, execution_date=2024-12-17 11:34:11.658947+00:00, run_id=manual__2024-12-17T11:34:11.658947+00:00, run_start_date=2024-12-17 11:34:11.658947+00:00, run_end_date=2024-12-17 11:34:17.042271+00:00, run_duration=5.383324, state=success, external_trigger=False, run_type=manual, data_interval_start=2024-12-16 11:34:11.658947+00:00, data_interval_end=2024-12-17 11:34:11.658947+00:00, dag_hash=None
[2024-12-17T17:04:17.052+0530] {processor.py:925} INFO - DAG(s) 'movie_pipeline' retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T17:04:17.063+0530] {logging_mixin.py:190} INFO - [2024-12-17T17:04:17.063+0530] {dag.py:3239} INFO - Sync 1 DAGs
[2024-12-17T17:04:17.080+0530] {logging_mixin.py:190} INFO - [2024-12-17T17:04:17.080+0530] {dag.py:4180} INFO - Setting next_dagrun for movie_pipeline to 2024-12-17 01:30:00+00:00, run_after=2024-12-18 01:30:00+00:00
[2024-12-17T17:04:17.098+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 5.487 seconds
[2024-12-17T17:06:59.329+0530] {processor.py:186} INFO - Started process (PID=77697) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T17:06:59.330+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-17T17:06:59.331+0530] {logging_mixin.py:190} INFO - [2024-12-17T17:06:59.331+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T17:06:59.490+0530] {logging_mixin.py:190} INFO - [2024-12-17T17:06:59.490+0530] {dag.py:4435} INFO - dagrun id: movie_pipeline
[2024-12-17T17:06:59.504+0530] {logging_mixin.py:190} INFO - [2024-12-17T17:06:59.503+0530] {dag.py:4451} INFO - created dagrun <DagRun movie_pipeline @ 2024-12-17 11:36:59.368818+00:00: manual__2024-12-17T11:36:59.368818+00:00, state:running, queued_at: None. externally triggered: False>
[2024-12-17T17:06:59.528+0530] {logging_mixin.py:190} INFO - [2024-12-17T17:06:59.528+0530] {dag.py:4396} INFO - [DAG TEST] starting task_id=scrape_collection_page map_index=-1
[2024-12-17T17:06:59.529+0530] {logging_mixin.py:190} INFO - [2024-12-17T17:06:59.528+0530] {dag.py:4399} INFO - [DAG TEST] running task <TaskInstance: movie_pipeline.scrape_collection_page manual__2024-12-17T11:36:59.368818+00:00 [scheduled]>
[2024-12-17T17:07:04.627+0530] {logging_mixin.py:190} INFO - [2024-12-17T17:07:04.627+0530] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-12-17T17:07:04.687+0530] {logging_mixin.py:190} INFO - [2024-12-17 17:07:04,686] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T11:36:59.368818+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T11:36:59.368818+00:00'
[2024-12-17T17:07:04.687+0530] {logging_mixin.py:190} INFO - [2024-12-17T17:07:04.686+0530] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T11:36:59.368818+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T11:36:59.368818+00:00'
[2024-12-17T17:07:04.690+0530] {logging_mixin.py:190} INFO - Task instance is in running state
[2024-12-17T17:07:04.690+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: queued
[2024-12-17T17:07:04.690+0530] {logging_mixin.py:190} INFO - Current task name:scrape_collection_page state:scheduled start_date:None
[2024-12-17T17:07:04.690+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline and current dag run status:running
[2024-12-17T17:07:04.691+0530] {logging_mixin.py:190} INFO - [2024-12-17T17:07:04.691+0530] {taskinstance.py:731} INFO - ::endgroup::
[2024-12-17T17:07:04.693+0530] {logging_mixin.py:190} INFO - 2024-12-17
[2024-12-17T17:07:04.693+0530] {logging_mixin.py:190} INFO - [2024-12-17 17:07:04,693] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T17:07:04.694+0530] {logging_mixin.py:190} INFO - [2024-12-17T17:07:04.693+0530] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T17:07:04.699+0530] {logging_mixin.py:190} INFO - [2024-12-17T17:07:04.699+0530] {taskinstance.py:340} INFO - ::group::Post task execution logs
[2024-12-17T17:07:04.699+0530] {logging_mixin.py:190} INFO - [2024-12-17T17:07:04.699+0530] {taskinstance.py:352} INFO - Marking task as SUCCESS. dag_id=movie_pipeline, task_id=scrape_collection_page, run_id=manual__2024-12-17T11:36:59.368818+00:00, execution_date=20241217T113659, start_date=, end_date=20241217T113704
[2024-12-17T17:07:04.708+0530] {logging_mixin.py:190} INFO - Task instance in success state
[2024-12-17T17:07:04.709+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: running
[2024-12-17T17:07:04.709+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline queued_at:None
[2024-12-17T17:07:04.709+0530] {logging_mixin.py:190} INFO - Task hostname:Jayeshs-MacBook-Air.local operator:PythonOperator
[2024-12-17T17:07:04.710+0530] {logging_mixin.py:190} INFO - [2024-12-17T17:07:04.709+0530] {dag.py:4410} INFO - [DAG TEST] end task task_id=scrape_collection_page map_index=-1
[2024-12-17T17:07:04.714+0530] {logging_mixin.py:190} INFO - [2024-12-17T17:07:04.713+0530] {dagrun.py:854} INFO - Marking run <DagRun movie_pipeline @ 2024-12-17 11:36:59.368818+00:00: manual__2024-12-17T11:36:59.368818+00:00, state:running, queued_at: None. externally triggered: False> successful
[2024-12-17T17:07:04.714+0530] {logging_mixin.py:190} INFO - Dag run in success state
[2024-12-17T17:07:04.715+0530] {logging_mixin.py:190} INFO - Dag run start:2024-12-17 11:36:59.368818+00:00 end:2024-12-17 11:37:04.714788+00:00
[2024-12-17T17:07:04.716+0530] {logging_mixin.py:190} INFO - [2024-12-17T17:07:04.715+0530] {dagrun.py:905} INFO - DagRun Finished: dag_id=movie_pipeline, execution_date=2024-12-17 11:36:59.368818+00:00, run_id=manual__2024-12-17T11:36:59.368818+00:00, run_start_date=2024-12-17 11:36:59.368818+00:00, run_end_date=2024-12-17 11:37:04.714788+00:00, run_duration=5.34597, state=success, external_trigger=False, run_type=manual, data_interval_start=2024-12-16 11:36:59.368818+00:00, data_interval_end=2024-12-17 11:36:59.368818+00:00, dag_hash=None
[2024-12-17T17:07:04.725+0530] {processor.py:925} INFO - DAG(s) 'movie_pipeline' retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T17:07:04.736+0530] {logging_mixin.py:190} INFO - [2024-12-17T17:07:04.736+0530] {dag.py:3239} INFO - Sync 1 DAGs
[2024-12-17T17:07:04.753+0530] {logging_mixin.py:190} INFO - [2024-12-17T17:07:04.753+0530] {dag.py:4180} INFO - Setting next_dagrun for movie_pipeline to 2024-12-17 01:30:00+00:00, run_after=2024-12-18 01:30:00+00:00
[2024-12-17T17:07:04.778+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 5.453 seconds
[2024-12-17T17:09:45.000+0530] {processor.py:186} INFO - Started process (PID=77886) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T17:09:45.001+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-17T17:09:45.003+0530] {logging_mixin.py:190} INFO - [2024-12-17T17:09:45.002+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T17:09:45.165+0530] {logging_mixin.py:190} INFO - [2024-12-17T17:09:45.165+0530] {dag.py:4435} INFO - dagrun id: movie_pipeline
[2024-12-17T17:09:45.179+0530] {logging_mixin.py:190} INFO - [2024-12-17T17:09:45.178+0530] {dag.py:4451} INFO - created dagrun <DagRun movie_pipeline @ 2024-12-17 11:39:45.040853+00:00: manual__2024-12-17T11:39:45.040853+00:00, state:running, queued_at: None. externally triggered: False>
[2024-12-17T17:09:45.204+0530] {logging_mixin.py:190} INFO - [2024-12-17T17:09:45.204+0530] {dag.py:4396} INFO - [DAG TEST] starting task_id=scrape_collection_page map_index=-1
[2024-12-17T17:09:45.205+0530] {logging_mixin.py:190} INFO - [2024-12-17T17:09:45.204+0530] {dag.py:4399} INFO - [DAG TEST] running task <TaskInstance: movie_pipeline.scrape_collection_page manual__2024-12-17T11:39:45.040853+00:00 [scheduled]>
[2024-12-17T17:09:50.296+0530] {logging_mixin.py:190} INFO - [2024-12-17T17:09:50.295+0530] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-12-17T17:09:50.354+0530] {logging_mixin.py:190} INFO - [2024-12-17 17:09:50,353] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T11:39:45.040853+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T11:39:45.040853+00:00'
[2024-12-17T17:09:50.354+0530] {logging_mixin.py:190} INFO - [2024-12-17T17:09:50.353+0530] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T11:39:45.040853+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T11:39:45.040853+00:00'
[2024-12-17T17:09:50.357+0530] {logging_mixin.py:190} INFO - Task instance is in running state
[2024-12-17T17:09:50.357+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: queued
[2024-12-17T17:09:50.357+0530] {logging_mixin.py:190} INFO - Current task name:scrape_collection_page state:scheduled start_date:None
[2024-12-17T17:09:50.357+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline and current dag run status:running
[2024-12-17T17:09:50.358+0530] {logging_mixin.py:190} INFO - [2024-12-17T17:09:50.357+0530] {taskinstance.py:731} INFO - ::endgroup::
[2024-12-17T17:09:50.360+0530] {logging_mixin.py:190} INFO - 2024-12-17
[2024-12-17T17:09:50.360+0530] {logging_mixin.py:190} INFO - [2024-12-17 17:09:50,360] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T17:09:50.361+0530] {logging_mixin.py:190} INFO - [2024-12-17T17:09:50.360+0530] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T17:09:50.365+0530] {logging_mixin.py:190} INFO - [2024-12-17T17:09:50.365+0530] {taskinstance.py:340} INFO - ::group::Post task execution logs
[2024-12-17T17:09:50.366+0530] {logging_mixin.py:190} INFO - [2024-12-17T17:09:50.366+0530] {taskinstance.py:352} INFO - Marking task as SUCCESS. dag_id=movie_pipeline, task_id=scrape_collection_page, run_id=manual__2024-12-17T11:39:45.040853+00:00, execution_date=20241217T113945, start_date=, end_date=20241217T113950
[2024-12-17T17:09:50.375+0530] {logging_mixin.py:190} INFO - Task instance in success state
[2024-12-17T17:09:50.376+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: running
[2024-12-17T17:09:50.376+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline queued_at:None
[2024-12-17T17:09:50.376+0530] {logging_mixin.py:190} INFO - Task hostname:Jayeshs-MacBook-Air.local operator:PythonOperator
[2024-12-17T17:09:50.376+0530] {logging_mixin.py:190} INFO - [2024-12-17T17:09:50.376+0530] {dag.py:4410} INFO - [DAG TEST] end task task_id=scrape_collection_page map_index=-1
[2024-12-17T17:09:50.380+0530] {logging_mixin.py:190} INFO - [2024-12-17T17:09:50.380+0530] {dagrun.py:854} INFO - Marking run <DagRun movie_pipeline @ 2024-12-17 11:39:45.040853+00:00: manual__2024-12-17T11:39:45.040853+00:00, state:running, queued_at: None. externally triggered: False> successful
[2024-12-17T17:09:50.381+0530] {logging_mixin.py:190} INFO - Dag run in success state
[2024-12-17T17:09:50.381+0530] {logging_mixin.py:190} INFO - Dag run start:2024-12-17 11:39:45.040853+00:00 end:2024-12-17 11:39:50.380951+00:00
[2024-12-17T17:09:50.381+0530] {logging_mixin.py:190} INFO - [2024-12-17T17:09:50.381+0530] {dagrun.py:905} INFO - DagRun Finished: dag_id=movie_pipeline, execution_date=2024-12-17 11:39:45.040853+00:00, run_id=manual__2024-12-17T11:39:45.040853+00:00, run_start_date=2024-12-17 11:39:45.040853+00:00, run_end_date=2024-12-17 11:39:50.380951+00:00, run_duration=5.340098, state=success, external_trigger=False, run_type=manual, data_interval_start=2024-12-16 11:39:45.040853+00:00, data_interval_end=2024-12-17 11:39:45.040853+00:00, dag_hash=None
[2024-12-17T17:09:50.390+0530] {processor.py:925} INFO - DAG(s) 'movie_pipeline' retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T17:09:50.401+0530] {logging_mixin.py:190} INFO - [2024-12-17T17:09:50.401+0530] {dag.py:3239} INFO - Sync 1 DAGs
[2024-12-17T17:09:50.419+0530] {logging_mixin.py:190} INFO - [2024-12-17T17:09:50.418+0530] {dag.py:4180} INFO - Setting next_dagrun for movie_pipeline to 2024-12-17 01:30:00+00:00, run_after=2024-12-18 01:30:00+00:00
[2024-12-17T17:09:50.436+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 5.439 seconds
[2024-12-17T17:12:33.598+0530] {processor.py:186} INFO - Started process (PID=78085) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T17:12:33.599+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-17T17:12:33.600+0530] {logging_mixin.py:190} INFO - [2024-12-17T17:12:33.600+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T17:12:33.772+0530] {logging_mixin.py:190} INFO - [2024-12-17T17:12:33.771+0530] {dag.py:4435} INFO - dagrun id: movie_pipeline
[2024-12-17T17:12:33.785+0530] {logging_mixin.py:190} INFO - [2024-12-17T17:12:33.785+0530] {dag.py:4451} INFO - created dagrun <DagRun movie_pipeline @ 2024-12-17 11:42:33.643231+00:00: manual__2024-12-17T11:42:33.643231+00:00, state:running, queued_at: None. externally triggered: False>
[2024-12-17T17:12:33.810+0530] {logging_mixin.py:190} INFO - [2024-12-17T17:12:33.810+0530] {dag.py:4396} INFO - [DAG TEST] starting task_id=scrape_collection_page map_index=-1
[2024-12-17T17:12:33.811+0530] {logging_mixin.py:190} INFO - [2024-12-17T17:12:33.810+0530] {dag.py:4399} INFO - [DAG TEST] running task <TaskInstance: movie_pipeline.scrape_collection_page manual__2024-12-17T11:42:33.643231+00:00 [scheduled]>
[2024-12-17T17:12:38.911+0530] {logging_mixin.py:190} INFO - [2024-12-17T17:12:38.910+0530] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-12-17T17:12:38.969+0530] {logging_mixin.py:190} INFO - [2024-12-17 17:12:38,969] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T11:42:33.643231+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T11:42:33.643231+00:00'
[2024-12-17T17:12:38.970+0530] {logging_mixin.py:190} INFO - [2024-12-17T17:12:38.969+0530] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T11:42:33.643231+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T11:42:33.643231+00:00'
[2024-12-17T17:12:38.973+0530] {logging_mixin.py:190} INFO - Task instance is in running state
[2024-12-17T17:12:38.973+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: queued
[2024-12-17T17:12:38.973+0530] {logging_mixin.py:190} INFO - Current task name:scrape_collection_page state:scheduled start_date:None
[2024-12-17T17:12:38.974+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline and current dag run status:running
[2024-12-17T17:12:38.974+0530] {logging_mixin.py:190} INFO - [2024-12-17T17:12:38.974+0530] {taskinstance.py:731} INFO - ::endgroup::
[2024-12-17T17:12:38.976+0530] {logging_mixin.py:190} INFO - 2024-12-17
[2024-12-17T17:12:38.976+0530] {logging_mixin.py:190} INFO - [2024-12-17 17:12:38,976] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T17:12:38.977+0530] {logging_mixin.py:190} INFO - [2024-12-17T17:12:38.976+0530] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T17:12:38.982+0530] {logging_mixin.py:190} INFO - [2024-12-17T17:12:38.981+0530] {taskinstance.py:340} INFO - ::group::Post task execution logs
[2024-12-17T17:12:38.982+0530] {logging_mixin.py:190} INFO - [2024-12-17T17:12:38.982+0530] {taskinstance.py:352} INFO - Marking task as SUCCESS. dag_id=movie_pipeline, task_id=scrape_collection_page, run_id=manual__2024-12-17T11:42:33.643231+00:00, execution_date=20241217T114233, start_date=, end_date=20241217T114238
[2024-12-17T17:12:38.991+0530] {logging_mixin.py:190} INFO - Task instance in success state
[2024-12-17T17:12:38.992+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: running
[2024-12-17T17:12:38.992+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline queued_at:None
[2024-12-17T17:12:38.992+0530] {logging_mixin.py:190} INFO - Task hostname:Jayeshs-MacBook-Air.local operator:PythonOperator
[2024-12-17T17:12:38.992+0530] {logging_mixin.py:190} INFO - [2024-12-17T17:12:38.992+0530] {dag.py:4410} INFO - [DAG TEST] end task task_id=scrape_collection_page map_index=-1
[2024-12-17T17:12:38.996+0530] {logging_mixin.py:190} INFO - [2024-12-17T17:12:38.996+0530] {dagrun.py:854} INFO - Marking run <DagRun movie_pipeline @ 2024-12-17 11:42:33.643231+00:00: manual__2024-12-17T11:42:33.643231+00:00, state:running, queued_at: None. externally triggered: False> successful
[2024-12-17T17:12:38.996+0530] {logging_mixin.py:190} INFO - Dag run in success state
[2024-12-17T17:12:38.996+0530] {logging_mixin.py:190} INFO - Dag run start:2024-12-17 11:42:33.643231+00:00 end:2024-12-17 11:42:38.996610+00:00
[2024-12-17T17:12:38.997+0530] {logging_mixin.py:190} INFO - [2024-12-17T17:12:38.997+0530] {dagrun.py:905} INFO - DagRun Finished: dag_id=movie_pipeline, execution_date=2024-12-17 11:42:33.643231+00:00, run_id=manual__2024-12-17T11:42:33.643231+00:00, run_start_date=2024-12-17 11:42:33.643231+00:00, run_end_date=2024-12-17 11:42:38.996610+00:00, run_duration=5.353379, state=success, external_trigger=False, run_type=manual, data_interval_start=2024-12-16 11:42:33.643231+00:00, data_interval_end=2024-12-17 11:42:33.643231+00:00, dag_hash=None
[2024-12-17T17:12:39.007+0530] {processor.py:925} INFO - DAG(s) 'movie_pipeline' retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T17:12:39.018+0530] {logging_mixin.py:190} INFO - [2024-12-17T17:12:39.018+0530] {dag.py:3239} INFO - Sync 1 DAGs
[2024-12-17T17:12:39.035+0530] {logging_mixin.py:190} INFO - [2024-12-17T17:12:39.035+0530] {dag.py:4180} INFO - Setting next_dagrun for movie_pipeline to 2024-12-17 01:30:00+00:00, run_after=2024-12-18 01:30:00+00:00
[2024-12-17T17:12:39.053+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 5.459 seconds
[2024-12-17T17:15:21.614+0530] {processor.py:186} INFO - Started process (PID=78277) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T17:15:21.615+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-17T17:15:21.616+0530] {logging_mixin.py:190} INFO - [2024-12-17T17:15:21.616+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T17:15:21.778+0530] {logging_mixin.py:190} INFO - [2024-12-17T17:15:21.777+0530] {dag.py:4435} INFO - dagrun id: movie_pipeline
[2024-12-17T17:15:21.791+0530] {logging_mixin.py:190} INFO - [2024-12-17T17:15:21.791+0530] {dag.py:4451} INFO - created dagrun <DagRun movie_pipeline @ 2024-12-17 11:45:21.654437+00:00: manual__2024-12-17T11:45:21.654437+00:00, state:running, queued_at: None. externally triggered: False>
[2024-12-17T17:15:21.816+0530] {logging_mixin.py:190} INFO - [2024-12-17T17:15:21.816+0530] {dag.py:4396} INFO - [DAG TEST] starting task_id=scrape_collection_page map_index=-1
[2024-12-17T17:15:21.817+0530] {logging_mixin.py:190} INFO - [2024-12-17T17:15:21.816+0530] {dag.py:4399} INFO - [DAG TEST] running task <TaskInstance: movie_pipeline.scrape_collection_page manual__2024-12-17T11:45:21.654437+00:00 [scheduled]>
[2024-12-17T17:15:26.910+0530] {logging_mixin.py:190} INFO - [2024-12-17T17:15:26.910+0530] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-12-17T17:15:26.970+0530] {logging_mixin.py:190} INFO - [2024-12-17 17:15:26,969] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T11:45:21.654437+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T11:45:21.654437+00:00'
[2024-12-17T17:15:26.970+0530] {logging_mixin.py:190} INFO - [2024-12-17T17:15:26.969+0530] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T11:45:21.654437+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T11:45:21.654437+00:00'
[2024-12-17T17:15:26.973+0530] {logging_mixin.py:190} INFO - Task instance is in running state
[2024-12-17T17:15:26.973+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: queued
[2024-12-17T17:15:26.974+0530] {logging_mixin.py:190} INFO - Current task name:scrape_collection_page state:scheduled start_date:None
[2024-12-17T17:15:26.974+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline and current dag run status:running
[2024-12-17T17:15:26.974+0530] {logging_mixin.py:190} INFO - [2024-12-17T17:15:26.974+0530] {taskinstance.py:731} INFO - ::endgroup::
[2024-12-17T17:15:26.976+0530] {logging_mixin.py:190} INFO - 2024-12-17
[2024-12-17T17:15:26.976+0530] {logging_mixin.py:190} INFO - [2024-12-17 17:15:26,976] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T17:15:26.977+0530] {logging_mixin.py:190} INFO - [2024-12-17T17:15:26.976+0530] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T17:15:26.982+0530] {logging_mixin.py:190} INFO - [2024-12-17T17:15:26.981+0530] {taskinstance.py:340} INFO - ::group::Post task execution logs
[2024-12-17T17:15:26.982+0530] {logging_mixin.py:190} INFO - [2024-12-17T17:15:26.982+0530] {taskinstance.py:352} INFO - Marking task as SUCCESS. dag_id=movie_pipeline, task_id=scrape_collection_page, run_id=manual__2024-12-17T11:45:21.654437+00:00, execution_date=20241217T114521, start_date=, end_date=20241217T114526
[2024-12-17T17:15:26.991+0530] {logging_mixin.py:190} INFO - Task instance in success state
[2024-12-17T17:15:26.991+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: running
[2024-12-17T17:15:26.991+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline queued_at:None
[2024-12-17T17:15:26.991+0530] {logging_mixin.py:190} INFO - Task hostname:Jayeshs-MacBook-Air.local operator:PythonOperator
[2024-12-17T17:15:26.992+0530] {logging_mixin.py:190} INFO - [2024-12-17T17:15:26.992+0530] {dag.py:4410} INFO - [DAG TEST] end task task_id=scrape_collection_page map_index=-1
[2024-12-17T17:15:26.996+0530] {logging_mixin.py:190} INFO - [2024-12-17T17:15:26.995+0530] {dagrun.py:854} INFO - Marking run <DagRun movie_pipeline @ 2024-12-17 11:45:21.654437+00:00: manual__2024-12-17T11:45:21.654437+00:00, state:running, queued_at: None. externally triggered: False> successful
[2024-12-17T17:15:26.996+0530] {logging_mixin.py:190} INFO - Dag run in success state
[2024-12-17T17:15:26.996+0530] {logging_mixin.py:190} INFO - Dag run start:2024-12-17 11:45:21.654437+00:00 end:2024-12-17 11:45:26.996292+00:00
[2024-12-17T17:15:26.996+0530] {logging_mixin.py:190} INFO - [2024-12-17T17:15:26.996+0530] {dagrun.py:905} INFO - DagRun Finished: dag_id=movie_pipeline, execution_date=2024-12-17 11:45:21.654437+00:00, run_id=manual__2024-12-17T11:45:21.654437+00:00, run_start_date=2024-12-17 11:45:21.654437+00:00, run_end_date=2024-12-17 11:45:26.996292+00:00, run_duration=5.341855, state=success, external_trigger=False, run_type=manual, data_interval_start=2024-12-16 11:45:21.654437+00:00, data_interval_end=2024-12-17 11:45:21.654437+00:00, dag_hash=None
[2024-12-17T17:15:27.007+0530] {processor.py:925} INFO - DAG(s) 'movie_pipeline' retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T17:15:27.021+0530] {logging_mixin.py:190} INFO - [2024-12-17T17:15:27.021+0530] {dag.py:3239} INFO - Sync 1 DAGs
[2024-12-17T17:15:27.039+0530] {logging_mixin.py:190} INFO - [2024-12-17T17:15:27.039+0530] {dag.py:4180} INFO - Setting next_dagrun for movie_pipeline to 2024-12-17 01:30:00+00:00, run_after=2024-12-18 01:30:00+00:00
[2024-12-17T17:15:27.057+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 5.447 seconds
[2024-12-17T17:18:09.755+0530] {processor.py:186} INFO - Started process (PID=78481) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T17:18:09.756+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-17T17:18:09.757+0530] {logging_mixin.py:190} INFO - [2024-12-17T17:18:09.757+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T17:18:09.922+0530] {logging_mixin.py:190} INFO - [2024-12-17T17:18:09.922+0530] {dag.py:4435} INFO - dagrun id: movie_pipeline
[2024-12-17T17:18:09.936+0530] {logging_mixin.py:190} INFO - [2024-12-17T17:18:09.935+0530] {dag.py:4451} INFO - created dagrun <DagRun movie_pipeline @ 2024-12-17 11:48:09.795016+00:00: manual__2024-12-17T11:48:09.795016+00:00, state:running, queued_at: None. externally triggered: False>
[2024-12-17T17:18:09.961+0530] {logging_mixin.py:190} INFO - [2024-12-17T17:18:09.960+0530] {dag.py:4396} INFO - [DAG TEST] starting task_id=scrape_collection_page map_index=-1
[2024-12-17T17:18:09.961+0530] {logging_mixin.py:190} INFO - [2024-12-17T17:18:09.961+0530] {dag.py:4399} INFO - [DAG TEST] running task <TaskInstance: movie_pipeline.scrape_collection_page manual__2024-12-17T11:48:09.795016+00:00 [scheduled]>
[2024-12-17T17:18:15.058+0530] {logging_mixin.py:190} INFO - [2024-12-17T17:18:15.057+0530] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-12-17T17:18:15.116+0530] {logging_mixin.py:190} INFO - [2024-12-17 17:18:15,116] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T11:48:09.795016+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T11:48:09.795016+00:00'
[2024-12-17T17:18:15.117+0530] {logging_mixin.py:190} INFO - [2024-12-17T17:18:15.116+0530] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T11:48:09.795016+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T11:48:09.795016+00:00'
[2024-12-17T17:18:15.119+0530] {logging_mixin.py:190} INFO - Task instance is in running state
[2024-12-17T17:18:15.120+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: queued
[2024-12-17T17:18:15.120+0530] {logging_mixin.py:190} INFO - Current task name:scrape_collection_page state:scheduled start_date:None
[2024-12-17T17:18:15.120+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline and current dag run status:running
[2024-12-17T17:18:15.120+0530] {logging_mixin.py:190} INFO - [2024-12-17T17:18:15.120+0530] {taskinstance.py:731} INFO - ::endgroup::
[2024-12-17T17:18:15.122+0530] {logging_mixin.py:190} INFO - 2024-12-17
[2024-12-17T17:18:15.123+0530] {logging_mixin.py:190} INFO - [2024-12-17 17:18:15,123] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T17:18:15.123+0530] {logging_mixin.py:190} INFO - [2024-12-17T17:18:15.123+0530] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T17:18:15.128+0530] {logging_mixin.py:190} INFO - [2024-12-17T17:18:15.128+0530] {taskinstance.py:340} INFO - ::group::Post task execution logs
[2024-12-17T17:18:15.129+0530] {logging_mixin.py:190} INFO - [2024-12-17T17:18:15.128+0530] {taskinstance.py:352} INFO - Marking task as SUCCESS. dag_id=movie_pipeline, task_id=scrape_collection_page, run_id=manual__2024-12-17T11:48:09.795016+00:00, execution_date=20241217T114809, start_date=, end_date=20241217T114815
[2024-12-17T17:18:15.137+0530] {logging_mixin.py:190} INFO - Task instance in success state
[2024-12-17T17:18:15.138+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: running
[2024-12-17T17:18:15.138+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline queued_at:None
[2024-12-17T17:18:15.138+0530] {logging_mixin.py:190} INFO - Task hostname:Jayeshs-MacBook-Air.local operator:PythonOperator
[2024-12-17T17:18:15.138+0530] {logging_mixin.py:190} INFO - [2024-12-17T17:18:15.138+0530] {dag.py:4410} INFO - [DAG TEST] end task task_id=scrape_collection_page map_index=-1
[2024-12-17T17:18:15.142+0530] {logging_mixin.py:190} INFO - [2024-12-17T17:18:15.142+0530] {dagrun.py:854} INFO - Marking run <DagRun movie_pipeline @ 2024-12-17 11:48:09.795016+00:00: manual__2024-12-17T11:48:09.795016+00:00, state:running, queued_at: None. externally triggered: False> successful
[2024-12-17T17:18:15.142+0530] {logging_mixin.py:190} INFO - Dag run in success state
[2024-12-17T17:18:15.143+0530] {logging_mixin.py:190} INFO - Dag run start:2024-12-17 11:48:09.795016+00:00 end:2024-12-17 11:48:15.142804+00:00
[2024-12-17T17:18:15.143+0530] {logging_mixin.py:190} INFO - [2024-12-17T17:18:15.143+0530] {dagrun.py:905} INFO - DagRun Finished: dag_id=movie_pipeline, execution_date=2024-12-17 11:48:09.795016+00:00, run_id=manual__2024-12-17T11:48:09.795016+00:00, run_start_date=2024-12-17 11:48:09.795016+00:00, run_end_date=2024-12-17 11:48:15.142804+00:00, run_duration=5.347788, state=success, external_trigger=False, run_type=manual, data_interval_start=2024-12-16 11:48:09.795016+00:00, data_interval_end=2024-12-17 11:48:09.795016+00:00, dag_hash=None
[2024-12-17T17:18:15.153+0530] {processor.py:925} INFO - DAG(s) 'movie_pipeline' retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T17:18:15.164+0530] {logging_mixin.py:190} INFO - [2024-12-17T17:18:15.164+0530] {dag.py:3239} INFO - Sync 1 DAGs
[2024-12-17T17:18:15.181+0530] {logging_mixin.py:190} INFO - [2024-12-17T17:18:15.181+0530] {dag.py:4180} INFO - Setting next_dagrun for movie_pipeline to 2024-12-17 01:30:00+00:00, run_after=2024-12-18 01:30:00+00:00
[2024-12-17T17:18:15.199+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 5.447 seconds
[2024-12-17T17:20:55.512+0530] {processor.py:186} INFO - Started process (PID=78672) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T17:20:55.513+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-17T17:20:55.514+0530] {logging_mixin.py:190} INFO - [2024-12-17T17:20:55.514+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T17:20:55.695+0530] {logging_mixin.py:190} INFO - [2024-12-17T17:20:55.694+0530] {dag.py:4435} INFO - dagrun id: movie_pipeline
[2024-12-17T17:20:55.708+0530] {logging_mixin.py:190} INFO - [2024-12-17T17:20:55.708+0530] {dag.py:4451} INFO - created dagrun <DagRun movie_pipeline @ 2024-12-17 11:50:55.558016+00:00: manual__2024-12-17T11:50:55.558016+00:00, state:running, queued_at: None. externally triggered: False>
[2024-12-17T17:20:55.733+0530] {logging_mixin.py:190} INFO - [2024-12-17T17:20:55.733+0530] {dag.py:4396} INFO - [DAG TEST] starting task_id=scrape_collection_page map_index=-1
[2024-12-17T17:20:55.734+0530] {logging_mixin.py:190} INFO - [2024-12-17T17:20:55.734+0530] {dag.py:4399} INFO - [DAG TEST] running task <TaskInstance: movie_pipeline.scrape_collection_page manual__2024-12-17T11:50:55.558016+00:00 [scheduled]>
[2024-12-17T17:21:00.832+0530] {logging_mixin.py:190} INFO - [2024-12-17T17:21:00.832+0530] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-12-17T17:21:00.894+0530] {logging_mixin.py:190} INFO - [2024-12-17 17:21:00,894] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T11:50:55.558016+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T11:50:55.558016+00:00'
[2024-12-17T17:21:00.894+0530] {logging_mixin.py:190} INFO - [2024-12-17T17:21:00.894+0530] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T11:50:55.558016+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T11:50:55.558016+00:00'
[2024-12-17T17:21:00.897+0530] {logging_mixin.py:190} INFO - Task instance is in running state
[2024-12-17T17:21:00.898+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: queued
[2024-12-17T17:21:00.898+0530] {logging_mixin.py:190} INFO - Current task name:scrape_collection_page state:scheduled start_date:None
[2024-12-17T17:21:00.898+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline and current dag run status:running
[2024-12-17T17:21:00.899+0530] {logging_mixin.py:190} INFO - [2024-12-17T17:21:00.899+0530] {taskinstance.py:731} INFO - ::endgroup::
[2024-12-17T17:21:00.901+0530] {logging_mixin.py:190} INFO - 2024-12-17
[2024-12-17T17:21:00.901+0530] {logging_mixin.py:190} INFO - [2024-12-17 17:21:00,901] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T17:21:00.901+0530] {logging_mixin.py:190} INFO - [2024-12-17T17:21:00.901+0530] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T17:21:00.907+0530] {logging_mixin.py:190} INFO - [2024-12-17T17:21:00.907+0530] {taskinstance.py:340} INFO - ::group::Post task execution logs
[2024-12-17T17:21:00.908+0530] {logging_mixin.py:190} INFO - [2024-12-17T17:21:00.907+0530] {taskinstance.py:352} INFO - Marking task as SUCCESS. dag_id=movie_pipeline, task_id=scrape_collection_page, run_id=manual__2024-12-17T11:50:55.558016+00:00, execution_date=20241217T115055, start_date=, end_date=20241217T115100
[2024-12-17T17:21:00.917+0530] {logging_mixin.py:190} INFO - Task instance in success state
[2024-12-17T17:21:00.917+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: running
[2024-12-17T17:21:00.917+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline queued_at:None
[2024-12-17T17:21:00.917+0530] {logging_mixin.py:190} INFO - Task hostname:Jayeshs-MacBook-Air.local operator:PythonOperator
[2024-12-17T17:21:00.918+0530] {logging_mixin.py:190} INFO - [2024-12-17T17:21:00.918+0530] {dag.py:4410} INFO - [DAG TEST] end task task_id=scrape_collection_page map_index=-1
[2024-12-17T17:21:00.921+0530] {logging_mixin.py:190} INFO - [2024-12-17T17:21:00.921+0530] {dagrun.py:854} INFO - Marking run <DagRun movie_pipeline @ 2024-12-17 11:50:55.558016+00:00: manual__2024-12-17T11:50:55.558016+00:00, state:running, queued_at: None. externally triggered: False> successful
[2024-12-17T17:21:00.922+0530] {logging_mixin.py:190} INFO - Dag run in success state
[2024-12-17T17:21:00.922+0530] {logging_mixin.py:190} INFO - Dag run start:2024-12-17 11:50:55.558016+00:00 end:2024-12-17 11:51:00.922098+00:00
[2024-12-17T17:21:00.922+0530] {logging_mixin.py:190} INFO - [2024-12-17T17:21:00.922+0530] {dagrun.py:905} INFO - DagRun Finished: dag_id=movie_pipeline, execution_date=2024-12-17 11:50:55.558016+00:00, run_id=manual__2024-12-17T11:50:55.558016+00:00, run_start_date=2024-12-17 11:50:55.558016+00:00, run_end_date=2024-12-17 11:51:00.922098+00:00, run_duration=5.364082, state=success, external_trigger=False, run_type=manual, data_interval_start=2024-12-16 11:50:55.558016+00:00, data_interval_end=2024-12-17 11:50:55.558016+00:00, dag_hash=None
[2024-12-17T17:21:00.932+0530] {processor.py:925} INFO - DAG(s) 'movie_pipeline' retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T17:21:00.945+0530] {logging_mixin.py:190} INFO - [2024-12-17T17:21:00.945+0530] {dag.py:3239} INFO - Sync 1 DAGs
[2024-12-17T17:21:00.966+0530] {logging_mixin.py:190} INFO - [2024-12-17T17:21:00.966+0530] {dag.py:4180} INFO - Setting next_dagrun for movie_pipeline to 2024-12-17 01:30:00+00:00, run_after=2024-12-18 01:30:00+00:00
[2024-12-17T17:21:00.984+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 5.475 seconds
[2024-12-17T17:23:40.383+0530] {processor.py:186} INFO - Started process (PID=78864) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T17:23:40.384+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-17T17:23:40.386+0530] {logging_mixin.py:190} INFO - [2024-12-17T17:23:40.385+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T17:23:40.548+0530] {logging_mixin.py:190} INFO - [2024-12-17T17:23:40.548+0530] {dag.py:4435} INFO - dagrun id: movie_pipeline
[2024-12-17T17:23:40.561+0530] {logging_mixin.py:190} INFO - [2024-12-17T17:23:40.561+0530] {dag.py:4451} INFO - created dagrun <DagRun movie_pipeline @ 2024-12-17 11:53:40.425524+00:00: manual__2024-12-17T11:53:40.425524+00:00, state:running, queued_at: None. externally triggered: False>
[2024-12-17T17:23:40.586+0530] {logging_mixin.py:190} INFO - [2024-12-17T17:23:40.586+0530] {dag.py:4396} INFO - [DAG TEST] starting task_id=scrape_collection_page map_index=-1
[2024-12-17T17:23:40.586+0530] {logging_mixin.py:190} INFO - [2024-12-17T17:23:40.586+0530] {dag.py:4399} INFO - [DAG TEST] running task <TaskInstance: movie_pipeline.scrape_collection_page manual__2024-12-17T11:53:40.425524+00:00 [scheduled]>
[2024-12-17T17:23:45.682+0530] {logging_mixin.py:190} INFO - [2024-12-17T17:23:45.682+0530] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-12-17T17:23:45.788+0530] {logging_mixin.py:190} INFO - [2024-12-17 17:23:45,788] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T11:53:40.425524+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T11:53:40.425524+00:00'
[2024-12-17T17:23:45.789+0530] {logging_mixin.py:190} INFO - [2024-12-17T17:23:45.788+0530] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T11:53:40.425524+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T11:53:40.425524+00:00'
[2024-12-17T17:23:45.793+0530] {logging_mixin.py:190} INFO - Task instance is in running state
[2024-12-17T17:23:45.793+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: queued
[2024-12-17T17:23:45.793+0530] {logging_mixin.py:190} INFO - Current task name:scrape_collection_page state:scheduled start_date:None
[2024-12-17T17:23:45.794+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline and current dag run status:running
[2024-12-17T17:23:45.795+0530] {logging_mixin.py:190} INFO - [2024-12-17T17:23:45.794+0530] {taskinstance.py:731} INFO - ::endgroup::
[2024-12-17T17:23:45.797+0530] {logging_mixin.py:190} INFO - 2024-12-17
[2024-12-17T17:23:45.798+0530] {logging_mixin.py:190} INFO - [2024-12-17 17:23:45,797] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T17:23:45.798+0530] {logging_mixin.py:190} INFO - [2024-12-17T17:23:45.797+0530] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T17:23:45.810+0530] {logging_mixin.py:190} INFO - [2024-12-17T17:23:45.810+0530] {taskinstance.py:340} INFO - ::group::Post task execution logs
[2024-12-17T17:23:45.811+0530] {logging_mixin.py:190} INFO - [2024-12-17T17:23:45.811+0530] {taskinstance.py:352} INFO - Marking task as SUCCESS. dag_id=movie_pipeline, task_id=scrape_collection_page, run_id=manual__2024-12-17T11:53:40.425524+00:00, execution_date=20241217T115340, start_date=, end_date=20241217T115345
[2024-12-17T17:23:45.824+0530] {logging_mixin.py:190} INFO - Task instance in success state
[2024-12-17T17:23:45.824+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: running
[2024-12-17T17:23:45.825+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline queued_at:None
[2024-12-17T17:23:45.825+0530] {logging_mixin.py:190} INFO - Task hostname:Jayeshs-MacBook-Air.local operator:PythonOperator
[2024-12-17T17:23:45.825+0530] {logging_mixin.py:190} INFO - [2024-12-17T17:23:45.825+0530] {dag.py:4410} INFO - [DAG TEST] end task task_id=scrape_collection_page map_index=-1
[2024-12-17T17:23:45.830+0530] {logging_mixin.py:190} INFO - [2024-12-17T17:23:45.829+0530] {dagrun.py:854} INFO - Marking run <DagRun movie_pipeline @ 2024-12-17 11:53:40.425524+00:00: manual__2024-12-17T11:53:40.425524+00:00, state:running, queued_at: None. externally triggered: False> successful
[2024-12-17T17:23:45.830+0530] {logging_mixin.py:190} INFO - Dag run in success state
[2024-12-17T17:23:45.830+0530] {logging_mixin.py:190} INFO - Dag run start:2024-12-17 11:53:40.425524+00:00 end:2024-12-17 11:53:45.830295+00:00
[2024-12-17T17:23:45.831+0530] {logging_mixin.py:190} INFO - [2024-12-17T17:23:45.831+0530] {dagrun.py:905} INFO - DagRun Finished: dag_id=movie_pipeline, execution_date=2024-12-17 11:53:40.425524+00:00, run_id=manual__2024-12-17T11:53:40.425524+00:00, run_start_date=2024-12-17 11:53:40.425524+00:00, run_end_date=2024-12-17 11:53:45.830295+00:00, run_duration=5.404771, state=success, external_trigger=False, run_type=manual, data_interval_start=2024-12-16 11:53:40.425524+00:00, data_interval_end=2024-12-17 11:53:40.425524+00:00, dag_hash=None
[2024-12-17T17:23:45.841+0530] {processor.py:925} INFO - DAG(s) 'movie_pipeline' retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T17:23:45.859+0530] {logging_mixin.py:190} INFO - [2024-12-17T17:23:45.858+0530] {dag.py:3239} INFO - Sync 1 DAGs
[2024-12-17T17:23:45.879+0530] {logging_mixin.py:190} INFO - [2024-12-17T17:23:45.878+0530] {dag.py:4180} INFO - Setting next_dagrun for movie_pipeline to 2024-12-17 01:30:00+00:00, run_after=2024-12-18 01:30:00+00:00
[2024-12-17T17:23:45.901+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 5.521 seconds
[2024-12-17T17:26:24.951+0530] {processor.py:186} INFO - Started process (PID=79066) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T17:26:24.952+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-17T17:26:24.953+0530] {logging_mixin.py:190} INFO - [2024-12-17T17:26:24.953+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T17:26:25.115+0530] {logging_mixin.py:190} INFO - [2024-12-17T17:26:25.115+0530] {dag.py:4435} INFO - dagrun id: movie_pipeline
[2024-12-17T17:26:25.128+0530] {logging_mixin.py:190} INFO - [2024-12-17T17:26:25.128+0530] {dag.py:4451} INFO - created dagrun <DagRun movie_pipeline @ 2024-12-17 11:56:24.991719+00:00: manual__2024-12-17T11:56:24.991719+00:00, state:running, queued_at: None. externally triggered: False>
[2024-12-17T17:26:25.153+0530] {logging_mixin.py:190} INFO - [2024-12-17T17:26:25.153+0530] {dag.py:4396} INFO - [DAG TEST] starting task_id=scrape_collection_page map_index=-1
[2024-12-17T17:26:25.153+0530] {logging_mixin.py:190} INFO - [2024-12-17T17:26:25.153+0530] {dag.py:4399} INFO - [DAG TEST] running task <TaskInstance: movie_pipeline.scrape_collection_page manual__2024-12-17T11:56:24.991719+00:00 [scheduled]>
[2024-12-17T17:26:30.252+0530] {logging_mixin.py:190} INFO - [2024-12-17T17:26:30.252+0530] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-12-17T17:26:30.312+0530] {logging_mixin.py:190} INFO - [2024-12-17 17:26:30,312] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T11:56:24.991719+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T11:56:24.991719+00:00'
[2024-12-17T17:26:30.312+0530] {logging_mixin.py:190} INFO - [2024-12-17T17:26:30.312+0530] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T11:56:24.991719+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T11:56:24.991719+00:00'
[2024-12-17T17:26:30.315+0530] {logging_mixin.py:190} INFO - Task instance is in running state
[2024-12-17T17:26:30.315+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: queued
[2024-12-17T17:26:30.316+0530] {logging_mixin.py:190} INFO - Current task name:scrape_collection_page state:scheduled start_date:None
[2024-12-17T17:26:30.316+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline and current dag run status:running
[2024-12-17T17:26:30.316+0530] {logging_mixin.py:190} INFO - [2024-12-17T17:26:30.316+0530] {taskinstance.py:731} INFO - ::endgroup::
[2024-12-17T17:26:30.318+0530] {logging_mixin.py:190} INFO - 2024-12-17
[2024-12-17T17:26:30.318+0530] {logging_mixin.py:190} INFO - [2024-12-17 17:26:30,318] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T17:26:30.319+0530] {logging_mixin.py:190} INFO - [2024-12-17T17:26:30.318+0530] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T17:26:30.323+0530] {logging_mixin.py:190} INFO - [2024-12-17T17:26:30.323+0530] {taskinstance.py:340} INFO - ::group::Post task execution logs
[2024-12-17T17:26:30.324+0530] {logging_mixin.py:190} INFO - [2024-12-17T17:26:30.324+0530] {taskinstance.py:352} INFO - Marking task as SUCCESS. dag_id=movie_pipeline, task_id=scrape_collection_page, run_id=manual__2024-12-17T11:56:24.991719+00:00, execution_date=20241217T115624, start_date=, end_date=20241217T115630
[2024-12-17T17:26:30.333+0530] {logging_mixin.py:190} INFO - Task instance in success state
[2024-12-17T17:26:30.333+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: running
[2024-12-17T17:26:30.334+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline queued_at:None
[2024-12-17T17:26:30.334+0530] {logging_mixin.py:190} INFO - Task hostname:Jayeshs-MacBook-Air.local operator:PythonOperator
[2024-12-17T17:26:30.334+0530] {logging_mixin.py:190} INFO - [2024-12-17T17:26:30.334+0530] {dag.py:4410} INFO - [DAG TEST] end task task_id=scrape_collection_page map_index=-1
[2024-12-17T17:26:30.338+0530] {logging_mixin.py:190} INFO - [2024-12-17T17:26:30.337+0530] {dagrun.py:854} INFO - Marking run <DagRun movie_pipeline @ 2024-12-17 11:56:24.991719+00:00: manual__2024-12-17T11:56:24.991719+00:00, state:running, queued_at: None. externally triggered: False> successful
[2024-12-17T17:26:30.338+0530] {logging_mixin.py:190} INFO - Dag run in success state
[2024-12-17T17:26:30.338+0530] {logging_mixin.py:190} INFO - Dag run start:2024-12-17 11:56:24.991719+00:00 end:2024-12-17 11:56:30.338477+00:00
[2024-12-17T17:26:30.339+0530] {logging_mixin.py:190} INFO - [2024-12-17T17:26:30.338+0530] {dagrun.py:905} INFO - DagRun Finished: dag_id=movie_pipeline, execution_date=2024-12-17 11:56:24.991719+00:00, run_id=manual__2024-12-17T11:56:24.991719+00:00, run_start_date=2024-12-17 11:56:24.991719+00:00, run_end_date=2024-12-17 11:56:30.338477+00:00, run_duration=5.346758, state=success, external_trigger=False, run_type=manual, data_interval_start=2024-12-16 11:56:24.991719+00:00, data_interval_end=2024-12-17 11:56:24.991719+00:00, dag_hash=None
[2024-12-17T17:26:30.349+0530] {processor.py:925} INFO - DAG(s) 'movie_pipeline' retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T17:26:30.360+0530] {logging_mixin.py:190} INFO - [2024-12-17T17:26:30.360+0530] {dag.py:3239} INFO - Sync 1 DAGs
[2024-12-17T17:26:30.377+0530] {logging_mixin.py:190} INFO - [2024-12-17T17:26:30.377+0530] {dag.py:4180} INFO - Setting next_dagrun for movie_pipeline to 2024-12-17 01:30:00+00:00, run_after=2024-12-18 01:30:00+00:00
[2024-12-17T17:26:30.395+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 5.447 seconds
[2024-12-17T17:29:08.522+0530] {processor.py:186} INFO - Started process (PID=79252) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T17:29:08.524+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-17T17:29:08.525+0530] {logging_mixin.py:190} INFO - [2024-12-17T17:29:08.525+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T17:29:08.685+0530] {logging_mixin.py:190} INFO - [2024-12-17T17:29:08.684+0530] {dag.py:4435} INFO - dagrun id: movie_pipeline
[2024-12-17T17:29:08.698+0530] {logging_mixin.py:190} INFO - [2024-12-17T17:29:08.698+0530] {dag.py:4451} INFO - created dagrun <DagRun movie_pipeline @ 2024-12-17 11:59:08.562423+00:00: manual__2024-12-17T11:59:08.562423+00:00, state:running, queued_at: None. externally triggered: False>
[2024-12-17T17:29:08.723+0530] {logging_mixin.py:190} INFO - [2024-12-17T17:29:08.722+0530] {dag.py:4396} INFO - [DAG TEST] starting task_id=scrape_collection_page map_index=-1
[2024-12-17T17:29:08.723+0530] {logging_mixin.py:190} INFO - [2024-12-17T17:29:08.723+0530] {dag.py:4399} INFO - [DAG TEST] running task <TaskInstance: movie_pipeline.scrape_collection_page manual__2024-12-17T11:59:08.562423+00:00 [scheduled]>
[2024-12-17T17:29:13.818+0530] {logging_mixin.py:190} INFO - [2024-12-17T17:29:13.818+0530] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-12-17T17:29:13.877+0530] {logging_mixin.py:190} INFO - [2024-12-17 17:29:13,877] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T11:59:08.562423+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T11:59:08.562423+00:00'
[2024-12-17T17:29:13.877+0530] {logging_mixin.py:190} INFO - [2024-12-17T17:29:13.877+0530] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T11:59:08.562423+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T11:59:08.562423+00:00'
[2024-12-17T17:29:13.880+0530] {logging_mixin.py:190} INFO - Task instance is in running state
[2024-12-17T17:29:13.880+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: queued
[2024-12-17T17:29:13.881+0530] {logging_mixin.py:190} INFO - Current task name:scrape_collection_page state:scheduled start_date:None
[2024-12-17T17:29:13.881+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline and current dag run status:running
[2024-12-17T17:29:13.882+0530] {logging_mixin.py:190} INFO - [2024-12-17T17:29:13.881+0530] {taskinstance.py:731} INFO - ::endgroup::
[2024-12-17T17:29:13.883+0530] {logging_mixin.py:190} INFO - 2024-12-17
[2024-12-17T17:29:13.884+0530] {logging_mixin.py:190} INFO - [2024-12-17 17:29:13,884] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T17:29:13.884+0530] {logging_mixin.py:190} INFO - [2024-12-17T17:29:13.884+0530] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T17:29:13.889+0530] {logging_mixin.py:190} INFO - [2024-12-17T17:29:13.889+0530] {taskinstance.py:340} INFO - ::group::Post task execution logs
[2024-12-17T17:29:13.890+0530] {logging_mixin.py:190} INFO - [2024-12-17T17:29:13.889+0530] {taskinstance.py:352} INFO - Marking task as SUCCESS. dag_id=movie_pipeline, task_id=scrape_collection_page, run_id=manual__2024-12-17T11:59:08.562423+00:00, execution_date=20241217T115908, start_date=, end_date=20241217T115913
[2024-12-17T17:29:13.899+0530] {logging_mixin.py:190} INFO - Task instance in success state
[2024-12-17T17:29:13.899+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: running
[2024-12-17T17:29:13.900+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline queued_at:None
[2024-12-17T17:29:13.900+0530] {logging_mixin.py:190} INFO - Task hostname:Jayeshs-MacBook-Air.local operator:PythonOperator
[2024-12-17T17:29:13.901+0530] {logging_mixin.py:190} INFO - [2024-12-17T17:29:13.900+0530] {dag.py:4410} INFO - [DAG TEST] end task task_id=scrape_collection_page map_index=-1
[2024-12-17T17:29:13.904+0530] {logging_mixin.py:190} INFO - [2024-12-17T17:29:13.904+0530] {dagrun.py:854} INFO - Marking run <DagRun movie_pipeline @ 2024-12-17 11:59:08.562423+00:00: manual__2024-12-17T11:59:08.562423+00:00, state:running, queued_at: None. externally triggered: False> successful
[2024-12-17T17:29:13.905+0530] {logging_mixin.py:190} INFO - Dag run in success state
[2024-12-17T17:29:13.905+0530] {logging_mixin.py:190} INFO - Dag run start:2024-12-17 11:59:08.562423+00:00 end:2024-12-17 11:59:13.905040+00:00
[2024-12-17T17:29:13.906+0530] {logging_mixin.py:190} INFO - [2024-12-17T17:29:13.905+0530] {dagrun.py:905} INFO - DagRun Finished: dag_id=movie_pipeline, execution_date=2024-12-17 11:59:08.562423+00:00, run_id=manual__2024-12-17T11:59:08.562423+00:00, run_start_date=2024-12-17 11:59:08.562423+00:00, run_end_date=2024-12-17 11:59:13.905040+00:00, run_duration=5.342617, state=success, external_trigger=False, run_type=manual, data_interval_start=2024-12-16 11:59:08.562423+00:00, data_interval_end=2024-12-17 11:59:08.562423+00:00, dag_hash=None
[2024-12-17T17:29:13.915+0530] {processor.py:925} INFO - DAG(s) 'movie_pipeline' retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T17:29:13.927+0530] {logging_mixin.py:190} INFO - [2024-12-17T17:29:13.926+0530] {dag.py:3239} INFO - Sync 1 DAGs
[2024-12-17T17:29:13.943+0530] {logging_mixin.py:190} INFO - [2024-12-17T17:29:13.943+0530] {dag.py:4180} INFO - Setting next_dagrun for movie_pipeline to 2024-12-17 01:30:00+00:00, run_after=2024-12-18 01:30:00+00:00
[2024-12-17T17:29:13.961+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 5.443 seconds
[2024-12-17T17:31:54.947+0530] {processor.py:186} INFO - Started process (PID=79445) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T17:31:54.949+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-17T17:31:54.950+0530] {logging_mixin.py:190} INFO - [2024-12-17T17:31:54.950+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T17:31:55.112+0530] {logging_mixin.py:190} INFO - [2024-12-17T17:31:55.112+0530] {dag.py:4435} INFO - dagrun id: movie_pipeline
[2024-12-17T17:31:55.126+0530] {logging_mixin.py:190} INFO - [2024-12-17T17:31:55.125+0530] {dag.py:4451} INFO - created dagrun <DagRun movie_pipeline @ 2024-12-17 12:01:54.988570+00:00: manual__2024-12-17T12:01:54.988570+00:00, state:running, queued_at: None. externally triggered: False>
[2024-12-17T17:31:55.151+0530] {logging_mixin.py:190} INFO - [2024-12-17T17:31:55.150+0530] {dag.py:4396} INFO - [DAG TEST] starting task_id=scrape_collection_page map_index=-1
[2024-12-17T17:31:55.151+0530] {logging_mixin.py:190} INFO - [2024-12-17T17:31:55.151+0530] {dag.py:4399} INFO - [DAG TEST] running task <TaskInstance: movie_pipeline.scrape_collection_page manual__2024-12-17T12:01:54.988570+00:00 [scheduled]>
[2024-12-17T17:32:00.252+0530] {logging_mixin.py:190} INFO - [2024-12-17T17:32:00.251+0530] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-12-17T17:32:00.320+0530] {logging_mixin.py:190} INFO - [2024-12-17 17:32:00,320] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T12:01:54.988570+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T12:01:54.988570+00:00'
[2024-12-17T17:32:00.320+0530] {logging_mixin.py:190} INFO - [2024-12-17T17:32:00.320+0530] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T12:01:54.988570+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T12:01:54.988570+00:00'
[2024-12-17T17:32:00.324+0530] {logging_mixin.py:190} INFO - Task instance is in running state
[2024-12-17T17:32:00.324+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: queued
[2024-12-17T17:32:00.324+0530] {logging_mixin.py:190} INFO - Current task name:scrape_collection_page state:scheduled start_date:None
[2024-12-17T17:32:00.325+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline and current dag run status:running
[2024-12-17T17:32:00.326+0530] {logging_mixin.py:190} INFO - [2024-12-17T17:32:00.326+0530] {taskinstance.py:731} INFO - ::endgroup::
[2024-12-17T17:32:00.328+0530] {logging_mixin.py:190} INFO - 2024-12-17
[2024-12-17T17:32:00.329+0530] {logging_mixin.py:190} INFO - [2024-12-17 17:32:00,329] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T17:32:00.329+0530] {logging_mixin.py:190} INFO - [2024-12-17T17:32:00.329+0530] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T17:32:00.335+0530] {logging_mixin.py:190} INFO - [2024-12-17T17:32:00.335+0530] {taskinstance.py:340} INFO - ::group::Post task execution logs
[2024-12-17T17:32:00.336+0530] {logging_mixin.py:190} INFO - [2024-12-17T17:32:00.336+0530] {taskinstance.py:352} INFO - Marking task as SUCCESS. dag_id=movie_pipeline, task_id=scrape_collection_page, run_id=manual__2024-12-17T12:01:54.988570+00:00, execution_date=20241217T120154, start_date=, end_date=20241217T120200
[2024-12-17T17:32:00.346+0530] {logging_mixin.py:190} INFO - Task instance in success state
[2024-12-17T17:32:00.346+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: running
[2024-12-17T17:32:00.346+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline queued_at:None
[2024-12-17T17:32:00.346+0530] {logging_mixin.py:190} INFO - Task hostname:Jayeshs-MacBook-Air.local operator:PythonOperator
[2024-12-17T17:32:00.347+0530] {logging_mixin.py:190} INFO - [2024-12-17T17:32:00.347+0530] {dag.py:4410} INFO - [DAG TEST] end task task_id=scrape_collection_page map_index=-1
[2024-12-17T17:32:00.351+0530] {logging_mixin.py:190} INFO - [2024-12-17T17:32:00.351+0530] {dagrun.py:854} INFO - Marking run <DagRun movie_pipeline @ 2024-12-17 12:01:54.988570+00:00: manual__2024-12-17T12:01:54.988570+00:00, state:running, queued_at: None. externally triggered: False> successful
[2024-12-17T17:32:00.351+0530] {logging_mixin.py:190} INFO - Dag run in success state
[2024-12-17T17:32:00.351+0530] {logging_mixin.py:190} INFO - Dag run start:2024-12-17 12:01:54.988570+00:00 end:2024-12-17 12:02:00.351710+00:00
[2024-12-17T17:32:00.352+0530] {logging_mixin.py:190} INFO - [2024-12-17T17:32:00.352+0530] {dagrun.py:905} INFO - DagRun Finished: dag_id=movie_pipeline, execution_date=2024-12-17 12:01:54.988570+00:00, run_id=manual__2024-12-17T12:01:54.988570+00:00, run_start_date=2024-12-17 12:01:54.988570+00:00, run_end_date=2024-12-17 12:02:00.351710+00:00, run_duration=5.36314, state=success, external_trigger=False, run_type=manual, data_interval_start=2024-12-16 12:01:54.988570+00:00, data_interval_end=2024-12-17 12:01:54.988570+00:00, dag_hash=None
[2024-12-17T17:32:00.363+0530] {processor.py:925} INFO - DAG(s) 'movie_pipeline' retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T17:32:00.377+0530] {logging_mixin.py:190} INFO - [2024-12-17T17:32:00.376+0530] {dag.py:3239} INFO - Sync 1 DAGs
[2024-12-17T17:32:00.398+0530] {logging_mixin.py:190} INFO - [2024-12-17T17:32:00.397+0530] {dag.py:4180} INFO - Setting next_dagrun for movie_pipeline to 2024-12-17 01:30:00+00:00, run_after=2024-12-18 01:30:00+00:00
[2024-12-17T17:32:00.418+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 5.474 seconds
[2024-12-17T17:34:39.150+0530] {processor.py:186} INFO - Started process (PID=79640) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T17:34:39.152+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-17T17:34:39.153+0530] {logging_mixin.py:190} INFO - [2024-12-17T17:34:39.153+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T17:34:39.331+0530] {logging_mixin.py:190} INFO - [2024-12-17T17:34:39.330+0530] {dag.py:4435} INFO - dagrun id: movie_pipeline
[2024-12-17T17:34:39.345+0530] {logging_mixin.py:190} INFO - [2024-12-17T17:34:39.344+0530] {dag.py:4451} INFO - created dagrun <DagRun movie_pipeline @ 2024-12-17 12:04:39.202674+00:00: manual__2024-12-17T12:04:39.202674+00:00, state:running, queued_at: None. externally triggered: False>
[2024-12-17T17:34:39.370+0530] {logging_mixin.py:190} INFO - [2024-12-17T17:34:39.370+0530] {dag.py:4396} INFO - [DAG TEST] starting task_id=scrape_collection_page map_index=-1
[2024-12-17T17:34:39.371+0530] {logging_mixin.py:190} INFO - [2024-12-17T17:34:39.370+0530] {dag.py:4399} INFO - [DAG TEST] running task <TaskInstance: movie_pipeline.scrape_collection_page manual__2024-12-17T12:04:39.202674+00:00 [scheduled]>
[2024-12-17T17:34:44.468+0530] {logging_mixin.py:190} INFO - [2024-12-17T17:34:44.468+0530] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-12-17T17:34:44.527+0530] {logging_mixin.py:190} INFO - [2024-12-17 17:34:44,527] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T12:04:39.202674+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T12:04:39.202674+00:00'
[2024-12-17T17:34:44.528+0530] {logging_mixin.py:190} INFO - [2024-12-17T17:34:44.527+0530] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T12:04:39.202674+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T12:04:39.202674+00:00'
[2024-12-17T17:34:44.530+0530] {logging_mixin.py:190} INFO - Task instance is in running state
[2024-12-17T17:34:44.530+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: queued
[2024-12-17T17:34:44.531+0530] {logging_mixin.py:190} INFO - Current task name:scrape_collection_page state:scheduled start_date:None
[2024-12-17T17:34:44.531+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline and current dag run status:running
[2024-12-17T17:34:44.531+0530] {logging_mixin.py:190} INFO - [2024-12-17T17:34:44.531+0530] {taskinstance.py:731} INFO - ::endgroup::
[2024-12-17T17:34:44.533+0530] {logging_mixin.py:190} INFO - 2024-12-17
[2024-12-17T17:34:44.534+0530] {logging_mixin.py:190} INFO - [2024-12-17 17:34:44,533] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T17:34:44.534+0530] {logging_mixin.py:190} INFO - [2024-12-17T17:34:44.533+0530] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T17:34:44.539+0530] {logging_mixin.py:190} INFO - [2024-12-17T17:34:44.539+0530] {taskinstance.py:340} INFO - ::group::Post task execution logs
[2024-12-17T17:34:44.540+0530] {logging_mixin.py:190} INFO - [2024-12-17T17:34:44.539+0530] {taskinstance.py:352} INFO - Marking task as SUCCESS. dag_id=movie_pipeline, task_id=scrape_collection_page, run_id=manual__2024-12-17T12:04:39.202674+00:00, execution_date=20241217T120439, start_date=, end_date=20241217T120444
[2024-12-17T17:34:44.549+0530] {logging_mixin.py:190} INFO - Task instance in success state
[2024-12-17T17:34:44.549+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: running
[2024-12-17T17:34:44.549+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline queued_at:None
[2024-12-17T17:34:44.549+0530] {logging_mixin.py:190} INFO - Task hostname:Jayeshs-MacBook-Air.local operator:PythonOperator
[2024-12-17T17:34:44.550+0530] {logging_mixin.py:190} INFO - [2024-12-17T17:34:44.550+0530] {dag.py:4410} INFO - [DAG TEST] end task task_id=scrape_collection_page map_index=-1
[2024-12-17T17:34:44.554+0530] {logging_mixin.py:190} INFO - [2024-12-17T17:34:44.553+0530] {dagrun.py:854} INFO - Marking run <DagRun movie_pipeline @ 2024-12-17 12:04:39.202674+00:00: manual__2024-12-17T12:04:39.202674+00:00, state:running, queued_at: None. externally triggered: False> successful
[2024-12-17T17:34:44.554+0530] {logging_mixin.py:190} INFO - Dag run in success state
[2024-12-17T17:34:44.554+0530] {logging_mixin.py:190} INFO - Dag run start:2024-12-17 12:04:39.202674+00:00 end:2024-12-17 12:04:44.554372+00:00
[2024-12-17T17:34:44.555+0530] {logging_mixin.py:190} INFO - [2024-12-17T17:34:44.554+0530] {dagrun.py:905} INFO - DagRun Finished: dag_id=movie_pipeline, execution_date=2024-12-17 12:04:39.202674+00:00, run_id=manual__2024-12-17T12:04:39.202674+00:00, run_start_date=2024-12-17 12:04:39.202674+00:00, run_end_date=2024-12-17 12:04:44.554372+00:00, run_duration=5.351698, state=success, external_trigger=False, run_type=manual, data_interval_start=2024-12-16 12:04:39.202674+00:00, data_interval_end=2024-12-17 12:04:39.202674+00:00, dag_hash=None
[2024-12-17T17:34:44.565+0530] {processor.py:925} INFO - DAG(s) 'movie_pipeline' retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T17:34:44.577+0530] {logging_mixin.py:190} INFO - [2024-12-17T17:34:44.576+0530] {dag.py:3239} INFO - Sync 1 DAGs
[2024-12-17T17:34:44.594+0530] {logging_mixin.py:190} INFO - [2024-12-17T17:34:44.593+0530] {dag.py:4180} INFO - Setting next_dagrun for movie_pipeline to 2024-12-17 01:30:00+00:00, run_after=2024-12-18 01:30:00+00:00
[2024-12-17T17:34:44.612+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 5.466 seconds
[2024-12-17T17:37:22.824+0530] {processor.py:186} INFO - Started process (PID=79831) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T17:37:22.825+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-17T17:37:22.827+0530] {logging_mixin.py:190} INFO - [2024-12-17T17:37:22.826+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T17:37:22.990+0530] {logging_mixin.py:190} INFO - [2024-12-17T17:37:22.990+0530] {dag.py:4435} INFO - dagrun id: movie_pipeline
[2024-12-17T17:37:23.005+0530] {logging_mixin.py:190} INFO - [2024-12-17T17:37:23.004+0530] {dag.py:4451} INFO - created dagrun <DagRun movie_pipeline @ 2024-12-17 12:07:22.865564+00:00: manual__2024-12-17T12:07:22.865564+00:00, state:running, queued_at: None. externally triggered: False>
[2024-12-17T17:37:23.030+0530] {logging_mixin.py:190} INFO - [2024-12-17T17:37:23.030+0530] {dag.py:4396} INFO - [DAG TEST] starting task_id=scrape_collection_page map_index=-1
[2024-12-17T17:37:23.031+0530] {logging_mixin.py:190} INFO - [2024-12-17T17:37:23.030+0530] {dag.py:4399} INFO - [DAG TEST] running task <TaskInstance: movie_pipeline.scrape_collection_page manual__2024-12-17T12:07:22.865564+00:00 [scheduled]>
[2024-12-17T17:37:28.128+0530] {logging_mixin.py:190} INFO - [2024-12-17T17:37:28.127+0530] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-12-17T17:37:28.195+0530] {logging_mixin.py:190} INFO - [2024-12-17 17:37:28,195] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T12:07:22.865564+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T12:07:22.865564+00:00'
[2024-12-17T17:37:28.195+0530] {logging_mixin.py:190} INFO - [2024-12-17T17:37:28.195+0530] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T12:07:22.865564+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T12:07:22.865564+00:00'
[2024-12-17T17:37:28.199+0530] {logging_mixin.py:190} INFO - Task instance is in running state
[2024-12-17T17:37:28.199+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: queued
[2024-12-17T17:37:28.200+0530] {logging_mixin.py:190} INFO - Current task name:scrape_collection_page state:scheduled start_date:None
[2024-12-17T17:37:28.200+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline and current dag run status:running
[2024-12-17T17:37:28.200+0530] {logging_mixin.py:190} INFO - [2024-12-17T17:37:28.200+0530] {taskinstance.py:731} INFO - ::endgroup::
[2024-12-17T17:37:28.202+0530] {logging_mixin.py:190} INFO - 2024-12-17
[2024-12-17T17:37:28.203+0530] {logging_mixin.py:190} INFO - [2024-12-17 17:37:28,203] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T17:37:28.204+0530] {logging_mixin.py:190} INFO - [2024-12-17T17:37:28.203+0530] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T17:37:28.210+0530] {logging_mixin.py:190} INFO - [2024-12-17T17:37:28.210+0530] {taskinstance.py:340} INFO - ::group::Post task execution logs
[2024-12-17T17:37:28.211+0530] {logging_mixin.py:190} INFO - [2024-12-17T17:37:28.210+0530] {taskinstance.py:352} INFO - Marking task as SUCCESS. dag_id=movie_pipeline, task_id=scrape_collection_page, run_id=manual__2024-12-17T12:07:22.865564+00:00, execution_date=20241217T120722, start_date=, end_date=20241217T120728
[2024-12-17T17:37:28.220+0530] {logging_mixin.py:190} INFO - Task instance in success state
[2024-12-17T17:37:28.220+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: running
[2024-12-17T17:37:28.221+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline queued_at:None
[2024-12-17T17:37:28.221+0530] {logging_mixin.py:190} INFO - Task hostname:Jayeshs-MacBook-Air.local operator:PythonOperator
[2024-12-17T17:37:28.221+0530] {logging_mixin.py:190} INFO - [2024-12-17T17:37:28.221+0530] {dag.py:4410} INFO - [DAG TEST] end task task_id=scrape_collection_page map_index=-1
[2024-12-17T17:37:28.225+0530] {logging_mixin.py:190} INFO - [2024-12-17T17:37:28.225+0530] {dagrun.py:854} INFO - Marking run <DagRun movie_pipeline @ 2024-12-17 12:07:22.865564+00:00: manual__2024-12-17T12:07:22.865564+00:00, state:running, queued_at: None. externally triggered: False> successful
[2024-12-17T17:37:28.225+0530] {logging_mixin.py:190} INFO - Dag run in success state
[2024-12-17T17:37:28.225+0530] {logging_mixin.py:190} INFO - Dag run start:2024-12-17 12:07:22.865564+00:00 end:2024-12-17 12:07:28.225509+00:00
[2024-12-17T17:37:28.226+0530] {logging_mixin.py:190} INFO - [2024-12-17T17:37:28.226+0530] {dagrun.py:905} INFO - DagRun Finished: dag_id=movie_pipeline, execution_date=2024-12-17 12:07:22.865564+00:00, run_id=manual__2024-12-17T12:07:22.865564+00:00, run_start_date=2024-12-17 12:07:22.865564+00:00, run_end_date=2024-12-17 12:07:28.225509+00:00, run_duration=5.359945, state=success, external_trigger=False, run_type=manual, data_interval_start=2024-12-16 12:07:22.865564+00:00, data_interval_end=2024-12-17 12:07:22.865564+00:00, dag_hash=None
[2024-12-17T17:37:28.240+0530] {processor.py:925} INFO - DAG(s) 'movie_pipeline' retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T17:37:28.252+0530] {logging_mixin.py:190} INFO - [2024-12-17T17:37:28.252+0530] {dag.py:3239} INFO - Sync 1 DAGs
[2024-12-17T17:37:28.269+0530] {logging_mixin.py:190} INFO - [2024-12-17T17:37:28.269+0530] {dag.py:4180} INFO - Setting next_dagrun for movie_pipeline to 2024-12-17 01:30:00+00:00, run_after=2024-12-18 01:30:00+00:00
[2024-12-17T17:37:28.287+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 5.467 seconds
[2024-12-17T17:40:11.578+0530] {processor.py:186} INFO - Started process (PID=80031) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T17:40:11.579+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-17T17:40:11.581+0530] {logging_mixin.py:190} INFO - [2024-12-17T17:40:11.580+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T17:40:11.744+0530] {logging_mixin.py:190} INFO - [2024-12-17T17:40:11.744+0530] {dag.py:4435} INFO - dagrun id: movie_pipeline
[2024-12-17T17:40:11.758+0530] {logging_mixin.py:190} INFO - [2024-12-17T17:40:11.757+0530] {dag.py:4451} INFO - created dagrun <DagRun movie_pipeline @ 2024-12-17 12:10:11.618746+00:00: manual__2024-12-17T12:10:11.618746+00:00, state:running, queued_at: None. externally triggered: False>
[2024-12-17T17:40:11.785+0530] {logging_mixin.py:190} INFO - [2024-12-17T17:40:11.784+0530] {dag.py:4396} INFO - [DAG TEST] starting task_id=scrape_collection_page map_index=-1
[2024-12-17T17:40:11.786+0530] {logging_mixin.py:190} INFO - [2024-12-17T17:40:11.785+0530] {dag.py:4399} INFO - [DAG TEST] running task <TaskInstance: movie_pipeline.scrape_collection_page manual__2024-12-17T12:10:11.618746+00:00 [scheduled]>
[2024-12-17T17:40:16.884+0530] {logging_mixin.py:190} INFO - [2024-12-17T17:40:16.884+0530] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-12-17T17:40:16.942+0530] {logging_mixin.py:190} INFO - [2024-12-17 17:40:16,942] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T12:10:11.618746+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T12:10:11.618746+00:00'
[2024-12-17T17:40:16.942+0530] {logging_mixin.py:190} INFO - [2024-12-17T17:40:16.942+0530] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T12:10:11.618746+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T12:10:11.618746+00:00'
[2024-12-17T17:40:16.945+0530] {logging_mixin.py:190} INFO - Task instance is in running state
[2024-12-17T17:40:16.945+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: queued
[2024-12-17T17:40:16.945+0530] {logging_mixin.py:190} INFO - Current task name:scrape_collection_page state:scheduled start_date:None
[2024-12-17T17:40:16.945+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline and current dag run status:running
[2024-12-17T17:40:16.946+0530] {logging_mixin.py:190} INFO - [2024-12-17T17:40:16.946+0530] {taskinstance.py:731} INFO - ::endgroup::
[2024-12-17T17:40:16.948+0530] {logging_mixin.py:190} INFO - 2024-12-17
[2024-12-17T17:40:16.948+0530] {logging_mixin.py:190} INFO - [2024-12-17 17:40:16,948] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T17:40:16.949+0530] {logging_mixin.py:190} INFO - [2024-12-17T17:40:16.948+0530] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T17:40:16.954+0530] {logging_mixin.py:190} INFO - [2024-12-17T17:40:16.954+0530] {taskinstance.py:340} INFO - ::group::Post task execution logs
[2024-12-17T17:40:16.954+0530] {logging_mixin.py:190} INFO - [2024-12-17T17:40:16.954+0530] {taskinstance.py:352} INFO - Marking task as SUCCESS. dag_id=movie_pipeline, task_id=scrape_collection_page, run_id=manual__2024-12-17T12:10:11.618746+00:00, execution_date=20241217T121011, start_date=, end_date=20241217T121016
[2024-12-17T17:40:16.964+0530] {logging_mixin.py:190} INFO - Task instance in success state
[2024-12-17T17:40:16.964+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: running
[2024-12-17T17:40:16.964+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline queued_at:None
[2024-12-17T17:40:16.964+0530] {logging_mixin.py:190} INFO - Task hostname:Jayeshs-MacBook-Air.local operator:PythonOperator
[2024-12-17T17:40:16.965+0530] {logging_mixin.py:190} INFO - [2024-12-17T17:40:16.965+0530] {dag.py:4410} INFO - [DAG TEST] end task task_id=scrape_collection_page map_index=-1
[2024-12-17T17:40:16.969+0530] {logging_mixin.py:190} INFO - [2024-12-17T17:40:16.968+0530] {dagrun.py:854} INFO - Marking run <DagRun movie_pipeline @ 2024-12-17 12:10:11.618746+00:00: manual__2024-12-17T12:10:11.618746+00:00, state:running, queued_at: None. externally triggered: False> successful
[2024-12-17T17:40:16.969+0530] {logging_mixin.py:190} INFO - Dag run in success state
[2024-12-17T17:40:16.969+0530] {logging_mixin.py:190} INFO - Dag run start:2024-12-17 12:10:11.618746+00:00 end:2024-12-17 12:10:16.969319+00:00
[2024-12-17T17:40:16.969+0530] {logging_mixin.py:190} INFO - [2024-12-17T17:40:16.969+0530] {dagrun.py:905} INFO - DagRun Finished: dag_id=movie_pipeline, execution_date=2024-12-17 12:10:11.618746+00:00, run_id=manual__2024-12-17T12:10:11.618746+00:00, run_start_date=2024-12-17 12:10:11.618746+00:00, run_end_date=2024-12-17 12:10:16.969319+00:00, run_duration=5.350573, state=success, external_trigger=False, run_type=manual, data_interval_start=2024-12-16 12:10:11.618746+00:00, data_interval_end=2024-12-17 12:10:11.618746+00:00, dag_hash=None
[2024-12-17T17:40:16.979+0530] {processor.py:925} INFO - DAG(s) 'movie_pipeline' retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T17:40:16.991+0530] {logging_mixin.py:190} INFO - [2024-12-17T17:40:16.990+0530] {dag.py:3239} INFO - Sync 1 DAGs
[2024-12-17T17:40:17.008+0530] {logging_mixin.py:190} INFO - [2024-12-17T17:40:17.007+0530] {dag.py:4180} INFO - Setting next_dagrun for movie_pipeline to 2024-12-17 01:30:00+00:00, run_after=2024-12-18 01:30:00+00:00
[2024-12-17T17:40:17.026+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 5.451 seconds
[2024-12-17T17:42:57.715+0530] {processor.py:186} INFO - Started process (PID=80220) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T17:42:57.716+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-17T17:42:57.717+0530] {logging_mixin.py:190} INFO - [2024-12-17T17:42:57.717+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T17:42:57.900+0530] {logging_mixin.py:190} INFO - [2024-12-17T17:42:57.900+0530] {dag.py:4435} INFO - dagrun id: movie_pipeline
[2024-12-17T17:42:57.915+0530] {logging_mixin.py:190} INFO - [2024-12-17T17:42:57.914+0530] {dag.py:4451} INFO - created dagrun <DagRun movie_pipeline @ 2024-12-17 12:12:57.755758+00:00: manual__2024-12-17T12:12:57.755758+00:00, state:running, queued_at: None. externally triggered: False>
[2024-12-17T17:42:57.940+0530] {logging_mixin.py:190} INFO - [2024-12-17T17:42:57.939+0530] {dag.py:4396} INFO - [DAG TEST] starting task_id=scrape_collection_page map_index=-1
[2024-12-17T17:42:57.940+0530] {logging_mixin.py:190} INFO - [2024-12-17T17:42:57.940+0530] {dag.py:4399} INFO - [DAG TEST] running task <TaskInstance: movie_pipeline.scrape_collection_page manual__2024-12-17T12:12:57.755758+00:00 [scheduled]>
[2024-12-17T17:43:03.035+0530] {logging_mixin.py:190} INFO - [2024-12-17T17:43:03.035+0530] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-12-17T17:43:03.092+0530] {logging_mixin.py:190} INFO - [2024-12-17 17:43:03,092] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T12:12:57.755758+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T12:12:57.755758+00:00'
[2024-12-17T17:43:03.093+0530] {logging_mixin.py:190} INFO - [2024-12-17T17:43:03.092+0530] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T12:12:57.755758+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T12:12:57.755758+00:00'
[2024-12-17T17:43:03.096+0530] {logging_mixin.py:190} INFO - Task instance is in running state
[2024-12-17T17:43:03.096+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: queued
[2024-12-17T17:43:03.096+0530] {logging_mixin.py:190} INFO - Current task name:scrape_collection_page state:scheduled start_date:None
[2024-12-17T17:43:03.096+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline and current dag run status:running
[2024-12-17T17:43:03.097+0530] {logging_mixin.py:190} INFO - [2024-12-17T17:43:03.097+0530] {taskinstance.py:731} INFO - ::endgroup::
[2024-12-17T17:43:03.099+0530] {logging_mixin.py:190} INFO - 2024-12-17
[2024-12-17T17:43:03.099+0530] {logging_mixin.py:190} INFO - [2024-12-17 17:43:03,099] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T17:43:03.100+0530] {logging_mixin.py:190} INFO - [2024-12-17T17:43:03.099+0530] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T17:43:03.105+0530] {logging_mixin.py:190} INFO - [2024-12-17T17:43:03.104+0530] {taskinstance.py:340} INFO - ::group::Post task execution logs
[2024-12-17T17:43:03.105+0530] {logging_mixin.py:190} INFO - [2024-12-17T17:43:03.105+0530] {taskinstance.py:352} INFO - Marking task as SUCCESS. dag_id=movie_pipeline, task_id=scrape_collection_page, run_id=manual__2024-12-17T12:12:57.755758+00:00, execution_date=20241217T121257, start_date=, end_date=20241217T121303
[2024-12-17T17:43:03.115+0530] {logging_mixin.py:190} INFO - Task instance in success state
[2024-12-17T17:43:03.115+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: running
[2024-12-17T17:43:03.115+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline queued_at:None
[2024-12-17T17:43:03.115+0530] {logging_mixin.py:190} INFO - Task hostname:Jayeshs-MacBook-Air.local operator:PythonOperator
[2024-12-17T17:43:03.116+0530] {logging_mixin.py:190} INFO - [2024-12-17T17:43:03.115+0530] {dag.py:4410} INFO - [DAG TEST] end task task_id=scrape_collection_page map_index=-1
[2024-12-17T17:43:03.119+0530] {logging_mixin.py:190} INFO - [2024-12-17T17:43:03.119+0530] {dagrun.py:854} INFO - Marking run <DagRun movie_pipeline @ 2024-12-17 12:12:57.755758+00:00: manual__2024-12-17T12:12:57.755758+00:00, state:running, queued_at: None. externally triggered: False> successful
[2024-12-17T17:43:03.120+0530] {logging_mixin.py:190} INFO - Dag run in success state
[2024-12-17T17:43:03.120+0530] {logging_mixin.py:190} INFO - Dag run start:2024-12-17 12:12:57.755758+00:00 end:2024-12-17 12:13:03.120097+00:00
[2024-12-17T17:43:03.120+0530] {logging_mixin.py:190} INFO - [2024-12-17T17:43:03.120+0530] {dagrun.py:905} INFO - DagRun Finished: dag_id=movie_pipeline, execution_date=2024-12-17 12:12:57.755758+00:00, run_id=manual__2024-12-17T12:12:57.755758+00:00, run_start_date=2024-12-17 12:12:57.755758+00:00, run_end_date=2024-12-17 12:13:03.120097+00:00, run_duration=5.364339, state=success, external_trigger=False, run_type=manual, data_interval_start=2024-12-16 12:12:57.755758+00:00, data_interval_end=2024-12-17 12:12:57.755758+00:00, dag_hash=None
[2024-12-17T17:43:03.130+0530] {processor.py:925} INFO - DAG(s) 'movie_pipeline' retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T17:43:03.142+0530] {logging_mixin.py:190} INFO - [2024-12-17T17:43:03.142+0530] {dag.py:3239} INFO - Sync 1 DAGs
[2024-12-17T17:43:03.160+0530] {logging_mixin.py:190} INFO - [2024-12-17T17:43:03.160+0530] {dag.py:4180} INFO - Setting next_dagrun for movie_pipeline to 2024-12-17 01:30:00+00:00, run_after=2024-12-18 01:30:00+00:00
[2024-12-17T17:43:03.178+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 5.467 seconds
[2024-12-17T17:45:45.354+0530] {processor.py:186} INFO - Started process (PID=80417) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T17:45:45.355+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-17T17:45:45.356+0530] {logging_mixin.py:190} INFO - [2024-12-17T17:45:45.356+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T17:45:45.520+0530] {logging_mixin.py:190} INFO - [2024-12-17T17:45:45.520+0530] {dag.py:4435} INFO - dagrun id: movie_pipeline
[2024-12-17T17:45:45.535+0530] {logging_mixin.py:190} INFO - [2024-12-17T17:45:45.535+0530] {dag.py:4451} INFO - created dagrun <DagRun movie_pipeline @ 2024-12-17 12:15:45.394829+00:00: manual__2024-12-17T12:15:45.394829+00:00, state:running, queued_at: None. externally triggered: False>
[2024-12-17T17:45:45.560+0530] {logging_mixin.py:190} INFO - [2024-12-17T17:45:45.560+0530] {dag.py:4396} INFO - [DAG TEST] starting task_id=scrape_collection_page map_index=-1
[2024-12-17T17:45:45.561+0530] {logging_mixin.py:190} INFO - [2024-12-17T17:45:45.560+0530] {dag.py:4399} INFO - [DAG TEST] running task <TaskInstance: movie_pipeline.scrape_collection_page manual__2024-12-17T12:15:45.394829+00:00 [scheduled]>
[2024-12-17T17:45:50.659+0530] {logging_mixin.py:190} INFO - [2024-12-17T17:45:50.658+0530] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-12-17T17:45:50.719+0530] {logging_mixin.py:190} INFO - [2024-12-17 17:45:50,719] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T12:15:45.394829+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T12:15:45.394829+00:00'
[2024-12-17T17:45:50.720+0530] {logging_mixin.py:190} INFO - [2024-12-17T17:45:50.719+0530] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T12:15:45.394829+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T12:15:45.394829+00:00'
[2024-12-17T17:45:50.723+0530] {logging_mixin.py:190} INFO - Task instance is in running state
[2024-12-17T17:45:50.723+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: queued
[2024-12-17T17:45:50.723+0530] {logging_mixin.py:190} INFO - Current task name:scrape_collection_page state:scheduled start_date:None
[2024-12-17T17:45:50.723+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline and current dag run status:running
[2024-12-17T17:45:50.724+0530] {logging_mixin.py:190} INFO - [2024-12-17T17:45:50.724+0530] {taskinstance.py:731} INFO - ::endgroup::
[2024-12-17T17:45:50.725+0530] {logging_mixin.py:190} INFO - 2024-12-17
[2024-12-17T17:45:50.726+0530] {logging_mixin.py:190} INFO - [2024-12-17 17:45:50,726] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T17:45:50.726+0530] {logging_mixin.py:190} INFO - [2024-12-17T17:45:50.726+0530] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T17:45:50.731+0530] {logging_mixin.py:190} INFO - [2024-12-17T17:45:50.731+0530] {taskinstance.py:340} INFO - ::group::Post task execution logs
[2024-12-17T17:45:50.732+0530] {logging_mixin.py:190} INFO - [2024-12-17T17:45:50.731+0530] {taskinstance.py:352} INFO - Marking task as SUCCESS. dag_id=movie_pipeline, task_id=scrape_collection_page, run_id=manual__2024-12-17T12:15:45.394829+00:00, execution_date=20241217T121545, start_date=, end_date=20241217T121550
[2024-12-17T17:45:50.741+0530] {logging_mixin.py:190} INFO - Task instance in success state
[2024-12-17T17:45:50.742+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: running
[2024-12-17T17:45:50.742+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline queued_at:None
[2024-12-17T17:45:50.742+0530] {logging_mixin.py:190} INFO - Task hostname:Jayeshs-MacBook-Air.local operator:PythonOperator
[2024-12-17T17:45:50.742+0530] {logging_mixin.py:190} INFO - [2024-12-17T17:45:50.742+0530] {dag.py:4410} INFO - [DAG TEST] end task task_id=scrape_collection_page map_index=-1
[2024-12-17T17:45:50.746+0530] {logging_mixin.py:190} INFO - [2024-12-17T17:45:50.746+0530] {dagrun.py:854} INFO - Marking run <DagRun movie_pipeline @ 2024-12-17 12:15:45.394829+00:00: manual__2024-12-17T12:15:45.394829+00:00, state:running, queued_at: None. externally triggered: False> successful
[2024-12-17T17:45:50.746+0530] {logging_mixin.py:190} INFO - Dag run in success state
[2024-12-17T17:45:50.747+0530] {logging_mixin.py:190} INFO - Dag run start:2024-12-17 12:15:45.394829+00:00 end:2024-12-17 12:15:50.746884+00:00
[2024-12-17T17:45:50.747+0530] {logging_mixin.py:190} INFO - [2024-12-17T17:45:50.747+0530] {dagrun.py:905} INFO - DagRun Finished: dag_id=movie_pipeline, execution_date=2024-12-17 12:15:45.394829+00:00, run_id=manual__2024-12-17T12:15:45.394829+00:00, run_start_date=2024-12-17 12:15:45.394829+00:00, run_end_date=2024-12-17 12:15:50.746884+00:00, run_duration=5.352055, state=success, external_trigger=False, run_type=manual, data_interval_start=2024-12-16 12:15:45.394829+00:00, data_interval_end=2024-12-17 12:15:45.394829+00:00, dag_hash=None
[2024-12-17T17:45:50.756+0530] {processor.py:925} INFO - DAG(s) 'movie_pipeline' retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T17:45:50.768+0530] {logging_mixin.py:190} INFO - [2024-12-17T17:45:50.768+0530] {dag.py:3239} INFO - Sync 1 DAGs
[2024-12-17T17:45:50.785+0530] {logging_mixin.py:190} INFO - [2024-12-17T17:45:50.785+0530] {dag.py:4180} INFO - Setting next_dagrun for movie_pipeline to 2024-12-17 01:30:00+00:00, run_after=2024-12-18 01:30:00+00:00
[2024-12-17T17:45:50.803+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 5.452 seconds
[2024-12-17T17:48:32.460+0530] {processor.py:186} INFO - Started process (PID=80617) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T17:48:32.461+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-17T17:48:32.462+0530] {logging_mixin.py:190} INFO - [2024-12-17T17:48:32.462+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T17:48:32.623+0530] {logging_mixin.py:190} INFO - [2024-12-17T17:48:32.623+0530] {dag.py:4435} INFO - dagrun id: movie_pipeline
[2024-12-17T17:48:32.637+0530] {logging_mixin.py:190} INFO - [2024-12-17T17:48:32.636+0530] {dag.py:4451} INFO - created dagrun <DagRun movie_pipeline @ 2024-12-17 12:18:32.501126+00:00: manual__2024-12-17T12:18:32.501126+00:00, state:running, queued_at: None. externally triggered: False>
[2024-12-17T17:48:32.662+0530] {logging_mixin.py:190} INFO - [2024-12-17T17:48:32.662+0530] {dag.py:4396} INFO - [DAG TEST] starting task_id=scrape_collection_page map_index=-1
[2024-12-17T17:48:32.663+0530] {logging_mixin.py:190} INFO - [2024-12-17T17:48:32.662+0530] {dag.py:4399} INFO - [DAG TEST] running task <TaskInstance: movie_pipeline.scrape_collection_page manual__2024-12-17T12:18:32.501126+00:00 [scheduled]>
[2024-12-17T17:48:37.756+0530] {logging_mixin.py:190} INFO - [2024-12-17T17:48:37.756+0530] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-12-17T17:48:37.814+0530] {logging_mixin.py:190} INFO - [2024-12-17 17:48:37,814] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T12:18:32.501126+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T12:18:32.501126+00:00'
[2024-12-17T17:48:37.815+0530] {logging_mixin.py:190} INFO - [2024-12-17T17:48:37.814+0530] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T12:18:32.501126+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T12:18:32.501126+00:00'
[2024-12-17T17:48:37.818+0530] {logging_mixin.py:190} INFO - Task instance is in running state
[2024-12-17T17:48:37.818+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: queued
[2024-12-17T17:48:37.818+0530] {logging_mixin.py:190} INFO - Current task name:scrape_collection_page state:scheduled start_date:None
[2024-12-17T17:48:37.818+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline and current dag run status:running
[2024-12-17T17:48:37.819+0530] {logging_mixin.py:190} INFO - [2024-12-17T17:48:37.818+0530] {taskinstance.py:731} INFO - ::endgroup::
[2024-12-17T17:48:37.821+0530] {logging_mixin.py:190} INFO - 2024-12-17
[2024-12-17T17:48:37.821+0530] {logging_mixin.py:190} INFO - [2024-12-17 17:48:37,821] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T17:48:37.821+0530] {logging_mixin.py:190} INFO - [2024-12-17T17:48:37.821+0530] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T17:48:37.826+0530] {logging_mixin.py:190} INFO - [2024-12-17T17:48:37.826+0530] {taskinstance.py:340} INFO - ::group::Post task execution logs
[2024-12-17T17:48:37.827+0530] {logging_mixin.py:190} INFO - [2024-12-17T17:48:37.827+0530] {taskinstance.py:352} INFO - Marking task as SUCCESS. dag_id=movie_pipeline, task_id=scrape_collection_page, run_id=manual__2024-12-17T12:18:32.501126+00:00, execution_date=20241217T121832, start_date=, end_date=20241217T121837
[2024-12-17T17:48:37.836+0530] {logging_mixin.py:190} INFO - Task instance in success state
[2024-12-17T17:48:37.836+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: running
[2024-12-17T17:48:37.836+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline queued_at:None
[2024-12-17T17:48:37.836+0530] {logging_mixin.py:190} INFO - Task hostname:Jayeshs-MacBook-Air.local operator:PythonOperator
[2024-12-17T17:48:37.837+0530] {logging_mixin.py:190} INFO - [2024-12-17T17:48:37.837+0530] {dag.py:4410} INFO - [DAG TEST] end task task_id=scrape_collection_page map_index=-1
[2024-12-17T17:48:37.841+0530] {logging_mixin.py:190} INFO - [2024-12-17T17:48:37.840+0530] {dagrun.py:854} INFO - Marking run <DagRun movie_pipeline @ 2024-12-17 12:18:32.501126+00:00: manual__2024-12-17T12:18:32.501126+00:00, state:running, queued_at: None. externally triggered: False> successful
[2024-12-17T17:48:37.841+0530] {logging_mixin.py:190} INFO - Dag run in success state
[2024-12-17T17:48:37.841+0530] {logging_mixin.py:190} INFO - Dag run start:2024-12-17 12:18:32.501126+00:00 end:2024-12-17 12:18:37.841352+00:00
[2024-12-17T17:48:37.842+0530] {logging_mixin.py:190} INFO - [2024-12-17T17:48:37.841+0530] {dagrun.py:905} INFO - DagRun Finished: dag_id=movie_pipeline, execution_date=2024-12-17 12:18:32.501126+00:00, run_id=manual__2024-12-17T12:18:32.501126+00:00, run_start_date=2024-12-17 12:18:32.501126+00:00, run_end_date=2024-12-17 12:18:37.841352+00:00, run_duration=5.340226, state=success, external_trigger=False, run_type=manual, data_interval_start=2024-12-16 12:18:32.501126+00:00, data_interval_end=2024-12-17 12:18:32.501126+00:00, dag_hash=None
[2024-12-17T17:48:37.851+0530] {processor.py:925} INFO - DAG(s) 'movie_pipeline' retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T17:48:37.863+0530] {logging_mixin.py:190} INFO - [2024-12-17T17:48:37.863+0530] {dag.py:3239} INFO - Sync 1 DAGs
[2024-12-17T17:48:37.880+0530] {logging_mixin.py:190} INFO - [2024-12-17T17:48:37.879+0530] {dag.py:4180} INFO - Setting next_dagrun for movie_pipeline to 2024-12-17 01:30:00+00:00, run_after=2024-12-18 01:30:00+00:00
[2024-12-17T17:48:37.897+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 5.441 seconds
[2024-12-17T17:51:18.737+0530] {processor.py:186} INFO - Started process (PID=80805) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T17:51:18.739+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-17T17:51:18.740+0530] {logging_mixin.py:190} INFO - [2024-12-17T17:51:18.740+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T17:51:18.908+0530] {logging_mixin.py:190} INFO - [2024-12-17T17:51:18.908+0530] {dag.py:4435} INFO - dagrun id: movie_pipeline
[2024-12-17T17:51:18.922+0530] {logging_mixin.py:190} INFO - [2024-12-17T17:51:18.921+0530] {dag.py:4451} INFO - created dagrun <DagRun movie_pipeline @ 2024-12-17 12:21:18.781610+00:00: manual__2024-12-17T12:21:18.781610+00:00, state:running, queued_at: None. externally triggered: False>
[2024-12-17T17:51:18.948+0530] {logging_mixin.py:190} INFO - [2024-12-17T17:51:18.947+0530] {dag.py:4396} INFO - [DAG TEST] starting task_id=scrape_collection_page map_index=-1
[2024-12-17T17:51:18.948+0530] {logging_mixin.py:190} INFO - [2024-12-17T17:51:18.948+0530] {dag.py:4399} INFO - [DAG TEST] running task <TaskInstance: movie_pipeline.scrape_collection_page manual__2024-12-17T12:21:18.781610+00:00 [scheduled]>
[2024-12-17T17:51:24.048+0530] {logging_mixin.py:190} INFO - [2024-12-17T17:51:24.048+0530] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-12-17T17:51:24.107+0530] {logging_mixin.py:190} INFO - [2024-12-17 17:51:24,106] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T12:21:18.781610+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T12:21:18.781610+00:00'
[2024-12-17T17:51:24.107+0530] {logging_mixin.py:190} INFO - [2024-12-17T17:51:24.106+0530] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T12:21:18.781610+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T12:21:18.781610+00:00'
[2024-12-17T17:51:24.110+0530] {logging_mixin.py:190} INFO - Task instance is in running state
[2024-12-17T17:51:24.111+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: queued
[2024-12-17T17:51:24.111+0530] {logging_mixin.py:190} INFO - Current task name:scrape_collection_page state:scheduled start_date:None
[2024-12-17T17:51:24.111+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline and current dag run status:running
[2024-12-17T17:51:24.112+0530] {logging_mixin.py:190} INFO - [2024-12-17T17:51:24.111+0530] {taskinstance.py:731} INFO - ::endgroup::
[2024-12-17T17:51:24.113+0530] {logging_mixin.py:190} INFO - 2024-12-17
[2024-12-17T17:51:24.114+0530] {logging_mixin.py:190} INFO - [2024-12-17 17:51:24,114] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T17:51:24.114+0530] {logging_mixin.py:190} INFO - [2024-12-17T17:51:24.114+0530] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T17:51:24.119+0530] {logging_mixin.py:190} INFO - [2024-12-17T17:51:24.119+0530] {taskinstance.py:340} INFO - ::group::Post task execution logs
[2024-12-17T17:51:24.120+0530] {logging_mixin.py:190} INFO - [2024-12-17T17:51:24.119+0530] {taskinstance.py:352} INFO - Marking task as SUCCESS. dag_id=movie_pipeline, task_id=scrape_collection_page, run_id=manual__2024-12-17T12:21:18.781610+00:00, execution_date=20241217T122118, start_date=, end_date=20241217T122124
[2024-12-17T17:51:24.129+0530] {logging_mixin.py:190} INFO - Task instance in success state
[2024-12-17T17:51:24.129+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: running
[2024-12-17T17:51:24.129+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline queued_at:None
[2024-12-17T17:51:24.130+0530] {logging_mixin.py:190} INFO - Task hostname:Jayeshs-MacBook-Air.local operator:PythonOperator
[2024-12-17T17:51:24.130+0530] {logging_mixin.py:190} INFO - [2024-12-17T17:51:24.130+0530] {dag.py:4410} INFO - [DAG TEST] end task task_id=scrape_collection_page map_index=-1
[2024-12-17T17:51:24.134+0530] {logging_mixin.py:190} INFO - [2024-12-17T17:51:24.133+0530] {dagrun.py:854} INFO - Marking run <DagRun movie_pipeline @ 2024-12-17 12:21:18.781610+00:00: manual__2024-12-17T12:21:18.781610+00:00, state:running, queued_at: None. externally triggered: False> successful
[2024-12-17T17:51:24.134+0530] {logging_mixin.py:190} INFO - Dag run in success state
[2024-12-17T17:51:24.134+0530] {logging_mixin.py:190} INFO - Dag run start:2024-12-17 12:21:18.781610+00:00 end:2024-12-17 12:21:24.134376+00:00
[2024-12-17T17:51:24.135+0530] {logging_mixin.py:190} INFO - [2024-12-17T17:51:24.134+0530] {dagrun.py:905} INFO - DagRun Finished: dag_id=movie_pipeline, execution_date=2024-12-17 12:21:18.781610+00:00, run_id=manual__2024-12-17T12:21:18.781610+00:00, run_start_date=2024-12-17 12:21:18.781610+00:00, run_end_date=2024-12-17 12:21:24.134376+00:00, run_duration=5.352766, state=success, external_trigger=False, run_type=manual, data_interval_start=2024-12-16 12:21:18.781610+00:00, data_interval_end=2024-12-17 12:21:18.781610+00:00, dag_hash=None
[2024-12-17T17:51:24.144+0530] {processor.py:925} INFO - DAG(s) 'movie_pipeline' retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T17:51:24.156+0530] {logging_mixin.py:190} INFO - [2024-12-17T17:51:24.156+0530] {dag.py:3239} INFO - Sync 1 DAGs
[2024-12-17T17:51:24.175+0530] {logging_mixin.py:190} INFO - [2024-12-17T17:51:24.174+0530] {dag.py:4180} INFO - Setting next_dagrun for movie_pipeline to 2024-12-17 01:30:00+00:00, run_after=2024-12-18 01:30:00+00:00
[2024-12-17T17:51:24.192+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 5.459 seconds
[2024-12-17T17:54:04.586+0530] {processor.py:186} INFO - Started process (PID=81014) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T17:54:04.587+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-17T17:54:04.588+0530] {logging_mixin.py:190} INFO - [2024-12-17T17:54:04.588+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T17:54:04.758+0530] {logging_mixin.py:190} INFO - [2024-12-17T17:54:04.758+0530] {dag.py:4435} INFO - dagrun id: movie_pipeline
[2024-12-17T17:54:04.774+0530] {logging_mixin.py:190} INFO - [2024-12-17T17:54:04.773+0530] {dag.py:4451} INFO - created dagrun <DagRun movie_pipeline @ 2024-12-17 12:24:04.631763+00:00: manual__2024-12-17T12:24:04.631763+00:00, state:running, queued_at: None. externally triggered: False>
[2024-12-17T17:54:04.799+0530] {logging_mixin.py:190} INFO - [2024-12-17T17:54:04.798+0530] {dag.py:4396} INFO - [DAG TEST] starting task_id=scrape_collection_page map_index=-1
[2024-12-17T17:54:04.799+0530] {logging_mixin.py:190} INFO - [2024-12-17T17:54:04.799+0530] {dag.py:4399} INFO - [DAG TEST] running task <TaskInstance: movie_pipeline.scrape_collection_page manual__2024-12-17T12:24:04.631763+00:00 [scheduled]>
[2024-12-17T17:54:09.896+0530] {logging_mixin.py:190} INFO - [2024-12-17T17:54:09.895+0530] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-12-17T17:54:09.955+0530] {logging_mixin.py:190} INFO - [2024-12-17 17:54:09,955] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T12:24:04.631763+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T12:24:04.631763+00:00'
[2024-12-17T17:54:09.955+0530] {logging_mixin.py:190} INFO - [2024-12-17T17:54:09.955+0530] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T12:24:04.631763+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T12:24:04.631763+00:00'
[2024-12-17T17:54:09.958+0530] {logging_mixin.py:190} INFO - Task instance is in running state
[2024-12-17T17:54:09.958+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: queued
[2024-12-17T17:54:09.958+0530] {logging_mixin.py:190} INFO - Current task name:scrape_collection_page state:scheduled start_date:None
[2024-12-17T17:54:09.959+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline and current dag run status:running
[2024-12-17T17:54:09.959+0530] {logging_mixin.py:190} INFO - [2024-12-17T17:54:09.959+0530] {taskinstance.py:731} INFO - ::endgroup::
[2024-12-17T17:54:09.961+0530] {logging_mixin.py:190} INFO - 2024-12-17
[2024-12-17T17:54:09.961+0530] {logging_mixin.py:190} INFO - [2024-12-17 17:54:09,961] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T17:54:09.962+0530] {logging_mixin.py:190} INFO - [2024-12-17T17:54:09.961+0530] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T17:54:09.967+0530] {logging_mixin.py:190} INFO - [2024-12-17T17:54:09.967+0530] {taskinstance.py:340} INFO - ::group::Post task execution logs
[2024-12-17T17:54:09.968+0530] {logging_mixin.py:190} INFO - [2024-12-17T17:54:09.968+0530] {taskinstance.py:352} INFO - Marking task as SUCCESS. dag_id=movie_pipeline, task_id=scrape_collection_page, run_id=manual__2024-12-17T12:24:04.631763+00:00, execution_date=20241217T122404, start_date=, end_date=20241217T122409
[2024-12-17T17:54:09.977+0530] {logging_mixin.py:190} INFO - Task instance in success state
[2024-12-17T17:54:09.977+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: running
[2024-12-17T17:54:09.977+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline queued_at:None
[2024-12-17T17:54:09.977+0530] {logging_mixin.py:190} INFO - Task hostname:Jayeshs-MacBook-Air.local operator:PythonOperator
[2024-12-17T17:54:09.978+0530] {logging_mixin.py:190} INFO - [2024-12-17T17:54:09.978+0530] {dag.py:4410} INFO - [DAG TEST] end task task_id=scrape_collection_page map_index=-1
[2024-12-17T17:54:09.982+0530] {logging_mixin.py:190} INFO - [2024-12-17T17:54:09.981+0530] {dagrun.py:854} INFO - Marking run <DagRun movie_pipeline @ 2024-12-17 12:24:04.631763+00:00: manual__2024-12-17T12:24:04.631763+00:00, state:running, queued_at: None. externally triggered: False> successful
[2024-12-17T17:54:09.982+0530] {logging_mixin.py:190} INFO - Dag run in success state
[2024-12-17T17:54:09.982+0530] {logging_mixin.py:190} INFO - Dag run start:2024-12-17 12:24:04.631763+00:00 end:2024-12-17 12:24:09.982263+00:00
[2024-12-17T17:54:09.982+0530] {logging_mixin.py:190} INFO - [2024-12-17T17:54:09.982+0530] {dagrun.py:905} INFO - DagRun Finished: dag_id=movie_pipeline, execution_date=2024-12-17 12:24:04.631763+00:00, run_id=manual__2024-12-17T12:24:04.631763+00:00, run_start_date=2024-12-17 12:24:04.631763+00:00, run_end_date=2024-12-17 12:24:09.982263+00:00, run_duration=5.3505, state=success, external_trigger=False, run_type=manual, data_interval_start=2024-12-16 12:24:04.631763+00:00, data_interval_end=2024-12-17 12:24:04.631763+00:00, dag_hash=None
[2024-12-17T17:54:09.992+0530] {processor.py:925} INFO - DAG(s) 'movie_pipeline' retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T17:54:10.003+0530] {logging_mixin.py:190} INFO - [2024-12-17T17:54:10.003+0530] {dag.py:3239} INFO - Sync 1 DAGs
[2024-12-17T17:54:10.021+0530] {logging_mixin.py:190} INFO - [2024-12-17T17:54:10.020+0530] {dag.py:4180} INFO - Setting next_dagrun for movie_pipeline to 2024-12-17 01:30:00+00:00, run_after=2024-12-18 01:30:00+00:00
[2024-12-17T17:54:10.039+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 5.456 seconds
[2024-12-17T17:56:47.058+0530] {processor.py:186} INFO - Started process (PID=81234) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T17:56:47.059+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-17T17:56:47.060+0530] {logging_mixin.py:190} INFO - [2024-12-17T17:56:47.060+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T17:56:47.222+0530] {logging_mixin.py:190} INFO - [2024-12-17T17:56:47.222+0530] {dag.py:4435} INFO - dagrun id: movie_pipeline
[2024-12-17T17:56:47.235+0530] {logging_mixin.py:190} INFO - [2024-12-17T17:56:47.235+0530] {dag.py:4451} INFO - created dagrun <DagRun movie_pipeline @ 2024-12-17 12:26:47.098278+00:00: manual__2024-12-17T12:26:47.098278+00:00, state:running, queued_at: None. externally triggered: False>
[2024-12-17T17:56:47.260+0530] {logging_mixin.py:190} INFO - [2024-12-17T17:56:47.260+0530] {dag.py:4396} INFO - [DAG TEST] starting task_id=scrape_collection_page map_index=-1
[2024-12-17T17:56:47.261+0530] {logging_mixin.py:190} INFO - [2024-12-17T17:56:47.261+0530] {dag.py:4399} INFO - [DAG TEST] running task <TaskInstance: movie_pipeline.scrape_collection_page manual__2024-12-17T12:26:47.098278+00:00 [scheduled]>
[2024-12-17T17:56:52.356+0530] {logging_mixin.py:190} INFO - [2024-12-17T17:56:52.356+0530] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-12-17T17:56:52.415+0530] {logging_mixin.py:190} INFO - [2024-12-17 17:56:52,414] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T12:26:47.098278+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T12:26:47.098278+00:00'
[2024-12-17T17:56:52.415+0530] {logging_mixin.py:190} INFO - [2024-12-17T17:56:52.414+0530] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T12:26:47.098278+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T12:26:47.098278+00:00'
[2024-12-17T17:56:52.418+0530] {logging_mixin.py:190} INFO - Task instance is in running state
[2024-12-17T17:56:52.418+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: queued
[2024-12-17T17:56:52.419+0530] {logging_mixin.py:190} INFO - Current task name:scrape_collection_page state:scheduled start_date:None
[2024-12-17T17:56:52.419+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline and current dag run status:running
[2024-12-17T17:56:52.419+0530] {logging_mixin.py:190} INFO - [2024-12-17T17:56:52.419+0530] {taskinstance.py:731} INFO - ::endgroup::
[2024-12-17T17:56:52.421+0530] {logging_mixin.py:190} INFO - 2024-12-17
[2024-12-17T17:56:52.422+0530] {logging_mixin.py:190} INFO - [2024-12-17 17:56:52,422] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T17:56:52.422+0530] {logging_mixin.py:190} INFO - [2024-12-17T17:56:52.422+0530] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T17:56:52.427+0530] {logging_mixin.py:190} INFO - [2024-12-17T17:56:52.427+0530] {taskinstance.py:340} INFO - ::group::Post task execution logs
[2024-12-17T17:56:52.427+0530] {logging_mixin.py:190} INFO - [2024-12-17T17:56:52.427+0530] {taskinstance.py:352} INFO - Marking task as SUCCESS. dag_id=movie_pipeline, task_id=scrape_collection_page, run_id=manual__2024-12-17T12:26:47.098278+00:00, execution_date=20241217T122647, start_date=, end_date=20241217T122652
[2024-12-17T17:56:52.437+0530] {logging_mixin.py:190} INFO - Task instance in success state
[2024-12-17T17:56:52.437+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: running
[2024-12-17T17:56:52.437+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline queued_at:None
[2024-12-17T17:56:52.438+0530] {logging_mixin.py:190} INFO - Task hostname:Jayeshs-MacBook-Air.local operator:PythonOperator
[2024-12-17T17:56:52.438+0530] {logging_mixin.py:190} INFO - [2024-12-17T17:56:52.438+0530] {dag.py:4410} INFO - [DAG TEST] end task task_id=scrape_collection_page map_index=-1
[2024-12-17T17:56:52.442+0530] {logging_mixin.py:190} INFO - [2024-12-17T17:56:52.441+0530] {dagrun.py:854} INFO - Marking run <DagRun movie_pipeline @ 2024-12-17 12:26:47.098278+00:00: manual__2024-12-17T12:26:47.098278+00:00, state:running, queued_at: None. externally triggered: False> successful
[2024-12-17T17:56:52.442+0530] {logging_mixin.py:190} INFO - Dag run in success state
[2024-12-17T17:56:52.442+0530] {logging_mixin.py:190} INFO - Dag run start:2024-12-17 12:26:47.098278+00:00 end:2024-12-17 12:26:52.442345+00:00
[2024-12-17T17:56:52.443+0530] {logging_mixin.py:190} INFO - [2024-12-17T17:56:52.442+0530] {dagrun.py:905} INFO - DagRun Finished: dag_id=movie_pipeline, execution_date=2024-12-17 12:26:47.098278+00:00, run_id=manual__2024-12-17T12:26:47.098278+00:00, run_start_date=2024-12-17 12:26:47.098278+00:00, run_end_date=2024-12-17 12:26:52.442345+00:00, run_duration=5.344067, state=success, external_trigger=False, run_type=manual, data_interval_start=2024-12-16 12:26:47.098278+00:00, data_interval_end=2024-12-17 12:26:47.098278+00:00, dag_hash=None
[2024-12-17T17:56:52.452+0530] {processor.py:925} INFO - DAG(s) 'movie_pipeline' retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T17:56:52.464+0530] {logging_mixin.py:190} INFO - [2024-12-17T17:56:52.463+0530] {dag.py:3239} INFO - Sync 1 DAGs
[2024-12-17T17:56:52.481+0530] {logging_mixin.py:190} INFO - [2024-12-17T17:56:52.480+0530] {dag.py:4180} INFO - Setting next_dagrun for movie_pipeline to 2024-12-17 01:30:00+00:00, run_after=2024-12-18 01:30:00+00:00
[2024-12-17T17:56:52.500+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 5.445 seconds
[2024-12-17T17:59:33.220+0530] {processor.py:186} INFO - Started process (PID=81424) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T17:59:33.222+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-17T17:59:33.223+0530] {logging_mixin.py:190} INFO - [2024-12-17T17:59:33.223+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T17:59:33.391+0530] {logging_mixin.py:190} INFO - [2024-12-17T17:59:33.391+0530] {dag.py:4435} INFO - dagrun id: movie_pipeline
[2024-12-17T17:59:33.405+0530] {logging_mixin.py:190} INFO - [2024-12-17T17:59:33.404+0530] {dag.py:4451} INFO - created dagrun <DagRun movie_pipeline @ 2024-12-17 12:29:33.262130+00:00: manual__2024-12-17T12:29:33.262130+00:00, state:running, queued_at: None. externally triggered: False>
[2024-12-17T17:59:33.431+0530] {logging_mixin.py:190} INFO - [2024-12-17T17:59:33.430+0530] {dag.py:4396} INFO - [DAG TEST] starting task_id=scrape_collection_page map_index=-1
[2024-12-17T17:59:33.432+0530] {logging_mixin.py:190} INFO - [2024-12-17T17:59:33.431+0530] {dag.py:4399} INFO - [DAG TEST] running task <TaskInstance: movie_pipeline.scrape_collection_page manual__2024-12-17T12:29:33.262130+00:00 [scheduled]>
[2024-12-17T17:59:38.526+0530] {logging_mixin.py:190} INFO - [2024-12-17T17:59:38.526+0530] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-12-17T17:59:38.584+0530] {logging_mixin.py:190} INFO - [2024-12-17 17:59:38,584] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T12:29:33.262130+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T12:29:33.262130+00:00'
[2024-12-17T17:59:38.585+0530] {logging_mixin.py:190} INFO - [2024-12-17T17:59:38.584+0530] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T12:29:33.262130+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T12:29:33.262130+00:00'
[2024-12-17T17:59:38.588+0530] {logging_mixin.py:190} INFO - Task instance is in running state
[2024-12-17T17:59:38.588+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: queued
[2024-12-17T17:59:38.588+0530] {logging_mixin.py:190} INFO - Current task name:scrape_collection_page state:scheduled start_date:None
[2024-12-17T17:59:38.589+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline and current dag run status:running
[2024-12-17T17:59:38.589+0530] {logging_mixin.py:190} INFO - [2024-12-17T17:59:38.589+0530] {taskinstance.py:731} INFO - ::endgroup::
[2024-12-17T17:59:38.591+0530] {logging_mixin.py:190} INFO - 2024-12-17
[2024-12-17T17:59:38.591+0530] {logging_mixin.py:190} INFO - [2024-12-17 17:59:38,591] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T17:59:38.591+0530] {logging_mixin.py:190} INFO - [2024-12-17T17:59:38.591+0530] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T17:59:38.596+0530] {logging_mixin.py:190} INFO - [2024-12-17T17:59:38.596+0530] {taskinstance.py:340} INFO - ::group::Post task execution logs
[2024-12-17T17:59:38.597+0530] {logging_mixin.py:190} INFO - [2024-12-17T17:59:38.597+0530] {taskinstance.py:352} INFO - Marking task as SUCCESS. dag_id=movie_pipeline, task_id=scrape_collection_page, run_id=manual__2024-12-17T12:29:33.262130+00:00, execution_date=20241217T122933, start_date=, end_date=20241217T122938
[2024-12-17T17:59:38.606+0530] {logging_mixin.py:190} INFO - Task instance in success state
[2024-12-17T17:59:38.606+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: running
[2024-12-17T17:59:38.607+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline queued_at:None
[2024-12-17T17:59:38.607+0530] {logging_mixin.py:190} INFO - Task hostname:Jayeshs-MacBook-Air.local operator:PythonOperator
[2024-12-17T17:59:38.607+0530] {logging_mixin.py:190} INFO - [2024-12-17T17:59:38.607+0530] {dag.py:4410} INFO - [DAG TEST] end task task_id=scrape_collection_page map_index=-1
[2024-12-17T17:59:38.611+0530] {logging_mixin.py:190} INFO - [2024-12-17T17:59:38.610+0530] {dagrun.py:854} INFO - Marking run <DagRun movie_pipeline @ 2024-12-17 12:29:33.262130+00:00: manual__2024-12-17T12:29:33.262130+00:00, state:running, queued_at: None. externally triggered: False> successful
[2024-12-17T17:59:38.611+0530] {logging_mixin.py:190} INFO - Dag run in success state
[2024-12-17T17:59:38.611+0530] {logging_mixin.py:190} INFO - Dag run start:2024-12-17 12:29:33.262130+00:00 end:2024-12-17 12:29:38.611480+00:00
[2024-12-17T17:59:38.612+0530] {logging_mixin.py:190} INFO - [2024-12-17T17:59:38.611+0530] {dagrun.py:905} INFO - DagRun Finished: dag_id=movie_pipeline, execution_date=2024-12-17 12:29:33.262130+00:00, run_id=manual__2024-12-17T12:29:33.262130+00:00, run_start_date=2024-12-17 12:29:33.262130+00:00, run_end_date=2024-12-17 12:29:38.611480+00:00, run_duration=5.34935, state=success, external_trigger=False, run_type=manual, data_interval_start=2024-12-16 12:29:33.262130+00:00, data_interval_end=2024-12-17 12:29:33.262130+00:00, dag_hash=None
[2024-12-17T17:59:38.621+0530] {processor.py:925} INFO - DAG(s) 'movie_pipeline' retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T17:59:38.632+0530] {logging_mixin.py:190} INFO - [2024-12-17T17:59:38.632+0530] {dag.py:3239} INFO - Sync 1 DAGs
[2024-12-17T17:59:38.649+0530] {logging_mixin.py:190} INFO - [2024-12-17T17:59:38.649+0530] {dag.py:4180} INFO - Setting next_dagrun for movie_pipeline to 2024-12-17 01:30:00+00:00, run_after=2024-12-18 01:30:00+00:00
[2024-12-17T17:59:38.668+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 5.452 seconds
[2024-12-17T18:02:15.882+0530] {processor.py:186} INFO - Started process (PID=81622) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T18:02:15.884+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-17T18:02:15.886+0530] {logging_mixin.py:190} INFO - [2024-12-17T18:02:15.885+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T18:02:16.050+0530] {logging_mixin.py:190} INFO - [2024-12-17T18:02:16.050+0530] {dag.py:4435} INFO - dagrun id: movie_pipeline
[2024-12-17T18:02:16.064+0530] {logging_mixin.py:190} INFO - [2024-12-17T18:02:16.063+0530] {dag.py:4451} INFO - created dagrun <DagRun movie_pipeline @ 2024-12-17 12:32:15.926372+00:00: manual__2024-12-17T12:32:15.926372+00:00, state:running, queued_at: None. externally triggered: False>
[2024-12-17T18:02:16.088+0530] {logging_mixin.py:190} INFO - [2024-12-17T18:02:16.088+0530] {dag.py:4396} INFO - [DAG TEST] starting task_id=scrape_collection_page map_index=-1
[2024-12-17T18:02:16.089+0530] {logging_mixin.py:190} INFO - [2024-12-17T18:02:16.088+0530] {dag.py:4399} INFO - [DAG TEST] running task <TaskInstance: movie_pipeline.scrape_collection_page manual__2024-12-17T12:32:15.926372+00:00 [scheduled]>
[2024-12-17T18:02:21.185+0530] {logging_mixin.py:190} INFO - [2024-12-17T18:02:21.184+0530] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-12-17T18:02:21.244+0530] {logging_mixin.py:190} INFO - [2024-12-17 18:02:21,244] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T12:32:15.926372+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T12:32:15.926372+00:00'
[2024-12-17T18:02:21.244+0530] {logging_mixin.py:190} INFO - [2024-12-17T18:02:21.244+0530] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T12:32:15.926372+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T12:32:15.926372+00:00'
[2024-12-17T18:02:21.247+0530] {logging_mixin.py:190} INFO - Task instance is in running state
[2024-12-17T18:02:21.248+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: queued
[2024-12-17T18:02:21.248+0530] {logging_mixin.py:190} INFO - Current task name:scrape_collection_page state:scheduled start_date:None
[2024-12-17T18:02:21.248+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline and current dag run status:running
[2024-12-17T18:02:21.248+0530] {logging_mixin.py:190} INFO - [2024-12-17T18:02:21.248+0530] {taskinstance.py:731} INFO - ::endgroup::
[2024-12-17T18:02:21.250+0530] {logging_mixin.py:190} INFO - 2024-12-17
[2024-12-17T18:02:21.251+0530] {logging_mixin.py:190} INFO - [2024-12-17 18:02:21,250] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T18:02:21.251+0530] {logging_mixin.py:190} INFO - [2024-12-17T18:02:21.250+0530] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T18:02:21.256+0530] {logging_mixin.py:190} INFO - [2024-12-17T18:02:21.256+0530] {taskinstance.py:340} INFO - ::group::Post task execution logs
[2024-12-17T18:02:21.256+0530] {logging_mixin.py:190} INFO - [2024-12-17T18:02:21.256+0530] {taskinstance.py:352} INFO - Marking task as SUCCESS. dag_id=movie_pipeline, task_id=scrape_collection_page, run_id=manual__2024-12-17T12:32:15.926372+00:00, execution_date=20241217T123215, start_date=, end_date=20241217T123221
[2024-12-17T18:02:21.265+0530] {logging_mixin.py:190} INFO - Task instance in success state
[2024-12-17T18:02:21.266+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: running
[2024-12-17T18:02:21.266+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline queued_at:None
[2024-12-17T18:02:21.266+0530] {logging_mixin.py:190} INFO - Task hostname:Jayeshs-MacBook-Air.local operator:PythonOperator
[2024-12-17T18:02:21.267+0530] {logging_mixin.py:190} INFO - [2024-12-17T18:02:21.266+0530] {dag.py:4410} INFO - [DAG TEST] end task task_id=scrape_collection_page map_index=-1
[2024-12-17T18:02:21.270+0530] {logging_mixin.py:190} INFO - [2024-12-17T18:02:21.270+0530] {dagrun.py:854} INFO - Marking run <DagRun movie_pipeline @ 2024-12-17 12:32:15.926372+00:00: manual__2024-12-17T12:32:15.926372+00:00, state:running, queued_at: None. externally triggered: False> successful
[2024-12-17T18:02:21.271+0530] {logging_mixin.py:190} INFO - Dag run in success state
[2024-12-17T18:02:21.271+0530] {logging_mixin.py:190} INFO - Dag run start:2024-12-17 12:32:15.926372+00:00 end:2024-12-17 12:32:21.271052+00:00
[2024-12-17T18:02:21.271+0530] {logging_mixin.py:190} INFO - [2024-12-17T18:02:21.271+0530] {dagrun.py:905} INFO - DagRun Finished: dag_id=movie_pipeline, execution_date=2024-12-17 12:32:15.926372+00:00, run_id=manual__2024-12-17T12:32:15.926372+00:00, run_start_date=2024-12-17 12:32:15.926372+00:00, run_end_date=2024-12-17 12:32:21.271052+00:00, run_duration=5.34468, state=success, external_trigger=False, run_type=manual, data_interval_start=2024-12-16 12:32:15.926372+00:00, data_interval_end=2024-12-17 12:32:15.926372+00:00, dag_hash=None
[2024-12-17T18:02:21.281+0530] {processor.py:925} INFO - DAG(s) 'movie_pipeline' retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T18:02:21.293+0530] {logging_mixin.py:190} INFO - [2024-12-17T18:02:21.293+0530] {dag.py:3239} INFO - Sync 1 DAGs
[2024-12-17T18:02:21.310+0530] {logging_mixin.py:190} INFO - [2024-12-17T18:02:21.309+0530] {dag.py:4180} INFO - Setting next_dagrun for movie_pipeline to 2024-12-17 01:30:00+00:00, run_after=2024-12-18 01:30:00+00:00
[2024-12-17T18:02:21.329+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 5.452 seconds
[2024-12-17T18:05:00.330+0530] {processor.py:186} INFO - Started process (PID=81814) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T18:05:00.331+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-17T18:05:00.332+0530] {logging_mixin.py:190} INFO - [2024-12-17T18:05:00.332+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T18:05:00.495+0530] {logging_mixin.py:190} INFO - [2024-12-17T18:05:00.495+0530] {dag.py:4435} INFO - dagrun id: movie_pipeline
[2024-12-17T18:05:00.509+0530] {logging_mixin.py:190} INFO - [2024-12-17T18:05:00.509+0530] {dag.py:4451} INFO - created dagrun <DagRun movie_pipeline @ 2024-12-17 12:35:00.371990+00:00: manual__2024-12-17T12:35:00.371990+00:00, state:running, queued_at: None. externally triggered: False>
[2024-12-17T18:05:00.535+0530] {logging_mixin.py:190} INFO - [2024-12-17T18:05:00.535+0530] {dag.py:4396} INFO - [DAG TEST] starting task_id=scrape_collection_page map_index=-1
[2024-12-17T18:05:00.536+0530] {logging_mixin.py:190} INFO - [2024-12-17T18:05:00.535+0530] {dag.py:4399} INFO - [DAG TEST] running task <TaskInstance: movie_pipeline.scrape_collection_page manual__2024-12-17T12:35:00.371990+00:00 [scheduled]>
[2024-12-17T18:05:05.633+0530] {logging_mixin.py:190} INFO - [2024-12-17T18:05:05.633+0530] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-12-17T18:05:05.694+0530] {logging_mixin.py:190} INFO - [2024-12-17 18:05:05,694] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T12:35:00.371990+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T12:35:00.371990+00:00'
[2024-12-17T18:05:05.695+0530] {logging_mixin.py:190} INFO - [2024-12-17T18:05:05.694+0530] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T12:35:00.371990+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T12:35:00.371990+00:00'
[2024-12-17T18:05:05.698+0530] {logging_mixin.py:190} INFO - Task instance is in running state
[2024-12-17T18:05:05.698+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: queued
[2024-12-17T18:05:05.698+0530] {logging_mixin.py:190} INFO - Current task name:scrape_collection_page state:scheduled start_date:None
[2024-12-17T18:05:05.699+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline and current dag run status:running
[2024-12-17T18:05:05.699+0530] {logging_mixin.py:190} INFO - [2024-12-17T18:05:05.699+0530] {taskinstance.py:731} INFO - ::endgroup::
[2024-12-17T18:05:05.701+0530] {logging_mixin.py:190} INFO - 2024-12-17
[2024-12-17T18:05:05.701+0530] {logging_mixin.py:190} INFO - [2024-12-17 18:05:05,701] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T18:05:05.702+0530] {logging_mixin.py:190} INFO - [2024-12-17T18:05:05.701+0530] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T18:05:05.706+0530] {logging_mixin.py:190} INFO - [2024-12-17T18:05:05.706+0530] {taskinstance.py:340} INFO - ::group::Post task execution logs
[2024-12-17T18:05:05.707+0530] {logging_mixin.py:190} INFO - [2024-12-17T18:05:05.707+0530] {taskinstance.py:352} INFO - Marking task as SUCCESS. dag_id=movie_pipeline, task_id=scrape_collection_page, run_id=manual__2024-12-17T12:35:00.371990+00:00, execution_date=20241217T123500, start_date=, end_date=20241217T123505
[2024-12-17T18:05:05.716+0530] {logging_mixin.py:190} INFO - Task instance in success state
[2024-12-17T18:05:05.716+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: running
[2024-12-17T18:05:05.716+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline queued_at:None
[2024-12-17T18:05:05.716+0530] {logging_mixin.py:190} INFO - Task hostname:Jayeshs-MacBook-Air.local operator:PythonOperator
[2024-12-17T18:05:05.717+0530] {logging_mixin.py:190} INFO - [2024-12-17T18:05:05.716+0530] {dag.py:4410} INFO - [DAG TEST] end task task_id=scrape_collection_page map_index=-1
[2024-12-17T18:05:05.721+0530] {logging_mixin.py:190} INFO - [2024-12-17T18:05:05.721+0530] {dagrun.py:854} INFO - Marking run <DagRun movie_pipeline @ 2024-12-17 12:35:00.371990+00:00: manual__2024-12-17T12:35:00.371990+00:00, state:running, queued_at: None. externally triggered: False> successful
[2024-12-17T18:05:05.721+0530] {logging_mixin.py:190} INFO - Dag run in success state
[2024-12-17T18:05:05.722+0530] {logging_mixin.py:190} INFO - Dag run start:2024-12-17 12:35:00.371990+00:00 end:2024-12-17 12:35:05.721915+00:00
[2024-12-17T18:05:05.723+0530] {logging_mixin.py:190} INFO - [2024-12-17T18:05:05.722+0530] {dagrun.py:905} INFO - DagRun Finished: dag_id=movie_pipeline, execution_date=2024-12-17 12:35:00.371990+00:00, run_id=manual__2024-12-17T12:35:00.371990+00:00, run_start_date=2024-12-17 12:35:00.371990+00:00, run_end_date=2024-12-17 12:35:05.721915+00:00, run_duration=5.349925, state=success, external_trigger=False, run_type=manual, data_interval_start=2024-12-16 12:35:00.371990+00:00, data_interval_end=2024-12-17 12:35:00.371990+00:00, dag_hash=None
[2024-12-17T18:05:05.732+0530] {processor.py:925} INFO - DAG(s) 'movie_pipeline' retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T18:05:05.746+0530] {logging_mixin.py:190} INFO - [2024-12-17T18:05:05.746+0530] {dag.py:3239} INFO - Sync 1 DAGs
[2024-12-17T18:05:05.772+0530] {logging_mixin.py:190} INFO - [2024-12-17T18:05:05.772+0530] {dag.py:4180} INFO - Setting next_dagrun for movie_pipeline to 2024-12-17 01:30:00+00:00, run_after=2024-12-18 01:30:00+00:00
[2024-12-17T18:05:05.792+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 5.466 seconds
[2024-12-17T18:07:45.176+0530] {processor.py:186} INFO - Started process (PID=82003) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T18:07:45.179+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-17T18:07:45.180+0530] {logging_mixin.py:190} INFO - [2024-12-17T18:07:45.180+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T18:07:45.344+0530] {logging_mixin.py:190} INFO - [2024-12-17T18:07:45.344+0530] {dag.py:4435} INFO - dagrun id: movie_pipeline
[2024-12-17T18:07:45.361+0530] {logging_mixin.py:190} INFO - [2024-12-17T18:07:45.360+0530] {dag.py:4451} INFO - created dagrun <DagRun movie_pipeline @ 2024-12-17 12:37:45.218981+00:00: manual__2024-12-17T12:37:45.218981+00:00, state:running, queued_at: None. externally triggered: False>
[2024-12-17T18:07:45.386+0530] {logging_mixin.py:190} INFO - [2024-12-17T18:07:45.386+0530] {dag.py:4396} INFO - [DAG TEST] starting task_id=scrape_collection_page map_index=-1
[2024-12-17T18:07:45.387+0530] {logging_mixin.py:190} INFO - [2024-12-17T18:07:45.387+0530] {dag.py:4399} INFO - [DAG TEST] running task <TaskInstance: movie_pipeline.scrape_collection_page manual__2024-12-17T12:37:45.218981+00:00 [scheduled]>
[2024-12-17T18:07:50.485+0530] {logging_mixin.py:190} INFO - [2024-12-17T18:07:50.485+0530] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-12-17T18:07:50.544+0530] {logging_mixin.py:190} INFO - [2024-12-17 18:07:50,544] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T12:37:45.218981+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T12:37:45.218981+00:00'
[2024-12-17T18:07:50.545+0530] {logging_mixin.py:190} INFO - [2024-12-17T18:07:50.544+0530] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T12:37:45.218981+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T12:37:45.218981+00:00'
[2024-12-17T18:07:50.547+0530] {logging_mixin.py:190} INFO - Task instance is in running state
[2024-12-17T18:07:50.548+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: queued
[2024-12-17T18:07:50.548+0530] {logging_mixin.py:190} INFO - Current task name:scrape_collection_page state:scheduled start_date:None
[2024-12-17T18:07:50.548+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline and current dag run status:running
[2024-12-17T18:07:50.549+0530] {logging_mixin.py:190} INFO - [2024-12-17T18:07:50.548+0530] {taskinstance.py:731} INFO - ::endgroup::
[2024-12-17T18:07:50.550+0530] {logging_mixin.py:190} INFO - 2024-12-17
[2024-12-17T18:07:50.551+0530] {logging_mixin.py:190} INFO - [2024-12-17 18:07:50,551] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T18:07:50.551+0530] {logging_mixin.py:190} INFO - [2024-12-17T18:07:50.551+0530] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T18:07:50.556+0530] {logging_mixin.py:190} INFO - [2024-12-17T18:07:50.556+0530] {taskinstance.py:340} INFO - ::group::Post task execution logs
[2024-12-17T18:07:50.557+0530] {logging_mixin.py:190} INFO - [2024-12-17T18:07:50.556+0530] {taskinstance.py:352} INFO - Marking task as SUCCESS. dag_id=movie_pipeline, task_id=scrape_collection_page, run_id=manual__2024-12-17T12:37:45.218981+00:00, execution_date=20241217T123745, start_date=, end_date=20241217T123750
[2024-12-17T18:07:50.566+0530] {logging_mixin.py:190} INFO - Task instance in success state
[2024-12-17T18:07:50.566+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: running
[2024-12-17T18:07:50.567+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline queued_at:None
[2024-12-17T18:07:50.567+0530] {logging_mixin.py:190} INFO - Task hostname:Jayeshs-MacBook-Air.local operator:PythonOperator
[2024-12-17T18:07:50.567+0530] {logging_mixin.py:190} INFO - [2024-12-17T18:07:50.567+0530] {dag.py:4410} INFO - [DAG TEST] end task task_id=scrape_collection_page map_index=-1
[2024-12-17T18:07:50.571+0530] {logging_mixin.py:190} INFO - [2024-12-17T18:07:50.571+0530] {dagrun.py:854} INFO - Marking run <DagRun movie_pipeline @ 2024-12-17 12:37:45.218981+00:00: manual__2024-12-17T12:37:45.218981+00:00, state:running, queued_at: None. externally triggered: False> successful
[2024-12-17T18:07:50.571+0530] {logging_mixin.py:190} INFO - Dag run in success state
[2024-12-17T18:07:50.572+0530] {logging_mixin.py:190} INFO - Dag run start:2024-12-17 12:37:45.218981+00:00 end:2024-12-17 12:37:50.571802+00:00
[2024-12-17T18:07:50.572+0530] {logging_mixin.py:190} INFO - [2024-12-17T18:07:50.572+0530] {dagrun.py:905} INFO - DagRun Finished: dag_id=movie_pipeline, execution_date=2024-12-17 12:37:45.218981+00:00, run_id=manual__2024-12-17T12:37:45.218981+00:00, run_start_date=2024-12-17 12:37:45.218981+00:00, run_end_date=2024-12-17 12:37:50.571802+00:00, run_duration=5.352821, state=success, external_trigger=False, run_type=manual, data_interval_start=2024-12-16 12:37:45.218981+00:00, data_interval_end=2024-12-17 12:37:45.218981+00:00, dag_hash=None
[2024-12-17T18:07:50.583+0530] {processor.py:925} INFO - DAG(s) 'movie_pipeline' retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T18:07:50.595+0530] {logging_mixin.py:190} INFO - [2024-12-17T18:07:50.594+0530] {dag.py:3239} INFO - Sync 1 DAGs
[2024-12-17T18:07:50.613+0530] {logging_mixin.py:190} INFO - [2024-12-17T18:07:50.612+0530] {dag.py:4180} INFO - Setting next_dagrun for movie_pipeline to 2024-12-17 01:30:00+00:00, run_after=2024-12-18 01:30:00+00:00
[2024-12-17T18:07:50.633+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 5.461 seconds
[2024-12-17T18:10:29.606+0530] {processor.py:186} INFO - Started process (PID=82203) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T18:10:29.607+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-17T18:10:29.608+0530] {logging_mixin.py:190} INFO - [2024-12-17T18:10:29.608+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T18:10:29.790+0530] {logging_mixin.py:190} INFO - [2024-12-17T18:10:29.790+0530] {dag.py:4435} INFO - dagrun id: movie_pipeline
[2024-12-17T18:10:29.804+0530] {logging_mixin.py:190} INFO - [2024-12-17T18:10:29.804+0530] {dag.py:4451} INFO - created dagrun <DagRun movie_pipeline @ 2024-12-17 12:40:29.650905+00:00: manual__2024-12-17T12:40:29.650905+00:00, state:running, queued_at: None. externally triggered: False>
[2024-12-17T18:10:29.830+0530] {logging_mixin.py:190} INFO - [2024-12-17T18:10:29.829+0530] {dag.py:4396} INFO - [DAG TEST] starting task_id=scrape_collection_page map_index=-1
[2024-12-17T18:10:29.830+0530] {logging_mixin.py:190} INFO - [2024-12-17T18:10:29.830+0530] {dag.py:4399} INFO - [DAG TEST] running task <TaskInstance: movie_pipeline.scrape_collection_page manual__2024-12-17T12:40:29.650905+00:00 [scheduled]>
[2024-12-17T18:10:34.927+0530] {logging_mixin.py:190} INFO - [2024-12-17T18:10:34.927+0530] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-12-17T18:10:34.985+0530] {logging_mixin.py:190} INFO - [2024-12-17 18:10:34,985] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T12:40:29.650905+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T12:40:29.650905+00:00'
[2024-12-17T18:10:34.986+0530] {logging_mixin.py:190} INFO - [2024-12-17T18:10:34.985+0530] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T12:40:29.650905+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T12:40:29.650905+00:00'
[2024-12-17T18:10:34.989+0530] {logging_mixin.py:190} INFO - Task instance is in running state
[2024-12-17T18:10:34.989+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: queued
[2024-12-17T18:10:34.989+0530] {logging_mixin.py:190} INFO - Current task name:scrape_collection_page state:scheduled start_date:None
[2024-12-17T18:10:34.989+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline and current dag run status:running
[2024-12-17T18:10:34.990+0530] {logging_mixin.py:190} INFO - [2024-12-17T18:10:34.990+0530] {taskinstance.py:731} INFO - ::endgroup::
[2024-12-17T18:10:34.992+0530] {logging_mixin.py:190} INFO - 2024-12-17
[2024-12-17T18:10:34.992+0530] {logging_mixin.py:190} INFO - [2024-12-17 18:10:34,992] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T18:10:34.993+0530] {logging_mixin.py:190} INFO - [2024-12-17T18:10:34.992+0530] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T18:10:34.997+0530] {logging_mixin.py:190} INFO - [2024-12-17T18:10:34.997+0530] {taskinstance.py:340} INFO - ::group::Post task execution logs
[2024-12-17T18:10:34.998+0530] {logging_mixin.py:190} INFO - [2024-12-17T18:10:34.998+0530] {taskinstance.py:352} INFO - Marking task as SUCCESS. dag_id=movie_pipeline, task_id=scrape_collection_page, run_id=manual__2024-12-17T12:40:29.650905+00:00, execution_date=20241217T124029, start_date=, end_date=20241217T124034
[2024-12-17T18:10:35.007+0530] {logging_mixin.py:190} INFO - Task instance in success state
[2024-12-17T18:10:35.008+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: running
[2024-12-17T18:10:35.008+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline queued_at:None
[2024-12-17T18:10:35.008+0530] {logging_mixin.py:190} INFO - Task hostname:Jayeshs-MacBook-Air.local operator:PythonOperator
[2024-12-17T18:10:35.008+0530] {logging_mixin.py:190} INFO - [2024-12-17T18:10:35.008+0530] {dag.py:4410} INFO - [DAG TEST] end task task_id=scrape_collection_page map_index=-1
[2024-12-17T18:10:35.012+0530] {logging_mixin.py:190} INFO - [2024-12-17T18:10:35.012+0530] {dagrun.py:854} INFO - Marking run <DagRun movie_pipeline @ 2024-12-17 12:40:29.650905+00:00: manual__2024-12-17T12:40:29.650905+00:00, state:running, queued_at: None. externally triggered: False> successful
[2024-12-17T18:10:35.012+0530] {logging_mixin.py:190} INFO - Dag run in success state
[2024-12-17T18:10:35.012+0530] {logging_mixin.py:190} INFO - Dag run start:2024-12-17 12:40:29.650905+00:00 end:2024-12-17 12:40:35.012639+00:00
[2024-12-17T18:10:35.013+0530] {logging_mixin.py:190} INFO - [2024-12-17T18:10:35.013+0530] {dagrun.py:905} INFO - DagRun Finished: dag_id=movie_pipeline, execution_date=2024-12-17 12:40:29.650905+00:00, run_id=manual__2024-12-17T12:40:29.650905+00:00, run_start_date=2024-12-17 12:40:29.650905+00:00, run_end_date=2024-12-17 12:40:35.012639+00:00, run_duration=5.361734, state=success, external_trigger=False, run_type=manual, data_interval_start=2024-12-16 12:40:29.650905+00:00, data_interval_end=2024-12-17 12:40:29.650905+00:00, dag_hash=None
[2024-12-17T18:10:35.022+0530] {processor.py:925} INFO - DAG(s) 'movie_pipeline' retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T18:10:35.034+0530] {logging_mixin.py:190} INFO - [2024-12-17T18:10:35.033+0530] {dag.py:3239} INFO - Sync 1 DAGs
[2024-12-17T18:10:35.051+0530] {logging_mixin.py:190} INFO - [2024-12-17T18:10:35.051+0530] {dag.py:4180} INFO - Setting next_dagrun for movie_pipeline to 2024-12-17 01:30:00+00:00, run_after=2024-12-18 01:30:00+00:00
[2024-12-17T18:10:35.069+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 5.466 seconds
[2024-12-17T18:13:15.073+0530] {processor.py:186} INFO - Started process (PID=82394) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T18:13:15.075+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-17T18:13:15.076+0530] {logging_mixin.py:190} INFO - [2024-12-17T18:13:15.075+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T18:13:15.238+0530] {logging_mixin.py:190} INFO - [2024-12-17T18:13:15.238+0530] {dag.py:4435} INFO - dagrun id: movie_pipeline
[2024-12-17T18:13:15.252+0530] {logging_mixin.py:190} INFO - [2024-12-17T18:13:15.252+0530] {dag.py:4451} INFO - created dagrun <DagRun movie_pipeline @ 2024-12-17 12:43:15.114818+00:00: manual__2024-12-17T12:43:15.114818+00:00, state:running, queued_at: None. externally triggered: False>
[2024-12-17T18:13:15.278+0530] {logging_mixin.py:190} INFO - [2024-12-17T18:13:15.277+0530] {dag.py:4396} INFO - [DAG TEST] starting task_id=scrape_collection_page map_index=-1
[2024-12-17T18:13:15.278+0530] {logging_mixin.py:190} INFO - [2024-12-17T18:13:15.278+0530] {dag.py:4399} INFO - [DAG TEST] running task <TaskInstance: movie_pipeline.scrape_collection_page manual__2024-12-17T12:43:15.114818+00:00 [scheduled]>
[2024-12-17T18:13:20.373+0530] {logging_mixin.py:190} INFO - [2024-12-17T18:13:20.373+0530] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-12-17T18:13:20.431+0530] {logging_mixin.py:190} INFO - [2024-12-17 18:13:20,430] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T12:43:15.114818+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T12:43:15.114818+00:00'
[2024-12-17T18:13:20.431+0530] {logging_mixin.py:190} INFO - [2024-12-17T18:13:20.430+0530] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T12:43:15.114818+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T12:43:15.114818+00:00'
[2024-12-17T18:13:20.434+0530] {logging_mixin.py:190} INFO - Task instance is in running state
[2024-12-17T18:13:20.434+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: queued
[2024-12-17T18:13:20.434+0530] {logging_mixin.py:190} INFO - Current task name:scrape_collection_page state:scheduled start_date:None
[2024-12-17T18:13:20.435+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline and current dag run status:running
[2024-12-17T18:13:20.435+0530] {logging_mixin.py:190} INFO - [2024-12-17T18:13:20.435+0530] {taskinstance.py:731} INFO - ::endgroup::
[2024-12-17T18:13:20.437+0530] {logging_mixin.py:190} INFO - 2024-12-17
[2024-12-17T18:13:20.438+0530] {logging_mixin.py:190} INFO - [2024-12-17 18:13:20,438] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T18:13:20.438+0530] {logging_mixin.py:190} INFO - [2024-12-17T18:13:20.438+0530] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T18:13:20.443+0530] {logging_mixin.py:190} INFO - [2024-12-17T18:13:20.443+0530] {taskinstance.py:340} INFO - ::group::Post task execution logs
[2024-12-17T18:13:20.444+0530] {logging_mixin.py:190} INFO - [2024-12-17T18:13:20.443+0530] {taskinstance.py:352} INFO - Marking task as SUCCESS. dag_id=movie_pipeline, task_id=scrape_collection_page, run_id=manual__2024-12-17T12:43:15.114818+00:00, execution_date=20241217T124315, start_date=, end_date=20241217T124320
[2024-12-17T18:13:20.453+0530] {logging_mixin.py:190} INFO - Task instance in success state
[2024-12-17T18:13:20.453+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: running
[2024-12-17T18:13:20.453+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline queued_at:None
[2024-12-17T18:13:20.453+0530] {logging_mixin.py:190} INFO - Task hostname:Jayeshs-MacBook-Air.local operator:PythonOperator
[2024-12-17T18:13:20.454+0530] {logging_mixin.py:190} INFO - [2024-12-17T18:13:20.454+0530] {dag.py:4410} INFO - [DAG TEST] end task task_id=scrape_collection_page map_index=-1
[2024-12-17T18:13:20.457+0530] {logging_mixin.py:190} INFO - [2024-12-17T18:13:20.457+0530] {dagrun.py:854} INFO - Marking run <DagRun movie_pipeline @ 2024-12-17 12:43:15.114818+00:00: manual__2024-12-17T12:43:15.114818+00:00, state:running, queued_at: None. externally triggered: False> successful
[2024-12-17T18:13:20.458+0530] {logging_mixin.py:190} INFO - Dag run in success state
[2024-12-17T18:13:20.458+0530] {logging_mixin.py:190} INFO - Dag run start:2024-12-17 12:43:15.114818+00:00 end:2024-12-17 12:43:20.458138+00:00
[2024-12-17T18:13:20.458+0530] {logging_mixin.py:190} INFO - [2024-12-17T18:13:20.458+0530] {dagrun.py:905} INFO - DagRun Finished: dag_id=movie_pipeline, execution_date=2024-12-17 12:43:15.114818+00:00, run_id=manual__2024-12-17T12:43:15.114818+00:00, run_start_date=2024-12-17 12:43:15.114818+00:00, run_end_date=2024-12-17 12:43:20.458138+00:00, run_duration=5.34332, state=success, external_trigger=False, run_type=manual, data_interval_start=2024-12-16 12:43:15.114818+00:00, data_interval_end=2024-12-17 12:43:15.114818+00:00, dag_hash=None
[2024-12-17T18:13:20.467+0530] {processor.py:925} INFO - DAG(s) 'movie_pipeline' retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T18:13:20.479+0530] {logging_mixin.py:190} INFO - [2024-12-17T18:13:20.479+0530] {dag.py:3239} INFO - Sync 1 DAGs
[2024-12-17T18:13:20.496+0530] {logging_mixin.py:190} INFO - [2024-12-17T18:13:20.496+0530] {dag.py:4180} INFO - Setting next_dagrun for movie_pipeline to 2024-12-17 01:30:00+00:00, run_after=2024-12-18 01:30:00+00:00
[2024-12-17T18:13:20.514+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 5.444 seconds
[2024-12-17T18:16:03.492+0530] {processor.py:186} INFO - Started process (PID=82592) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T18:16:03.493+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-17T18:16:03.494+0530] {logging_mixin.py:190} INFO - [2024-12-17T18:16:03.494+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T18:16:03.660+0530] {logging_mixin.py:190} INFO - [2024-12-17T18:16:03.660+0530] {dag.py:4435} INFO - dagrun id: movie_pipeline
[2024-12-17T18:16:03.674+0530] {logging_mixin.py:190} INFO - [2024-12-17T18:16:03.673+0530] {dag.py:4451} INFO - created dagrun <DagRun movie_pipeline @ 2024-12-17 12:46:03.533333+00:00: manual__2024-12-17T12:46:03.533333+00:00, state:running, queued_at: None. externally triggered: False>
[2024-12-17T18:16:03.699+0530] {logging_mixin.py:190} INFO - [2024-12-17T18:16:03.699+0530] {dag.py:4396} INFO - [DAG TEST] starting task_id=scrape_collection_page map_index=-1
[2024-12-17T18:16:03.700+0530] {logging_mixin.py:190} INFO - [2024-12-17T18:16:03.699+0530] {dag.py:4399} INFO - [DAG TEST] running task <TaskInstance: movie_pipeline.scrape_collection_page manual__2024-12-17T12:46:03.533333+00:00 [scheduled]>
[2024-12-17T18:16:08.795+0530] {logging_mixin.py:190} INFO - [2024-12-17T18:16:08.795+0530] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-12-17T18:16:08.855+0530] {logging_mixin.py:190} INFO - [2024-12-17 18:16:08,854] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T12:46:03.533333+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T12:46:03.533333+00:00'
[2024-12-17T18:16:08.855+0530] {logging_mixin.py:190} INFO - [2024-12-17T18:16:08.854+0530] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T12:46:03.533333+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T12:46:03.533333+00:00'
[2024-12-17T18:16:08.858+0530] {logging_mixin.py:190} INFO - Task instance is in running state
[2024-12-17T18:16:08.858+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: queued
[2024-12-17T18:16:08.858+0530] {logging_mixin.py:190} INFO - Current task name:scrape_collection_page state:scheduled start_date:None
[2024-12-17T18:16:08.858+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline and current dag run status:running
[2024-12-17T18:16:08.859+0530] {logging_mixin.py:190} INFO - [2024-12-17T18:16:08.859+0530] {taskinstance.py:731} INFO - ::endgroup::
[2024-12-17T18:16:08.861+0530] {logging_mixin.py:190} INFO - 2024-12-17
[2024-12-17T18:16:08.861+0530] {logging_mixin.py:190} INFO - [2024-12-17 18:16:08,861] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T18:16:08.862+0530] {logging_mixin.py:190} INFO - [2024-12-17T18:16:08.861+0530] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T18:16:08.867+0530] {logging_mixin.py:190} INFO - [2024-12-17T18:16:08.866+0530] {taskinstance.py:340} INFO - ::group::Post task execution logs
[2024-12-17T18:16:08.867+0530] {logging_mixin.py:190} INFO - [2024-12-17T18:16:08.867+0530] {taskinstance.py:352} INFO - Marking task as SUCCESS. dag_id=movie_pipeline, task_id=scrape_collection_page, run_id=manual__2024-12-17T12:46:03.533333+00:00, execution_date=20241217T124603, start_date=, end_date=20241217T124608
[2024-12-17T18:16:08.877+0530] {logging_mixin.py:190} INFO - Task instance in success state
[2024-12-17T18:16:08.877+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: running
[2024-12-17T18:16:08.877+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline queued_at:None
[2024-12-17T18:16:08.877+0530] {logging_mixin.py:190} INFO - Task hostname:Jayeshs-MacBook-Air.local operator:PythonOperator
[2024-12-17T18:16:08.878+0530] {logging_mixin.py:190} INFO - [2024-12-17T18:16:08.877+0530] {dag.py:4410} INFO - [DAG TEST] end task task_id=scrape_collection_page map_index=-1
[2024-12-17T18:16:08.881+0530] {logging_mixin.py:190} INFO - [2024-12-17T18:16:08.881+0530] {dagrun.py:854} INFO - Marking run <DagRun movie_pipeline @ 2024-12-17 12:46:03.533333+00:00: manual__2024-12-17T12:46:03.533333+00:00, state:running, queued_at: None. externally triggered: False> successful
[2024-12-17T18:16:08.882+0530] {logging_mixin.py:190} INFO - Dag run in success state
[2024-12-17T18:16:08.882+0530] {logging_mixin.py:190} INFO - Dag run start:2024-12-17 12:46:03.533333+00:00 end:2024-12-17 12:46:08.882070+00:00
[2024-12-17T18:16:08.882+0530] {logging_mixin.py:190} INFO - [2024-12-17T18:16:08.882+0530] {dagrun.py:905} INFO - DagRun Finished: dag_id=movie_pipeline, execution_date=2024-12-17 12:46:03.533333+00:00, run_id=manual__2024-12-17T12:46:03.533333+00:00, run_start_date=2024-12-17 12:46:03.533333+00:00, run_end_date=2024-12-17 12:46:08.882070+00:00, run_duration=5.348737, state=success, external_trigger=False, run_type=manual, data_interval_start=2024-12-16 12:46:03.533333+00:00, data_interval_end=2024-12-17 12:46:03.533333+00:00, dag_hash=None
[2024-12-17T18:16:08.892+0530] {processor.py:925} INFO - DAG(s) 'movie_pipeline' retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T18:16:08.904+0530] {logging_mixin.py:190} INFO - [2024-12-17T18:16:08.904+0530] {dag.py:3239} INFO - Sync 1 DAGs
[2024-12-17T18:16:08.921+0530] {logging_mixin.py:190} INFO - [2024-12-17T18:16:08.921+0530] {dag.py:4180} INFO - Setting next_dagrun for movie_pipeline to 2024-12-17 01:30:00+00:00, run_after=2024-12-18 01:30:00+00:00
[2024-12-17T18:16:08.939+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 5.450 seconds
[2024-12-17T18:18:44.791+0530] {processor.py:186} INFO - Started process (PID=82778) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T18:18:44.792+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-17T18:18:44.793+0530] {logging_mixin.py:190} INFO - [2024-12-17T18:18:44.793+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T18:18:44.956+0530] {logging_mixin.py:190} INFO - [2024-12-17T18:18:44.956+0530] {dag.py:4435} INFO - dagrun id: movie_pipeline
[2024-12-17T18:18:44.969+0530] {logging_mixin.py:190} INFO - [2024-12-17T18:18:44.969+0530] {dag.py:4451} INFO - created dagrun <DagRun movie_pipeline @ 2024-12-17 12:48:44.832807+00:00: manual__2024-12-17T12:48:44.832807+00:00, state:running, queued_at: None. externally triggered: False>
[2024-12-17T18:18:44.994+0530] {logging_mixin.py:190} INFO - [2024-12-17T18:18:44.994+0530] {dag.py:4396} INFO - [DAG TEST] starting task_id=scrape_collection_page map_index=-1
[2024-12-17T18:18:44.995+0530] {logging_mixin.py:190} INFO - [2024-12-17T18:18:44.994+0530] {dag.py:4399} INFO - [DAG TEST] running task <TaskInstance: movie_pipeline.scrape_collection_page manual__2024-12-17T12:48:44.832807+00:00 [scheduled]>
[2024-12-17T18:18:50.088+0530] {logging_mixin.py:190} INFO - [2024-12-17T18:18:50.088+0530] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-12-17T18:18:50.147+0530] {logging_mixin.py:190} INFO - [2024-12-17 18:18:50,147] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T12:48:44.832807+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T12:48:44.832807+00:00'
[2024-12-17T18:18:50.147+0530] {logging_mixin.py:190} INFO - [2024-12-17T18:18:50.147+0530] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T12:48:44.832807+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T12:48:44.832807+00:00'
[2024-12-17T18:18:50.150+0530] {logging_mixin.py:190} INFO - Task instance is in running state
[2024-12-17T18:18:50.150+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: queued
[2024-12-17T18:18:50.151+0530] {logging_mixin.py:190} INFO - Current task name:scrape_collection_page state:scheduled start_date:None
[2024-12-17T18:18:50.151+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline and current dag run status:running
[2024-12-17T18:18:50.152+0530] {logging_mixin.py:190} INFO - [2024-12-17T18:18:50.151+0530] {taskinstance.py:731} INFO - ::endgroup::
[2024-12-17T18:18:50.153+0530] {logging_mixin.py:190} INFO - 2024-12-17
[2024-12-17T18:18:50.154+0530] {logging_mixin.py:190} INFO - [2024-12-17 18:18:50,154] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T18:18:50.154+0530] {logging_mixin.py:190} INFO - [2024-12-17T18:18:50.154+0530] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T18:18:50.159+0530] {logging_mixin.py:190} INFO - [2024-12-17T18:18:50.159+0530] {taskinstance.py:340} INFO - ::group::Post task execution logs
[2024-12-17T18:18:50.160+0530] {logging_mixin.py:190} INFO - [2024-12-17T18:18:50.159+0530] {taskinstance.py:352} INFO - Marking task as SUCCESS. dag_id=movie_pipeline, task_id=scrape_collection_page, run_id=manual__2024-12-17T12:48:44.832807+00:00, execution_date=20241217T124844, start_date=, end_date=20241217T124850
[2024-12-17T18:18:50.169+0530] {logging_mixin.py:190} INFO - Task instance in success state
[2024-12-17T18:18:50.169+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: running
[2024-12-17T18:18:50.169+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline queued_at:None
[2024-12-17T18:18:50.169+0530] {logging_mixin.py:190} INFO - Task hostname:Jayeshs-MacBook-Air.local operator:PythonOperator
[2024-12-17T18:18:50.170+0530] {logging_mixin.py:190} INFO - [2024-12-17T18:18:50.170+0530] {dag.py:4410} INFO - [DAG TEST] end task task_id=scrape_collection_page map_index=-1
[2024-12-17T18:18:50.174+0530] {logging_mixin.py:190} INFO - [2024-12-17T18:18:50.173+0530] {dagrun.py:854} INFO - Marking run <DagRun movie_pipeline @ 2024-12-17 12:48:44.832807+00:00: manual__2024-12-17T12:48:44.832807+00:00, state:running, queued_at: None. externally triggered: False> successful
[2024-12-17T18:18:50.174+0530] {logging_mixin.py:190} INFO - Dag run in success state
[2024-12-17T18:18:50.174+0530] {logging_mixin.py:190} INFO - Dag run start:2024-12-17 12:48:44.832807+00:00 end:2024-12-17 12:48:50.174217+00:00
[2024-12-17T18:18:50.174+0530] {logging_mixin.py:190} INFO - [2024-12-17T18:18:50.174+0530] {dagrun.py:905} INFO - DagRun Finished: dag_id=movie_pipeline, execution_date=2024-12-17 12:48:44.832807+00:00, run_id=manual__2024-12-17T12:48:44.832807+00:00, run_start_date=2024-12-17 12:48:44.832807+00:00, run_end_date=2024-12-17 12:48:50.174217+00:00, run_duration=5.34141, state=success, external_trigger=False, run_type=manual, data_interval_start=2024-12-16 12:48:44.832807+00:00, data_interval_end=2024-12-17 12:48:44.832807+00:00, dag_hash=None
[2024-12-17T18:18:50.183+0530] {processor.py:925} INFO - DAG(s) 'movie_pipeline' retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T18:18:50.195+0530] {logging_mixin.py:190} INFO - [2024-12-17T18:18:50.195+0530] {dag.py:3239} INFO - Sync 1 DAGs
[2024-12-17T18:18:50.213+0530] {logging_mixin.py:190} INFO - [2024-12-17T18:18:50.213+0530] {dag.py:4180} INFO - Setting next_dagrun for movie_pipeline to 2024-12-17 01:30:00+00:00, run_after=2024-12-18 01:30:00+00:00
[2024-12-17T18:18:50.231+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 5.444 seconds
[2024-12-17T18:21:29.963+0530] {processor.py:186} INFO - Started process (PID=82971) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T18:21:29.964+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-17T18:21:29.965+0530] {logging_mixin.py:190} INFO - [2024-12-17T18:21:29.965+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T18:21:30.129+0530] {logging_mixin.py:190} INFO - [2024-12-17T18:21:30.129+0530] {dag.py:4435} INFO - dagrun id: movie_pipeline
[2024-12-17T18:21:30.146+0530] {logging_mixin.py:190} INFO - [2024-12-17T18:21:30.145+0530] {dag.py:4451} INFO - created dagrun <DagRun movie_pipeline @ 2024-12-17 12:51:30.003791+00:00: manual__2024-12-17T12:51:30.003791+00:00, state:running, queued_at: None. externally triggered: False>
[2024-12-17T18:21:30.171+0530] {logging_mixin.py:190} INFO - [2024-12-17T18:21:30.170+0530] {dag.py:4396} INFO - [DAG TEST] starting task_id=scrape_collection_page map_index=-1
[2024-12-17T18:21:30.171+0530] {logging_mixin.py:190} INFO - [2024-12-17T18:21:30.171+0530] {dag.py:4399} INFO - [DAG TEST] running task <TaskInstance: movie_pipeline.scrape_collection_page manual__2024-12-17T12:51:30.003791+00:00 [scheduled]>
[2024-12-17T18:21:35.265+0530] {logging_mixin.py:190} INFO - [2024-12-17T18:21:35.265+0530] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-12-17T18:21:35.323+0530] {logging_mixin.py:190} INFO - [2024-12-17 18:21:35,323] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T12:51:30.003791+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T12:51:30.003791+00:00'
[2024-12-17T18:21:35.324+0530] {logging_mixin.py:190} INFO - [2024-12-17T18:21:35.323+0530] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T12:51:30.003791+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T12:51:30.003791+00:00'
[2024-12-17T18:21:35.327+0530] {logging_mixin.py:190} INFO - Task instance is in running state
[2024-12-17T18:21:35.327+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: queued
[2024-12-17T18:21:35.327+0530] {logging_mixin.py:190} INFO - Current task name:scrape_collection_page state:scheduled start_date:None
[2024-12-17T18:21:35.327+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline and current dag run status:running
[2024-12-17T18:21:35.328+0530] {logging_mixin.py:190} INFO - [2024-12-17T18:21:35.328+0530] {taskinstance.py:731} INFO - ::endgroup::
[2024-12-17T18:21:35.330+0530] {logging_mixin.py:190} INFO - 2024-12-17
[2024-12-17T18:21:35.330+0530] {logging_mixin.py:190} INFO - [2024-12-17 18:21:35,330] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T18:21:35.330+0530] {logging_mixin.py:190} INFO - [2024-12-17T18:21:35.330+0530] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T18:21:35.335+0530] {logging_mixin.py:190} INFO - [2024-12-17T18:21:35.335+0530] {taskinstance.py:340} INFO - ::group::Post task execution logs
[2024-12-17T18:21:35.336+0530] {logging_mixin.py:190} INFO - [2024-12-17T18:21:35.336+0530] {taskinstance.py:352} INFO - Marking task as SUCCESS. dag_id=movie_pipeline, task_id=scrape_collection_page, run_id=manual__2024-12-17T12:51:30.003791+00:00, execution_date=20241217T125130, start_date=, end_date=20241217T125135
[2024-12-17T18:21:35.345+0530] {logging_mixin.py:190} INFO - Task instance in success state
[2024-12-17T18:21:35.345+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: running
[2024-12-17T18:21:35.345+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline queued_at:None
[2024-12-17T18:21:35.346+0530] {logging_mixin.py:190} INFO - Task hostname:Jayeshs-MacBook-Air.local operator:PythonOperator
[2024-12-17T18:21:35.346+0530] {logging_mixin.py:190} INFO - [2024-12-17T18:21:35.346+0530] {dag.py:4410} INFO - [DAG TEST] end task task_id=scrape_collection_page map_index=-1
[2024-12-17T18:21:35.350+0530] {logging_mixin.py:190} INFO - [2024-12-17T18:21:35.350+0530] {dagrun.py:854} INFO - Marking run <DagRun movie_pipeline @ 2024-12-17 12:51:30.003791+00:00: manual__2024-12-17T12:51:30.003791+00:00, state:running, queued_at: None. externally triggered: False> successful
[2024-12-17T18:21:35.350+0530] {logging_mixin.py:190} INFO - Dag run in success state
[2024-12-17T18:21:35.350+0530] {logging_mixin.py:190} INFO - Dag run start:2024-12-17 12:51:30.003791+00:00 end:2024-12-17 12:51:35.350485+00:00
[2024-12-17T18:21:35.351+0530] {logging_mixin.py:190} INFO - [2024-12-17T18:21:35.351+0530] {dagrun.py:905} INFO - DagRun Finished: dag_id=movie_pipeline, execution_date=2024-12-17 12:51:30.003791+00:00, run_id=manual__2024-12-17T12:51:30.003791+00:00, run_start_date=2024-12-17 12:51:30.003791+00:00, run_end_date=2024-12-17 12:51:35.350485+00:00, run_duration=5.346694, state=success, external_trigger=False, run_type=manual, data_interval_start=2024-12-16 12:51:30.003791+00:00, data_interval_end=2024-12-17 12:51:30.003791+00:00, dag_hash=None
[2024-12-17T18:21:35.361+0530] {processor.py:925} INFO - DAG(s) 'movie_pipeline' retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T18:21:35.373+0530] {logging_mixin.py:190} INFO - [2024-12-17T18:21:35.372+0530] {dag.py:3239} INFO - Sync 1 DAGs
[2024-12-17T18:21:35.389+0530] {logging_mixin.py:190} INFO - [2024-12-17T18:21:35.389+0530] {dag.py:4180} INFO - Setting next_dagrun for movie_pipeline to 2024-12-17 01:30:00+00:00, run_after=2024-12-18 01:30:00+00:00
[2024-12-17T18:21:35.407+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 5.448 seconds
[2024-12-17T18:24:11.811+0530] {processor.py:186} INFO - Started process (PID=83179) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T18:24:11.813+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-17T18:24:11.815+0530] {logging_mixin.py:190} INFO - [2024-12-17T18:24:11.815+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T18:24:11.978+0530] {logging_mixin.py:190} INFO - [2024-12-17T18:24:11.978+0530] {dag.py:4435} INFO - dagrun id: movie_pipeline
[2024-12-17T18:24:11.992+0530] {logging_mixin.py:190} INFO - [2024-12-17T18:24:11.992+0530] {dag.py:4451} INFO - created dagrun <DagRun movie_pipeline @ 2024-12-17 12:54:11.855101+00:00: manual__2024-12-17T12:54:11.855101+00:00, state:running, queued_at: None. externally triggered: False>
[2024-12-17T18:24:12.017+0530] {logging_mixin.py:190} INFO - [2024-12-17T18:24:12.017+0530] {dag.py:4396} INFO - [DAG TEST] starting task_id=scrape_collection_page map_index=-1
[2024-12-17T18:24:12.018+0530] {logging_mixin.py:190} INFO - [2024-12-17T18:24:12.017+0530] {dag.py:4399} INFO - [DAG TEST] running task <TaskInstance: movie_pipeline.scrape_collection_page manual__2024-12-17T12:54:11.855101+00:00 [scheduled]>
[2024-12-17T18:24:17.110+0530] {logging_mixin.py:190} INFO - [2024-12-17T18:24:17.110+0530] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-12-17T18:24:17.169+0530] {logging_mixin.py:190} INFO - [2024-12-17 18:24:17,169] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T12:54:11.855101+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T12:54:11.855101+00:00'
[2024-12-17T18:24:17.170+0530] {logging_mixin.py:190} INFO - [2024-12-17T18:24:17.169+0530] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T12:54:11.855101+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T12:54:11.855101+00:00'
[2024-12-17T18:24:17.173+0530] {logging_mixin.py:190} INFO - Task instance is in running state
[2024-12-17T18:24:17.173+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: queued
[2024-12-17T18:24:17.173+0530] {logging_mixin.py:190} INFO - Current task name:scrape_collection_page state:scheduled start_date:None
[2024-12-17T18:24:17.173+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline and current dag run status:running
[2024-12-17T18:24:17.174+0530] {logging_mixin.py:190} INFO - [2024-12-17T18:24:17.174+0530] {taskinstance.py:731} INFO - ::endgroup::
[2024-12-17T18:24:17.176+0530] {logging_mixin.py:190} INFO - 2024-12-17
[2024-12-17T18:24:17.176+0530] {logging_mixin.py:190} INFO - [2024-12-17 18:24:17,176] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T18:24:17.176+0530] {logging_mixin.py:190} INFO - [2024-12-17T18:24:17.176+0530] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T18:24:17.181+0530] {logging_mixin.py:190} INFO - [2024-12-17T18:24:17.181+0530] {taskinstance.py:340} INFO - ::group::Post task execution logs
[2024-12-17T18:24:17.182+0530] {logging_mixin.py:190} INFO - [2024-12-17T18:24:17.181+0530] {taskinstance.py:352} INFO - Marking task as SUCCESS. dag_id=movie_pipeline, task_id=scrape_collection_page, run_id=manual__2024-12-17T12:54:11.855101+00:00, execution_date=20241217T125411, start_date=, end_date=20241217T125417
[2024-12-17T18:24:17.191+0530] {logging_mixin.py:190} INFO - Task instance in success state
[2024-12-17T18:24:17.191+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: running
[2024-12-17T18:24:17.191+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline queued_at:None
[2024-12-17T18:24:17.192+0530] {logging_mixin.py:190} INFO - Task hostname:Jayeshs-MacBook-Air.local operator:PythonOperator
[2024-12-17T18:24:17.192+0530] {logging_mixin.py:190} INFO - [2024-12-17T18:24:17.192+0530] {dag.py:4410} INFO - [DAG TEST] end task task_id=scrape_collection_page map_index=-1
[2024-12-17T18:24:17.196+0530] {logging_mixin.py:190} INFO - [2024-12-17T18:24:17.196+0530] {dagrun.py:854} INFO - Marking run <DagRun movie_pipeline @ 2024-12-17 12:54:11.855101+00:00: manual__2024-12-17T12:54:11.855101+00:00, state:running, queued_at: None. externally triggered: False> successful
[2024-12-17T18:24:17.196+0530] {logging_mixin.py:190} INFO - Dag run in success state
[2024-12-17T18:24:17.196+0530] {logging_mixin.py:190} INFO - Dag run start:2024-12-17 12:54:11.855101+00:00 end:2024-12-17 12:54:17.196658+00:00
[2024-12-17T18:24:17.197+0530] {logging_mixin.py:190} INFO - [2024-12-17T18:24:17.197+0530] {dagrun.py:905} INFO - DagRun Finished: dag_id=movie_pipeline, execution_date=2024-12-17 12:54:11.855101+00:00, run_id=manual__2024-12-17T12:54:11.855101+00:00, run_start_date=2024-12-17 12:54:11.855101+00:00, run_end_date=2024-12-17 12:54:17.196658+00:00, run_duration=5.341557, state=success, external_trigger=False, run_type=manual, data_interval_start=2024-12-16 12:54:11.855101+00:00, data_interval_end=2024-12-17 12:54:11.855101+00:00, dag_hash=None
[2024-12-17T18:24:17.205+0530] {processor.py:925} INFO - DAG(s) 'movie_pipeline' retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T18:24:17.217+0530] {logging_mixin.py:190} INFO - [2024-12-17T18:24:17.217+0530] {dag.py:3239} INFO - Sync 1 DAGs
[2024-12-17T18:24:17.234+0530] {logging_mixin.py:190} INFO - [2024-12-17T18:24:17.234+0530] {dag.py:4180} INFO - Setting next_dagrun for movie_pipeline to 2024-12-17 01:30:00+00:00, run_after=2024-12-18 01:30:00+00:00
[2024-12-17T18:24:17.252+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 5.445 seconds
[2024-12-17T18:26:55.664+0530] {processor.py:186} INFO - Started process (PID=83366) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T18:26:55.665+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-17T18:26:55.666+0530] {logging_mixin.py:190} INFO - [2024-12-17T18:26:55.666+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T18:26:55.828+0530] {logging_mixin.py:190} INFO - [2024-12-17T18:26:55.828+0530] {dag.py:4435} INFO - dagrun id: movie_pipeline
[2024-12-17T18:26:55.842+0530] {logging_mixin.py:190} INFO - [2024-12-17T18:26:55.841+0530] {dag.py:4451} INFO - created dagrun <DagRun movie_pipeline @ 2024-12-17 12:56:55.704981+00:00: manual__2024-12-17T12:56:55.704981+00:00, state:running, queued_at: None. externally triggered: False>
[2024-12-17T18:26:55.867+0530] {logging_mixin.py:190} INFO - [2024-12-17T18:26:55.866+0530] {dag.py:4396} INFO - [DAG TEST] starting task_id=scrape_collection_page map_index=-1
[2024-12-17T18:26:55.868+0530] {logging_mixin.py:190} INFO - [2024-12-17T18:26:55.867+0530] {dag.py:4399} INFO - [DAG TEST] running task <TaskInstance: movie_pipeline.scrape_collection_page manual__2024-12-17T12:56:55.704981+00:00 [scheduled]>
[2024-12-17T18:27:00.961+0530] {logging_mixin.py:190} INFO - [2024-12-17T18:27:00.961+0530] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-12-17T18:27:01.020+0530] {logging_mixin.py:190} INFO - [2024-12-17 18:27:01,020] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T12:56:55.704981+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T12:56:55.704981+00:00'
[2024-12-17T18:27:01.021+0530] {logging_mixin.py:190} INFO - [2024-12-17T18:27:01.020+0530] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T12:56:55.704981+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T12:56:55.704981+00:00'
[2024-12-17T18:27:01.024+0530] {logging_mixin.py:190} INFO - Task instance is in running state
[2024-12-17T18:27:01.024+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: queued
[2024-12-17T18:27:01.024+0530] {logging_mixin.py:190} INFO - Current task name:scrape_collection_page state:scheduled start_date:None
[2024-12-17T18:27:01.025+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline and current dag run status:running
[2024-12-17T18:27:01.025+0530] {logging_mixin.py:190} INFO - [2024-12-17T18:27:01.025+0530] {taskinstance.py:731} INFO - ::endgroup::
[2024-12-17T18:27:01.027+0530] {logging_mixin.py:190} INFO - 2024-12-17
[2024-12-17T18:27:01.028+0530] {logging_mixin.py:190} INFO - [2024-12-17 18:27:01,027] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T18:27:01.028+0530] {logging_mixin.py:190} INFO - [2024-12-17T18:27:01.027+0530] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T18:27:01.033+0530] {logging_mixin.py:190} INFO - [2024-12-17T18:27:01.033+0530] {taskinstance.py:340} INFO - ::group::Post task execution logs
[2024-12-17T18:27:01.033+0530] {logging_mixin.py:190} INFO - [2024-12-17T18:27:01.033+0530] {taskinstance.py:352} INFO - Marking task as SUCCESS. dag_id=movie_pipeline, task_id=scrape_collection_page, run_id=manual__2024-12-17T12:56:55.704981+00:00, execution_date=20241217T125655, start_date=, end_date=20241217T125701
[2024-12-17T18:27:01.042+0530] {logging_mixin.py:190} INFO - Task instance in success state
[2024-12-17T18:27:01.043+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: running
[2024-12-17T18:27:01.043+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline queued_at:None
[2024-12-17T18:27:01.043+0530] {logging_mixin.py:190} INFO - Task hostname:Jayeshs-MacBook-Air.local operator:PythonOperator
[2024-12-17T18:27:01.043+0530] {logging_mixin.py:190} INFO - [2024-12-17T18:27:01.043+0530] {dag.py:4410} INFO - [DAG TEST] end task task_id=scrape_collection_page map_index=-1
[2024-12-17T18:27:01.047+0530] {logging_mixin.py:190} INFO - [2024-12-17T18:27:01.047+0530] {dagrun.py:854} INFO - Marking run <DagRun movie_pipeline @ 2024-12-17 12:56:55.704981+00:00: manual__2024-12-17T12:56:55.704981+00:00, state:running, queued_at: None. externally triggered: False> successful
[2024-12-17T18:27:01.048+0530] {logging_mixin.py:190} INFO - Dag run in success state
[2024-12-17T18:27:01.048+0530] {logging_mixin.py:190} INFO - Dag run start:2024-12-17 12:56:55.704981+00:00 end:2024-12-17 12:57:01.048218+00:00
[2024-12-17T18:27:01.048+0530] {logging_mixin.py:190} INFO - [2024-12-17T18:27:01.048+0530] {dagrun.py:905} INFO - DagRun Finished: dag_id=movie_pipeline, execution_date=2024-12-17 12:56:55.704981+00:00, run_id=manual__2024-12-17T12:56:55.704981+00:00, run_start_date=2024-12-17 12:56:55.704981+00:00, run_end_date=2024-12-17 12:57:01.048218+00:00, run_duration=5.343237, state=success, external_trigger=False, run_type=manual, data_interval_start=2024-12-16 12:56:55.704981+00:00, data_interval_end=2024-12-17 12:56:55.704981+00:00, dag_hash=None
[2024-12-17T18:27:01.058+0530] {processor.py:925} INFO - DAG(s) 'movie_pipeline' retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T18:27:01.070+0530] {logging_mixin.py:190} INFO - [2024-12-17T18:27:01.069+0530] {dag.py:3239} INFO - Sync 1 DAGs
[2024-12-17T18:27:01.087+0530] {logging_mixin.py:190} INFO - [2024-12-17T18:27:01.086+0530] {dag.py:4180} INFO - Setting next_dagrun for movie_pipeline to 2024-12-17 01:30:00+00:00, run_after=2024-12-18 01:30:00+00:00
[2024-12-17T18:27:01.106+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 5.445 seconds
[2024-12-17T18:29:35.404+0530] {processor.py:186} INFO - Started process (PID=83553) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T18:29:35.405+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-17T18:29:35.407+0530] {logging_mixin.py:190} INFO - [2024-12-17T18:29:35.406+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T18:29:35.570+0530] {logging_mixin.py:190} INFO - [2024-12-17T18:29:35.570+0530] {dag.py:4435} INFO - dagrun id: movie_pipeline
[2024-12-17T18:29:35.585+0530] {logging_mixin.py:190} INFO - [2024-12-17T18:29:35.584+0530] {dag.py:4451} INFO - created dagrun <DagRun movie_pipeline @ 2024-12-17 12:59:35.444802+00:00: manual__2024-12-17T12:59:35.444802+00:00, state:running, queued_at: None. externally triggered: False>
[2024-12-17T18:29:35.610+0530] {logging_mixin.py:190} INFO - [2024-12-17T18:29:35.610+0530] {dag.py:4396} INFO - [DAG TEST] starting task_id=scrape_collection_page map_index=-1
[2024-12-17T18:29:35.611+0530] {logging_mixin.py:190} INFO - [2024-12-17T18:29:35.611+0530] {dag.py:4399} INFO - [DAG TEST] running task <TaskInstance: movie_pipeline.scrape_collection_page manual__2024-12-17T12:59:35.444802+00:00 [scheduled]>
[2024-12-17T18:29:40.709+0530] {logging_mixin.py:190} INFO - [2024-12-17T18:29:40.709+0530] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-12-17T18:29:40.767+0530] {logging_mixin.py:190} INFO - [2024-12-17 18:29:40,767] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T12:59:35.444802+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T12:59:35.444802+00:00'
[2024-12-17T18:29:40.767+0530] {logging_mixin.py:190} INFO - [2024-12-17T18:29:40.767+0530] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T12:59:35.444802+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T12:59:35.444802+00:00'
[2024-12-17T18:29:40.770+0530] {logging_mixin.py:190} INFO - Task instance is in running state
[2024-12-17T18:29:40.770+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: queued
[2024-12-17T18:29:40.770+0530] {logging_mixin.py:190} INFO - Current task name:scrape_collection_page state:scheduled start_date:None
[2024-12-17T18:29:40.771+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline and current dag run status:running
[2024-12-17T18:29:40.771+0530] {logging_mixin.py:190} INFO - [2024-12-17T18:29:40.771+0530] {taskinstance.py:731} INFO - ::endgroup::
[2024-12-17T18:29:40.773+0530] {logging_mixin.py:190} INFO - 2024-12-17
[2024-12-17T18:29:40.774+0530] {logging_mixin.py:190} INFO - [2024-12-17 18:29:40,773] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T18:29:40.774+0530] {logging_mixin.py:190} INFO - [2024-12-17T18:29:40.773+0530] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T18:29:40.779+0530] {logging_mixin.py:190} INFO - [2024-12-17T18:29:40.779+0530] {taskinstance.py:340} INFO - ::group::Post task execution logs
[2024-12-17T18:29:40.779+0530] {logging_mixin.py:190} INFO - [2024-12-17T18:29:40.779+0530] {taskinstance.py:352} INFO - Marking task as SUCCESS. dag_id=movie_pipeline, task_id=scrape_collection_page, run_id=manual__2024-12-17T12:59:35.444802+00:00, execution_date=20241217T125935, start_date=, end_date=20241217T125940
[2024-12-17T18:29:40.789+0530] {logging_mixin.py:190} INFO - Task instance in success state
[2024-12-17T18:29:40.789+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: running
[2024-12-17T18:29:40.789+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline queued_at:None
[2024-12-17T18:29:40.789+0530] {logging_mixin.py:190} INFO - Task hostname:Jayeshs-MacBook-Air.local operator:PythonOperator
[2024-12-17T18:29:40.790+0530] {logging_mixin.py:190} INFO - [2024-12-17T18:29:40.789+0530] {dag.py:4410} INFO - [DAG TEST] end task task_id=scrape_collection_page map_index=-1
[2024-12-17T18:29:40.793+0530] {logging_mixin.py:190} INFO - [2024-12-17T18:29:40.793+0530] {dagrun.py:854} INFO - Marking run <DagRun movie_pipeline @ 2024-12-17 12:59:35.444802+00:00: manual__2024-12-17T12:59:35.444802+00:00, state:running, queued_at: None. externally triggered: False> successful
[2024-12-17T18:29:40.794+0530] {logging_mixin.py:190} INFO - Dag run in success state
[2024-12-17T18:29:40.794+0530] {logging_mixin.py:190} INFO - Dag run start:2024-12-17 12:59:35.444802+00:00 end:2024-12-17 12:59:40.794186+00:00
[2024-12-17T18:29:40.795+0530] {logging_mixin.py:190} INFO - [2024-12-17T18:29:40.794+0530] {dagrun.py:905} INFO - DagRun Finished: dag_id=movie_pipeline, execution_date=2024-12-17 12:59:35.444802+00:00, run_id=manual__2024-12-17T12:59:35.444802+00:00, run_start_date=2024-12-17 12:59:35.444802+00:00, run_end_date=2024-12-17 12:59:40.794186+00:00, run_duration=5.349384, state=success, external_trigger=False, run_type=manual, data_interval_start=2024-12-16 12:59:35.444802+00:00, data_interval_end=2024-12-17 12:59:35.444802+00:00, dag_hash=None
[2024-12-17T18:29:40.804+0530] {processor.py:925} INFO - DAG(s) 'movie_pipeline' retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T18:29:40.817+0530] {logging_mixin.py:190} INFO - [2024-12-17T18:29:40.816+0530] {dag.py:3239} INFO - Sync 1 DAGs
[2024-12-17T18:29:40.835+0530] {logging_mixin.py:190} INFO - [2024-12-17T18:29:40.835+0530] {dag.py:4180} INFO - Setting next_dagrun for movie_pipeline to 2024-12-17 01:30:00+00:00, run_after=2024-12-18 01:30:00+00:00
[2024-12-17T18:29:40.852+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 5.451 seconds
[2024-12-17T18:32:21.534+0530] {processor.py:186} INFO - Started process (PID=83754) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T18:32:21.535+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-17T18:32:21.536+0530] {logging_mixin.py:190} INFO - [2024-12-17T18:32:21.536+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T18:32:21.698+0530] {logging_mixin.py:190} INFO - [2024-12-17T18:32:21.697+0530] {dag.py:4435} INFO - dagrun id: movie_pipeline
[2024-12-17T18:32:21.711+0530] {logging_mixin.py:190} INFO - [2024-12-17T18:32:21.711+0530] {dag.py:4451} INFO - created dagrun <DagRun movie_pipeline @ 2024-12-17 13:02:21.574445+00:00: manual__2024-12-17T13:02:21.574445+00:00, state:running, queued_at: None. externally triggered: False>
[2024-12-17T18:32:21.736+0530] {logging_mixin.py:190} INFO - [2024-12-17T18:32:21.736+0530] {dag.py:4396} INFO - [DAG TEST] starting task_id=scrape_collection_page map_index=-1
[2024-12-17T18:32:21.737+0530] {logging_mixin.py:190} INFO - [2024-12-17T18:32:21.736+0530] {dag.py:4399} INFO - [DAG TEST] running task <TaskInstance: movie_pipeline.scrape_collection_page manual__2024-12-17T13:02:21.574445+00:00 [scheduled]>
[2024-12-17T18:32:26.832+0530] {logging_mixin.py:190} INFO - [2024-12-17T18:32:26.832+0530] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-12-17T18:32:26.891+0530] {logging_mixin.py:190} INFO - [2024-12-17 18:32:26,890] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T13:02:21.574445+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T13:02:21.574445+00:00'
[2024-12-17T18:32:26.891+0530] {logging_mixin.py:190} INFO - [2024-12-17T18:32:26.890+0530] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T13:02:21.574445+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T13:02:21.574445+00:00'
[2024-12-17T18:32:26.894+0530] {logging_mixin.py:190} INFO - Task instance is in running state
[2024-12-17T18:32:26.895+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: queued
[2024-12-17T18:32:26.895+0530] {logging_mixin.py:190} INFO - Current task name:scrape_collection_page state:scheduled start_date:None
[2024-12-17T18:32:26.895+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline and current dag run status:running
[2024-12-17T18:32:26.895+0530] {logging_mixin.py:190} INFO - [2024-12-17T18:32:26.895+0530] {taskinstance.py:731} INFO - ::endgroup::
[2024-12-17T18:32:26.897+0530] {logging_mixin.py:190} INFO - 2024-12-17
[2024-12-17T18:32:26.897+0530] {logging_mixin.py:190} INFO - [2024-12-17 18:32:26,897] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T18:32:26.898+0530] {logging_mixin.py:190} INFO - [2024-12-17T18:32:26.897+0530] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T18:32:26.903+0530] {logging_mixin.py:190} INFO - [2024-12-17T18:32:26.903+0530] {taskinstance.py:340} INFO - ::group::Post task execution logs
[2024-12-17T18:32:26.904+0530] {logging_mixin.py:190} INFO - [2024-12-17T18:32:26.904+0530] {taskinstance.py:352} INFO - Marking task as SUCCESS. dag_id=movie_pipeline, task_id=scrape_collection_page, run_id=manual__2024-12-17T13:02:21.574445+00:00, execution_date=20241217T130221, start_date=, end_date=20241217T130226
[2024-12-17T18:32:26.913+0530] {logging_mixin.py:190} INFO - Task instance in success state
[2024-12-17T18:32:26.913+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: running
[2024-12-17T18:32:26.914+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline queued_at:None
[2024-12-17T18:32:26.914+0530] {logging_mixin.py:190} INFO - Task hostname:Jayeshs-MacBook-Air.local operator:PythonOperator
[2024-12-17T18:32:26.914+0530] {logging_mixin.py:190} INFO - [2024-12-17T18:32:26.914+0530] {dag.py:4410} INFO - [DAG TEST] end task task_id=scrape_collection_page map_index=-1
[2024-12-17T18:32:26.918+0530] {logging_mixin.py:190} INFO - [2024-12-17T18:32:26.918+0530] {dagrun.py:854} INFO - Marking run <DagRun movie_pipeline @ 2024-12-17 13:02:21.574445+00:00: manual__2024-12-17T13:02:21.574445+00:00, state:running, queued_at: None. externally triggered: False> successful
[2024-12-17T18:32:26.919+0530] {logging_mixin.py:190} INFO - Dag run in success state
[2024-12-17T18:32:26.919+0530] {logging_mixin.py:190} INFO - Dag run start:2024-12-17 13:02:21.574445+00:00 end:2024-12-17 13:02:26.919187+00:00
[2024-12-17T18:32:26.920+0530] {logging_mixin.py:190} INFO - [2024-12-17T18:32:26.920+0530] {dagrun.py:905} INFO - DagRun Finished: dag_id=movie_pipeline, execution_date=2024-12-17 13:02:21.574445+00:00, run_id=manual__2024-12-17T13:02:21.574445+00:00, run_start_date=2024-12-17 13:02:21.574445+00:00, run_end_date=2024-12-17 13:02:26.919187+00:00, run_duration=5.344742, state=success, external_trigger=False, run_type=manual, data_interval_start=2024-12-16 13:02:21.574445+00:00, data_interval_end=2024-12-17 13:02:21.574445+00:00, dag_hash=None
[2024-12-17T18:32:26.929+0530] {processor.py:925} INFO - DAG(s) 'movie_pipeline' retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T18:32:26.941+0530] {logging_mixin.py:190} INFO - [2024-12-17T18:32:26.940+0530] {dag.py:3239} INFO - Sync 1 DAGs
[2024-12-17T18:32:26.958+0530] {logging_mixin.py:190} INFO - [2024-12-17T18:32:26.957+0530] {dag.py:4180} INFO - Setting next_dagrun for movie_pipeline to 2024-12-17 01:30:00+00:00, run_after=2024-12-18 01:30:00+00:00
[2024-12-17T18:32:26.976+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 5.445 seconds
[2024-12-17T18:35:04.534+0530] {processor.py:186} INFO - Started process (PID=83941) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T18:35:04.535+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-17T18:35:04.536+0530] {logging_mixin.py:190} INFO - [2024-12-17T18:35:04.536+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T18:35:04.699+0530] {logging_mixin.py:190} INFO - [2024-12-17T18:35:04.698+0530] {dag.py:4435} INFO - dagrun id: movie_pipeline
[2024-12-17T18:35:04.712+0530] {logging_mixin.py:190} INFO - [2024-12-17T18:35:04.711+0530] {dag.py:4451} INFO - created dagrun <DagRun movie_pipeline @ 2024-12-17 13:05:04.574924+00:00: manual__2024-12-17T13:05:04.574924+00:00, state:running, queued_at: None. externally triggered: False>
[2024-12-17T18:35:04.737+0530] {logging_mixin.py:190} INFO - [2024-12-17T18:35:04.736+0530] {dag.py:4396} INFO - [DAG TEST] starting task_id=scrape_collection_page map_index=-1
[2024-12-17T18:35:04.737+0530] {logging_mixin.py:190} INFO - [2024-12-17T18:35:04.737+0530] {dag.py:4399} INFO - [DAG TEST] running task <TaskInstance: movie_pipeline.scrape_collection_page manual__2024-12-17T13:05:04.574924+00:00 [scheduled]>
[2024-12-17T18:35:09.837+0530] {logging_mixin.py:190} INFO - [2024-12-17T18:35:09.836+0530] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-12-17T18:35:09.895+0530] {logging_mixin.py:190} INFO - [2024-12-17 18:35:09,895] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T13:05:04.574924+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T13:05:04.574924+00:00'
[2024-12-17T18:35:09.895+0530] {logging_mixin.py:190} INFO - [2024-12-17T18:35:09.895+0530] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T13:05:04.574924+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T13:05:04.574924+00:00'
[2024-12-17T18:35:09.898+0530] {logging_mixin.py:190} INFO - Task instance is in running state
[2024-12-17T18:35:09.899+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: queued
[2024-12-17T18:35:09.899+0530] {logging_mixin.py:190} INFO - Current task name:scrape_collection_page state:scheduled start_date:None
[2024-12-17T18:35:09.899+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline and current dag run status:running
[2024-12-17T18:35:09.900+0530] {logging_mixin.py:190} INFO - [2024-12-17T18:35:09.899+0530] {taskinstance.py:731} INFO - ::endgroup::
[2024-12-17T18:35:09.901+0530] {logging_mixin.py:190} INFO - 2024-12-17
[2024-12-17T18:35:09.902+0530] {logging_mixin.py:190} INFO - [2024-12-17 18:35:09,902] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T18:35:09.902+0530] {logging_mixin.py:190} INFO - [2024-12-17T18:35:09.902+0530] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T18:35:09.908+0530] {logging_mixin.py:190} INFO - [2024-12-17T18:35:09.907+0530] {taskinstance.py:340} INFO - ::group::Post task execution logs
[2024-12-17T18:35:09.908+0530] {logging_mixin.py:190} INFO - [2024-12-17T18:35:09.908+0530] {taskinstance.py:352} INFO - Marking task as SUCCESS. dag_id=movie_pipeline, task_id=scrape_collection_page, run_id=manual__2024-12-17T13:05:04.574924+00:00, execution_date=20241217T130504, start_date=, end_date=20241217T130509
[2024-12-17T18:35:09.918+0530] {logging_mixin.py:190} INFO - Task instance in success state
[2024-12-17T18:35:09.918+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: running
[2024-12-17T18:35:09.918+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline queued_at:None
[2024-12-17T18:35:09.918+0530] {logging_mixin.py:190} INFO - Task hostname:Jayeshs-MacBook-Air.local operator:PythonOperator
[2024-12-17T18:35:09.919+0530] {logging_mixin.py:190} INFO - [2024-12-17T18:35:09.919+0530] {dag.py:4410} INFO - [DAG TEST] end task task_id=scrape_collection_page map_index=-1
[2024-12-17T18:35:09.923+0530] {logging_mixin.py:190} INFO - [2024-12-17T18:35:09.922+0530] {dagrun.py:854} INFO - Marking run <DagRun movie_pipeline @ 2024-12-17 13:05:04.574924+00:00: manual__2024-12-17T13:05:04.574924+00:00, state:running, queued_at: None. externally triggered: False> successful
[2024-12-17T18:35:09.923+0530] {logging_mixin.py:190} INFO - Dag run in success state
[2024-12-17T18:35:09.923+0530] {logging_mixin.py:190} INFO - Dag run start:2024-12-17 13:05:04.574924+00:00 end:2024-12-17 13:05:09.923216+00:00
[2024-12-17T18:35:09.924+0530] {logging_mixin.py:190} INFO - [2024-12-17T18:35:09.923+0530] {dagrun.py:905} INFO - DagRun Finished: dag_id=movie_pipeline, execution_date=2024-12-17 13:05:04.574924+00:00, run_id=manual__2024-12-17T13:05:04.574924+00:00, run_start_date=2024-12-17 13:05:04.574924+00:00, run_end_date=2024-12-17 13:05:09.923216+00:00, run_duration=5.348292, state=success, external_trigger=False, run_type=manual, data_interval_start=2024-12-16 13:05:04.574924+00:00, data_interval_end=2024-12-17 13:05:04.574924+00:00, dag_hash=None
[2024-12-17T18:35:09.933+0530] {processor.py:925} INFO - DAG(s) 'movie_pipeline' retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T18:35:09.945+0530] {logging_mixin.py:190} INFO - [2024-12-17T18:35:09.944+0530] {dag.py:3239} INFO - Sync 1 DAGs
[2024-12-17T18:35:09.962+0530] {logging_mixin.py:190} INFO - [2024-12-17T18:35:09.961+0530] {dag.py:4180} INFO - Setting next_dagrun for movie_pipeline to 2024-12-17 01:30:00+00:00, run_after=2024-12-18 01:30:00+00:00
[2024-12-17T18:35:09.979+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 5.449 seconds
[2024-12-17T18:37:47.116+0530] {processor.py:186} INFO - Started process (PID=84128) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T18:37:47.117+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-17T18:37:47.118+0530] {logging_mixin.py:190} INFO - [2024-12-17T18:37:47.118+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T18:37:47.282+0530] {logging_mixin.py:190} INFO - [2024-12-17T18:37:47.281+0530] {dag.py:4435} INFO - dagrun id: movie_pipeline
[2024-12-17T18:37:47.295+0530] {logging_mixin.py:190} INFO - [2024-12-17T18:37:47.295+0530] {dag.py:4451} INFO - created dagrun <DagRun movie_pipeline @ 2024-12-17 13:07:47.156414+00:00: manual__2024-12-17T13:07:47.156414+00:00, state:running, queued_at: None. externally triggered: False>
[2024-12-17T18:37:47.321+0530] {logging_mixin.py:190} INFO - [2024-12-17T18:37:47.321+0530] {dag.py:4396} INFO - [DAG TEST] starting task_id=scrape_collection_page map_index=-1
[2024-12-17T18:37:47.322+0530] {logging_mixin.py:190} INFO - [2024-12-17T18:37:47.321+0530] {dag.py:4399} INFO - [DAG TEST] running task <TaskInstance: movie_pipeline.scrape_collection_page manual__2024-12-17T13:07:47.156414+00:00 [scheduled]>
[2024-12-17T18:37:52.415+0530] {logging_mixin.py:190} INFO - [2024-12-17T18:37:52.415+0530] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-12-17T18:37:52.473+0530] {logging_mixin.py:190} INFO - [2024-12-17 18:37:52,473] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T13:07:47.156414+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T13:07:47.156414+00:00'
[2024-12-17T18:37:52.473+0530] {logging_mixin.py:190} INFO - [2024-12-17T18:37:52.473+0530] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T13:07:47.156414+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T13:07:47.156414+00:00'
[2024-12-17T18:37:52.476+0530] {logging_mixin.py:190} INFO - Task instance is in running state
[2024-12-17T18:37:52.476+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: queued
[2024-12-17T18:37:52.476+0530] {logging_mixin.py:190} INFO - Current task name:scrape_collection_page state:scheduled start_date:None
[2024-12-17T18:37:52.477+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline and current dag run status:running
[2024-12-17T18:37:52.477+0530] {logging_mixin.py:190} INFO - [2024-12-17T18:37:52.477+0530] {taskinstance.py:731} INFO - ::endgroup::
[2024-12-17T18:37:52.479+0530] {logging_mixin.py:190} INFO - 2024-12-17
[2024-12-17T18:37:52.479+0530] {logging_mixin.py:190} INFO - [2024-12-17 18:37:52,479] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T18:37:52.480+0530] {logging_mixin.py:190} INFO - [2024-12-17T18:37:52.479+0530] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T18:37:52.485+0530] {logging_mixin.py:190} INFO - [2024-12-17T18:37:52.484+0530] {taskinstance.py:340} INFO - ::group::Post task execution logs
[2024-12-17T18:37:52.485+0530] {logging_mixin.py:190} INFO - [2024-12-17T18:37:52.485+0530] {taskinstance.py:352} INFO - Marking task as SUCCESS. dag_id=movie_pipeline, task_id=scrape_collection_page, run_id=manual__2024-12-17T13:07:47.156414+00:00, execution_date=20241217T130747, start_date=, end_date=20241217T130752
[2024-12-17T18:37:52.494+0530] {logging_mixin.py:190} INFO - Task instance in success state
[2024-12-17T18:37:52.494+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: running
[2024-12-17T18:37:52.494+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline queued_at:None
[2024-12-17T18:37:52.495+0530] {logging_mixin.py:190} INFO - Task hostname:Jayeshs-MacBook-Air.local operator:PythonOperator
[2024-12-17T18:37:52.495+0530] {logging_mixin.py:190} INFO - [2024-12-17T18:37:52.495+0530] {dag.py:4410} INFO - [DAG TEST] end task task_id=scrape_collection_page map_index=-1
[2024-12-17T18:37:52.499+0530] {logging_mixin.py:190} INFO - [2024-12-17T18:37:52.498+0530] {dagrun.py:854} INFO - Marking run <DagRun movie_pipeline @ 2024-12-17 13:07:47.156414+00:00: manual__2024-12-17T13:07:47.156414+00:00, state:running, queued_at: None. externally triggered: False> successful
[2024-12-17T18:37:52.499+0530] {logging_mixin.py:190} INFO - Dag run in success state
[2024-12-17T18:37:52.499+0530] {logging_mixin.py:190} INFO - Dag run start:2024-12-17 13:07:47.156414+00:00 end:2024-12-17 13:07:52.499421+00:00
[2024-12-17T18:37:52.500+0530] {logging_mixin.py:190} INFO - [2024-12-17T18:37:52.499+0530] {dagrun.py:905} INFO - DagRun Finished: dag_id=movie_pipeline, execution_date=2024-12-17 13:07:47.156414+00:00, run_id=manual__2024-12-17T13:07:47.156414+00:00, run_start_date=2024-12-17 13:07:47.156414+00:00, run_end_date=2024-12-17 13:07:52.499421+00:00, run_duration=5.343007, state=success, external_trigger=False, run_type=manual, data_interval_start=2024-12-16 13:07:47.156414+00:00, data_interval_end=2024-12-17 13:07:47.156414+00:00, dag_hash=None
[2024-12-17T18:37:52.508+0530] {processor.py:925} INFO - DAG(s) 'movie_pipeline' retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T18:37:52.520+0530] {logging_mixin.py:190} INFO - [2024-12-17T18:37:52.520+0530] {dag.py:3239} INFO - Sync 1 DAGs
[2024-12-17T18:37:52.537+0530] {logging_mixin.py:190} INFO - [2024-12-17T18:37:52.536+0530] {dag.py:4180} INFO - Setting next_dagrun for movie_pipeline to 2024-12-17 01:30:00+00:00, run_after=2024-12-18 01:30:00+00:00
[2024-12-17T18:37:52.555+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 5.442 seconds
[2024-12-17T18:40:31.424+0530] {processor.py:186} INFO - Started process (PID=84329) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T18:40:31.425+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-17T18:40:31.427+0530] {logging_mixin.py:190} INFO - [2024-12-17T18:40:31.426+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T18:40:31.592+0530] {logging_mixin.py:190} INFO - [2024-12-17T18:40:31.591+0530] {dag.py:4435} INFO - dagrun id: movie_pipeline
[2024-12-17T18:40:31.606+0530] {logging_mixin.py:190} INFO - [2024-12-17T18:40:31.606+0530] {dag.py:4451} INFO - created dagrun <DagRun movie_pipeline @ 2024-12-17 13:10:31.465259+00:00: manual__2024-12-17T13:10:31.465259+00:00, state:running, queued_at: None. externally triggered: False>
[2024-12-17T18:40:31.632+0530] {logging_mixin.py:190} INFO - [2024-12-17T18:40:31.632+0530] {dag.py:4396} INFO - [DAG TEST] starting task_id=scrape_collection_page map_index=-1
[2024-12-17T18:40:31.633+0530] {logging_mixin.py:190} INFO - [2024-12-17T18:40:31.632+0530] {dag.py:4399} INFO - [DAG TEST] running task <TaskInstance: movie_pipeline.scrape_collection_page manual__2024-12-17T13:10:31.465259+00:00 [scheduled]>
[2024-12-17T18:40:36.727+0530] {logging_mixin.py:190} INFO - [2024-12-17T18:40:36.727+0530] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-12-17T18:40:36.785+0530] {logging_mixin.py:190} INFO - [2024-12-17 18:40:36,785] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T13:10:31.465259+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T13:10:31.465259+00:00'
[2024-12-17T18:40:36.786+0530] {logging_mixin.py:190} INFO - [2024-12-17T18:40:36.785+0530] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T13:10:31.465259+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T13:10:31.465259+00:00'
[2024-12-17T18:40:36.789+0530] {logging_mixin.py:190} INFO - Task instance is in running state
[2024-12-17T18:40:36.789+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: queued
[2024-12-17T18:40:36.789+0530] {logging_mixin.py:190} INFO - Current task name:scrape_collection_page state:scheduled start_date:None
[2024-12-17T18:40:36.789+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline and current dag run status:running
[2024-12-17T18:40:36.790+0530] {logging_mixin.py:190} INFO - [2024-12-17T18:40:36.790+0530] {taskinstance.py:731} INFO - ::endgroup::
[2024-12-17T18:40:36.792+0530] {logging_mixin.py:190} INFO - 2024-12-17
[2024-12-17T18:40:36.792+0530] {logging_mixin.py:190} INFO - [2024-12-17 18:40:36,792] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T18:40:36.792+0530] {logging_mixin.py:190} INFO - [2024-12-17T18:40:36.792+0530] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T18:40:36.798+0530] {logging_mixin.py:190} INFO - [2024-12-17T18:40:36.797+0530] {taskinstance.py:340} INFO - ::group::Post task execution logs
[2024-12-17T18:40:36.798+0530] {logging_mixin.py:190} INFO - [2024-12-17T18:40:36.798+0530] {taskinstance.py:352} INFO - Marking task as SUCCESS. dag_id=movie_pipeline, task_id=scrape_collection_page, run_id=manual__2024-12-17T13:10:31.465259+00:00, execution_date=20241217T131031, start_date=, end_date=20241217T131036
[2024-12-17T18:40:36.807+0530] {logging_mixin.py:190} INFO - Task instance in success state
[2024-12-17T18:40:36.808+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: running
[2024-12-17T18:40:36.808+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline queued_at:None
[2024-12-17T18:40:36.808+0530] {logging_mixin.py:190} INFO - Task hostname:Jayeshs-MacBook-Air.local operator:PythonOperator
[2024-12-17T18:40:36.809+0530] {logging_mixin.py:190} INFO - [2024-12-17T18:40:36.808+0530] {dag.py:4410} INFO - [DAG TEST] end task task_id=scrape_collection_page map_index=-1
[2024-12-17T18:40:36.812+0530] {logging_mixin.py:190} INFO - [2024-12-17T18:40:36.812+0530] {dagrun.py:854} INFO - Marking run <DagRun movie_pipeline @ 2024-12-17 13:10:31.465259+00:00: manual__2024-12-17T13:10:31.465259+00:00, state:running, queued_at: None. externally triggered: False> successful
[2024-12-17T18:40:36.812+0530] {logging_mixin.py:190} INFO - Dag run in success state
[2024-12-17T18:40:36.813+0530] {logging_mixin.py:190} INFO - Dag run start:2024-12-17 13:10:31.465259+00:00 end:2024-12-17 13:10:36.812896+00:00
[2024-12-17T18:40:36.813+0530] {logging_mixin.py:190} INFO - [2024-12-17T18:40:36.813+0530] {dagrun.py:905} INFO - DagRun Finished: dag_id=movie_pipeline, execution_date=2024-12-17 13:10:31.465259+00:00, run_id=manual__2024-12-17T13:10:31.465259+00:00, run_start_date=2024-12-17 13:10:31.465259+00:00, run_end_date=2024-12-17 13:10:36.812896+00:00, run_duration=5.347637, state=success, external_trigger=False, run_type=manual, data_interval_start=2024-12-16 13:10:31.465259+00:00, data_interval_end=2024-12-17 13:10:31.465259+00:00, dag_hash=None
[2024-12-17T18:40:36.823+0530] {processor.py:925} INFO - DAG(s) 'movie_pipeline' retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T18:40:36.834+0530] {logging_mixin.py:190} INFO - [2024-12-17T18:40:36.834+0530] {dag.py:3239} INFO - Sync 1 DAGs
[2024-12-17T18:40:36.851+0530] {logging_mixin.py:190} INFO - [2024-12-17T18:40:36.851+0530] {dag.py:4180} INFO - Setting next_dagrun for movie_pipeline to 2024-12-17 01:30:00+00:00, run_after=2024-12-18 01:30:00+00:00
[2024-12-17T18:40:36.869+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 5.448 seconds
[2024-12-17T18:43:17.230+0530] {processor.py:186} INFO - Started process (PID=84518) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T18:43:17.232+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-17T18:43:17.234+0530] {logging_mixin.py:190} INFO - [2024-12-17T18:43:17.234+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T18:43:17.394+0530] {logging_mixin.py:190} INFO - [2024-12-17T18:43:17.394+0530] {dag.py:4435} INFO - dagrun id: movie_pipeline
[2024-12-17T18:43:17.407+0530] {logging_mixin.py:190} INFO - [2024-12-17T18:43:17.407+0530] {dag.py:4451} INFO - created dagrun <DagRun movie_pipeline @ 2024-12-17 13:13:17.271954+00:00: manual__2024-12-17T13:13:17.271954+00:00, state:running, queued_at: None. externally triggered: False>
[2024-12-17T18:43:17.432+0530] {logging_mixin.py:190} INFO - [2024-12-17T18:43:17.432+0530] {dag.py:4396} INFO - [DAG TEST] starting task_id=scrape_collection_page map_index=-1
[2024-12-17T18:43:17.433+0530] {logging_mixin.py:190} INFO - [2024-12-17T18:43:17.433+0530] {dag.py:4399} INFO - [DAG TEST] running task <TaskInstance: movie_pipeline.scrape_collection_page manual__2024-12-17T13:13:17.271954+00:00 [scheduled]>
[2024-12-17T18:43:22.531+0530] {logging_mixin.py:190} INFO - [2024-12-17T18:43:22.531+0530] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-12-17T18:43:22.589+0530] {logging_mixin.py:190} INFO - [2024-12-17 18:43:22,589] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T13:13:17.271954+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T13:13:17.271954+00:00'
[2024-12-17T18:43:22.589+0530] {logging_mixin.py:190} INFO - [2024-12-17T18:43:22.589+0530] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T13:13:17.271954+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T13:13:17.271954+00:00'
[2024-12-17T18:43:22.592+0530] {logging_mixin.py:190} INFO - Task instance is in running state
[2024-12-17T18:43:22.592+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: queued
[2024-12-17T18:43:22.593+0530] {logging_mixin.py:190} INFO - Current task name:scrape_collection_page state:scheduled start_date:None
[2024-12-17T18:43:22.593+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline and current dag run status:running
[2024-12-17T18:43:22.593+0530] {logging_mixin.py:190} INFO - [2024-12-17T18:43:22.593+0530] {taskinstance.py:731} INFO - ::endgroup::
[2024-12-17T18:43:22.595+0530] {logging_mixin.py:190} INFO - 2024-12-17
[2024-12-17T18:43:22.596+0530] {logging_mixin.py:190} INFO - [2024-12-17 18:43:22,595] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T18:43:22.596+0530] {logging_mixin.py:190} INFO - [2024-12-17T18:43:22.595+0530] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T18:43:22.601+0530] {logging_mixin.py:190} INFO - [2024-12-17T18:43:22.601+0530] {taskinstance.py:340} INFO - ::group::Post task execution logs
[2024-12-17T18:43:22.601+0530] {logging_mixin.py:190} INFO - [2024-12-17T18:43:22.601+0530] {taskinstance.py:352} INFO - Marking task as SUCCESS. dag_id=movie_pipeline, task_id=scrape_collection_page, run_id=manual__2024-12-17T13:13:17.271954+00:00, execution_date=20241217T131317, start_date=, end_date=20241217T131322
[2024-12-17T18:43:22.611+0530] {logging_mixin.py:190} INFO - Task instance in success state
[2024-12-17T18:43:22.611+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: running
[2024-12-17T18:43:22.612+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline queued_at:None
[2024-12-17T18:43:22.612+0530] {logging_mixin.py:190} INFO - Task hostname:Jayeshs-MacBook-Air.local operator:PythonOperator
[2024-12-17T18:43:22.612+0530] {logging_mixin.py:190} INFO - [2024-12-17T18:43:22.612+0530] {dag.py:4410} INFO - [DAG TEST] end task task_id=scrape_collection_page map_index=-1
[2024-12-17T18:43:22.616+0530] {logging_mixin.py:190} INFO - [2024-12-17T18:43:22.616+0530] {dagrun.py:854} INFO - Marking run <DagRun movie_pipeline @ 2024-12-17 13:13:17.271954+00:00: manual__2024-12-17T13:13:17.271954+00:00, state:running, queued_at: None. externally triggered: False> successful
[2024-12-17T18:43:22.616+0530] {logging_mixin.py:190} INFO - Dag run in success state
[2024-12-17T18:43:22.616+0530] {logging_mixin.py:190} INFO - Dag run start:2024-12-17 13:13:17.271954+00:00 end:2024-12-17 13:13:22.616627+00:00
[2024-12-17T18:43:22.617+0530] {logging_mixin.py:190} INFO - [2024-12-17T18:43:22.617+0530] {dagrun.py:905} INFO - DagRun Finished: dag_id=movie_pipeline, execution_date=2024-12-17 13:13:17.271954+00:00, run_id=manual__2024-12-17T13:13:17.271954+00:00, run_start_date=2024-12-17 13:13:17.271954+00:00, run_end_date=2024-12-17 13:13:22.616627+00:00, run_duration=5.344673, state=success, external_trigger=False, run_type=manual, data_interval_start=2024-12-16 13:13:17.271954+00:00, data_interval_end=2024-12-17 13:13:17.271954+00:00, dag_hash=None
[2024-12-17T18:43:22.626+0530] {processor.py:925} INFO - DAG(s) 'movie_pipeline' retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T18:43:22.638+0530] {logging_mixin.py:190} INFO - [2024-12-17T18:43:22.637+0530] {dag.py:3239} INFO - Sync 1 DAGs
[2024-12-17T18:43:22.654+0530] {logging_mixin.py:190} INFO - [2024-12-17T18:43:22.654+0530] {dag.py:4180} INFO - Setting next_dagrun for movie_pipeline to 2024-12-17 01:30:00+00:00, run_after=2024-12-18 01:30:00+00:00
[2024-12-17T18:43:22.674+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 5.447 seconds
[2024-12-17T18:46:00.676+0530] {processor.py:186} INFO - Started process (PID=84712) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T18:46:00.677+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-17T18:46:00.678+0530] {logging_mixin.py:190} INFO - [2024-12-17T18:46:00.678+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T18:46:00.842+0530] {logging_mixin.py:190} INFO - [2024-12-17T18:46:00.841+0530] {dag.py:4435} INFO - dagrun id: movie_pipeline
[2024-12-17T18:46:00.856+0530] {logging_mixin.py:190} INFO - [2024-12-17T18:46:00.855+0530] {dag.py:4451} INFO - created dagrun <DagRun movie_pipeline @ 2024-12-17 13:16:00.716403+00:00: manual__2024-12-17T13:16:00.716403+00:00, state:running, queued_at: None. externally triggered: False>
[2024-12-17T18:46:00.881+0530] {logging_mixin.py:190} INFO - [2024-12-17T18:46:00.880+0530] {dag.py:4396} INFO - [DAG TEST] starting task_id=scrape_collection_page map_index=-1
[2024-12-17T18:46:00.881+0530] {logging_mixin.py:190} INFO - [2024-12-17T18:46:00.881+0530] {dag.py:4399} INFO - [DAG TEST] running task <TaskInstance: movie_pipeline.scrape_collection_page manual__2024-12-17T13:16:00.716403+00:00 [scheduled]>
[2024-12-17T18:46:05.976+0530] {logging_mixin.py:190} INFO - [2024-12-17T18:46:05.976+0530] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-12-17T18:46:06.034+0530] {logging_mixin.py:190} INFO - [2024-12-17 18:46:06,034] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T13:16:00.716403+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T13:16:00.716403+00:00'
[2024-12-17T18:46:06.034+0530] {logging_mixin.py:190} INFO - [2024-12-17T18:46:06.034+0530] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T13:16:00.716403+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T13:16:00.716403+00:00'
[2024-12-17T18:46:06.037+0530] {logging_mixin.py:190} INFO - Task instance is in running state
[2024-12-17T18:46:06.038+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: queued
[2024-12-17T18:46:06.038+0530] {logging_mixin.py:190} INFO - Current task name:scrape_collection_page state:scheduled start_date:None
[2024-12-17T18:46:06.038+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline and current dag run status:running
[2024-12-17T18:46:06.039+0530] {logging_mixin.py:190} INFO - [2024-12-17T18:46:06.038+0530] {taskinstance.py:731} INFO - ::endgroup::
[2024-12-17T18:46:06.040+0530] {logging_mixin.py:190} INFO - 2024-12-17
[2024-12-17T18:46:06.041+0530] {logging_mixin.py:190} INFO - [2024-12-17 18:46:06,041] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T18:46:06.041+0530] {logging_mixin.py:190} INFO - [2024-12-17T18:46:06.041+0530] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T18:46:06.046+0530] {logging_mixin.py:190} INFO - [2024-12-17T18:46:06.046+0530] {taskinstance.py:340} INFO - ::group::Post task execution logs
[2024-12-17T18:46:06.047+0530] {logging_mixin.py:190} INFO - [2024-12-17T18:46:06.046+0530] {taskinstance.py:352} INFO - Marking task as SUCCESS. dag_id=movie_pipeline, task_id=scrape_collection_page, run_id=manual__2024-12-17T13:16:00.716403+00:00, execution_date=20241217T131600, start_date=, end_date=20241217T131606
[2024-12-17T18:46:06.056+0530] {logging_mixin.py:190} INFO - Task instance in success state
[2024-12-17T18:46:06.056+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: running
[2024-12-17T18:46:06.056+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline queued_at:None
[2024-12-17T18:46:06.057+0530] {logging_mixin.py:190} INFO - Task hostname:Jayeshs-MacBook-Air.local operator:PythonOperator
[2024-12-17T18:46:06.057+0530] {logging_mixin.py:190} INFO - [2024-12-17T18:46:06.057+0530] {dag.py:4410} INFO - [DAG TEST] end task task_id=scrape_collection_page map_index=-1
[2024-12-17T18:46:06.061+0530] {logging_mixin.py:190} INFO - [2024-12-17T18:46:06.060+0530] {dagrun.py:854} INFO - Marking run <DagRun movie_pipeline @ 2024-12-17 13:16:00.716403+00:00: manual__2024-12-17T13:16:00.716403+00:00, state:running, queued_at: None. externally triggered: False> successful
[2024-12-17T18:46:06.061+0530] {logging_mixin.py:190} INFO - Dag run in success state
[2024-12-17T18:46:06.061+0530] {logging_mixin.py:190} INFO - Dag run start:2024-12-17 13:16:00.716403+00:00 end:2024-12-17 13:16:06.061320+00:00
[2024-12-17T18:46:06.061+0530] {logging_mixin.py:190} INFO - [2024-12-17T18:46:06.061+0530] {dagrun.py:905} INFO - DagRun Finished: dag_id=movie_pipeline, execution_date=2024-12-17 13:16:00.716403+00:00, run_id=manual__2024-12-17T13:16:00.716403+00:00, run_start_date=2024-12-17 13:16:00.716403+00:00, run_end_date=2024-12-17 13:16:06.061320+00:00, run_duration=5.344917, state=success, external_trigger=False, run_type=manual, data_interval_start=2024-12-16 13:16:00.716403+00:00, data_interval_end=2024-12-17 13:16:00.716403+00:00, dag_hash=None
[2024-12-17T18:46:06.070+0530] {processor.py:925} INFO - DAG(s) 'movie_pipeline' retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T18:46:06.083+0530] {logging_mixin.py:190} INFO - [2024-12-17T18:46:06.082+0530] {dag.py:3239} INFO - Sync 1 DAGs
[2024-12-17T18:46:06.099+0530] {logging_mixin.py:190} INFO - [2024-12-17T18:46:06.099+0530] {dag.py:4180} INFO - Setting next_dagrun for movie_pipeline to 2024-12-17 01:30:00+00:00, run_after=2024-12-18 01:30:00+00:00
[2024-12-17T18:46:06.119+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 5.447 seconds
[2024-12-17T18:48:46.127+0530] {processor.py:186} INFO - Started process (PID=84911) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T18:48:46.128+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-17T18:48:46.129+0530] {logging_mixin.py:190} INFO - [2024-12-17T18:48:46.129+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T18:48:46.289+0530] {logging_mixin.py:190} INFO - [2024-12-17T18:48:46.288+0530] {dag.py:4435} INFO - dagrun id: movie_pipeline
[2024-12-17T18:48:46.302+0530] {logging_mixin.py:190} INFO - [2024-12-17T18:48:46.302+0530] {dag.py:4451} INFO - created dagrun <DagRun movie_pipeline @ 2024-12-17 13:18:46.166953+00:00: manual__2024-12-17T13:18:46.166953+00:00, state:running, queued_at: None. externally triggered: False>
[2024-12-17T18:48:46.327+0530] {logging_mixin.py:190} INFO - [2024-12-17T18:48:46.326+0530] {dag.py:4396} INFO - [DAG TEST] starting task_id=scrape_collection_page map_index=-1
[2024-12-17T18:48:46.327+0530] {logging_mixin.py:190} INFO - [2024-12-17T18:48:46.327+0530] {dag.py:4399} INFO - [DAG TEST] running task <TaskInstance: movie_pipeline.scrape_collection_page manual__2024-12-17T13:18:46.166953+00:00 [scheduled]>
[2024-12-17T18:48:51.421+0530] {logging_mixin.py:190} INFO - [2024-12-17T18:48:51.421+0530] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-12-17T18:48:51.479+0530] {logging_mixin.py:190} INFO - [2024-12-17 18:48:51,479] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T13:18:46.166953+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T13:18:46.166953+00:00'
[2024-12-17T18:48:51.480+0530] {logging_mixin.py:190} INFO - [2024-12-17T18:48:51.479+0530] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T13:18:46.166953+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T13:18:46.166953+00:00'
[2024-12-17T18:48:51.482+0530] {logging_mixin.py:190} INFO - Task instance is in running state
[2024-12-17T18:48:51.483+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: queued
[2024-12-17T18:48:51.483+0530] {logging_mixin.py:190} INFO - Current task name:scrape_collection_page state:scheduled start_date:None
[2024-12-17T18:48:51.483+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline and current dag run status:running
[2024-12-17T18:48:51.484+0530] {logging_mixin.py:190} INFO - [2024-12-17T18:48:51.483+0530] {taskinstance.py:731} INFO - ::endgroup::
[2024-12-17T18:48:51.485+0530] {logging_mixin.py:190} INFO - 2024-12-17
[2024-12-17T18:48:51.486+0530] {logging_mixin.py:190} INFO - [2024-12-17 18:48:51,486] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T18:48:51.486+0530] {logging_mixin.py:190} INFO - [2024-12-17T18:48:51.486+0530] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T18:48:51.491+0530] {logging_mixin.py:190} INFO - [2024-12-17T18:48:51.491+0530] {taskinstance.py:340} INFO - ::group::Post task execution logs
[2024-12-17T18:48:51.491+0530] {logging_mixin.py:190} INFO - [2024-12-17T18:48:51.491+0530] {taskinstance.py:352} INFO - Marking task as SUCCESS. dag_id=movie_pipeline, task_id=scrape_collection_page, run_id=manual__2024-12-17T13:18:46.166953+00:00, execution_date=20241217T131846, start_date=, end_date=20241217T131851
[2024-12-17T18:48:51.501+0530] {logging_mixin.py:190} INFO - Task instance in success state
[2024-12-17T18:48:51.501+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: running
[2024-12-17T18:48:51.502+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline queued_at:None
[2024-12-17T18:48:51.502+0530] {logging_mixin.py:190} INFO - Task hostname:Jayeshs-MacBook-Air.local operator:PythonOperator
[2024-12-17T18:48:51.502+0530] {logging_mixin.py:190} INFO - [2024-12-17T18:48:51.502+0530] {dag.py:4410} INFO - [DAG TEST] end task task_id=scrape_collection_page map_index=-1
[2024-12-17T18:48:51.506+0530] {logging_mixin.py:190} INFO - [2024-12-17T18:48:51.506+0530] {dagrun.py:854} INFO - Marking run <DagRun movie_pipeline @ 2024-12-17 13:18:46.166953+00:00: manual__2024-12-17T13:18:46.166953+00:00, state:running, queued_at: None. externally triggered: False> successful
[2024-12-17T18:48:51.507+0530] {logging_mixin.py:190} INFO - Dag run in success state
[2024-12-17T18:48:51.507+0530] {logging_mixin.py:190} INFO - Dag run start:2024-12-17 13:18:46.166953+00:00 end:2024-12-17 13:18:51.506906+00:00
[2024-12-17T18:48:51.508+0530] {logging_mixin.py:190} INFO - [2024-12-17T18:48:51.507+0530] {dagrun.py:905} INFO - DagRun Finished: dag_id=movie_pipeline, execution_date=2024-12-17 13:18:46.166953+00:00, run_id=manual__2024-12-17T13:18:46.166953+00:00, run_start_date=2024-12-17 13:18:46.166953+00:00, run_end_date=2024-12-17 13:18:51.506906+00:00, run_duration=5.339953, state=success, external_trigger=False, run_type=manual, data_interval_start=2024-12-16 13:18:46.166953+00:00, data_interval_end=2024-12-17 13:18:46.166953+00:00, dag_hash=None
[2024-12-17T18:48:51.517+0530] {processor.py:925} INFO - DAG(s) 'movie_pipeline' retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T18:48:51.528+0530] {logging_mixin.py:190} INFO - [2024-12-17T18:48:51.528+0530] {dag.py:3239} INFO - Sync 1 DAGs
[2024-12-17T18:48:51.545+0530] {logging_mixin.py:190} INFO - [2024-12-17T18:48:51.545+0530] {dag.py:4180} INFO - Setting next_dagrun for movie_pipeline to 2024-12-17 01:30:00+00:00, run_after=2024-12-18 01:30:00+00:00
[2024-12-17T18:48:51.562+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 5.439 seconds
[2024-12-17T18:51:32.678+0530] {processor.py:186} INFO - Started process (PID=85106) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T18:51:32.680+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-17T18:51:32.681+0530] {logging_mixin.py:190} INFO - [2024-12-17T18:51:32.681+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T18:51:32.842+0530] {logging_mixin.py:190} INFO - [2024-12-17T18:51:32.842+0530] {dag.py:4435} INFO - dagrun id: movie_pipeline
[2024-12-17T18:51:32.856+0530] {logging_mixin.py:190} INFO - [2024-12-17T18:51:32.856+0530] {dag.py:4451} INFO - created dagrun <DagRun movie_pipeline @ 2024-12-17 13:21:32.719412+00:00: manual__2024-12-17T13:21:32.719412+00:00, state:running, queued_at: None. externally triggered: False>
[2024-12-17T18:51:32.881+0530] {logging_mixin.py:190} INFO - [2024-12-17T18:51:32.881+0530] {dag.py:4396} INFO - [DAG TEST] starting task_id=scrape_collection_page map_index=-1
[2024-12-17T18:51:32.882+0530] {logging_mixin.py:190} INFO - [2024-12-17T18:51:32.882+0530] {dag.py:4399} INFO - [DAG TEST] running task <TaskInstance: movie_pipeline.scrape_collection_page manual__2024-12-17T13:21:32.719412+00:00 [scheduled]>
[2024-12-17T18:51:37.980+0530] {logging_mixin.py:190} INFO - [2024-12-17T18:51:37.980+0530] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-12-17T18:51:38.041+0530] {logging_mixin.py:190} INFO - [2024-12-17 18:51:38,041] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T13:21:32.719412+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T13:21:32.719412+00:00'
[2024-12-17T18:51:38.041+0530] {logging_mixin.py:190} INFO - [2024-12-17T18:51:38.041+0530] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T13:21:32.719412+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T13:21:32.719412+00:00'
[2024-12-17T18:51:38.044+0530] {logging_mixin.py:190} INFO - Task instance is in running state
[2024-12-17T18:51:38.044+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: queued
[2024-12-17T18:51:38.045+0530] {logging_mixin.py:190} INFO - Current task name:scrape_collection_page state:scheduled start_date:None
[2024-12-17T18:51:38.045+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline and current dag run status:running
[2024-12-17T18:51:38.045+0530] {logging_mixin.py:190} INFO - [2024-12-17T18:51:38.045+0530] {taskinstance.py:731} INFO - ::endgroup::
[2024-12-17T18:51:38.047+0530] {logging_mixin.py:190} INFO - 2024-12-17
[2024-12-17T18:51:38.048+0530] {logging_mixin.py:190} INFO - [2024-12-17 18:51:38,047] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T18:51:38.048+0530] {logging_mixin.py:190} INFO - [2024-12-17T18:51:38.047+0530] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T18:51:38.053+0530] {logging_mixin.py:190} INFO - [2024-12-17T18:51:38.053+0530] {taskinstance.py:340} INFO - ::group::Post task execution logs
[2024-12-17T18:51:38.053+0530] {logging_mixin.py:190} INFO - [2024-12-17T18:51:38.053+0530] {taskinstance.py:352} INFO - Marking task as SUCCESS. dag_id=movie_pipeline, task_id=scrape_collection_page, run_id=manual__2024-12-17T13:21:32.719412+00:00, execution_date=20241217T132132, start_date=, end_date=20241217T132138
[2024-12-17T18:51:38.063+0530] {logging_mixin.py:190} INFO - Task instance in success state
[2024-12-17T18:51:38.063+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: running
[2024-12-17T18:51:38.063+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline queued_at:None
[2024-12-17T18:51:38.063+0530] {logging_mixin.py:190} INFO - Task hostname:Jayeshs-MacBook-Air.local operator:PythonOperator
[2024-12-17T18:51:38.064+0530] {logging_mixin.py:190} INFO - [2024-12-17T18:51:38.063+0530] {dag.py:4410} INFO - [DAG TEST] end task task_id=scrape_collection_page map_index=-1
[2024-12-17T18:51:38.068+0530] {logging_mixin.py:190} INFO - [2024-12-17T18:51:38.068+0530] {dagrun.py:854} INFO - Marking run <DagRun movie_pipeline @ 2024-12-17 13:21:32.719412+00:00: manual__2024-12-17T13:21:32.719412+00:00, state:running, queued_at: None. externally triggered: False> successful
[2024-12-17T18:51:38.069+0530] {logging_mixin.py:190} INFO - Dag run in success state
[2024-12-17T18:51:38.069+0530] {logging_mixin.py:190} INFO - Dag run start:2024-12-17 13:21:32.719412+00:00 end:2024-12-17 13:21:38.069121+00:00
[2024-12-17T18:51:38.069+0530] {logging_mixin.py:190} INFO - [2024-12-17T18:51:38.069+0530] {dagrun.py:905} INFO - DagRun Finished: dag_id=movie_pipeline, execution_date=2024-12-17 13:21:32.719412+00:00, run_id=manual__2024-12-17T13:21:32.719412+00:00, run_start_date=2024-12-17 13:21:32.719412+00:00, run_end_date=2024-12-17 13:21:38.069121+00:00, run_duration=5.349709, state=success, external_trigger=False, run_type=manual, data_interval_start=2024-12-16 13:21:32.719412+00:00, data_interval_end=2024-12-17 13:21:32.719412+00:00, dag_hash=None
[2024-12-17T18:51:38.079+0530] {processor.py:925} INFO - DAG(s) 'movie_pipeline' retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T18:51:38.090+0530] {logging_mixin.py:190} INFO - [2024-12-17T18:51:38.090+0530] {dag.py:3239} INFO - Sync 1 DAGs
[2024-12-17T18:51:38.107+0530] {logging_mixin.py:190} INFO - [2024-12-17T18:51:38.106+0530] {dag.py:4180} INFO - Setting next_dagrun for movie_pipeline to 2024-12-17 01:30:00+00:00, run_after=2024-12-18 01:30:00+00:00
[2024-12-17T18:51:38.124+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 5.450 seconds
[2024-12-17T18:54:17.668+0530] {processor.py:186} INFO - Started process (PID=85332) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T18:54:17.670+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-17T18:54:17.672+0530] {logging_mixin.py:190} INFO - [2024-12-17T18:54:17.671+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T18:54:17.834+0530] {logging_mixin.py:190} INFO - [2024-12-17T18:54:17.834+0530] {dag.py:4435} INFO - dagrun id: movie_pipeline
[2024-12-17T18:54:17.848+0530] {logging_mixin.py:190} INFO - [2024-12-17T18:54:17.848+0530] {dag.py:4451} INFO - created dagrun <DagRun movie_pipeline @ 2024-12-17 13:24:17.709638+00:00: manual__2024-12-17T13:24:17.709638+00:00, state:running, queued_at: None. externally triggered: False>
[2024-12-17T18:54:17.873+0530] {logging_mixin.py:190} INFO - [2024-12-17T18:54:17.873+0530] {dag.py:4396} INFO - [DAG TEST] starting task_id=scrape_collection_page map_index=-1
[2024-12-17T18:54:17.874+0530] {logging_mixin.py:190} INFO - [2024-12-17T18:54:17.874+0530] {dag.py:4399} INFO - [DAG TEST] running task <TaskInstance: movie_pipeline.scrape_collection_page manual__2024-12-17T13:24:17.709638+00:00 [scheduled]>
[2024-12-17T18:54:22.968+0530] {logging_mixin.py:190} INFO - [2024-12-17T18:54:22.968+0530] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-12-17T18:54:23.026+0530] {logging_mixin.py:190} INFO - [2024-12-17 18:54:23,026] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T13:24:17.709638+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T13:24:17.709638+00:00'
[2024-12-17T18:54:23.027+0530] {logging_mixin.py:190} INFO - [2024-12-17T18:54:23.026+0530] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T13:24:17.709638+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T13:24:17.709638+00:00'
[2024-12-17T18:54:23.029+0530] {logging_mixin.py:190} INFO - Task instance is in running state
[2024-12-17T18:54:23.030+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: queued
[2024-12-17T18:54:23.030+0530] {logging_mixin.py:190} INFO - Current task name:scrape_collection_page state:scheduled start_date:None
[2024-12-17T18:54:23.030+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline and current dag run status:running
[2024-12-17T18:54:23.031+0530] {logging_mixin.py:190} INFO - [2024-12-17T18:54:23.031+0530] {taskinstance.py:731} INFO - ::endgroup::
[2024-12-17T18:54:23.033+0530] {logging_mixin.py:190} INFO - 2024-12-17
[2024-12-17T18:54:23.033+0530] {logging_mixin.py:190} INFO - [2024-12-17 18:54:23,033] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T18:54:23.033+0530] {logging_mixin.py:190} INFO - [2024-12-17T18:54:23.033+0530] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T18:54:23.038+0530] {logging_mixin.py:190} INFO - [2024-12-17T18:54:23.038+0530] {taskinstance.py:340} INFO - ::group::Post task execution logs
[2024-12-17T18:54:23.039+0530] {logging_mixin.py:190} INFO - [2024-12-17T18:54:23.039+0530] {taskinstance.py:352} INFO - Marking task as SUCCESS. dag_id=movie_pipeline, task_id=scrape_collection_page, run_id=manual__2024-12-17T13:24:17.709638+00:00, execution_date=20241217T132417, start_date=, end_date=20241217T132423
[2024-12-17T18:54:23.048+0530] {logging_mixin.py:190} INFO - Task instance in success state
[2024-12-17T18:54:23.048+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: running
[2024-12-17T18:54:23.049+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline queued_at:None
[2024-12-17T18:54:23.049+0530] {logging_mixin.py:190} INFO - Task hostname:Jayeshs-MacBook-Air.local operator:PythonOperator
[2024-12-17T18:54:23.049+0530] {logging_mixin.py:190} INFO - [2024-12-17T18:54:23.049+0530] {dag.py:4410} INFO - [DAG TEST] end task task_id=scrape_collection_page map_index=-1
[2024-12-17T18:54:23.053+0530] {logging_mixin.py:190} INFO - [2024-12-17T18:54:23.053+0530] {dagrun.py:854} INFO - Marking run <DagRun movie_pipeline @ 2024-12-17 13:24:17.709638+00:00: manual__2024-12-17T13:24:17.709638+00:00, state:running, queued_at: None. externally triggered: False> successful
[2024-12-17T18:54:23.053+0530] {logging_mixin.py:190} INFO - Dag run in success state
[2024-12-17T18:54:23.053+0530] {logging_mixin.py:190} INFO - Dag run start:2024-12-17 13:24:17.709638+00:00 end:2024-12-17 13:24:23.053564+00:00
[2024-12-17T18:54:23.054+0530] {logging_mixin.py:190} INFO - [2024-12-17T18:54:23.054+0530] {dagrun.py:905} INFO - DagRun Finished: dag_id=movie_pipeline, execution_date=2024-12-17 13:24:17.709638+00:00, run_id=manual__2024-12-17T13:24:17.709638+00:00, run_start_date=2024-12-17 13:24:17.709638+00:00, run_end_date=2024-12-17 13:24:23.053564+00:00, run_duration=5.343926, state=success, external_trigger=False, run_type=manual, data_interval_start=2024-12-16 13:24:17.709638+00:00, data_interval_end=2024-12-17 13:24:17.709638+00:00, dag_hash=None
[2024-12-17T18:54:23.063+0530] {processor.py:925} INFO - DAG(s) 'movie_pipeline' retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T18:54:23.074+0530] {logging_mixin.py:190} INFO - [2024-12-17T18:54:23.074+0530] {dag.py:3239} INFO - Sync 1 DAGs
[2024-12-17T18:54:23.092+0530] {logging_mixin.py:190} INFO - [2024-12-17T18:54:23.092+0530] {dag.py:4180} INFO - Setting next_dagrun for movie_pipeline to 2024-12-17 01:30:00+00:00, run_after=2024-12-18 01:30:00+00:00
[2024-12-17T18:54:23.109+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 5.445 seconds
[2024-12-17T18:57:02.385+0530] {processor.py:186} INFO - Started process (PID=85521) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T18:57:02.386+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-17T18:57:02.387+0530] {logging_mixin.py:190} INFO - [2024-12-17T18:57:02.387+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T18:57:02.550+0530] {logging_mixin.py:190} INFO - [2024-12-17T18:57:02.550+0530] {dag.py:4435} INFO - dagrun id: movie_pipeline
[2024-12-17T18:57:02.564+0530] {logging_mixin.py:190} INFO - [2024-12-17T18:57:02.564+0530] {dag.py:4451} INFO - created dagrun <DagRun movie_pipeline @ 2024-12-17 13:27:02.425833+00:00: manual__2024-12-17T13:27:02.425833+00:00, state:running, queued_at: None. externally triggered: False>
[2024-12-17T18:57:02.589+0530] {logging_mixin.py:190} INFO - [2024-12-17T18:57:02.588+0530] {dag.py:4396} INFO - [DAG TEST] starting task_id=scrape_collection_page map_index=-1
[2024-12-17T18:57:02.589+0530] {logging_mixin.py:190} INFO - [2024-12-17T18:57:02.589+0530] {dag.py:4399} INFO - [DAG TEST] running task <TaskInstance: movie_pipeline.scrape_collection_page manual__2024-12-17T13:27:02.425833+00:00 [scheduled]>
[2024-12-17T18:57:07.685+0530] {logging_mixin.py:190} INFO - [2024-12-17T18:57:07.685+0530] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-12-17T18:57:07.745+0530] {logging_mixin.py:190} INFO - [2024-12-17 18:57:07,745] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T13:27:02.425833+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T13:27:02.425833+00:00'
[2024-12-17T18:57:07.746+0530] {logging_mixin.py:190} INFO - [2024-12-17T18:57:07.745+0530] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T13:27:02.425833+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T13:27:02.425833+00:00'
[2024-12-17T18:57:07.748+0530] {logging_mixin.py:190} INFO - Task instance is in running state
[2024-12-17T18:57:07.749+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: queued
[2024-12-17T18:57:07.749+0530] {logging_mixin.py:190} INFO - Current task name:scrape_collection_page state:scheduled start_date:None
[2024-12-17T18:57:07.749+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline and current dag run status:running
[2024-12-17T18:57:07.750+0530] {logging_mixin.py:190} INFO - [2024-12-17T18:57:07.750+0530] {taskinstance.py:731} INFO - ::endgroup::
[2024-12-17T18:57:07.752+0530] {logging_mixin.py:190} INFO - 2024-12-17
[2024-12-17T18:57:07.752+0530] {logging_mixin.py:190} INFO - [2024-12-17 18:57:07,752] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T18:57:07.753+0530] {logging_mixin.py:190} INFO - [2024-12-17T18:57:07.752+0530] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T18:57:07.758+0530] {logging_mixin.py:190} INFO - [2024-12-17T18:57:07.758+0530] {taskinstance.py:340} INFO - ::group::Post task execution logs
[2024-12-17T18:57:07.758+0530] {logging_mixin.py:190} INFO - [2024-12-17T18:57:07.758+0530] {taskinstance.py:352} INFO - Marking task as SUCCESS. dag_id=movie_pipeline, task_id=scrape_collection_page, run_id=manual__2024-12-17T13:27:02.425833+00:00, execution_date=20241217T132702, start_date=, end_date=20241217T132707
[2024-12-17T18:57:07.768+0530] {logging_mixin.py:190} INFO - Task instance in success state
[2024-12-17T18:57:07.768+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: running
[2024-12-17T18:57:07.769+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline queued_at:None
[2024-12-17T18:57:07.769+0530] {logging_mixin.py:190} INFO - Task hostname:Jayeshs-MacBook-Air.local operator:PythonOperator
[2024-12-17T18:57:07.769+0530] {logging_mixin.py:190} INFO - [2024-12-17T18:57:07.769+0530] {dag.py:4410} INFO - [DAG TEST] end task task_id=scrape_collection_page map_index=-1
[2024-12-17T18:57:07.773+0530] {logging_mixin.py:190} INFO - [2024-12-17T18:57:07.773+0530] {dagrun.py:854} INFO - Marking run <DagRun movie_pipeline @ 2024-12-17 13:27:02.425833+00:00: manual__2024-12-17T13:27:02.425833+00:00, state:running, queued_at: None. externally triggered: False> successful
[2024-12-17T18:57:07.773+0530] {logging_mixin.py:190} INFO - Dag run in success state
[2024-12-17T18:57:07.773+0530] {logging_mixin.py:190} INFO - Dag run start:2024-12-17 13:27:02.425833+00:00 end:2024-12-17 13:27:07.773617+00:00
[2024-12-17T18:57:07.774+0530] {logging_mixin.py:190} INFO - [2024-12-17T18:57:07.774+0530] {dagrun.py:905} INFO - DagRun Finished: dag_id=movie_pipeline, execution_date=2024-12-17 13:27:02.425833+00:00, run_id=manual__2024-12-17T13:27:02.425833+00:00, run_start_date=2024-12-17 13:27:02.425833+00:00, run_end_date=2024-12-17 13:27:07.773617+00:00, run_duration=5.347784, state=success, external_trigger=False, run_type=manual, data_interval_start=2024-12-16 13:27:02.425833+00:00, data_interval_end=2024-12-17 13:27:02.425833+00:00, dag_hash=None
[2024-12-17T18:57:07.784+0530] {processor.py:925} INFO - DAG(s) 'movie_pipeline' retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T18:57:07.798+0530] {logging_mixin.py:190} INFO - [2024-12-17T18:57:07.798+0530] {dag.py:3239} INFO - Sync 1 DAGs
[2024-12-17T18:57:07.815+0530] {logging_mixin.py:190} INFO - [2024-12-17T18:57:07.815+0530] {dag.py:4180} INFO - Setting next_dagrun for movie_pipeline to 2024-12-17 01:30:00+00:00, run_after=2024-12-18 01:30:00+00:00
[2024-12-17T18:57:07.833+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 5.452 seconds
[2024-12-17T18:59:47.372+0530] {processor.py:186} INFO - Started process (PID=85712) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T18:59:47.373+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-17T18:59:47.375+0530] {logging_mixin.py:190} INFO - [2024-12-17T18:59:47.374+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T18:59:47.536+0530] {logging_mixin.py:190} INFO - [2024-12-17T18:59:47.536+0530] {dag.py:4435} INFO - dagrun id: movie_pipeline
[2024-12-17T18:59:47.550+0530] {logging_mixin.py:190} INFO - [2024-12-17T18:59:47.550+0530] {dag.py:4451} INFO - created dagrun <DagRun movie_pipeline @ 2024-12-17 13:29:47.413355+00:00: manual__2024-12-17T13:29:47.413355+00:00, state:running, queued_at: None. externally triggered: False>
[2024-12-17T18:59:47.575+0530] {logging_mixin.py:190} INFO - [2024-12-17T18:59:47.574+0530] {dag.py:4396} INFO - [DAG TEST] starting task_id=scrape_collection_page map_index=-1
[2024-12-17T18:59:47.575+0530] {logging_mixin.py:190} INFO - [2024-12-17T18:59:47.575+0530] {dag.py:4399} INFO - [DAG TEST] running task <TaskInstance: movie_pipeline.scrape_collection_page manual__2024-12-17T13:29:47.413355+00:00 [scheduled]>
[2024-12-17T18:59:52.670+0530] {logging_mixin.py:190} INFO - [2024-12-17T18:59:52.669+0530] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-12-17T18:59:52.733+0530] {logging_mixin.py:190} INFO - [2024-12-17 18:59:52,733] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T13:29:47.413355+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T13:29:47.413355+00:00'
[2024-12-17T18:59:52.733+0530] {logging_mixin.py:190} INFO - [2024-12-17T18:59:52.733+0530] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T13:29:47.413355+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T13:29:47.413355+00:00'
[2024-12-17T18:59:52.737+0530] {logging_mixin.py:190} INFO - Task instance is in running state
[2024-12-17T18:59:52.737+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: queued
[2024-12-17T18:59:52.737+0530] {logging_mixin.py:190} INFO - Current task name:scrape_collection_page state:scheduled start_date:None
[2024-12-17T18:59:52.738+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline and current dag run status:running
[2024-12-17T18:59:52.738+0530] {logging_mixin.py:190} INFO - [2024-12-17T18:59:52.738+0530] {taskinstance.py:731} INFO - ::endgroup::
[2024-12-17T18:59:52.740+0530] {logging_mixin.py:190} INFO - 2024-12-17
[2024-12-17T18:59:52.740+0530] {logging_mixin.py:190} INFO - [2024-12-17 18:59:52,740] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T18:59:52.740+0530] {logging_mixin.py:190} INFO - [2024-12-17T18:59:52.740+0530] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T18:59:52.745+0530] {logging_mixin.py:190} INFO - [2024-12-17T18:59:52.745+0530] {taskinstance.py:340} INFO - ::group::Post task execution logs
[2024-12-17T18:59:52.746+0530] {logging_mixin.py:190} INFO - [2024-12-17T18:59:52.746+0530] {taskinstance.py:352} INFO - Marking task as SUCCESS. dag_id=movie_pipeline, task_id=scrape_collection_page, run_id=manual__2024-12-17T13:29:47.413355+00:00, execution_date=20241217T132947, start_date=, end_date=20241217T132952
[2024-12-17T18:59:52.755+0530] {logging_mixin.py:190} INFO - Task instance in success state
[2024-12-17T18:59:52.755+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: running
[2024-12-17T18:59:52.756+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline queued_at:None
[2024-12-17T18:59:52.756+0530] {logging_mixin.py:190} INFO - Task hostname:Jayeshs-MacBook-Air.local operator:PythonOperator
[2024-12-17T18:59:52.756+0530] {logging_mixin.py:190} INFO - [2024-12-17T18:59:52.756+0530] {dag.py:4410} INFO - [DAG TEST] end task task_id=scrape_collection_page map_index=-1
[2024-12-17T18:59:52.760+0530] {logging_mixin.py:190} INFO - [2024-12-17T18:59:52.760+0530] {dagrun.py:854} INFO - Marking run <DagRun movie_pipeline @ 2024-12-17 13:29:47.413355+00:00: manual__2024-12-17T13:29:47.413355+00:00, state:running, queued_at: None. externally triggered: False> successful
[2024-12-17T18:59:52.760+0530] {logging_mixin.py:190} INFO - Dag run in success state
[2024-12-17T18:59:52.761+0530] {logging_mixin.py:190} INFO - Dag run start:2024-12-17 13:29:47.413355+00:00 end:2024-12-17 13:29:52.760811+00:00
[2024-12-17T18:59:52.762+0530] {logging_mixin.py:190} INFO - [2024-12-17T18:59:52.761+0530] {dagrun.py:905} INFO - DagRun Finished: dag_id=movie_pipeline, execution_date=2024-12-17 13:29:47.413355+00:00, run_id=manual__2024-12-17T13:29:47.413355+00:00, run_start_date=2024-12-17 13:29:47.413355+00:00, run_end_date=2024-12-17 13:29:52.760811+00:00, run_duration=5.347456, state=success, external_trigger=False, run_type=manual, data_interval_start=2024-12-16 13:29:47.413355+00:00, data_interval_end=2024-12-17 13:29:47.413355+00:00, dag_hash=None
[2024-12-17T18:59:52.771+0530] {processor.py:925} INFO - DAG(s) 'movie_pipeline' retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T18:59:52.783+0530] {logging_mixin.py:190} INFO - [2024-12-17T18:59:52.782+0530] {dag.py:3239} INFO - Sync 1 DAGs
[2024-12-17T18:59:52.801+0530] {logging_mixin.py:190} INFO - [2024-12-17T18:59:52.800+0530] {dag.py:4180} INFO - Setting next_dagrun for movie_pipeline to 2024-12-17 01:30:00+00:00, run_after=2024-12-18 01:30:00+00:00
[2024-12-17T18:59:52.824+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 5.456 seconds
[2024-12-17T19:02:32.566+0530] {processor.py:186} INFO - Started process (PID=85911) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T19:02:32.567+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-17T19:02:32.568+0530] {logging_mixin.py:190} INFO - [2024-12-17T19:02:32.568+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T19:02:32.730+0530] {logging_mixin.py:190} INFO - [2024-12-17T19:02:32.730+0530] {dag.py:4435} INFO - dagrun id: movie_pipeline
[2024-12-17T19:02:32.745+0530] {logging_mixin.py:190} INFO - [2024-12-17T19:02:32.744+0530] {dag.py:4451} INFO - created dagrun <DagRun movie_pipeline @ 2024-12-17 13:32:32.606285+00:00: manual__2024-12-17T13:32:32.606285+00:00, state:running, queued_at: None. externally triggered: False>
[2024-12-17T19:02:32.770+0530] {logging_mixin.py:190} INFO - [2024-12-17T19:02:32.770+0530] {dag.py:4396} INFO - [DAG TEST] starting task_id=scrape_collection_page map_index=-1
[2024-12-17T19:02:32.771+0530] {logging_mixin.py:190} INFO - [2024-12-17T19:02:32.770+0530] {dag.py:4399} INFO - [DAG TEST] running task <TaskInstance: movie_pipeline.scrape_collection_page manual__2024-12-17T13:32:32.606285+00:00 [scheduled]>
[2024-12-17T19:02:37.864+0530] {logging_mixin.py:190} INFO - [2024-12-17T19:02:37.863+0530] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-12-17T19:02:37.927+0530] {logging_mixin.py:190} INFO - [2024-12-17 19:02:37,926] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T13:32:32.606285+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T13:32:32.606285+00:00'
[2024-12-17T19:02:37.927+0530] {logging_mixin.py:190} INFO - [2024-12-17T19:02:37.926+0530] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T13:32:32.606285+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T13:32:32.606285+00:00'
[2024-12-17T19:02:37.930+0530] {logging_mixin.py:190} INFO - Task instance is in running state
[2024-12-17T19:02:37.930+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: queued
[2024-12-17T19:02:37.931+0530] {logging_mixin.py:190} INFO - Current task name:scrape_collection_page state:scheduled start_date:None
[2024-12-17T19:02:37.931+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline and current dag run status:running
[2024-12-17T19:02:37.931+0530] {logging_mixin.py:190} INFO - [2024-12-17T19:02:37.931+0530] {taskinstance.py:731} INFO - ::endgroup::
[2024-12-17T19:02:37.933+0530] {logging_mixin.py:190} INFO - 2024-12-17
[2024-12-17T19:02:37.934+0530] {logging_mixin.py:190} INFO - [2024-12-17 19:02:37,933] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T19:02:37.934+0530] {logging_mixin.py:190} INFO - [2024-12-17T19:02:37.933+0530] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T19:02:37.939+0530] {logging_mixin.py:190} INFO - [2024-12-17T19:02:37.939+0530] {taskinstance.py:340} INFO - ::group::Post task execution logs
[2024-12-17T19:02:37.940+0530] {logging_mixin.py:190} INFO - [2024-12-17T19:02:37.940+0530] {taskinstance.py:352} INFO - Marking task as SUCCESS. dag_id=movie_pipeline, task_id=scrape_collection_page, run_id=manual__2024-12-17T13:32:32.606285+00:00, execution_date=20241217T133232, start_date=, end_date=20241217T133237
[2024-12-17T19:02:37.950+0530] {logging_mixin.py:190} INFO - Task instance in success state
[2024-12-17T19:02:37.950+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: running
[2024-12-17T19:02:37.951+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline queued_at:None
[2024-12-17T19:02:37.951+0530] {logging_mixin.py:190} INFO - Task hostname:Jayeshs-MacBook-Air.local operator:PythonOperator
[2024-12-17T19:02:37.951+0530] {logging_mixin.py:190} INFO - [2024-12-17T19:02:37.951+0530] {dag.py:4410} INFO - [DAG TEST] end task task_id=scrape_collection_page map_index=-1
[2024-12-17T19:02:37.957+0530] {logging_mixin.py:190} INFO - [2024-12-17T19:02:37.955+0530] {dagrun.py:854} INFO - Marking run <DagRun movie_pipeline @ 2024-12-17 13:32:32.606285+00:00: manual__2024-12-17T13:32:32.606285+00:00, state:running, queued_at: None. externally triggered: False> successful
[2024-12-17T19:02:37.957+0530] {logging_mixin.py:190} INFO - Dag run in success state
[2024-12-17T19:02:37.957+0530] {logging_mixin.py:190} INFO - Dag run start:2024-12-17 13:32:32.606285+00:00 end:2024-12-17 13:32:37.957546+00:00
[2024-12-17T19:02:37.958+0530] {logging_mixin.py:190} INFO - [2024-12-17T19:02:37.958+0530] {dagrun.py:905} INFO - DagRun Finished: dag_id=movie_pipeline, execution_date=2024-12-17 13:32:32.606285+00:00, run_id=manual__2024-12-17T13:32:32.606285+00:00, run_start_date=2024-12-17 13:32:32.606285+00:00, run_end_date=2024-12-17 13:32:37.957546+00:00, run_duration=5.351261, state=success, external_trigger=False, run_type=manual, data_interval_start=2024-12-16 13:32:32.606285+00:00, data_interval_end=2024-12-17 13:32:32.606285+00:00, dag_hash=None
[2024-12-17T19:02:37.967+0530] {processor.py:925} INFO - DAG(s) 'movie_pipeline' retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T19:02:37.978+0530] {logging_mixin.py:190} INFO - [2024-12-17T19:02:37.978+0530] {dag.py:3239} INFO - Sync 1 DAGs
[2024-12-17T19:02:37.996+0530] {logging_mixin.py:190} INFO - [2024-12-17T19:02:37.995+0530] {dag.py:4180} INFO - Setting next_dagrun for movie_pipeline to 2024-12-17 01:30:00+00:00, run_after=2024-12-18 01:30:00+00:00
[2024-12-17T19:02:38.014+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 5.451 seconds
[2024-12-17T19:05:31.855+0530] {processor.py:186} INFO - Started process (PID=86117) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T19:05:31.859+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-17T19:05:31.860+0530] {logging_mixin.py:190} INFO - [2024-12-17T19:05:31.860+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T19:05:32.260+0530] {logging_mixin.py:190} INFO - [2024-12-17T19:05:32.260+0530] {dag.py:4435} INFO - dagrun id: movie_pipeline
[2024-12-17T19:05:32.293+0530] {logging_mixin.py:190} INFO - [2024-12-17T19:05:32.292+0530] {dag.py:4451} INFO - created dagrun <DagRun movie_pipeline @ 2024-12-17 13:35:31.946160+00:00: manual__2024-12-17T13:35:31.946160+00:00, state:running, queued_at: None. externally triggered: False>
[2024-12-17T19:05:32.348+0530] {logging_mixin.py:190} INFO - [2024-12-17T19:05:32.348+0530] {dag.py:4396} INFO - [DAG TEST] starting task_id=scrape_collection_page map_index=-1
[2024-12-17T19:05:32.349+0530] {logging_mixin.py:190} INFO - [2024-12-17T19:05:32.348+0530] {dag.py:4399} INFO - [DAG TEST] running task <TaskInstance: movie_pipeline.scrape_collection_page manual__2024-12-17T13:35:31.946160+00:00 [scheduled]>
[2024-12-17T19:05:37.508+0530] {logging_mixin.py:190} INFO - [2024-12-17T19:05:37.507+0530] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-12-17T19:05:37.644+0530] {logging_mixin.py:190} INFO - [2024-12-17 19:05:37,643] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T13:35:31.946160+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T13:35:31.946160+00:00'
[2024-12-17T19:05:37.646+0530] {logging_mixin.py:190} INFO - [2024-12-17T19:05:37.643+0530] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T13:35:31.946160+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T13:35:31.946160+00:00'
[2024-12-17T19:05:37.658+0530] {logging_mixin.py:190} INFO - Task instance is in running state
[2024-12-17T19:05:37.659+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: queued
[2024-12-17T19:05:37.660+0530] {logging_mixin.py:190} INFO - Current task name:scrape_collection_page state:scheduled start_date:None
[2024-12-17T19:05:37.660+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline and current dag run status:running
[2024-12-17T19:05:37.663+0530] {logging_mixin.py:190} INFO - [2024-12-17T19:05:37.661+0530] {taskinstance.py:731} INFO - ::endgroup::
[2024-12-17T19:05:37.672+0530] {logging_mixin.py:190} INFO - 2024-12-17
[2024-12-17T19:05:37.673+0530] {logging_mixin.py:190} INFO - [2024-12-17 19:05:37,673] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T19:05:37.675+0530] {logging_mixin.py:190} INFO - [2024-12-17T19:05:37.673+0530] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T19:05:37.686+0530] {logging_mixin.py:190} INFO - [2024-12-17T19:05:37.685+0530] {taskinstance.py:340} INFO - ::group::Post task execution logs
[2024-12-17T19:05:37.687+0530] {logging_mixin.py:190} INFO - [2024-12-17T19:05:37.686+0530] {taskinstance.py:352} INFO - Marking task as SUCCESS. dag_id=movie_pipeline, task_id=scrape_collection_page, run_id=manual__2024-12-17T13:35:31.946160+00:00, execution_date=20241217T133531, start_date=, end_date=20241217T133537
[2024-12-17T19:05:37.702+0530] {logging_mixin.py:190} INFO - Task instance in success state
[2024-12-17T19:05:37.702+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: running
[2024-12-17T19:05:37.703+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline queued_at:None
[2024-12-17T19:05:37.703+0530] {logging_mixin.py:190} INFO - Task hostname:Jayeshs-MacBook-Air.local operator:PythonOperator
[2024-12-17T19:05:37.704+0530] {logging_mixin.py:190} INFO - [2024-12-17T19:05:37.703+0530] {dag.py:4410} INFO - [DAG TEST] end task task_id=scrape_collection_page map_index=-1
[2024-12-17T19:05:37.712+0530] {logging_mixin.py:190} INFO - [2024-12-17T19:05:37.712+0530] {dagrun.py:854} INFO - Marking run <DagRun movie_pipeline @ 2024-12-17 13:35:31.946160+00:00: manual__2024-12-17T13:35:31.946160+00:00, state:running, queued_at: None. externally triggered: False> successful
[2024-12-17T19:05:37.713+0530] {logging_mixin.py:190} INFO - Dag run in success state
[2024-12-17T19:05:37.714+0530] {logging_mixin.py:190} INFO - Dag run start:2024-12-17 13:35:31.946160+00:00 end:2024-12-17 13:35:37.713389+00:00
[2024-12-17T19:05:37.715+0530] {logging_mixin.py:190} INFO - [2024-12-17T19:05:37.715+0530] {dagrun.py:905} INFO - DagRun Finished: dag_id=movie_pipeline, execution_date=2024-12-17 13:35:31.946160+00:00, run_id=manual__2024-12-17T13:35:31.946160+00:00, run_start_date=2024-12-17 13:35:31.946160+00:00, run_end_date=2024-12-17 13:35:37.713389+00:00, run_duration=5.767229, state=success, external_trigger=False, run_type=manual, data_interval_start=2024-12-16 13:35:31.946160+00:00, data_interval_end=2024-12-17 13:35:31.946160+00:00, dag_hash=None
[2024-12-17T19:05:37.729+0530] {processor.py:925} INFO - DAG(s) 'movie_pipeline' retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T19:05:37.753+0530] {logging_mixin.py:190} INFO - [2024-12-17T19:05:37.753+0530] {dag.py:3239} INFO - Sync 1 DAGs
[2024-12-17T19:05:37.786+0530] {logging_mixin.py:190} INFO - [2024-12-17T19:05:37.786+0530] {dag.py:4180} INFO - Setting next_dagrun for movie_pipeline to 2024-12-17 01:30:00+00:00, run_after=2024-12-18 01:30:00+00:00
[2024-12-17T19:05:37.839+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 5.991 seconds
[2024-12-17T19:11:00.696+0530] {processor.py:186} INFO - Started process (PID=86425) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T19:11:00.699+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-17T19:11:00.702+0530] {logging_mixin.py:190} INFO - [2024-12-17T19:11:00.702+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T19:11:00.939+0530] {logging_mixin.py:190} INFO - [2024-12-17T19:11:00.939+0530] {dag.py:4435} INFO - dagrun id: movie_pipeline
[2024-12-17T19:11:00.958+0530] {logging_mixin.py:190} INFO - [2024-12-17T19:11:00.957+0530] {dag.py:4451} INFO - created dagrun <DagRun movie_pipeline @ 2024-12-17 13:41:00.755615+00:00: manual__2024-12-17T13:41:00.755615+00:00, state:running, queued_at: None. externally triggered: False>
[2024-12-17T19:11:00.994+0530] {logging_mixin.py:190} INFO - [2024-12-17T19:11:00.994+0530] {dag.py:4396} INFO - [DAG TEST] starting task_id=scrape_collection_page map_index=-1
[2024-12-17T19:11:00.995+0530] {logging_mixin.py:190} INFO - [2024-12-17T19:11:00.995+0530] {dag.py:4399} INFO - [DAG TEST] running task <TaskInstance: movie_pipeline.scrape_collection_page manual__2024-12-17T13:41:00.755615+00:00 [scheduled]>
[2024-12-17T19:11:06.124+0530] {logging_mixin.py:190} INFO - [2024-12-17T19:11:06.123+0530] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-12-17T19:11:06.217+0530] {logging_mixin.py:190} INFO - [2024-12-17 19:11:06,217] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T13:41:00.755615+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T13:41:00.755615+00:00'
[2024-12-17T19:11:06.218+0530] {logging_mixin.py:190} INFO - [2024-12-17T19:11:06.217+0530] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T13:41:00.755615+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T13:41:00.755615+00:00'
[2024-12-17T19:11:06.231+0530] {logging_mixin.py:190} INFO - Task instance is in running state
[2024-12-17T19:11:06.231+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: queued
[2024-12-17T19:11:06.231+0530] {logging_mixin.py:190} INFO - Current task name:scrape_collection_page state:scheduled start_date:None
[2024-12-17T19:11:06.232+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline and current dag run status:running
[2024-12-17T19:11:06.232+0530] {logging_mixin.py:190} INFO - [2024-12-17T19:11:06.232+0530] {taskinstance.py:731} INFO - ::endgroup::
[2024-12-17T19:11:06.238+0530] {logging_mixin.py:190} INFO - 2024-12-17
[2024-12-17T19:11:06.239+0530] {logging_mixin.py:190} INFO - [2024-12-17 19:11:06,239] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T19:11:06.239+0530] {logging_mixin.py:190} INFO - [2024-12-17T19:11:06.239+0530] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T19:11:06.248+0530] {logging_mixin.py:190} INFO - [2024-12-17T19:11:06.248+0530] {taskinstance.py:340} INFO - ::group::Post task execution logs
[2024-12-17T19:11:06.250+0530] {logging_mixin.py:190} INFO - [2024-12-17T19:11:06.249+0530] {taskinstance.py:352} INFO - Marking task as SUCCESS. dag_id=movie_pipeline, task_id=scrape_collection_page, run_id=manual__2024-12-17T13:41:00.755615+00:00, execution_date=20241217T134100, start_date=, end_date=20241217T134106
[2024-12-17T19:11:06.272+0530] {logging_mixin.py:190} INFO - Task instance in success state
[2024-12-17T19:11:06.272+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: running
[2024-12-17T19:11:06.273+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline queued_at:None
[2024-12-17T19:11:06.274+0530] {logging_mixin.py:190} INFO - Task hostname:Jayeshs-MacBook-Air.local operator:PythonOperator
[2024-12-17T19:11:06.274+0530] {logging_mixin.py:190} INFO - [2024-12-17T19:11:06.274+0530] {dag.py:4410} INFO - [DAG TEST] end task task_id=scrape_collection_page map_index=-1
[2024-12-17T19:11:06.281+0530] {logging_mixin.py:190} INFO - [2024-12-17T19:11:06.280+0530] {dagrun.py:854} INFO - Marking run <DagRun movie_pipeline @ 2024-12-17 13:41:00.755615+00:00: manual__2024-12-17T13:41:00.755615+00:00, state:running, queued_at: None. externally triggered: False> successful
[2024-12-17T19:11:06.281+0530] {logging_mixin.py:190} INFO - Dag run in success state
[2024-12-17T19:11:06.282+0530] {logging_mixin.py:190} INFO - Dag run start:2024-12-17 13:41:00.755615+00:00 end:2024-12-17 13:41:06.281331+00:00
[2024-12-17T19:11:06.283+0530] {logging_mixin.py:190} INFO - [2024-12-17T19:11:06.282+0530] {dagrun.py:905} INFO - DagRun Finished: dag_id=movie_pipeline, execution_date=2024-12-17 13:41:00.755615+00:00, run_id=manual__2024-12-17T13:41:00.755615+00:00, run_start_date=2024-12-17 13:41:00.755615+00:00, run_end_date=2024-12-17 13:41:06.281331+00:00, run_duration=5.525716, state=success, external_trigger=False, run_type=manual, data_interval_start=2024-12-16 13:41:00.755615+00:00, data_interval_end=2024-12-17 13:41:00.755615+00:00, dag_hash=None
[2024-12-17T19:11:06.295+0530] {processor.py:925} INFO - DAG(s) 'movie_pipeline' retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T19:11:06.319+0530] {logging_mixin.py:190} INFO - [2024-12-17T19:11:06.319+0530] {dag.py:3239} INFO - Sync 1 DAGs
[2024-12-17T19:11:06.359+0530] {logging_mixin.py:190} INFO - [2024-12-17T19:11:06.359+0530] {dag.py:4180} INFO - Setting next_dagrun for movie_pipeline to 2024-12-17 01:30:00+00:00, run_after=2024-12-18 01:30:00+00:00
[2024-12-17T19:11:06.393+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 5.703 seconds
[2024-12-17T19:15:17.887+0530] {processor.py:186} INFO - Started process (PID=86671) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T19:15:17.890+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-17T19:15:17.895+0530] {logging_mixin.py:190} INFO - [2024-12-17T19:15:17.894+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T19:15:18.195+0530] {logging_mixin.py:190} INFO - [2024-12-17T19:15:18.195+0530] {dag.py:4435} INFO - dagrun id: movie_pipeline
[2024-12-17T19:15:18.212+0530] {logging_mixin.py:190} INFO - [2024-12-17T19:15:18.212+0530] {dag.py:4451} INFO - created dagrun <DagRun movie_pipeline @ 2024-12-17 13:45:17.963643+00:00: manual__2024-12-17T13:45:17.963643+00:00, state:running, queued_at: None. externally triggered: False>
[2024-12-17T19:15:18.257+0530] {logging_mixin.py:190} INFO - [2024-12-17T19:15:18.257+0530] {dag.py:4396} INFO - [DAG TEST] starting task_id=scrape_collection_page map_index=-1
[2024-12-17T19:15:18.257+0530] {logging_mixin.py:190} INFO - [2024-12-17T19:15:18.257+0530] {dag.py:4399} INFO - [DAG TEST] running task <TaskInstance: movie_pipeline.scrape_collection_page manual__2024-12-17T13:45:17.963643+00:00 [scheduled]>
[2024-12-17T19:15:23.393+0530] {logging_mixin.py:190} INFO - [2024-12-17T19:15:23.393+0530] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-12-17T19:15:23.491+0530] {logging_mixin.py:190} INFO - [2024-12-17 19:15:23,491] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T13:45:17.963643+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T13:45:17.963643+00:00'
[2024-12-17T19:15:23.492+0530] {logging_mixin.py:190} INFO - [2024-12-17T19:15:23.491+0530] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T13:45:17.963643+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T13:45:17.963643+00:00'
[2024-12-17T19:15:23.499+0530] {logging_mixin.py:190} INFO - Task instance is in running state
[2024-12-17T19:15:23.499+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: queued
[2024-12-17T19:15:23.499+0530] {logging_mixin.py:190} INFO - Current task name:scrape_collection_page state:scheduled start_date:None
[2024-12-17T19:15:23.499+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline and current dag run status:running
[2024-12-17T19:15:23.501+0530] {logging_mixin.py:190} INFO - [2024-12-17T19:15:23.500+0530] {taskinstance.py:731} INFO - ::endgroup::
[2024-12-17T19:15:23.504+0530] {logging_mixin.py:190} INFO - 2024-12-17
[2024-12-17T19:15:23.504+0530] {logging_mixin.py:190} INFO - [2024-12-17 19:15:23,504] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T19:15:23.505+0530] {logging_mixin.py:190} INFO - [2024-12-17T19:15:23.504+0530] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T19:15:23.511+0530] {logging_mixin.py:190} INFO - [2024-12-17T19:15:23.511+0530] {taskinstance.py:340} INFO - ::group::Post task execution logs
[2024-12-17T19:15:23.512+0530] {logging_mixin.py:190} INFO - [2024-12-17T19:15:23.512+0530] {taskinstance.py:352} INFO - Marking task as SUCCESS. dag_id=movie_pipeline, task_id=scrape_collection_page, run_id=manual__2024-12-17T13:45:17.963643+00:00, execution_date=20241217T134517, start_date=, end_date=20241217T134523
[2024-12-17T19:15:23.526+0530] {logging_mixin.py:190} INFO - Task instance in success state
[2024-12-17T19:15:23.526+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: running
[2024-12-17T19:15:23.526+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline queued_at:None
[2024-12-17T19:15:23.527+0530] {logging_mixin.py:190} INFO - Task hostname:Jayeshs-MacBook-Air.local operator:PythonOperator
[2024-12-17T19:15:23.527+0530] {logging_mixin.py:190} INFO - [2024-12-17T19:15:23.527+0530] {dag.py:4410} INFO - [DAG TEST] end task task_id=scrape_collection_page map_index=-1
[2024-12-17T19:15:23.533+0530] {logging_mixin.py:190} INFO - [2024-12-17T19:15:23.533+0530] {dagrun.py:854} INFO - Marking run <DagRun movie_pipeline @ 2024-12-17 13:45:17.963643+00:00: manual__2024-12-17T13:45:17.963643+00:00, state:running, queued_at: None. externally triggered: False> successful
[2024-12-17T19:15:23.533+0530] {logging_mixin.py:190} INFO - Dag run in success state
[2024-12-17T19:15:23.534+0530] {logging_mixin.py:190} INFO - Dag run start:2024-12-17 13:45:17.963643+00:00 end:2024-12-17 13:45:23.533651+00:00
[2024-12-17T19:15:23.534+0530] {logging_mixin.py:190} INFO - [2024-12-17T19:15:23.534+0530] {dagrun.py:905} INFO - DagRun Finished: dag_id=movie_pipeline, execution_date=2024-12-17 13:45:17.963643+00:00, run_id=manual__2024-12-17T13:45:17.963643+00:00, run_start_date=2024-12-17 13:45:17.963643+00:00, run_end_date=2024-12-17 13:45:23.533651+00:00, run_duration=5.570008, state=success, external_trigger=False, run_type=manual, data_interval_start=2024-12-16 13:45:17.963643+00:00, data_interval_end=2024-12-17 13:45:17.963643+00:00, dag_hash=None
[2024-12-17T19:15:23.546+0530] {processor.py:925} INFO - DAG(s) 'movie_pipeline' retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T19:15:23.566+0530] {logging_mixin.py:190} INFO - [2024-12-17T19:15:23.565+0530] {dag.py:3239} INFO - Sync 1 DAGs
[2024-12-17T19:15:23.595+0530] {logging_mixin.py:190} INFO - [2024-12-17T19:15:23.594+0530] {dag.py:4180} INFO - Setting next_dagrun for movie_pipeline to 2024-12-17 01:30:00+00:00, run_after=2024-12-18 01:30:00+00:00
[2024-12-17T19:15:23.620+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 5.740 seconds
[2024-12-17T19:19:07.826+0530] {processor.py:186} INFO - Started process (PID=86918) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T19:19:07.829+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-17T19:19:07.830+0530] {logging_mixin.py:190} INFO - [2024-12-17T19:19:07.830+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T19:19:08.011+0530] {logging_mixin.py:190} INFO - [2024-12-17T19:19:08.010+0530] {dag.py:4435} INFO - dagrun id: movie_pipeline
[2024-12-17T19:19:08.026+0530] {logging_mixin.py:190} INFO - [2024-12-17T19:19:08.025+0530] {dag.py:4451} INFO - created dagrun <DagRun movie_pipeline @ 2024-12-17 13:49:07.873808+00:00: manual__2024-12-17T13:49:07.873808+00:00, state:running, queued_at: None. externally triggered: False>
[2024-12-17T19:19:08.060+0530] {logging_mixin.py:190} INFO - [2024-12-17T19:19:08.059+0530] {dag.py:4396} INFO - [DAG TEST] starting task_id=scrape_collection_page map_index=-1
[2024-12-17T19:19:08.060+0530] {logging_mixin.py:190} INFO - [2024-12-17T19:19:08.060+0530] {dag.py:4399} INFO - [DAG TEST] running task <TaskInstance: movie_pipeline.scrape_collection_page manual__2024-12-17T13:49:07.873808+00:00 [scheduled]>
[2024-12-17T19:19:13.159+0530] {logging_mixin.py:190} INFO - [2024-12-17T19:19:13.159+0530] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-12-17T19:19:13.226+0530] {logging_mixin.py:190} INFO - [2024-12-17 19:19:13,226] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T13:49:07.873808+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T13:49:07.873808+00:00'
[2024-12-17T19:19:13.226+0530] {logging_mixin.py:190} INFO - [2024-12-17T19:19:13.226+0530] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T13:49:07.873808+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T13:49:07.873808+00:00'
[2024-12-17T19:19:13.239+0530] {logging_mixin.py:190} INFO - Task instance is in running state
[2024-12-17T19:19:13.240+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: queued
[2024-12-17T19:19:13.240+0530] {logging_mixin.py:190} INFO - Current task name:scrape_collection_page state:scheduled start_date:None
[2024-12-17T19:19:13.240+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline and current dag run status:running
[2024-12-17T19:19:13.240+0530] {logging_mixin.py:190} INFO - [2024-12-17T19:19:13.240+0530] {taskinstance.py:731} INFO - ::endgroup::
[2024-12-17T19:19:13.244+0530] {logging_mixin.py:190} INFO - 2024-12-17
[2024-12-17T19:19:13.245+0530] {logging_mixin.py:190} INFO - [2024-12-17 19:19:13,245] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T19:19:13.245+0530] {logging_mixin.py:190} INFO - [2024-12-17T19:19:13.245+0530] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T19:19:13.252+0530] {logging_mixin.py:190} INFO - [2024-12-17T19:19:13.251+0530] {taskinstance.py:340} INFO - ::group::Post task execution logs
[2024-12-17T19:19:13.252+0530] {logging_mixin.py:190} INFO - [2024-12-17T19:19:13.252+0530] {taskinstance.py:352} INFO - Marking task as SUCCESS. dag_id=movie_pipeline, task_id=scrape_collection_page, run_id=manual__2024-12-17T13:49:07.873808+00:00, execution_date=20241217T134907, start_date=, end_date=20241217T134913
[2024-12-17T19:19:13.262+0530] {logging_mixin.py:190} INFO - Task instance in success state
[2024-12-17T19:19:13.263+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: running
[2024-12-17T19:19:13.263+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline queued_at:None
[2024-12-17T19:19:13.263+0530] {logging_mixin.py:190} INFO - Task hostname:Jayeshs-MacBook-Air.local operator:PythonOperator
[2024-12-17T19:19:13.263+0530] {logging_mixin.py:190} INFO - [2024-12-17T19:19:13.263+0530] {dag.py:4410} INFO - [DAG TEST] end task task_id=scrape_collection_page map_index=-1
[2024-12-17T19:19:13.267+0530] {logging_mixin.py:190} INFO - [2024-12-17T19:19:13.267+0530] {dagrun.py:854} INFO - Marking run <DagRun movie_pipeline @ 2024-12-17 13:49:07.873808+00:00: manual__2024-12-17T13:49:07.873808+00:00, state:running, queued_at: None. externally triggered: False> successful
[2024-12-17T19:19:13.267+0530] {logging_mixin.py:190} INFO - Dag run in success state
[2024-12-17T19:19:13.268+0530] {logging_mixin.py:190} INFO - Dag run start:2024-12-17 13:49:07.873808+00:00 end:2024-12-17 13:49:13.267766+00:00
[2024-12-17T19:19:13.268+0530] {logging_mixin.py:190} INFO - [2024-12-17T19:19:13.268+0530] {dagrun.py:905} INFO - DagRun Finished: dag_id=movie_pipeline, execution_date=2024-12-17 13:49:07.873808+00:00, run_id=manual__2024-12-17T13:49:07.873808+00:00, run_start_date=2024-12-17 13:49:07.873808+00:00, run_end_date=2024-12-17 13:49:13.267766+00:00, run_duration=5.393958, state=success, external_trigger=False, run_type=manual, data_interval_start=2024-12-16 13:49:07.873808+00:00, data_interval_end=2024-12-17 13:49:07.873808+00:00, dag_hash=None
[2024-12-17T19:19:13.278+0530] {processor.py:925} INFO - DAG(s) 'movie_pipeline' retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T19:19:13.292+0530] {logging_mixin.py:190} INFO - [2024-12-17T19:19:13.291+0530] {dag.py:3239} INFO - Sync 1 DAGs
[2024-12-17T19:19:13.312+0530] {logging_mixin.py:190} INFO - [2024-12-17T19:19:13.311+0530] {dag.py:4180} INFO - Setting next_dagrun for movie_pipeline to 2024-12-17 01:30:00+00:00, run_after=2024-12-18 01:30:00+00:00
[2024-12-17T19:19:13.332+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 5.509 seconds
[2024-12-17T19:22:05.811+0530] {processor.py:186} INFO - Started process (PID=87125) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T19:22:05.813+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-17T19:22:05.814+0530] {logging_mixin.py:190} INFO - [2024-12-17T19:22:05.814+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T19:22:05.986+0530] {logging_mixin.py:190} INFO - [2024-12-17T19:22:05.986+0530] {dag.py:4435} INFO - dagrun id: movie_pipeline
[2024-12-17T19:22:06.001+0530] {logging_mixin.py:190} INFO - [2024-12-17T19:22:06.000+0530] {dag.py:4451} INFO - created dagrun <DagRun movie_pipeline @ 2024-12-17 13:52:05.855210+00:00: manual__2024-12-17T13:52:05.855210+00:00, state:running, queued_at: None. externally triggered: False>
[2024-12-17T19:22:06.028+0530] {logging_mixin.py:190} INFO - [2024-12-17T19:22:06.027+0530] {dag.py:4396} INFO - [DAG TEST] starting task_id=scrape_collection_page map_index=-1
[2024-12-17T19:22:06.029+0530] {logging_mixin.py:190} INFO - [2024-12-17T19:22:06.028+0530] {dag.py:4399} INFO - [DAG TEST] running task <TaskInstance: movie_pipeline.scrape_collection_page manual__2024-12-17T13:52:05.855210+00:00 [scheduled]>
[2024-12-17T19:22:11.128+0530] {logging_mixin.py:190} INFO - [2024-12-17T19:22:11.128+0530] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-12-17T19:22:11.190+0530] {logging_mixin.py:190} INFO - [2024-12-17 19:22:11,190] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T13:52:05.855210+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T13:52:05.855210+00:00'
[2024-12-17T19:22:11.190+0530] {logging_mixin.py:190} INFO - [2024-12-17T19:22:11.190+0530] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T13:52:05.855210+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T13:52:05.855210+00:00'
[2024-12-17T19:22:11.194+0530] {logging_mixin.py:190} INFO - Task instance is in running state
[2024-12-17T19:22:11.194+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: queued
[2024-12-17T19:22:11.194+0530] {logging_mixin.py:190} INFO - Current task name:scrape_collection_page state:scheduled start_date:None
[2024-12-17T19:22:11.194+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline and current dag run status:running
[2024-12-17T19:22:11.195+0530] {logging_mixin.py:190} INFO - [2024-12-17T19:22:11.195+0530] {taskinstance.py:731} INFO - ::endgroup::
[2024-12-17T19:22:11.197+0530] {logging_mixin.py:190} INFO - 2024-12-17
[2024-12-17T19:22:11.197+0530] {logging_mixin.py:190} INFO - [2024-12-17 19:22:11,197] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T19:22:11.198+0530] {logging_mixin.py:190} INFO - [2024-12-17T19:22:11.197+0530] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T19:22:11.203+0530] {logging_mixin.py:190} INFO - [2024-12-17T19:22:11.202+0530] {taskinstance.py:340} INFO - ::group::Post task execution logs
[2024-12-17T19:22:11.203+0530] {logging_mixin.py:190} INFO - [2024-12-17T19:22:11.203+0530] {taskinstance.py:352} INFO - Marking task as SUCCESS. dag_id=movie_pipeline, task_id=scrape_collection_page, run_id=manual__2024-12-17T13:52:05.855210+00:00, execution_date=20241217T135205, start_date=, end_date=20241217T135211
[2024-12-17T19:22:11.214+0530] {logging_mixin.py:190} INFO - Task instance in success state
[2024-12-17T19:22:11.214+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: running
[2024-12-17T19:22:11.215+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline queued_at:None
[2024-12-17T19:22:11.215+0530] {logging_mixin.py:190} INFO - Task hostname:Jayeshs-MacBook-Air.local operator:PythonOperator
[2024-12-17T19:22:11.215+0530] {logging_mixin.py:190} INFO - [2024-12-17T19:22:11.215+0530] {dag.py:4410} INFO - [DAG TEST] end task task_id=scrape_collection_page map_index=-1
[2024-12-17T19:22:11.219+0530] {logging_mixin.py:190} INFO - [2024-12-17T19:22:11.219+0530] {dagrun.py:854} INFO - Marking run <DagRun movie_pipeline @ 2024-12-17 13:52:05.855210+00:00: manual__2024-12-17T13:52:05.855210+00:00, state:running, queued_at: None. externally triggered: False> successful
[2024-12-17T19:22:11.220+0530] {logging_mixin.py:190} INFO - Dag run in success state
[2024-12-17T19:22:11.220+0530] {logging_mixin.py:190} INFO - Dag run start:2024-12-17 13:52:05.855210+00:00 end:2024-12-17 13:52:11.219940+00:00
[2024-12-17T19:22:11.220+0530] {logging_mixin.py:190} INFO - [2024-12-17T19:22:11.220+0530] {dagrun.py:905} INFO - DagRun Finished: dag_id=movie_pipeline, execution_date=2024-12-17 13:52:05.855210+00:00, run_id=manual__2024-12-17T13:52:05.855210+00:00, run_start_date=2024-12-17 13:52:05.855210+00:00, run_end_date=2024-12-17 13:52:11.219940+00:00, run_duration=5.36473, state=success, external_trigger=False, run_type=manual, data_interval_start=2024-12-16 13:52:05.855210+00:00, data_interval_end=2024-12-17 13:52:05.855210+00:00, dag_hash=None
[2024-12-17T19:22:11.232+0530] {processor.py:925} INFO - DAG(s) 'movie_pipeline' retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T19:22:11.246+0530] {logging_mixin.py:190} INFO - [2024-12-17T19:22:11.246+0530] {dag.py:3239} INFO - Sync 1 DAGs
[2024-12-17T19:22:11.265+0530] {logging_mixin.py:190} INFO - [2024-12-17T19:22:11.265+0530] {dag.py:4180} INFO - Setting next_dagrun for movie_pipeline to 2024-12-17 01:30:00+00:00, run_after=2024-12-18 01:30:00+00:00
[2024-12-17T19:22:11.285+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 5.477 seconds
[2024-12-17T19:25:05.899+0530] {processor.py:186} INFO - Started process (PID=87333) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T19:25:05.901+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-17T19:25:05.904+0530] {logging_mixin.py:190} INFO - [2024-12-17T19:25:05.904+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T19:25:06.189+0530] {logging_mixin.py:190} INFO - [2024-12-17T19:25:06.188+0530] {dag.py:4435} INFO - dagrun id: movie_pipeline
[2024-12-17T19:25:06.210+0530] {logging_mixin.py:190} INFO - [2024-12-17T19:25:06.209+0530] {dag.py:4451} INFO - created dagrun <DagRun movie_pipeline @ 2024-12-17 13:55:06.005319+00:00: manual__2024-12-17T13:55:06.005319+00:00, state:running, queued_at: None. externally triggered: False>
[2024-12-17T19:25:06.245+0530] {logging_mixin.py:190} INFO - [2024-12-17T19:25:06.245+0530] {dag.py:4396} INFO - [DAG TEST] starting task_id=scrape_collection_page map_index=-1
[2024-12-17T19:25:06.250+0530] {logging_mixin.py:190} INFO - [2024-12-17T19:25:06.246+0530] {dag.py:4399} INFO - [DAG TEST] running task <TaskInstance: movie_pipeline.scrape_collection_page manual__2024-12-17T13:55:06.005319+00:00 [scheduled]>
[2024-12-17T19:25:11.384+0530] {logging_mixin.py:190} INFO - [2024-12-17T19:25:11.383+0530] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-12-17T19:25:11.491+0530] {logging_mixin.py:190} INFO - [2024-12-17 19:25:11,490] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T13:55:06.005319+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T13:55:06.005319+00:00'
[2024-12-17T19:25:11.491+0530] {logging_mixin.py:190} INFO - [2024-12-17T19:25:11.490+0530] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T13:55:06.005319+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T13:55:06.005319+00:00'
[2024-12-17T19:25:11.508+0530] {logging_mixin.py:190} INFO - Task instance is in running state
[2024-12-17T19:25:11.509+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: queued
[2024-12-17T19:25:11.509+0530] {logging_mixin.py:190} INFO - Current task name:scrape_collection_page state:scheduled start_date:None
[2024-12-17T19:25:11.509+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline and current dag run status:running
[2024-12-17T19:25:11.509+0530] {logging_mixin.py:190} INFO - [2024-12-17T19:25:11.509+0530] {taskinstance.py:731} INFO - ::endgroup::
[2024-12-17T19:25:11.517+0530] {logging_mixin.py:190} INFO - 2024-12-17
[2024-12-17T19:25:11.518+0530] {logging_mixin.py:190} INFO - [2024-12-17 19:25:11,517] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T19:25:11.518+0530] {logging_mixin.py:190} INFO - [2024-12-17T19:25:11.517+0530] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T19:25:11.534+0530] {logging_mixin.py:190} INFO - [2024-12-17T19:25:11.532+0530] {taskinstance.py:340} INFO - ::group::Post task execution logs
[2024-12-17T19:25:11.536+0530] {logging_mixin.py:190} INFO - [2024-12-17T19:25:11.536+0530] {taskinstance.py:352} INFO - Marking task as SUCCESS. dag_id=movie_pipeline, task_id=scrape_collection_page, run_id=manual__2024-12-17T13:55:06.005319+00:00, execution_date=20241217T135506, start_date=, end_date=20241217T135511
[2024-12-17T19:25:11.561+0530] {logging_mixin.py:190} INFO - Task instance in success state
[2024-12-17T19:25:11.561+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: running
[2024-12-17T19:25:11.561+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline queued_at:None
[2024-12-17T19:25:11.561+0530] {logging_mixin.py:190} INFO - Task hostname:Jayeshs-MacBook-Air.local operator:PythonOperator
[2024-12-17T19:25:11.562+0530] {logging_mixin.py:190} INFO - [2024-12-17T19:25:11.562+0530] {dag.py:4410} INFO - [DAG TEST] end task task_id=scrape_collection_page map_index=-1
[2024-12-17T19:25:11.568+0530] {logging_mixin.py:190} INFO - [2024-12-17T19:25:11.568+0530] {dagrun.py:854} INFO - Marking run <DagRun movie_pipeline @ 2024-12-17 13:55:06.005319+00:00: manual__2024-12-17T13:55:06.005319+00:00, state:running, queued_at: None. externally triggered: False> successful
[2024-12-17T19:25:11.569+0530] {logging_mixin.py:190} INFO - Dag run in success state
[2024-12-17T19:25:11.570+0530] {logging_mixin.py:190} INFO - Dag run start:2024-12-17 13:55:06.005319+00:00 end:2024-12-17 13:55:11.569596+00:00
[2024-12-17T19:25:11.571+0530] {logging_mixin.py:190} INFO - [2024-12-17T19:25:11.570+0530] {dagrun.py:905} INFO - DagRun Finished: dag_id=movie_pipeline, execution_date=2024-12-17 13:55:06.005319+00:00, run_id=manual__2024-12-17T13:55:06.005319+00:00, run_start_date=2024-12-17 13:55:06.005319+00:00, run_end_date=2024-12-17 13:55:11.569596+00:00, run_duration=5.564277, state=success, external_trigger=False, run_type=manual, data_interval_start=2024-12-16 13:55:06.005319+00:00, data_interval_end=2024-12-17 13:55:06.005319+00:00, dag_hash=None
[2024-12-17T19:25:11.586+0530] {processor.py:925} INFO - DAG(s) 'movie_pipeline' retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T19:25:11.610+0530] {logging_mixin.py:190} INFO - [2024-12-17T19:25:11.609+0530] {dag.py:3239} INFO - Sync 1 DAGs
[2024-12-17T19:25:11.657+0530] {logging_mixin.py:190} INFO - [2024-12-17T19:25:11.657+0530] {dag.py:4180} INFO - Setting next_dagrun for movie_pipeline to 2024-12-17 01:30:00+00:00, run_after=2024-12-18 01:30:00+00:00
[2024-12-17T19:25:11.690+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 5.797 seconds
[2024-12-17T19:29:02.849+0530] {processor.py:186} INFO - Started process (PID=87565) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T19:29:02.852+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-17T19:29:02.855+0530] {logging_mixin.py:190} INFO - [2024-12-17T19:29:02.853+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T19:29:03.097+0530] {logging_mixin.py:190} INFO - [2024-12-17T19:29:03.096+0530] {dag.py:4435} INFO - dagrun id: movie_pipeline
[2024-12-17T19:29:03.118+0530] {logging_mixin.py:190} INFO - [2024-12-17T19:29:03.118+0530] {dag.py:4451} INFO - created dagrun <DagRun movie_pipeline @ 2024-12-17 13:59:02.907148+00:00: manual__2024-12-17T13:59:02.907148+00:00, state:running, queued_at: None. externally triggered: False>
[2024-12-17T19:29:03.160+0530] {logging_mixin.py:190} INFO - [2024-12-17T19:29:03.160+0530] {dag.py:4396} INFO - [DAG TEST] starting task_id=scrape_collection_page map_index=-1
[2024-12-17T19:29:03.161+0530] {logging_mixin.py:190} INFO - [2024-12-17T19:29:03.161+0530] {dag.py:4399} INFO - [DAG TEST] running task <TaskInstance: movie_pipeline.scrape_collection_page manual__2024-12-17T13:59:02.907148+00:00 [scheduled]>
[2024-12-17T19:29:08.298+0530] {logging_mixin.py:190} INFO - [2024-12-17T19:29:08.297+0530] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-12-17T19:29:08.552+0530] {logging_mixin.py:190} INFO - [2024-12-17 19:29:08,551] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T13:59:02.907148+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T13:59:02.907148+00:00'
[2024-12-17T19:29:08.552+0530] {logging_mixin.py:190} INFO - [2024-12-17T19:29:08.551+0530] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T13:59:02.907148+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T13:59:02.907148+00:00'
[2024-12-17T19:29:08.568+0530] {logging_mixin.py:190} INFO - Task instance is in running state
[2024-12-17T19:29:08.568+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: queued
[2024-12-17T19:29:08.568+0530] {logging_mixin.py:190} INFO - Current task name:scrape_collection_page state:scheduled start_date:None
[2024-12-17T19:29:08.569+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline and current dag run status:running
[2024-12-17T19:29:08.569+0530] {logging_mixin.py:190} INFO - [2024-12-17T19:29:08.569+0530] {taskinstance.py:731} INFO - ::endgroup::
[2024-12-17T19:29:08.574+0530] {logging_mixin.py:190} INFO - 2024-12-17
[2024-12-17T19:29:08.575+0530] {logging_mixin.py:190} INFO - [2024-12-17 19:29:08,575] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T19:29:08.575+0530] {logging_mixin.py:190} INFO - [2024-12-17T19:29:08.575+0530] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T19:29:08.584+0530] {logging_mixin.py:190} INFO - [2024-12-17T19:29:08.584+0530] {taskinstance.py:340} INFO - ::group::Post task execution logs
[2024-12-17T19:29:08.587+0530] {logging_mixin.py:190} INFO - [2024-12-17T19:29:08.586+0530] {taskinstance.py:352} INFO - Marking task as SUCCESS. dag_id=movie_pipeline, task_id=scrape_collection_page, run_id=manual__2024-12-17T13:59:02.907148+00:00, execution_date=20241217T135902, start_date=, end_date=20241217T135908
[2024-12-17T19:29:08.605+0530] {logging_mixin.py:190} INFO - Task instance in success state
[2024-12-17T19:29:08.605+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: running
[2024-12-17T19:29:08.606+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline queued_at:None
[2024-12-17T19:29:08.606+0530] {logging_mixin.py:190} INFO - Task hostname:Jayeshs-MacBook-Air.local operator:PythonOperator
[2024-12-17T19:29:08.606+0530] {logging_mixin.py:190} INFO - [2024-12-17T19:29:08.606+0530] {dag.py:4410} INFO - [DAG TEST] end task task_id=scrape_collection_page map_index=-1
[2024-12-17T19:29:08.612+0530] {logging_mixin.py:190} INFO - [2024-12-17T19:29:08.611+0530] {dagrun.py:854} INFO - Marking run <DagRun movie_pipeline @ 2024-12-17 13:59:02.907148+00:00: manual__2024-12-17T13:59:02.907148+00:00, state:running, queued_at: None. externally triggered: False> successful
[2024-12-17T19:29:08.612+0530] {logging_mixin.py:190} INFO - Dag run in success state
[2024-12-17T19:29:08.612+0530] {logging_mixin.py:190} INFO - Dag run start:2024-12-17 13:59:02.907148+00:00 end:2024-12-17 13:59:08.612301+00:00
[2024-12-17T19:29:08.613+0530] {logging_mixin.py:190} INFO - [2024-12-17T19:29:08.612+0530] {dagrun.py:905} INFO - DagRun Finished: dag_id=movie_pipeline, execution_date=2024-12-17 13:59:02.907148+00:00, run_id=manual__2024-12-17T13:59:02.907148+00:00, run_start_date=2024-12-17 13:59:02.907148+00:00, run_end_date=2024-12-17 13:59:08.612301+00:00, run_duration=5.705153, state=success, external_trigger=False, run_type=manual, data_interval_start=2024-12-16 13:59:02.907148+00:00, data_interval_end=2024-12-17 13:59:02.907148+00:00, dag_hash=None
[2024-12-17T19:29:08.626+0530] {processor.py:925} INFO - DAG(s) 'movie_pipeline' retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T19:29:08.645+0530] {logging_mixin.py:190} INFO - [2024-12-17T19:29:08.644+0530] {dag.py:3239} INFO - Sync 1 DAGs
[2024-12-17T19:29:08.675+0530] {logging_mixin.py:190} INFO - [2024-12-17T19:29:08.675+0530] {dag.py:4180} INFO - Setting next_dagrun for movie_pipeline to 2024-12-17 01:30:00+00:00, run_after=2024-12-18 01:30:00+00:00
[2024-12-17T19:29:08.713+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 5.868 seconds
[2024-12-17T19:33:06.106+0530] {processor.py:186} INFO - Started process (PID=87818) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T19:33:06.109+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-17T19:33:06.114+0530] {logging_mixin.py:190} INFO - [2024-12-17T19:33:06.110+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T19:33:06.353+0530] {logging_mixin.py:190} INFO - [2024-12-17T19:33:06.353+0530] {dag.py:4435} INFO - dagrun id: movie_pipeline
[2024-12-17T19:33:06.375+0530] {logging_mixin.py:190} INFO - [2024-12-17T19:33:06.374+0530] {dag.py:4451} INFO - created dagrun <DagRun movie_pipeline @ 2024-12-17 14:03:06.166707+00:00: manual__2024-12-17T14:03:06.166707+00:00, state:running, queued_at: None. externally triggered: False>
[2024-12-17T19:33:06.418+0530] {logging_mixin.py:190} INFO - [2024-12-17T19:33:06.417+0530] {dag.py:4396} INFO - [DAG TEST] starting task_id=scrape_collection_page map_index=-1
[2024-12-17T19:33:06.419+0530] {logging_mixin.py:190} INFO - [2024-12-17T19:33:06.418+0530] {dag.py:4399} INFO - [DAG TEST] running task <TaskInstance: movie_pipeline.scrape_collection_page manual__2024-12-17T14:03:06.166707+00:00 [scheduled]>
[2024-12-17T19:33:11.543+0530] {logging_mixin.py:190} INFO - [2024-12-17T19:33:11.542+0530] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-12-17T19:33:11.675+0530] {logging_mixin.py:190} INFO - [2024-12-17 19:33:11,674] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T14:03:06.166707+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T14:03:06.166707+00:00'
[2024-12-17T19:33:11.675+0530] {logging_mixin.py:190} INFO - [2024-12-17T19:33:11.674+0530] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T14:03:06.166707+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T14:03:06.166707+00:00'
[2024-12-17T19:33:11.687+0530] {logging_mixin.py:190} INFO - Task instance is in running state
[2024-12-17T19:33:11.687+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: queued
[2024-12-17T19:33:11.687+0530] {logging_mixin.py:190} INFO - Current task name:scrape_collection_page state:scheduled start_date:None
[2024-12-17T19:33:11.688+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline and current dag run status:running
[2024-12-17T19:33:11.688+0530] {logging_mixin.py:190} INFO - [2024-12-17T19:33:11.688+0530] {taskinstance.py:731} INFO - ::endgroup::
[2024-12-17T19:33:11.691+0530] {logging_mixin.py:190} INFO - 2024-12-17
[2024-12-17T19:33:11.692+0530] {logging_mixin.py:190} INFO - [2024-12-17 19:33:11,691] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T19:33:11.692+0530] {logging_mixin.py:190} INFO - [2024-12-17T19:33:11.691+0530] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T19:33:11.702+0530] {logging_mixin.py:190} INFO - [2024-12-17T19:33:11.702+0530] {taskinstance.py:340} INFO - ::group::Post task execution logs
[2024-12-17T19:33:11.703+0530] {logging_mixin.py:190} INFO - [2024-12-17T19:33:11.703+0530] {taskinstance.py:352} INFO - Marking task as SUCCESS. dag_id=movie_pipeline, task_id=scrape_collection_page, run_id=manual__2024-12-17T14:03:06.166707+00:00, execution_date=20241217T140306, start_date=, end_date=20241217T140311
[2024-12-17T19:33:11.715+0530] {logging_mixin.py:190} INFO - Task instance in success state
[2024-12-17T19:33:11.715+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: running
[2024-12-17T19:33:11.715+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline queued_at:None
[2024-12-17T19:33:11.716+0530] {logging_mixin.py:190} INFO - Task hostname:Jayeshs-MacBook-Air.local operator:PythonOperator
[2024-12-17T19:33:11.716+0530] {logging_mixin.py:190} INFO - [2024-12-17T19:33:11.716+0530] {dag.py:4410} INFO - [DAG TEST] end task task_id=scrape_collection_page map_index=-1
[2024-12-17T19:33:11.722+0530] {logging_mixin.py:190} INFO - [2024-12-17T19:33:11.722+0530] {dagrun.py:854} INFO - Marking run <DagRun movie_pipeline @ 2024-12-17 14:03:06.166707+00:00: manual__2024-12-17T14:03:06.166707+00:00, state:running, queued_at: None. externally triggered: False> successful
[2024-12-17T19:33:11.723+0530] {logging_mixin.py:190} INFO - Dag run in success state
[2024-12-17T19:33:11.842+0530] {logging_mixin.py:190} INFO - Dag run start:2024-12-17 14:03:06.166707+00:00 end:2024-12-17 14:03:11.723338+00:00
[2024-12-17T19:33:11.842+0530] {logging_mixin.py:190} INFO - [2024-12-17T19:33:11.842+0530] {dagrun.py:905} INFO - DagRun Finished: dag_id=movie_pipeline, execution_date=2024-12-17 14:03:06.166707+00:00, run_id=manual__2024-12-17T14:03:06.166707+00:00, run_start_date=2024-12-17 14:03:06.166707+00:00, run_end_date=2024-12-17 14:03:11.723338+00:00, run_duration=5.556631, state=success, external_trigger=False, run_type=manual, data_interval_start=2024-12-16 14:03:06.166707+00:00, data_interval_end=2024-12-17 14:03:06.166707+00:00, dag_hash=None
[2024-12-17T19:33:11.858+0530] {processor.py:925} INFO - DAG(s) 'movie_pipeline' retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T19:33:11.880+0530] {logging_mixin.py:190} INFO - [2024-12-17T19:33:11.880+0530] {dag.py:3239} INFO - Sync 1 DAGs
[2024-12-17T19:33:12.071+0530] {logging_mixin.py:190} INFO - [2024-12-17T19:33:12.070+0530] {dag.py:4180} INFO - Setting next_dagrun for movie_pipeline to 2024-12-17 01:30:00+00:00, run_after=2024-12-18 01:30:00+00:00
[2024-12-17T19:33:12.122+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 6.022 seconds
[2024-12-17T19:38:00.766+0530] {processor.py:186} INFO - Started process (PID=88088) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T19:38:00.769+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-17T19:38:00.772+0530] {logging_mixin.py:190} INFO - [2024-12-17T19:38:00.772+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T19:38:01.003+0530] {logging_mixin.py:190} INFO - [2024-12-17T19:38:01.003+0530] {dag.py:4435} INFO - dagrun id: movie_pipeline
[2024-12-17T19:38:01.028+0530] {logging_mixin.py:190} INFO - [2024-12-17T19:38:01.026+0530] {dag.py:4451} INFO - created dagrun <DagRun movie_pipeline @ 2024-12-17 14:08:00.827103+00:00: manual__2024-12-17T14:08:00.827103+00:00, state:running, queued_at: None. externally triggered: False>
[2024-12-17T19:38:01.064+0530] {logging_mixin.py:190} INFO - [2024-12-17T19:38:01.064+0530] {dag.py:4396} INFO - [DAG TEST] starting task_id=scrape_collection_page map_index=-1
[2024-12-17T19:38:01.065+0530] {logging_mixin.py:190} INFO - [2024-12-17T19:38:01.064+0530] {dag.py:4399} INFO - [DAG TEST] running task <TaskInstance: movie_pipeline.scrape_collection_page manual__2024-12-17T14:08:00.827103+00:00 [scheduled]>
[2024-12-17T19:38:06.199+0530] {logging_mixin.py:190} INFO - [2024-12-17T19:38:06.199+0530] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-12-17T19:38:06.478+0530] {logging_mixin.py:190} INFO - [2024-12-17 19:38:06,474] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T14:08:00.827103+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T14:08:00.827103+00:00'
[2024-12-17T19:38:06.478+0530] {logging_mixin.py:190} INFO - [2024-12-17T19:38:06.474+0530] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T14:08:00.827103+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T14:08:00.827103+00:00'
[2024-12-17T19:38:06.489+0530] {logging_mixin.py:190} INFO - Task instance is in running state
[2024-12-17T19:38:06.489+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: queued
[2024-12-17T19:38:06.490+0530] {logging_mixin.py:190} INFO - Current task name:scrape_collection_page state:scheduled start_date:None
[2024-12-17T19:38:06.490+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline and current dag run status:running
[2024-12-17T19:38:06.490+0530] {logging_mixin.py:190} INFO - [2024-12-17T19:38:06.490+0530] {taskinstance.py:731} INFO - ::endgroup::
[2024-12-17T19:38:06.501+0530] {logging_mixin.py:190} INFO - 2024-12-17
[2024-12-17T19:38:06.502+0530] {logging_mixin.py:190} INFO - [2024-12-17 19:38:06,501] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T19:38:06.502+0530] {logging_mixin.py:190} INFO - [2024-12-17T19:38:06.501+0530] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T19:38:06.512+0530] {logging_mixin.py:190} INFO - [2024-12-17T19:38:06.512+0530] {taskinstance.py:340} INFO - ::group::Post task execution logs
[2024-12-17T19:38:06.513+0530] {logging_mixin.py:190} INFO - [2024-12-17T19:38:06.513+0530] {taskinstance.py:352} INFO - Marking task as SUCCESS. dag_id=movie_pipeline, task_id=scrape_collection_page, run_id=manual__2024-12-17T14:08:00.827103+00:00, execution_date=20241217T140800, start_date=, end_date=20241217T140806
[2024-12-17T19:38:06.523+0530] {logging_mixin.py:190} INFO - Task instance in success state
[2024-12-17T19:38:06.523+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: running
[2024-12-17T19:38:06.523+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline queued_at:None
[2024-12-17T19:38:06.523+0530] {logging_mixin.py:190} INFO - Task hostname:Jayeshs-MacBook-Air.local operator:PythonOperator
[2024-12-17T19:38:06.524+0530] {logging_mixin.py:190} INFO - [2024-12-17T19:38:06.523+0530] {dag.py:4410} INFO - [DAG TEST] end task task_id=scrape_collection_page map_index=-1
[2024-12-17T19:38:06.532+0530] {logging_mixin.py:190} INFO - [2024-12-17T19:38:06.532+0530] {dagrun.py:854} INFO - Marking run <DagRun movie_pipeline @ 2024-12-17 14:08:00.827103+00:00: manual__2024-12-17T14:08:00.827103+00:00, state:running, queued_at: None. externally triggered: False> successful
[2024-12-17T19:38:06.532+0530] {logging_mixin.py:190} INFO - Dag run in success state
[2024-12-17T19:38:06.532+0530] {logging_mixin.py:190} INFO - Dag run start:2024-12-17 14:08:00.827103+00:00 end:2024-12-17 14:08:06.532685+00:00
[2024-12-17T19:38:06.533+0530] {logging_mixin.py:190} INFO - [2024-12-17T19:38:06.533+0530] {dagrun.py:905} INFO - DagRun Finished: dag_id=movie_pipeline, execution_date=2024-12-17 14:08:00.827103+00:00, run_id=manual__2024-12-17T14:08:00.827103+00:00, run_start_date=2024-12-17 14:08:00.827103+00:00, run_end_date=2024-12-17 14:08:06.532685+00:00, run_duration=5.705582, state=success, external_trigger=False, run_type=manual, data_interval_start=2024-12-16 14:08:00.827103+00:00, data_interval_end=2024-12-17 14:08:00.827103+00:00, dag_hash=None
[2024-12-17T19:38:06.544+0530] {processor.py:925} INFO - DAG(s) 'movie_pipeline' retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T19:38:06.561+0530] {logging_mixin.py:190} INFO - [2024-12-17T19:38:06.560+0530] {dag.py:3239} INFO - Sync 1 DAGs
[2024-12-17T19:38:06.584+0530] {logging_mixin.py:190} INFO - [2024-12-17T19:38:06.584+0530] {dag.py:4180} INFO - Setting next_dagrun for movie_pipeline to 2024-12-17 01:30:00+00:00, run_after=2024-12-18 01:30:00+00:00
[2024-12-17T19:38:06.615+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 5.853 seconds
[2024-12-17T19:42:25.823+0530] {processor.py:186} INFO - Started process (PID=88356) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T19:42:25.826+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-17T19:42:25.828+0530] {logging_mixin.py:190} INFO - [2024-12-17T19:42:25.828+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T19:42:26.070+0530] {logging_mixin.py:190} INFO - [2024-12-17T19:42:26.070+0530] {dag.py:4435} INFO - dagrun id: movie_pipeline
[2024-12-17T19:42:26.091+0530] {logging_mixin.py:190} INFO - [2024-12-17T19:42:26.090+0530] {dag.py:4451} INFO - created dagrun <DagRun movie_pipeline @ 2024-12-17 14:12:25.888651+00:00: manual__2024-12-17T14:12:25.888651+00:00, state:running, queued_at: None. externally triggered: False>
[2024-12-17T19:42:26.171+0530] {logging_mixin.py:190} INFO - [2024-12-17T19:42:26.170+0530] {dag.py:4396} INFO - [DAG TEST] starting task_id=scrape_collection_page map_index=-1
[2024-12-17T19:42:26.171+0530] {logging_mixin.py:190} INFO - [2024-12-17T19:42:26.171+0530] {dag.py:4399} INFO - [DAG TEST] running task <TaskInstance: movie_pipeline.scrape_collection_page manual__2024-12-17T14:12:25.888651+00:00 [scheduled]>
[2024-12-17T19:42:31.634+0530] {logging_mixin.py:190} INFO - [2024-12-17T19:42:31.633+0530] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-12-17T19:42:31.723+0530] {logging_mixin.py:190} INFO - [2024-12-17 19:42:31,723] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T14:12:25.888651+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T14:12:25.888651+00:00'
[2024-12-17T19:42:31.723+0530] {logging_mixin.py:190} INFO - [2024-12-17T19:42:31.723+0530] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T14:12:25.888651+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T14:12:25.888651+00:00'
[2024-12-17T19:42:31.732+0530] {logging_mixin.py:190} INFO - Task instance is in running state
[2024-12-17T19:42:31.733+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: queued
[2024-12-17T19:42:31.733+0530] {logging_mixin.py:190} INFO - Current task name:scrape_collection_page state:scheduled start_date:None
[2024-12-17T19:42:31.733+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline and current dag run status:running
[2024-12-17T19:42:31.734+0530] {logging_mixin.py:190} INFO - [2024-12-17T19:42:31.734+0530] {taskinstance.py:731} INFO - ::endgroup::
[2024-12-17T19:42:31.737+0530] {logging_mixin.py:190} INFO - 2024-12-17
[2024-12-17T19:42:31.738+0530] {logging_mixin.py:190} INFO - [2024-12-17 19:42:31,737] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T19:42:31.738+0530] {logging_mixin.py:190} INFO - [2024-12-17T19:42:31.737+0530] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T19:42:31.743+0530] {logging_mixin.py:190} INFO - [2024-12-17T19:42:31.743+0530] {taskinstance.py:340} INFO - ::group::Post task execution logs
[2024-12-17T19:42:31.744+0530] {logging_mixin.py:190} INFO - [2024-12-17T19:42:31.743+0530] {taskinstance.py:352} INFO - Marking task as SUCCESS. dag_id=movie_pipeline, task_id=scrape_collection_page, run_id=manual__2024-12-17T14:12:25.888651+00:00, execution_date=20241217T141225, start_date=, end_date=20241217T141231
[2024-12-17T19:42:31.754+0530] {logging_mixin.py:190} INFO - Task instance in success state
[2024-12-17T19:42:31.755+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: running
[2024-12-17T19:42:31.755+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline queued_at:None
[2024-12-17T19:42:31.755+0530] {logging_mixin.py:190} INFO - Task hostname:Jayeshs-MacBook-Air.local operator:PythonOperator
[2024-12-17T19:42:31.755+0530] {logging_mixin.py:190} INFO - [2024-12-17T19:42:31.755+0530] {dag.py:4410} INFO - [DAG TEST] end task task_id=scrape_collection_page map_index=-1
[2024-12-17T19:42:31.759+0530] {logging_mixin.py:190} INFO - [2024-12-17T19:42:31.759+0530] {dagrun.py:854} INFO - Marking run <DagRun movie_pipeline @ 2024-12-17 14:12:25.888651+00:00: manual__2024-12-17T14:12:25.888651+00:00, state:running, queued_at: None. externally triggered: False> successful
[2024-12-17T19:42:31.760+0530] {logging_mixin.py:190} INFO - Dag run in success state
[2024-12-17T19:42:31.760+0530] {logging_mixin.py:190} INFO - Dag run start:2024-12-17 14:12:25.888651+00:00 end:2024-12-17 14:12:31.760262+00:00
[2024-12-17T19:42:31.761+0530] {logging_mixin.py:190} INFO - [2024-12-17T19:42:31.760+0530] {dagrun.py:905} INFO - DagRun Finished: dag_id=movie_pipeline, execution_date=2024-12-17 14:12:25.888651+00:00, run_id=manual__2024-12-17T14:12:25.888651+00:00, run_start_date=2024-12-17 14:12:25.888651+00:00, run_end_date=2024-12-17 14:12:31.760262+00:00, run_duration=5.871611, state=success, external_trigger=False, run_type=manual, data_interval_start=2024-12-16 14:12:25.888651+00:00, data_interval_end=2024-12-17 14:12:25.888651+00:00, dag_hash=None
[2024-12-17T19:42:31.773+0530] {processor.py:925} INFO - DAG(s) 'movie_pipeline' retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T19:42:31.789+0530] {logging_mixin.py:190} INFO - [2024-12-17T19:42:31.789+0530] {dag.py:3239} INFO - Sync 1 DAGs
[2024-12-17T19:42:31.816+0530] {logging_mixin.py:190} INFO - [2024-12-17T19:42:31.816+0530] {dag.py:4180} INFO - Setting next_dagrun for movie_pipeline to 2024-12-17 01:30:00+00:00, run_after=2024-12-18 01:30:00+00:00
[2024-12-17T19:42:31.837+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 6.018 seconds
[2024-12-17T19:46:26.924+0530] {processor.py:186} INFO - Started process (PID=88596) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T19:46:26.927+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-17T19:46:26.929+0530] {logging_mixin.py:190} INFO - [2024-12-17T19:46:26.929+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T19:46:27.173+0530] {logging_mixin.py:190} INFO - [2024-12-17T19:46:27.173+0530] {dag.py:4435} INFO - dagrun id: movie_pipeline
[2024-12-17T19:46:27.194+0530] {logging_mixin.py:190} INFO - [2024-12-17T19:46:27.194+0530] {dag.py:4451} INFO - created dagrun <DagRun movie_pipeline @ 2024-12-17 14:16:26.983532+00:00: manual__2024-12-17T14:16:26.983532+00:00, state:running, queued_at: None. externally triggered: False>
[2024-12-17T19:46:27.231+0530] {logging_mixin.py:190} INFO - [2024-12-17T19:46:27.230+0530] {dag.py:4396} INFO - [DAG TEST] starting task_id=scrape_collection_page map_index=-1
[2024-12-17T19:46:27.231+0530] {logging_mixin.py:190} INFO - [2024-12-17T19:46:27.231+0530] {dag.py:4399} INFO - [DAG TEST] running task <TaskInstance: movie_pipeline.scrape_collection_page manual__2024-12-17T14:16:26.983532+00:00 [scheduled]>
[2024-12-17T19:46:32.361+0530] {logging_mixin.py:190} INFO - [2024-12-17T19:46:32.361+0530] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-12-17T19:46:32.454+0530] {logging_mixin.py:190} INFO - [2024-12-17 19:46:32,454] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T14:16:26.983532+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T14:16:26.983532+00:00'
[2024-12-17T19:46:32.454+0530] {logging_mixin.py:190} INFO - [2024-12-17T19:46:32.454+0530] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T14:16:26.983532+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T14:16:26.983532+00:00'
[2024-12-17T19:46:32.467+0530] {logging_mixin.py:190} INFO - Task instance is in running state
[2024-12-17T19:46:32.468+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: queued
[2024-12-17T19:46:32.469+0530] {logging_mixin.py:190} INFO - Current task name:scrape_collection_page state:scheduled start_date:None
[2024-12-17T19:46:32.469+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline and current dag run status:running
[2024-12-17T19:46:32.470+0530] {logging_mixin.py:190} INFO - [2024-12-17T19:46:32.470+0530] {taskinstance.py:731} INFO - ::endgroup::
[2024-12-17T19:46:32.474+0530] {logging_mixin.py:190} INFO - 2024-12-17
[2024-12-17T19:46:32.475+0530] {logging_mixin.py:190} INFO - [2024-12-17 19:46:32,475] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T19:46:32.476+0530] {logging_mixin.py:190} INFO - [2024-12-17T19:46:32.475+0530] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T19:46:32.481+0530] {logging_mixin.py:190} INFO - [2024-12-17T19:46:32.481+0530] {taskinstance.py:340} INFO - ::group::Post task execution logs
[2024-12-17T19:46:32.481+0530] {logging_mixin.py:190} INFO - [2024-12-17T19:46:32.481+0530] {taskinstance.py:352} INFO - Marking task as SUCCESS. dag_id=movie_pipeline, task_id=scrape_collection_page, run_id=manual__2024-12-17T14:16:26.983532+00:00, execution_date=20241217T141626, start_date=, end_date=20241217T141632
[2024-12-17T19:46:32.496+0530] {logging_mixin.py:190} INFO - Task instance in success state
[2024-12-17T19:46:32.496+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: running
[2024-12-17T19:46:32.496+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline queued_at:None
[2024-12-17T19:46:32.496+0530] {logging_mixin.py:190} INFO - Task hostname:Jayeshs-MacBook-Air.local operator:PythonOperator
[2024-12-17T19:46:32.497+0530] {logging_mixin.py:190} INFO - [2024-12-17T19:46:32.497+0530] {dag.py:4410} INFO - [DAG TEST] end task task_id=scrape_collection_page map_index=-1
[2024-12-17T19:46:32.501+0530] {logging_mixin.py:190} INFO - [2024-12-17T19:46:32.501+0530] {dagrun.py:854} INFO - Marking run <DagRun movie_pipeline @ 2024-12-17 14:16:26.983532+00:00: manual__2024-12-17T14:16:26.983532+00:00, state:running, queued_at: None. externally triggered: False> successful
[2024-12-17T19:46:32.501+0530] {logging_mixin.py:190} INFO - Dag run in success state
[2024-12-17T19:46:32.502+0530] {logging_mixin.py:190} INFO - Dag run start:2024-12-17 14:16:26.983532+00:00 end:2024-12-17 14:16:32.501654+00:00
[2024-12-17T19:46:32.502+0530] {logging_mixin.py:190} INFO - [2024-12-17T19:46:32.502+0530] {dagrun.py:905} INFO - DagRun Finished: dag_id=movie_pipeline, execution_date=2024-12-17 14:16:26.983532+00:00, run_id=manual__2024-12-17T14:16:26.983532+00:00, run_start_date=2024-12-17 14:16:26.983532+00:00, run_end_date=2024-12-17 14:16:32.501654+00:00, run_duration=5.518122, state=success, external_trigger=False, run_type=manual, data_interval_start=2024-12-16 14:16:26.983532+00:00, data_interval_end=2024-12-17 14:16:26.983532+00:00, dag_hash=None
[2024-12-17T19:46:32.515+0530] {processor.py:925} INFO - DAG(s) 'movie_pipeline' retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T19:46:32.533+0530] {logging_mixin.py:190} INFO - [2024-12-17T19:46:32.533+0530] {dag.py:3239} INFO - Sync 1 DAGs
[2024-12-17T19:46:32.554+0530] {logging_mixin.py:190} INFO - [2024-12-17T19:46:32.554+0530] {dag.py:4180} INFO - Setting next_dagrun for movie_pipeline to 2024-12-17 01:30:00+00:00, run_after=2024-12-18 01:30:00+00:00
[2024-12-17T19:46:32.581+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 5.663 seconds
[2024-12-17T19:50:24.581+0530] {processor.py:186} INFO - Started process (PID=88838) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T19:50:24.583+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-17T19:50:24.585+0530] {logging_mixin.py:190} INFO - [2024-12-17T19:50:24.584+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T19:50:24.825+0530] {logging_mixin.py:190} INFO - [2024-12-17T19:50:24.824+0530] {dag.py:4435} INFO - dagrun id: movie_pipeline
[2024-12-17T19:50:24.849+0530] {logging_mixin.py:190} INFO - [2024-12-17T19:50:24.848+0530] {dag.py:4451} INFO - created dagrun <DagRun movie_pipeline @ 2024-12-17 14:20:24.636884+00:00: manual__2024-12-17T14:20:24.636884+00:00, state:running, queued_at: None. externally triggered: False>
[2024-12-17T19:50:24.887+0530] {logging_mixin.py:190} INFO - [2024-12-17T19:50:24.887+0530] {dag.py:4396} INFO - [DAG TEST] starting task_id=scrape_collection_page map_index=-1
[2024-12-17T19:50:24.888+0530] {logging_mixin.py:190} INFO - [2024-12-17T19:50:24.887+0530] {dag.py:4399} INFO - [DAG TEST] running task <TaskInstance: movie_pipeline.scrape_collection_page manual__2024-12-17T14:20:24.636884+00:00 [scheduled]>
[2024-12-17T19:50:30.032+0530] {logging_mixin.py:190} INFO - [2024-12-17T19:50:30.030+0530] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-12-17T19:50:30.164+0530] {logging_mixin.py:190} INFO - [2024-12-17 19:50:30,164] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T14:20:24.636884+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T14:20:24.636884+00:00'
[2024-12-17T19:50:30.165+0530] {logging_mixin.py:190} INFO - [2024-12-17T19:50:30.164+0530] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T14:20:24.636884+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T14:20:24.636884+00:00'
[2024-12-17T19:50:30.181+0530] {logging_mixin.py:190} INFO - Task instance is in running state
[2024-12-17T19:50:30.182+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: queued
[2024-12-17T19:50:30.182+0530] {logging_mixin.py:190} INFO - Current task name:scrape_collection_page state:scheduled start_date:None
[2024-12-17T19:50:30.182+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline and current dag run status:running
[2024-12-17T19:50:30.184+0530] {logging_mixin.py:190} INFO - [2024-12-17T19:50:30.183+0530] {taskinstance.py:731} INFO - ::endgroup::
[2024-12-17T19:50:30.188+0530] {logging_mixin.py:190} INFO - 2024-12-17
[2024-12-17T19:50:30.188+0530] {logging_mixin.py:190} INFO - [2024-12-17 19:50:30,188] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T19:50:30.189+0530] {logging_mixin.py:190} INFO - [2024-12-17T19:50:30.188+0530] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T19:50:30.198+0530] {logging_mixin.py:190} INFO - [2024-12-17T19:50:30.198+0530] {taskinstance.py:340} INFO - ::group::Post task execution logs
[2024-12-17T19:50:30.199+0530] {logging_mixin.py:190} INFO - [2024-12-17T19:50:30.199+0530] {taskinstance.py:352} INFO - Marking task as SUCCESS. dag_id=movie_pipeline, task_id=scrape_collection_page, run_id=manual__2024-12-17T14:20:24.636884+00:00, execution_date=20241217T142024, start_date=, end_date=20241217T142030
[2024-12-17T19:50:30.212+0530] {logging_mixin.py:190} INFO - Task instance in success state
[2024-12-17T19:50:30.213+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: running
[2024-12-17T19:50:30.213+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline queued_at:None
[2024-12-17T19:50:30.213+0530] {logging_mixin.py:190} INFO - Task hostname:Jayeshs-MacBook-Air.local operator:PythonOperator
[2024-12-17T19:50:30.214+0530] {logging_mixin.py:190} INFO - [2024-12-17T19:50:30.213+0530] {dag.py:4410} INFO - [DAG TEST] end task task_id=scrape_collection_page map_index=-1
[2024-12-17T19:50:30.219+0530] {logging_mixin.py:190} INFO - [2024-12-17T19:50:30.219+0530] {dagrun.py:854} INFO - Marking run <DagRun movie_pipeline @ 2024-12-17 14:20:24.636884+00:00: manual__2024-12-17T14:20:24.636884+00:00, state:running, queued_at: None. externally triggered: False> successful
[2024-12-17T19:50:30.219+0530] {logging_mixin.py:190} INFO - Dag run in success state
[2024-12-17T19:50:30.220+0530] {logging_mixin.py:190} INFO - Dag run start:2024-12-17 14:20:24.636884+00:00 end:2024-12-17 14:20:30.219887+00:00
[2024-12-17T19:50:30.220+0530] {logging_mixin.py:190} INFO - [2024-12-17T19:50:30.220+0530] {dagrun.py:905} INFO - DagRun Finished: dag_id=movie_pipeline, execution_date=2024-12-17 14:20:24.636884+00:00, run_id=manual__2024-12-17T14:20:24.636884+00:00, run_start_date=2024-12-17 14:20:24.636884+00:00, run_end_date=2024-12-17 14:20:30.219887+00:00, run_duration=5.583003, state=success, external_trigger=False, run_type=manual, data_interval_start=2024-12-16 14:20:24.636884+00:00, data_interval_end=2024-12-17 14:20:24.636884+00:00, dag_hash=None
[2024-12-17T19:50:30.233+0530] {processor.py:925} INFO - DAG(s) 'movie_pipeline' retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T19:50:30.258+0530] {logging_mixin.py:190} INFO - [2024-12-17T19:50:30.257+0530] {dag.py:3239} INFO - Sync 1 DAGs
[2024-12-17T19:50:30.289+0530] {logging_mixin.py:190} INFO - [2024-12-17T19:50:30.289+0530] {dag.py:4180} INFO - Setting next_dagrun for movie_pipeline to 2024-12-17 01:30:00+00:00, run_after=2024-12-18 01:30:00+00:00
[2024-12-17T19:50:30.326+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 5.754 seconds
[2024-12-17T19:54:46.652+0530] {processor.py:186} INFO - Started process (PID=89116) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T19:54:46.657+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-17T19:54:46.659+0530] {logging_mixin.py:190} INFO - [2024-12-17T19:54:46.659+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T19:54:47.019+0530] {logging_mixin.py:190} INFO - [2024-12-17T19:54:47.019+0530] {dag.py:4435} INFO - dagrun id: movie_pipeline
[2024-12-17T19:54:47.042+0530] {logging_mixin.py:190} INFO - [2024-12-17T19:54:47.041+0530] {dag.py:4451} INFO - created dagrun <DagRun movie_pipeline @ 2024-12-17 14:24:46.740294+00:00: manual__2024-12-17T14:24:46.740294+00:00, state:running, queued_at: None. externally triggered: False>
[2024-12-17T19:54:47.271+0530] {logging_mixin.py:190} INFO - [2024-12-17T19:54:47.270+0530] {dag.py:4396} INFO - [DAG TEST] starting task_id=scrape_collection_page map_index=-1
[2024-12-17T19:54:47.276+0530] {logging_mixin.py:190} INFO - [2024-12-17T19:54:47.275+0530] {dag.py:4399} INFO - [DAG TEST] running task <TaskInstance: movie_pipeline.scrape_collection_page manual__2024-12-17T14:24:46.740294+00:00 [scheduled]>
[2024-12-17T19:54:52.453+0530] {logging_mixin.py:190} INFO - [2024-12-17T19:54:52.452+0530] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-12-17T19:54:52.543+0530] {logging_mixin.py:190} INFO - [2024-12-17 19:54:52,543] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T14:24:46.740294+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T14:24:46.740294+00:00'
[2024-12-17T19:54:52.543+0530] {logging_mixin.py:190} INFO - [2024-12-17T19:54:52.543+0530] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T14:24:46.740294+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T14:24:46.740294+00:00'
[2024-12-17T19:54:52.553+0530] {logging_mixin.py:190} INFO - Task instance is in running state
[2024-12-17T19:54:52.553+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: queued
[2024-12-17T19:54:52.554+0530] {logging_mixin.py:190} INFO - Current task name:scrape_collection_page state:scheduled start_date:None
[2024-12-17T19:54:52.554+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline and current dag run status:running
[2024-12-17T19:54:52.554+0530] {logging_mixin.py:190} INFO - [2024-12-17T19:54:52.554+0530] {taskinstance.py:731} INFO - ::endgroup::
[2024-12-17T19:54:52.560+0530] {logging_mixin.py:190} INFO - 2024-12-17
[2024-12-17T19:54:52.561+0530] {logging_mixin.py:190} INFO - [2024-12-17 19:54:52,560] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T19:54:52.562+0530] {logging_mixin.py:190} INFO - [2024-12-17T19:54:52.560+0530] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T19:54:52.575+0530] {logging_mixin.py:190} INFO - [2024-12-17T19:54:52.574+0530] {taskinstance.py:340} INFO - ::group::Post task execution logs
[2024-12-17T19:54:52.576+0530] {logging_mixin.py:190} INFO - [2024-12-17T19:54:52.576+0530] {taskinstance.py:352} INFO - Marking task as SUCCESS. dag_id=movie_pipeline, task_id=scrape_collection_page, run_id=manual__2024-12-17T14:24:46.740294+00:00, execution_date=20241217T142446, start_date=, end_date=20241217T142452
[2024-12-17T19:54:52.597+0530] {logging_mixin.py:190} INFO - Task instance in success state
[2024-12-17T19:54:52.598+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: running
[2024-12-17T19:54:52.598+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline queued_at:None
[2024-12-17T19:54:52.599+0530] {logging_mixin.py:190} INFO - Task hostname:Jayeshs-MacBook-Air.local operator:PythonOperator
[2024-12-17T19:54:52.600+0530] {logging_mixin.py:190} INFO - [2024-12-17T19:54:52.600+0530] {dag.py:4410} INFO - [DAG TEST] end task task_id=scrape_collection_page map_index=-1
[2024-12-17T19:54:52.605+0530] {logging_mixin.py:190} INFO - [2024-12-17T19:54:52.605+0530] {dagrun.py:854} INFO - Marking run <DagRun movie_pipeline @ 2024-12-17 14:24:46.740294+00:00: manual__2024-12-17T14:24:46.740294+00:00, state:running, queued_at: None. externally triggered: False> successful
[2024-12-17T19:54:52.606+0530] {logging_mixin.py:190} INFO - Dag run in success state
[2024-12-17T19:54:52.606+0530] {logging_mixin.py:190} INFO - Dag run start:2024-12-17 14:24:46.740294+00:00 end:2024-12-17 14:24:52.606121+00:00
[2024-12-17T19:54:52.607+0530] {logging_mixin.py:190} INFO - [2024-12-17T19:54:52.606+0530] {dagrun.py:905} INFO - DagRun Finished: dag_id=movie_pipeline, execution_date=2024-12-17 14:24:46.740294+00:00, run_id=manual__2024-12-17T14:24:46.740294+00:00, run_start_date=2024-12-17 14:24:46.740294+00:00, run_end_date=2024-12-17 14:24:52.606121+00:00, run_duration=5.865827, state=success, external_trigger=False, run_type=manual, data_interval_start=2024-12-16 14:24:46.740294+00:00, data_interval_end=2024-12-17 14:24:46.740294+00:00, dag_hash=None
[2024-12-17T19:54:52.617+0530] {processor.py:925} INFO - DAG(s) 'movie_pipeline' retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T19:54:52.642+0530] {logging_mixin.py:190} INFO - [2024-12-17T19:54:52.642+0530] {dag.py:3239} INFO - Sync 1 DAGs
[2024-12-17T19:54:52.671+0530] {logging_mixin.py:190} INFO - [2024-12-17T19:54:52.671+0530] {dag.py:4180} INFO - Setting next_dagrun for movie_pipeline to 2024-12-17 01:30:00+00:00, run_after=2024-12-18 01:30:00+00:00
[2024-12-17T19:54:52.693+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 6.046 seconds
[2024-12-17T19:59:35.652+0530] {processor.py:186} INFO - Started process (PID=89389) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T19:59:35.655+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-17T19:59:35.657+0530] {logging_mixin.py:190} INFO - [2024-12-17T19:59:35.656+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T19:59:35.910+0530] {logging_mixin.py:190} INFO - [2024-12-17T19:59:35.910+0530] {dag.py:4435} INFO - dagrun id: movie_pipeline
[2024-12-17T19:59:35.929+0530] {logging_mixin.py:190} INFO - [2024-12-17T19:59:35.928+0530] {dag.py:4451} INFO - created dagrun <DagRun movie_pipeline @ 2024-12-17 14:29:35.719395+00:00: manual__2024-12-17T14:29:35.719395+00:00, state:running, queued_at: None. externally triggered: False>
[2024-12-17T19:59:35.984+0530] {logging_mixin.py:190} INFO - [2024-12-17T19:59:35.984+0530] {dag.py:4396} INFO - [DAG TEST] starting task_id=scrape_collection_page map_index=-1
[2024-12-17T19:59:35.985+0530] {logging_mixin.py:190} INFO - [2024-12-17T19:59:35.985+0530] {dag.py:4399} INFO - [DAG TEST] running task <TaskInstance: movie_pipeline.scrape_collection_page manual__2024-12-17T14:29:35.719395+00:00 [scheduled]>
[2024-12-17T19:59:41.128+0530] {logging_mixin.py:190} INFO - [2024-12-17T19:59:41.128+0530] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-12-17T19:59:41.219+0530] {logging_mixin.py:190} INFO - [2024-12-17 19:59:41,219] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T14:29:35.719395+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T14:29:35.719395+00:00'
[2024-12-17T19:59:41.219+0530] {logging_mixin.py:190} INFO - [2024-12-17T19:59:41.219+0530] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T14:29:35.719395+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T14:29:35.719395+00:00'
[2024-12-17T19:59:41.232+0530] {logging_mixin.py:190} INFO - Task instance is in running state
[2024-12-17T19:59:41.233+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: queued
[2024-12-17T19:59:41.233+0530] {logging_mixin.py:190} INFO - Current task name:scrape_collection_page state:scheduled start_date:None
[2024-12-17T19:59:41.233+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline and current dag run status:running
[2024-12-17T19:59:41.233+0530] {logging_mixin.py:190} INFO - [2024-12-17T19:59:41.233+0530] {taskinstance.py:731} INFO - ::endgroup::
[2024-12-17T19:59:41.237+0530] {logging_mixin.py:190} INFO - 2024-12-17
[2024-12-17T19:59:41.237+0530] {logging_mixin.py:190} INFO - [2024-12-17 19:59:41,237] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T19:59:41.238+0530] {logging_mixin.py:190} INFO - [2024-12-17T19:59:41.237+0530] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T19:59:41.248+0530] {logging_mixin.py:190} INFO - [2024-12-17T19:59:41.248+0530] {taskinstance.py:340} INFO - ::group::Post task execution logs
[2024-12-17T19:59:41.249+0530] {logging_mixin.py:190} INFO - [2024-12-17T19:59:41.249+0530] {taskinstance.py:352} INFO - Marking task as SUCCESS. dag_id=movie_pipeline, task_id=scrape_collection_page, run_id=manual__2024-12-17T14:29:35.719395+00:00, execution_date=20241217T142935, start_date=, end_date=20241217T142941
[2024-12-17T19:59:41.263+0530] {logging_mixin.py:190} INFO - Task instance in success state
[2024-12-17T19:59:41.263+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: running
[2024-12-17T19:59:41.264+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline queued_at:None
[2024-12-17T19:59:41.264+0530] {logging_mixin.py:190} INFO - Task hostname:Jayeshs-MacBook-Air.local operator:PythonOperator
[2024-12-17T19:59:41.266+0530] {logging_mixin.py:190} INFO - [2024-12-17T19:59:41.265+0530] {dag.py:4410} INFO - [DAG TEST] end task task_id=scrape_collection_page map_index=-1
[2024-12-17T19:59:41.270+0530] {logging_mixin.py:190} INFO - [2024-12-17T19:59:41.270+0530] {dagrun.py:854} INFO - Marking run <DagRun movie_pipeline @ 2024-12-17 14:29:35.719395+00:00: manual__2024-12-17T14:29:35.719395+00:00, state:running, queued_at: None. externally triggered: False> successful
[2024-12-17T19:59:41.271+0530] {logging_mixin.py:190} INFO - Dag run in success state
[2024-12-17T19:59:41.271+0530] {logging_mixin.py:190} INFO - Dag run start:2024-12-17 14:29:35.719395+00:00 end:2024-12-17 14:29:41.271082+00:00
[2024-12-17T19:59:41.271+0530] {logging_mixin.py:190} INFO - [2024-12-17T19:59:41.271+0530] {dagrun.py:905} INFO - DagRun Finished: dag_id=movie_pipeline, execution_date=2024-12-17 14:29:35.719395+00:00, run_id=manual__2024-12-17T14:29:35.719395+00:00, run_start_date=2024-12-17 14:29:35.719395+00:00, run_end_date=2024-12-17 14:29:41.271082+00:00, run_duration=5.551687, state=success, external_trigger=False, run_type=manual, data_interval_start=2024-12-16 14:29:35.719395+00:00, data_interval_end=2024-12-17 14:29:35.719395+00:00, dag_hash=None
[2024-12-17T19:59:41.286+0530] {processor.py:925} INFO - DAG(s) 'movie_pipeline' retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T19:59:41.302+0530] {logging_mixin.py:190} INFO - [2024-12-17T19:59:41.302+0530] {dag.py:3239} INFO - Sync 1 DAGs
[2024-12-17T19:59:41.321+0530] {logging_mixin.py:190} INFO - [2024-12-17T19:59:41.321+0530] {dag.py:4180} INFO - Setting next_dagrun for movie_pipeline to 2024-12-17 01:30:00+00:00, run_after=2024-12-18 01:30:00+00:00
[2024-12-17T19:59:41.351+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 5.707 seconds
[2024-12-17T20:03:43.611+0530] {processor.py:186} INFO - Started process (PID=89646) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T20:03:43.613+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-17T20:03:43.615+0530] {logging_mixin.py:190} INFO - [2024-12-17T20:03:43.615+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T20:03:43.855+0530] {logging_mixin.py:190} INFO - [2024-12-17T20:03:43.855+0530] {dag.py:4435} INFO - dagrun id: movie_pipeline
[2024-12-17T20:03:43.879+0530] {logging_mixin.py:190} INFO - [2024-12-17T20:03:43.879+0530] {dag.py:4451} INFO - created dagrun <DagRun movie_pipeline @ 2024-12-17 14:33:43.677620+00:00: manual__2024-12-17T14:33:43.677620+00:00, state:running, queued_at: None. externally triggered: False>
[2024-12-17T20:03:43.921+0530] {logging_mixin.py:190} INFO - [2024-12-17T20:03:43.921+0530] {dag.py:4396} INFO - [DAG TEST] starting task_id=scrape_collection_page map_index=-1
[2024-12-17T20:03:43.922+0530] {logging_mixin.py:190} INFO - [2024-12-17T20:03:43.922+0530] {dag.py:4399} INFO - [DAG TEST] running task <TaskInstance: movie_pipeline.scrape_collection_page manual__2024-12-17T14:33:43.677620+00:00 [scheduled]>
[2024-12-17T20:03:49.103+0530] {logging_mixin.py:190} INFO - [2024-12-17T20:03:49.102+0530] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-12-17T20:03:49.246+0530] {logging_mixin.py:190} INFO - [2024-12-17 20:03:49,246] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T14:33:43.677620+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T14:33:43.677620+00:00'
[2024-12-17T20:03:49.247+0530] {logging_mixin.py:190} INFO - [2024-12-17T20:03:49.246+0530] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T14:33:43.677620+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T14:33:43.677620+00:00'
[2024-12-17T20:03:49.262+0530] {logging_mixin.py:190} INFO - Task instance is in running state
[2024-12-17T20:03:49.263+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: queued
[2024-12-17T20:03:49.263+0530] {logging_mixin.py:190} INFO - Current task name:scrape_collection_page state:scheduled start_date:None
[2024-12-17T20:03:49.263+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline and current dag run status:running
[2024-12-17T20:03:49.264+0530] {logging_mixin.py:190} INFO - [2024-12-17T20:03:49.263+0530] {taskinstance.py:731} INFO - ::endgroup::
[2024-12-17T20:03:49.268+0530] {logging_mixin.py:190} INFO - 2024-12-17
[2024-12-17T20:03:49.269+0530] {logging_mixin.py:190} INFO - [2024-12-17 20:03:49,269] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T20:03:49.270+0530] {logging_mixin.py:190} INFO - [2024-12-17T20:03:49.269+0530] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T20:03:49.277+0530] {logging_mixin.py:190} INFO - [2024-12-17T20:03:49.277+0530] {taskinstance.py:340} INFO - ::group::Post task execution logs
[2024-12-17T20:03:49.277+0530] {logging_mixin.py:190} INFO - [2024-12-17T20:03:49.277+0530] {taskinstance.py:352} INFO - Marking task as SUCCESS. dag_id=movie_pipeline, task_id=scrape_collection_page, run_id=manual__2024-12-17T14:33:43.677620+00:00, execution_date=20241217T143343, start_date=, end_date=20241217T143349
[2024-12-17T20:03:49.294+0530] {logging_mixin.py:190} INFO - Task instance in success state
[2024-12-17T20:03:49.295+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: running
[2024-12-17T20:03:49.295+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline queued_at:None
[2024-12-17T20:03:49.295+0530] {logging_mixin.py:190} INFO - Task hostname:Jayeshs-MacBook-Air.local operator:PythonOperator
[2024-12-17T20:03:49.296+0530] {logging_mixin.py:190} INFO - [2024-12-17T20:03:49.296+0530] {dag.py:4410} INFO - [DAG TEST] end task task_id=scrape_collection_page map_index=-1
[2024-12-17T20:03:49.307+0530] {logging_mixin.py:190} INFO - [2024-12-17T20:03:49.306+0530] {dagrun.py:854} INFO - Marking run <DagRun movie_pipeline @ 2024-12-17 14:33:43.677620+00:00: manual__2024-12-17T14:33:43.677620+00:00, state:running, queued_at: None. externally triggered: False> successful
[2024-12-17T20:03:49.307+0530] {logging_mixin.py:190} INFO - Dag run in success state
[2024-12-17T20:03:49.308+0530] {logging_mixin.py:190} INFO - Dag run start:2024-12-17 14:33:43.677620+00:00 end:2024-12-17 14:33:49.307823+00:00
[2024-12-17T20:03:49.308+0530] {logging_mixin.py:190} INFO - [2024-12-17T20:03:49.308+0530] {dagrun.py:905} INFO - DagRun Finished: dag_id=movie_pipeline, execution_date=2024-12-17 14:33:43.677620+00:00, run_id=manual__2024-12-17T14:33:43.677620+00:00, run_start_date=2024-12-17 14:33:43.677620+00:00, run_end_date=2024-12-17 14:33:49.307823+00:00, run_duration=5.630203, state=success, external_trigger=False, run_type=manual, data_interval_start=2024-12-16 14:33:43.677620+00:00, data_interval_end=2024-12-17 14:33:43.677620+00:00, dag_hash=None
[2024-12-17T20:03:49.537+0530] {processor.py:925} INFO - DAG(s) 'movie_pipeline' retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T20:03:49.590+0530] {logging_mixin.py:190} INFO - [2024-12-17T20:03:49.586+0530] {dag.py:3239} INFO - Sync 1 DAGs
[2024-12-17T20:03:49.718+0530] {logging_mixin.py:190} INFO - [2024-12-17T20:03:49.718+0530] {dag.py:4180} INFO - Setting next_dagrun for movie_pipeline to 2024-12-17 01:30:00+00:00, run_after=2024-12-18 01:30:00+00:00
[2024-12-17T20:03:49.769+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 6.163 seconds
[2024-12-17T20:07:54.348+0530] {processor.py:186} INFO - Started process (PID=89892) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T20:07:54.351+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-17T20:07:54.354+0530] {logging_mixin.py:190} INFO - [2024-12-17T20:07:54.353+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T20:07:54.645+0530] {logging_mixin.py:190} INFO - [2024-12-17T20:07:54.645+0530] {dag.py:4435} INFO - dagrun id: movie_pipeline
[2024-12-17T20:07:54.661+0530] {logging_mixin.py:190} INFO - [2024-12-17T20:07:54.660+0530] {dag.py:4451} INFO - created dagrun <DagRun movie_pipeline @ 2024-12-17 14:37:54.454287+00:00: manual__2024-12-17T14:37:54.454287+00:00, state:running, queued_at: None. externally triggered: False>
[2024-12-17T20:07:54.704+0530] {logging_mixin.py:190} INFO - [2024-12-17T20:07:54.704+0530] {dag.py:4396} INFO - [DAG TEST] starting task_id=scrape_collection_page map_index=-1
[2024-12-17T20:07:54.705+0530] {logging_mixin.py:190} INFO - [2024-12-17T20:07:54.705+0530] {dag.py:4399} INFO - [DAG TEST] running task <TaskInstance: movie_pipeline.scrape_collection_page manual__2024-12-17T14:37:54.454287+00:00 [scheduled]>
[2024-12-17T20:07:59.836+0530] {logging_mixin.py:190} INFO - [2024-12-17T20:07:59.835+0530] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-12-17T20:07:59.956+0530] {logging_mixin.py:190} INFO - [2024-12-17 20:07:59,956] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T14:37:54.454287+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T14:37:54.454287+00:00'
[2024-12-17T20:07:59.957+0530] {logging_mixin.py:190} INFO - [2024-12-17T20:07:59.956+0530] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T14:37:54.454287+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T14:37:54.454287+00:00'
[2024-12-17T20:07:59.968+0530] {logging_mixin.py:190} INFO - Task instance is in running state
[2024-12-17T20:07:59.969+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: queued
[2024-12-17T20:07:59.970+0530] {logging_mixin.py:190} INFO - Current task name:scrape_collection_page state:scheduled start_date:None
[2024-12-17T20:07:59.970+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline and current dag run status:running
[2024-12-17T20:07:59.970+0530] {logging_mixin.py:190} INFO - [2024-12-17T20:07:59.970+0530] {taskinstance.py:731} INFO - ::endgroup::
[2024-12-17T20:07:59.974+0530] {logging_mixin.py:190} INFO - 2024-12-17
[2024-12-17T20:07:59.975+0530] {logging_mixin.py:190} INFO - [2024-12-17 20:07:59,974] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T20:07:59.975+0530] {logging_mixin.py:190} INFO - [2024-12-17T20:07:59.974+0530] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T20:07:59.983+0530] {logging_mixin.py:190} INFO - [2024-12-17T20:07:59.983+0530] {taskinstance.py:340} INFO - ::group::Post task execution logs
[2024-12-17T20:07:59.985+0530] {logging_mixin.py:190} INFO - [2024-12-17T20:07:59.985+0530] {taskinstance.py:352} INFO - Marking task as SUCCESS. dag_id=movie_pipeline, task_id=scrape_collection_page, run_id=manual__2024-12-17T14:37:54.454287+00:00, execution_date=20241217T143754, start_date=, end_date=20241217T143759
[2024-12-17T20:08:00.002+0530] {logging_mixin.py:190} INFO - Task instance in success state
[2024-12-17T20:08:00.002+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: running
[2024-12-17T20:08:00.003+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline queued_at:None
[2024-12-17T20:08:00.003+0530] {logging_mixin.py:190} INFO - Task hostname:Jayeshs-MacBook-Air.local operator:PythonOperator
[2024-12-17T20:08:00.004+0530] {logging_mixin.py:190} INFO - [2024-12-17T20:08:00.003+0530] {dag.py:4410} INFO - [DAG TEST] end task task_id=scrape_collection_page map_index=-1
[2024-12-17T20:08:00.012+0530] {logging_mixin.py:190} INFO - [2024-12-17T20:08:00.012+0530] {dagrun.py:854} INFO - Marking run <DagRun movie_pipeline @ 2024-12-17 14:37:54.454287+00:00: manual__2024-12-17T14:37:54.454287+00:00, state:running, queued_at: None. externally triggered: False> successful
[2024-12-17T20:08:00.015+0530] {logging_mixin.py:190} INFO - Dag run in success state
[2024-12-17T20:08:00.016+0530] {logging_mixin.py:190} INFO - Dag run start:2024-12-17 14:37:54.454287+00:00 end:2024-12-17 14:38:00.015185+00:00
[2024-12-17T20:08:00.016+0530] {logging_mixin.py:190} INFO - [2024-12-17T20:08:00.016+0530] {dagrun.py:905} INFO - DagRun Finished: dag_id=movie_pipeline, execution_date=2024-12-17 14:37:54.454287+00:00, run_id=manual__2024-12-17T14:37:54.454287+00:00, run_start_date=2024-12-17 14:37:54.454287+00:00, run_end_date=2024-12-17 14:38:00.015185+00:00, run_duration=5.560898, state=success, external_trigger=False, run_type=manual, data_interval_start=2024-12-16 14:37:54.454287+00:00, data_interval_end=2024-12-17 14:37:54.454287+00:00, dag_hash=None
[2024-12-17T20:08:00.033+0530] {processor.py:925} INFO - DAG(s) 'movie_pipeline' retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T20:08:00.057+0530] {logging_mixin.py:190} INFO - [2024-12-17T20:08:00.056+0530] {dag.py:3239} INFO - Sync 1 DAGs
[2024-12-17T20:08:00.084+0530] {logging_mixin.py:190} INFO - [2024-12-17T20:08:00.084+0530] {dag.py:4180} INFO - Setting next_dagrun for movie_pipeline to 2024-12-17 01:30:00+00:00, run_after=2024-12-18 01:30:00+00:00
[2024-12-17T20:08:00.116+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 5.774 seconds
[2024-12-17T20:12:03.401+0530] {processor.py:186} INFO - Started process (PID=90158) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T20:12:03.404+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-17T20:12:03.406+0530] {logging_mixin.py:190} INFO - [2024-12-17T20:12:03.405+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T20:12:03.655+0530] {logging_mixin.py:190} INFO - [2024-12-17T20:12:03.655+0530] {dag.py:4435} INFO - dagrun id: movie_pipeline
[2024-12-17T20:12:03.675+0530] {logging_mixin.py:190} INFO - [2024-12-17T20:12:03.675+0530] {dag.py:4451} INFO - created dagrun <DagRun movie_pipeline @ 2024-12-17 14:42:03.461847+00:00: manual__2024-12-17T14:42:03.461847+00:00, state:running, queued_at: None. externally triggered: False>
[2024-12-17T20:12:03.714+0530] {logging_mixin.py:190} INFO - [2024-12-17T20:12:03.714+0530] {dag.py:4396} INFO - [DAG TEST] starting task_id=scrape_collection_page map_index=-1
[2024-12-17T20:12:03.716+0530] {logging_mixin.py:190} INFO - [2024-12-17T20:12:03.714+0530] {dag.py:4399} INFO - [DAG TEST] running task <TaskInstance: movie_pipeline.scrape_collection_page manual__2024-12-17T14:42:03.461847+00:00 [scheduled]>
[2024-12-17T20:12:08.838+0530] {logging_mixin.py:190} INFO - [2024-12-17T20:12:08.837+0530] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-12-17T20:12:08.969+0530] {logging_mixin.py:190} INFO - [2024-12-17 20:12:08,969] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T14:42:03.461847+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T14:42:03.461847+00:00'
[2024-12-17T20:12:08.970+0530] {logging_mixin.py:190} INFO - [2024-12-17T20:12:08.969+0530] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T14:42:03.461847+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T14:42:03.461847+00:00'
[2024-12-17T20:12:08.985+0530] {logging_mixin.py:190} INFO - Task instance is in running state
[2024-12-17T20:12:08.986+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: queued
[2024-12-17T20:12:08.986+0530] {logging_mixin.py:190} INFO - Current task name:scrape_collection_page state:scheduled start_date:None
[2024-12-17T20:12:08.987+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline and current dag run status:running
[2024-12-17T20:12:08.987+0530] {logging_mixin.py:190} INFO - [2024-12-17T20:12:08.987+0530] {taskinstance.py:731} INFO - ::endgroup::
[2024-12-17T20:12:08.989+0530] {logging_mixin.py:190} INFO - 2024-12-17
[2024-12-17T20:12:08.990+0530] {logging_mixin.py:190} INFO - [2024-12-17 20:12:08,989] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T20:12:08.990+0530] {logging_mixin.py:190} INFO - [2024-12-17T20:12:08.989+0530] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T20:12:08.996+0530] {logging_mixin.py:190} INFO - [2024-12-17T20:12:08.996+0530] {taskinstance.py:340} INFO - ::group::Post task execution logs
[2024-12-17T20:12:08.997+0530] {logging_mixin.py:190} INFO - [2024-12-17T20:12:08.996+0530] {taskinstance.py:352} INFO - Marking task as SUCCESS. dag_id=movie_pipeline, task_id=scrape_collection_page, run_id=manual__2024-12-17T14:42:03.461847+00:00, execution_date=20241217T144203, start_date=, end_date=20241217T144208
[2024-12-17T20:12:09.023+0530] {logging_mixin.py:190} INFO - Task instance in success state
[2024-12-17T20:12:09.024+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: running
[2024-12-17T20:12:09.024+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline queued_at:None
[2024-12-17T20:12:09.025+0530] {logging_mixin.py:190} INFO - Task hostname:Jayeshs-MacBook-Air.local operator:PythonOperator
[2024-12-17T20:12:09.027+0530] {logging_mixin.py:190} INFO - [2024-12-17T20:12:09.025+0530] {dag.py:4410} INFO - [DAG TEST] end task task_id=scrape_collection_page map_index=-1
[2024-12-17T20:12:09.037+0530] {logging_mixin.py:190} INFO - [2024-12-17T20:12:09.034+0530] {dagrun.py:854} INFO - Marking run <DagRun movie_pipeline @ 2024-12-17 14:42:03.461847+00:00: manual__2024-12-17T14:42:03.461847+00:00, state:running, queued_at: None. externally triggered: False> successful
[2024-12-17T20:12:09.037+0530] {logging_mixin.py:190} INFO - Dag run in success state
[2024-12-17T20:12:09.038+0530] {logging_mixin.py:190} INFO - Dag run start:2024-12-17 14:42:03.461847+00:00 end:2024-12-17 14:42:09.037635+00:00
[2024-12-17T20:12:09.038+0530] {logging_mixin.py:190} INFO - [2024-12-17T20:12:09.038+0530] {dagrun.py:905} INFO - DagRun Finished: dag_id=movie_pipeline, execution_date=2024-12-17 14:42:03.461847+00:00, run_id=manual__2024-12-17T14:42:03.461847+00:00, run_start_date=2024-12-17 14:42:03.461847+00:00, run_end_date=2024-12-17 14:42:09.037635+00:00, run_duration=5.575788, state=success, external_trigger=False, run_type=manual, data_interval_start=2024-12-16 14:42:03.461847+00:00, data_interval_end=2024-12-17 14:42:03.461847+00:00, dag_hash=None
[2024-12-17T20:12:09.051+0530] {processor.py:925} INFO - DAG(s) 'movie_pipeline' retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T20:12:09.074+0530] {logging_mixin.py:190} INFO - [2024-12-17T20:12:09.073+0530] {dag.py:3239} INFO - Sync 1 DAGs
[2024-12-17T20:12:09.112+0530] {logging_mixin.py:190} INFO - [2024-12-17T20:12:09.111+0530] {dag.py:4180} INFO - Setting next_dagrun for movie_pipeline to 2024-12-17 01:30:00+00:00, run_after=2024-12-18 01:30:00+00:00
[2024-12-17T20:12:09.154+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 5.757 seconds
[2024-12-17T20:16:15.652+0530] {processor.py:186} INFO - Started process (PID=90412) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T20:16:15.654+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-17T20:16:15.655+0530] {logging_mixin.py:190} INFO - [2024-12-17T20:16:15.655+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T20:16:15.899+0530] {logging_mixin.py:190} INFO - [2024-12-17T20:16:15.898+0530] {dag.py:4435} INFO - dagrun id: movie_pipeline
[2024-12-17T20:16:15.916+0530] {logging_mixin.py:190} INFO - [2024-12-17T20:16:15.916+0530] {dag.py:4451} INFO - created dagrun <DagRun movie_pipeline @ 2024-12-17 14:46:15.709280+00:00: manual__2024-12-17T14:46:15.709280+00:00, state:running, queued_at: None. externally triggered: False>
[2024-12-17T20:16:15.956+0530] {logging_mixin.py:190} INFO - [2024-12-17T20:16:15.956+0530] {dag.py:4396} INFO - [DAG TEST] starting task_id=scrape_collection_page map_index=-1
[2024-12-17T20:16:15.957+0530] {logging_mixin.py:190} INFO - [2024-12-17T20:16:15.957+0530] {dag.py:4399} INFO - [DAG TEST] running task <TaskInstance: movie_pipeline.scrape_collection_page manual__2024-12-17T14:46:15.709280+00:00 [scheduled]>
[2024-12-17T20:16:21.071+0530] {logging_mixin.py:190} INFO - [2024-12-17T20:16:21.071+0530] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-12-17T20:16:21.178+0530] {logging_mixin.py:190} INFO - [2024-12-17 20:16:21,178] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T14:46:15.709280+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T14:46:15.709280+00:00'
[2024-12-17T20:16:21.178+0530] {logging_mixin.py:190} INFO - [2024-12-17T20:16:21.178+0530] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T14:46:15.709280+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T14:46:15.709280+00:00'
[2024-12-17T20:16:21.187+0530] {logging_mixin.py:190} INFO - Task instance is in running state
[2024-12-17T20:16:21.189+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: queued
[2024-12-17T20:16:21.190+0530] {logging_mixin.py:190} INFO - Current task name:scrape_collection_page state:scheduled start_date:None
[2024-12-17T20:16:21.190+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline and current dag run status:running
[2024-12-17T20:16:21.192+0530] {logging_mixin.py:190} INFO - [2024-12-17T20:16:21.191+0530] {taskinstance.py:731} INFO - ::endgroup::
[2024-12-17T20:16:21.197+0530] {logging_mixin.py:190} INFO - 2024-12-17
[2024-12-17T20:16:21.200+0530] {logging_mixin.py:190} INFO - [2024-12-17 20:16:21,199] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T20:16:21.201+0530] {logging_mixin.py:190} INFO - [2024-12-17T20:16:21.199+0530] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T20:16:21.208+0530] {logging_mixin.py:190} INFO - [2024-12-17T20:16:21.208+0530] {taskinstance.py:340} INFO - ::group::Post task execution logs
[2024-12-17T20:16:21.209+0530] {logging_mixin.py:190} INFO - [2024-12-17T20:16:21.208+0530] {taskinstance.py:352} INFO - Marking task as SUCCESS. dag_id=movie_pipeline, task_id=scrape_collection_page, run_id=manual__2024-12-17T14:46:15.709280+00:00, execution_date=20241217T144615, start_date=, end_date=20241217T144621
[2024-12-17T20:16:21.224+0530] {logging_mixin.py:190} INFO - Task instance in success state
[2024-12-17T20:16:21.225+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: running
[2024-12-17T20:16:21.225+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline queued_at:None
[2024-12-17T20:16:21.226+0530] {logging_mixin.py:190} INFO - Task hostname:Jayeshs-MacBook-Air.local operator:PythonOperator
[2024-12-17T20:16:21.227+0530] {logging_mixin.py:190} INFO - [2024-12-17T20:16:21.226+0530] {dag.py:4410} INFO - [DAG TEST] end task task_id=scrape_collection_page map_index=-1
[2024-12-17T20:16:21.237+0530] {logging_mixin.py:190} INFO - [2024-12-17T20:16:21.236+0530] {dagrun.py:854} INFO - Marking run <DagRun movie_pipeline @ 2024-12-17 14:46:15.709280+00:00: manual__2024-12-17T14:46:15.709280+00:00, state:running, queued_at: None. externally triggered: False> successful
[2024-12-17T20:16:21.237+0530] {logging_mixin.py:190} INFO - Dag run in success state
[2024-12-17T20:16:21.237+0530] {logging_mixin.py:190} INFO - Dag run start:2024-12-17 14:46:15.709280+00:00 end:2024-12-17 14:46:21.237369+00:00
[2024-12-17T20:16:21.238+0530] {logging_mixin.py:190} INFO - [2024-12-17T20:16:21.237+0530] {dagrun.py:905} INFO - DagRun Finished: dag_id=movie_pipeline, execution_date=2024-12-17 14:46:15.709280+00:00, run_id=manual__2024-12-17T14:46:15.709280+00:00, run_start_date=2024-12-17 14:46:15.709280+00:00, run_end_date=2024-12-17 14:46:21.237369+00:00, run_duration=5.528089, state=success, external_trigger=False, run_type=manual, data_interval_start=2024-12-16 14:46:15.709280+00:00, data_interval_end=2024-12-17 14:46:15.709280+00:00, dag_hash=None
[2024-12-17T20:16:21.261+0530] {processor.py:925} INFO - DAG(s) 'movie_pipeline' retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T20:16:21.285+0530] {logging_mixin.py:190} INFO - [2024-12-17T20:16:21.284+0530] {dag.py:3239} INFO - Sync 1 DAGs
[2024-12-17T20:16:21.324+0530] {logging_mixin.py:190} INFO - [2024-12-17T20:16:21.323+0530] {dag.py:4180} INFO - Setting next_dagrun for movie_pipeline to 2024-12-17 01:30:00+00:00, run_after=2024-12-18 01:30:00+00:00
[2024-12-17T20:16:21.351+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 5.703 seconds
[2024-12-17T20:20:00.519+0530] {processor.py:186} INFO - Started process (PID=90658) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T20:20:00.521+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-17T20:20:00.523+0530] {logging_mixin.py:190} INFO - [2024-12-17T20:20:00.523+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T20:20:00.692+0530] {logging_mixin.py:190} INFO - [2024-12-17T20:20:00.692+0530] {dag.py:4435} INFO - dagrun id: movie_pipeline
[2024-12-17T20:20:00.706+0530] {logging_mixin.py:190} INFO - [2024-12-17T20:20:00.705+0530] {dag.py:4451} INFO - created dagrun <DagRun movie_pipeline @ 2024-12-17 14:50:00.564909+00:00: manual__2024-12-17T14:50:00.564909+00:00, state:running, queued_at: None. externally triggered: False>
[2024-12-17T20:20:00.732+0530] {logging_mixin.py:190} INFO - [2024-12-17T20:20:00.731+0530] {dag.py:4396} INFO - [DAG TEST] starting task_id=scrape_collection_page map_index=-1
[2024-12-17T20:20:00.732+0530] {logging_mixin.py:190} INFO - [2024-12-17T20:20:00.732+0530] {dag.py:4399} INFO - [DAG TEST] running task <TaskInstance: movie_pipeline.scrape_collection_page manual__2024-12-17T14:50:00.564909+00:00 [scheduled]>
[2024-12-17T20:20:05.898+0530] {logging_mixin.py:190} INFO - [2024-12-17T20:20:05.897+0530] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-12-17T20:20:05.959+0530] {logging_mixin.py:190} INFO - [2024-12-17 20:20:05,959] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T14:50:00.564909+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T14:50:00.564909+00:00'
[2024-12-17T20:20:05.960+0530] {logging_mixin.py:190} INFO - [2024-12-17T20:20:05.959+0530] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T14:50:00.564909+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T14:50:00.564909+00:00'
[2024-12-17T20:20:05.964+0530] {logging_mixin.py:190} INFO - Task instance is in running state
[2024-12-17T20:20:05.964+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: queued
[2024-12-17T20:20:05.964+0530] {logging_mixin.py:190} INFO - Current task name:scrape_collection_page state:scheduled start_date:None
[2024-12-17T20:20:05.965+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline and current dag run status:running
[2024-12-17T20:20:05.965+0530] {logging_mixin.py:190} INFO - [2024-12-17T20:20:05.965+0530] {taskinstance.py:731} INFO - ::endgroup::
[2024-12-17T20:20:05.967+0530] {logging_mixin.py:190} INFO - 2024-12-17
[2024-12-17T20:20:05.967+0530] {logging_mixin.py:190} INFO - [2024-12-17 20:20:05,967] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T20:20:05.967+0530] {logging_mixin.py:190} INFO - [2024-12-17T20:20:05.967+0530] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T20:20:05.973+0530] {logging_mixin.py:190} INFO - [2024-12-17T20:20:05.972+0530] {taskinstance.py:340} INFO - ::group::Post task execution logs
[2024-12-17T20:20:05.973+0530] {logging_mixin.py:190} INFO - [2024-12-17T20:20:05.973+0530] {taskinstance.py:352} INFO - Marking task as SUCCESS. dag_id=movie_pipeline, task_id=scrape_collection_page, run_id=manual__2024-12-17T14:50:00.564909+00:00, execution_date=20241217T145000, start_date=, end_date=20241217T145005
[2024-12-17T20:20:05.983+0530] {logging_mixin.py:190} INFO - Task instance in success state
[2024-12-17T20:20:05.983+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: running
[2024-12-17T20:20:05.984+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline queued_at:None
[2024-12-17T20:20:05.984+0530] {logging_mixin.py:190} INFO - Task hostname:Jayeshs-MacBook-Air.local operator:PythonOperator
[2024-12-17T20:20:05.984+0530] {logging_mixin.py:190} INFO - [2024-12-17T20:20:05.984+0530] {dag.py:4410} INFO - [DAG TEST] end task task_id=scrape_collection_page map_index=-1
[2024-12-17T20:20:05.988+0530] {logging_mixin.py:190} INFO - [2024-12-17T20:20:05.988+0530] {dagrun.py:854} INFO - Marking run <DagRun movie_pipeline @ 2024-12-17 14:50:00.564909+00:00: manual__2024-12-17T14:50:00.564909+00:00, state:running, queued_at: None. externally triggered: False> successful
[2024-12-17T20:20:05.988+0530] {logging_mixin.py:190} INFO - Dag run in success state
[2024-12-17T20:20:05.989+0530] {logging_mixin.py:190} INFO - Dag run start:2024-12-17 14:50:00.564909+00:00 end:2024-12-17 14:50:05.988895+00:00
[2024-12-17T20:20:05.989+0530] {logging_mixin.py:190} INFO - [2024-12-17T20:20:05.989+0530] {dagrun.py:905} INFO - DagRun Finished: dag_id=movie_pipeline, execution_date=2024-12-17 14:50:00.564909+00:00, run_id=manual__2024-12-17T14:50:00.564909+00:00, run_start_date=2024-12-17 14:50:00.564909+00:00, run_end_date=2024-12-17 14:50:05.988895+00:00, run_duration=5.423986, state=success, external_trigger=False, run_type=manual, data_interval_start=2024-12-16 14:50:00.564909+00:00, data_interval_end=2024-12-17 14:50:00.564909+00:00, dag_hash=None
[2024-12-17T20:20:05.999+0530] {processor.py:925} INFO - DAG(s) 'movie_pipeline' retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T20:20:06.010+0530] {logging_mixin.py:190} INFO - [2024-12-17T20:20:06.010+0530] {dag.py:3239} INFO - Sync 1 DAGs
[2024-12-17T20:20:06.028+0530] {logging_mixin.py:190} INFO - [2024-12-17T20:20:06.028+0530] {dag.py:4180} INFO - Setting next_dagrun for movie_pipeline to 2024-12-17 01:30:00+00:00, run_after=2024-12-18 01:30:00+00:00
[2024-12-17T20:20:06.047+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 5.532 seconds
[2024-12-17T20:22:59.163+0530] {processor.py:186} INFO - Started process (PID=90879) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T20:22:59.165+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-17T20:22:59.166+0530] {logging_mixin.py:190} INFO - [2024-12-17T20:22:59.165+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T20:22:59.344+0530] {logging_mixin.py:190} INFO - [2024-12-17T20:22:59.344+0530] {dag.py:4435} INFO - dagrun id: movie_pipeline
[2024-12-17T20:22:59.358+0530] {logging_mixin.py:190} INFO - [2024-12-17T20:22:59.357+0530] {dag.py:4451} INFO - created dagrun <DagRun movie_pipeline @ 2024-12-17 14:52:59.213061+00:00: manual__2024-12-17T14:52:59.213061+00:00, state:running, queued_at: None. externally triggered: False>
[2024-12-17T20:22:59.383+0530] {logging_mixin.py:190} INFO - [2024-12-17T20:22:59.383+0530] {dag.py:4396} INFO - [DAG TEST] starting task_id=scrape_collection_page map_index=-1
[2024-12-17T20:22:59.384+0530] {logging_mixin.py:190} INFO - [2024-12-17T20:22:59.383+0530] {dag.py:4399} INFO - [DAG TEST] running task <TaskInstance: movie_pipeline.scrape_collection_page manual__2024-12-17T14:52:59.213061+00:00 [scheduled]>
[2024-12-17T20:23:04.480+0530] {logging_mixin.py:190} INFO - [2024-12-17T20:23:04.480+0530] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-12-17T20:23:04.540+0530] {logging_mixin.py:190} INFO - [2024-12-17 20:23:04,540] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T14:52:59.213061+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T14:52:59.213061+00:00'
[2024-12-17T20:23:04.541+0530] {logging_mixin.py:190} INFO - [2024-12-17T20:23:04.540+0530] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T14:52:59.213061+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T14:52:59.213061+00:00'
[2024-12-17T20:23:04.543+0530] {logging_mixin.py:190} INFO - Task instance is in running state
[2024-12-17T20:23:04.544+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: queued
[2024-12-17T20:23:04.544+0530] {logging_mixin.py:190} INFO - Current task name:scrape_collection_page state:scheduled start_date:None
[2024-12-17T20:23:04.544+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline and current dag run status:running
[2024-12-17T20:23:04.545+0530] {logging_mixin.py:190} INFO - [2024-12-17T20:23:04.544+0530] {taskinstance.py:731} INFO - ::endgroup::
[2024-12-17T20:23:04.546+0530] {logging_mixin.py:190} INFO - 2024-12-17
[2024-12-17T20:23:04.547+0530] {logging_mixin.py:190} INFO - [2024-12-17 20:23:04,547] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T20:23:04.547+0530] {logging_mixin.py:190} INFO - [2024-12-17T20:23:04.547+0530] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T20:23:04.553+0530] {logging_mixin.py:190} INFO - [2024-12-17T20:23:04.552+0530] {taskinstance.py:340} INFO - ::group::Post task execution logs
[2024-12-17T20:23:04.553+0530] {logging_mixin.py:190} INFO - [2024-12-17T20:23:04.553+0530] {taskinstance.py:352} INFO - Marking task as SUCCESS. dag_id=movie_pipeline, task_id=scrape_collection_page, run_id=manual__2024-12-17T14:52:59.213061+00:00, execution_date=20241217T145259, start_date=, end_date=20241217T145304
[2024-12-17T20:23:04.562+0530] {logging_mixin.py:190} INFO - Task instance in success state
[2024-12-17T20:23:04.563+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: running
[2024-12-17T20:23:04.563+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline queued_at:None
[2024-12-17T20:23:04.563+0530] {logging_mixin.py:190} INFO - Task hostname:Jayeshs-MacBook-Air.local operator:PythonOperator
[2024-12-17T20:23:04.563+0530] {logging_mixin.py:190} INFO - [2024-12-17T20:23:04.563+0530] {dag.py:4410} INFO - [DAG TEST] end task task_id=scrape_collection_page map_index=-1
[2024-12-17T20:23:04.568+0530] {logging_mixin.py:190} INFO - [2024-12-17T20:23:04.567+0530] {dagrun.py:854} INFO - Marking run <DagRun movie_pipeline @ 2024-12-17 14:52:59.213061+00:00: manual__2024-12-17T14:52:59.213061+00:00, state:running, queued_at: None. externally triggered: False> successful
[2024-12-17T20:23:04.568+0530] {logging_mixin.py:190} INFO - Dag run in success state
[2024-12-17T20:23:04.569+0530] {logging_mixin.py:190} INFO - Dag run start:2024-12-17 14:52:59.213061+00:00 end:2024-12-17 14:53:04.568690+00:00
[2024-12-17T20:23:04.569+0530] {logging_mixin.py:190} INFO - [2024-12-17T20:23:04.569+0530] {dagrun.py:905} INFO - DagRun Finished: dag_id=movie_pipeline, execution_date=2024-12-17 14:52:59.213061+00:00, run_id=manual__2024-12-17T14:52:59.213061+00:00, run_start_date=2024-12-17 14:52:59.213061+00:00, run_end_date=2024-12-17 14:53:04.568690+00:00, run_duration=5.355629, state=success, external_trigger=False, run_type=manual, data_interval_start=2024-12-16 14:52:59.213061+00:00, data_interval_end=2024-12-17 14:52:59.213061+00:00, dag_hash=None
[2024-12-17T20:23:04.580+0530] {processor.py:925} INFO - DAG(s) 'movie_pipeline' retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T20:23:04.592+0530] {logging_mixin.py:190} INFO - [2024-12-17T20:23:04.592+0530] {dag.py:3239} INFO - Sync 1 DAGs
[2024-12-17T20:23:04.610+0530] {logging_mixin.py:190} INFO - [2024-12-17T20:23:04.609+0530] {dag.py:4180} INFO - Setting next_dagrun for movie_pipeline to 2024-12-17 01:30:00+00:00, run_after=2024-12-18 01:30:00+00:00
[2024-12-17T20:23:04.628+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 5.468 seconds
[2024-12-17T20:25:53.590+0530] {processor.py:186} INFO - Started process (PID=91072) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T20:25:53.591+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-17T20:25:53.593+0530] {logging_mixin.py:190} INFO - [2024-12-17T20:25:53.592+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T20:25:53.768+0530] {logging_mixin.py:190} INFO - [2024-12-17T20:25:53.768+0530] {dag.py:4435} INFO - dagrun id: movie_pipeline
[2024-12-17T20:25:53.782+0530] {logging_mixin.py:190} INFO - [2024-12-17T20:25:53.782+0530] {dag.py:4451} INFO - created dagrun <DagRun movie_pipeline @ 2024-12-17 14:55:53.632191+00:00: manual__2024-12-17T14:55:53.632191+00:00, state:running, queued_at: None. externally triggered: False>
[2024-12-17T20:25:53.808+0530] {logging_mixin.py:190} INFO - [2024-12-17T20:25:53.808+0530] {dag.py:4396} INFO - [DAG TEST] starting task_id=scrape_collection_page map_index=-1
[2024-12-17T20:25:53.809+0530] {logging_mixin.py:190} INFO - [2024-12-17T20:25:53.808+0530] {dag.py:4399} INFO - [DAG TEST] running task <TaskInstance: movie_pipeline.scrape_collection_page manual__2024-12-17T14:55:53.632191+00:00 [scheduled]>
[2024-12-17T20:25:58.910+0530] {logging_mixin.py:190} INFO - [2024-12-17T20:25:58.910+0530] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-12-17T20:25:58.970+0530] {logging_mixin.py:190} INFO - [2024-12-17 20:25:58,970] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T14:55:53.632191+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T14:55:53.632191+00:00'
[2024-12-17T20:25:58.970+0530] {logging_mixin.py:190} INFO - [2024-12-17T20:25:58.970+0530] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T14:55:53.632191+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T14:55:53.632191+00:00'
[2024-12-17T20:25:58.974+0530] {logging_mixin.py:190} INFO - Task instance is in running state
[2024-12-17T20:25:58.974+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: queued
[2024-12-17T20:25:58.974+0530] {logging_mixin.py:190} INFO - Current task name:scrape_collection_page state:scheduled start_date:None
[2024-12-17T20:25:58.975+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline and current dag run status:running
[2024-12-17T20:25:58.975+0530] {logging_mixin.py:190} INFO - [2024-12-17T20:25:58.975+0530] {taskinstance.py:731} INFO - ::endgroup::
[2024-12-17T20:25:58.977+0530] {logging_mixin.py:190} INFO - 2024-12-17
[2024-12-17T20:25:58.977+0530] {logging_mixin.py:190} INFO - [2024-12-17 20:25:58,977] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T20:25:58.977+0530] {logging_mixin.py:190} INFO - [2024-12-17T20:25:58.977+0530] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T20:25:58.982+0530] {logging_mixin.py:190} INFO - [2024-12-17T20:25:58.982+0530] {taskinstance.py:340} INFO - ::group::Post task execution logs
[2024-12-17T20:25:58.983+0530] {logging_mixin.py:190} INFO - [2024-12-17T20:25:58.983+0530] {taskinstance.py:352} INFO - Marking task as SUCCESS. dag_id=movie_pipeline, task_id=scrape_collection_page, run_id=manual__2024-12-17T14:55:53.632191+00:00, execution_date=20241217T145553, start_date=, end_date=20241217T145558
[2024-12-17T20:25:58.994+0530] {logging_mixin.py:190} INFO - Task instance in success state
[2024-12-17T20:25:58.995+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: running
[2024-12-17T20:25:58.995+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline queued_at:None
[2024-12-17T20:25:58.995+0530] {logging_mixin.py:190} INFO - Task hostname:Jayeshs-MacBook-Air.local operator:PythonOperator
[2024-12-17T20:25:58.996+0530] {logging_mixin.py:190} INFO - [2024-12-17T20:25:58.996+0530] {dag.py:4410} INFO - [DAG TEST] end task task_id=scrape_collection_page map_index=-1
[2024-12-17T20:25:59.000+0530] {logging_mixin.py:190} INFO - [2024-12-17T20:25:59.000+0530] {dagrun.py:854} INFO - Marking run <DagRun movie_pipeline @ 2024-12-17 14:55:53.632191+00:00: manual__2024-12-17T14:55:53.632191+00:00, state:running, queued_at: None. externally triggered: False> successful
[2024-12-17T20:25:59.001+0530] {logging_mixin.py:190} INFO - Dag run in success state
[2024-12-17T20:25:59.001+0530] {logging_mixin.py:190} INFO - Dag run start:2024-12-17 14:55:53.632191+00:00 end:2024-12-17 14:55:59.001175+00:00
[2024-12-17T20:25:59.001+0530] {logging_mixin.py:190} INFO - [2024-12-17T20:25:59.001+0530] {dagrun.py:905} INFO - DagRun Finished: dag_id=movie_pipeline, execution_date=2024-12-17 14:55:53.632191+00:00, run_id=manual__2024-12-17T14:55:53.632191+00:00, run_start_date=2024-12-17 14:55:53.632191+00:00, run_end_date=2024-12-17 14:55:59.001175+00:00, run_duration=5.368984, state=success, external_trigger=False, run_type=manual, data_interval_start=2024-12-16 14:55:53.632191+00:00, data_interval_end=2024-12-17 14:55:53.632191+00:00, dag_hash=None
[2024-12-17T20:25:59.010+0530] {processor.py:925} INFO - DAG(s) 'movie_pipeline' retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T20:25:59.023+0530] {logging_mixin.py:190} INFO - [2024-12-17T20:25:59.022+0530] {dag.py:3239} INFO - Sync 1 DAGs
[2024-12-17T20:25:59.040+0530] {logging_mixin.py:190} INFO - [2024-12-17T20:25:59.040+0530] {dag.py:4180} INFO - Setting next_dagrun for movie_pipeline to 2024-12-17 01:30:00+00:00, run_after=2024-12-18 01:30:00+00:00
[2024-12-17T20:25:59.059+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 5.472 seconds
[2024-12-17T20:28:56.154+0530] {processor.py:186} INFO - Started process (PID=91299) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T20:28:56.156+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-17T20:28:56.158+0530] {logging_mixin.py:190} INFO - [2024-12-17T20:28:56.158+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T20:28:56.338+0530] {logging_mixin.py:190} INFO - [2024-12-17T20:28:56.338+0530] {dag.py:4435} INFO - dagrun id: movie_pipeline
[2024-12-17T20:28:56.354+0530] {logging_mixin.py:190} INFO - [2024-12-17T20:28:56.354+0530] {dag.py:4451} INFO - created dagrun <DagRun movie_pipeline @ 2024-12-17 14:58:56.202519+00:00: manual__2024-12-17T14:58:56.202519+00:00, state:running, queued_at: None. externally triggered: False>
[2024-12-17T20:28:56.387+0530] {logging_mixin.py:190} INFO - [2024-12-17T20:28:56.387+0530] {dag.py:4396} INFO - [DAG TEST] starting task_id=scrape_collection_page map_index=-1
[2024-12-17T20:28:56.388+0530] {logging_mixin.py:190} INFO - [2024-12-17T20:28:56.387+0530] {dag.py:4399} INFO - [DAG TEST] running task <TaskInstance: movie_pipeline.scrape_collection_page manual__2024-12-17T14:58:56.202519+00:00 [scheduled]>
[2024-12-17T20:29:01.486+0530] {logging_mixin.py:190} INFO - [2024-12-17T20:29:01.486+0530] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-12-17T20:29:01.550+0530] {logging_mixin.py:190} INFO - [2024-12-17 20:29:01,550] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T14:58:56.202519+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T14:58:56.202519+00:00'
[2024-12-17T20:29:01.550+0530] {logging_mixin.py:190} INFO - [2024-12-17T20:29:01.550+0530] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T14:58:56.202519+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T14:58:56.202519+00:00'
[2024-12-17T20:29:01.564+0530] {logging_mixin.py:190} INFO - Task instance is in running state
[2024-12-17T20:29:01.564+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: queued
[2024-12-17T20:29:01.564+0530] {logging_mixin.py:190} INFO - Current task name:scrape_collection_page state:scheduled start_date:None
[2024-12-17T20:29:01.565+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline and current dag run status:running
[2024-12-17T20:29:01.565+0530] {logging_mixin.py:190} INFO - [2024-12-17T20:29:01.565+0530] {taskinstance.py:731} INFO - ::endgroup::
[2024-12-17T20:29:01.570+0530] {logging_mixin.py:190} INFO - 2024-12-17
[2024-12-17T20:29:01.571+0530] {logging_mixin.py:190} INFO - [2024-12-17 20:29:01,570] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T20:29:01.572+0530] {logging_mixin.py:190} INFO - [2024-12-17T20:29:01.570+0530] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T20:29:01.581+0530] {logging_mixin.py:190} INFO - [2024-12-17T20:29:01.581+0530] {taskinstance.py:340} INFO - ::group::Post task execution logs
[2024-12-17T20:29:01.582+0530] {logging_mixin.py:190} INFO - [2024-12-17T20:29:01.582+0530] {taskinstance.py:352} INFO - Marking task as SUCCESS. dag_id=movie_pipeline, task_id=scrape_collection_page, run_id=manual__2024-12-17T14:58:56.202519+00:00, execution_date=20241217T145856, start_date=, end_date=20241217T145901
[2024-12-17T20:29:01.592+0530] {logging_mixin.py:190} INFO - Task instance in success state
[2024-12-17T20:29:01.592+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: running
[2024-12-17T20:29:01.592+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline queued_at:None
[2024-12-17T20:29:01.593+0530] {logging_mixin.py:190} INFO - Task hostname:Jayeshs-MacBook-Air.local operator:PythonOperator
[2024-12-17T20:29:01.593+0530] {logging_mixin.py:190} INFO - [2024-12-17T20:29:01.593+0530] {dag.py:4410} INFO - [DAG TEST] end task task_id=scrape_collection_page map_index=-1
[2024-12-17T20:29:01.598+0530] {logging_mixin.py:190} INFO - [2024-12-17T20:29:01.598+0530] {dagrun.py:854} INFO - Marking run <DagRun movie_pipeline @ 2024-12-17 14:58:56.202519+00:00: manual__2024-12-17T14:58:56.202519+00:00, state:running, queued_at: None. externally triggered: False> successful
[2024-12-17T20:29:01.598+0530] {logging_mixin.py:190} INFO - Dag run in success state
[2024-12-17T20:29:01.599+0530] {logging_mixin.py:190} INFO - Dag run start:2024-12-17 14:58:56.202519+00:00 end:2024-12-17 14:59:01.598677+00:00
[2024-12-17T20:29:01.599+0530] {logging_mixin.py:190} INFO - [2024-12-17T20:29:01.599+0530] {dagrun.py:905} INFO - DagRun Finished: dag_id=movie_pipeline, execution_date=2024-12-17 14:58:56.202519+00:00, run_id=manual__2024-12-17T14:58:56.202519+00:00, run_start_date=2024-12-17 14:58:56.202519+00:00, run_end_date=2024-12-17 14:59:01.598677+00:00, run_duration=5.396158, state=success, external_trigger=False, run_type=manual, data_interval_start=2024-12-16 14:58:56.202519+00:00, data_interval_end=2024-12-17 14:58:56.202519+00:00, dag_hash=None
[2024-12-17T20:29:01.607+0530] {processor.py:925} INFO - DAG(s) 'movie_pipeline' retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T20:29:01.620+0530] {logging_mixin.py:190} INFO - [2024-12-17T20:29:01.619+0530] {dag.py:3239} INFO - Sync 1 DAGs
[2024-12-17T20:29:01.637+0530] {logging_mixin.py:190} INFO - [2024-12-17T20:29:01.637+0530] {dag.py:4180} INFO - Setting next_dagrun for movie_pipeline to 2024-12-17 01:30:00+00:00, run_after=2024-12-18 01:30:00+00:00
[2024-12-17T20:29:01.655+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 5.505 seconds
[2024-12-17T20:31:53.887+0530] {processor.py:186} INFO - Started process (PID=91500) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T20:31:53.889+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-17T20:31:53.891+0530] {logging_mixin.py:190} INFO - [2024-12-17T20:31:53.891+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T20:31:54.062+0530] {logging_mixin.py:190} INFO - [2024-12-17T20:31:54.061+0530] {dag.py:4435} INFO - dagrun id: movie_pipeline
[2024-12-17T20:31:54.076+0530] {logging_mixin.py:190} INFO - [2024-12-17T20:31:54.076+0530] {dag.py:4451} INFO - created dagrun <DagRun movie_pipeline @ 2024-12-17 15:01:53.931288+00:00: manual__2024-12-17T15:01:53.931288+00:00, state:running, queued_at: None. externally triggered: False>
[2024-12-17T20:31:54.103+0530] {logging_mixin.py:190} INFO - [2024-12-17T20:31:54.102+0530] {dag.py:4396} INFO - [DAG TEST] starting task_id=scrape_collection_page map_index=-1
[2024-12-17T20:31:54.103+0530] {logging_mixin.py:190} INFO - [2024-12-17T20:31:54.103+0530] {dag.py:4399} INFO - [DAG TEST] running task <TaskInstance: movie_pipeline.scrape_collection_page manual__2024-12-17T15:01:53.931288+00:00 [scheduled]>
[2024-12-17T20:31:59.207+0530] {logging_mixin.py:190} INFO - [2024-12-17T20:31:59.207+0530] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-12-17T20:31:59.275+0530] {logging_mixin.py:190} INFO - [2024-12-17 20:31:59,274] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T15:01:53.931288+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T15:01:53.931288+00:00'
[2024-12-17T20:31:59.275+0530] {logging_mixin.py:190} INFO - [2024-12-17T20:31:59.274+0530] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T15:01:53.931288+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T15:01:53.931288+00:00'
[2024-12-17T20:31:59.278+0530] {logging_mixin.py:190} INFO - Task instance is in running state
[2024-12-17T20:31:59.278+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: queued
[2024-12-17T20:31:59.279+0530] {logging_mixin.py:190} INFO - Current task name:scrape_collection_page state:scheduled start_date:None
[2024-12-17T20:31:59.279+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline and current dag run status:running
[2024-12-17T20:31:59.279+0530] {logging_mixin.py:190} INFO - [2024-12-17T20:31:59.279+0530] {taskinstance.py:731} INFO - ::endgroup::
[2024-12-17T20:31:59.281+0530] {logging_mixin.py:190} INFO - 2024-12-17
[2024-12-17T20:31:59.281+0530] {logging_mixin.py:190} INFO - [2024-12-17 20:31:59,281] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T20:31:59.282+0530] {logging_mixin.py:190} INFO - [2024-12-17T20:31:59.281+0530] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T20:31:59.287+0530] {logging_mixin.py:190} INFO - [2024-12-17T20:31:59.287+0530] {taskinstance.py:340} INFO - ::group::Post task execution logs
[2024-12-17T20:31:59.288+0530] {logging_mixin.py:190} INFO - [2024-12-17T20:31:59.288+0530] {taskinstance.py:352} INFO - Marking task as SUCCESS. dag_id=movie_pipeline, task_id=scrape_collection_page, run_id=manual__2024-12-17T15:01:53.931288+00:00, execution_date=20241217T150153, start_date=, end_date=20241217T150159
[2024-12-17T20:31:59.297+0530] {logging_mixin.py:190} INFO - Task instance in success state
[2024-12-17T20:31:59.298+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: running
[2024-12-17T20:31:59.298+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline queued_at:None
[2024-12-17T20:31:59.298+0530] {logging_mixin.py:190} INFO - Task hostname:Jayeshs-MacBook-Air.local operator:PythonOperator
[2024-12-17T20:31:59.298+0530] {logging_mixin.py:190} INFO - [2024-12-17T20:31:59.298+0530] {dag.py:4410} INFO - [DAG TEST] end task task_id=scrape_collection_page map_index=-1
[2024-12-17T20:31:59.303+0530] {logging_mixin.py:190} INFO - [2024-12-17T20:31:59.302+0530] {dagrun.py:854} INFO - Marking run <DagRun movie_pipeline @ 2024-12-17 15:01:53.931288+00:00: manual__2024-12-17T15:01:53.931288+00:00, state:running, queued_at: None. externally triggered: False> successful
[2024-12-17T20:31:59.303+0530] {logging_mixin.py:190} INFO - Dag run in success state
[2024-12-17T20:31:59.303+0530] {logging_mixin.py:190} INFO - Dag run start:2024-12-17 15:01:53.931288+00:00 end:2024-12-17 15:01:59.303351+00:00
[2024-12-17T20:31:59.304+0530] {logging_mixin.py:190} INFO - [2024-12-17T20:31:59.303+0530] {dagrun.py:905} INFO - DagRun Finished: dag_id=movie_pipeline, execution_date=2024-12-17 15:01:53.931288+00:00, run_id=manual__2024-12-17T15:01:53.931288+00:00, run_start_date=2024-12-17 15:01:53.931288+00:00, run_end_date=2024-12-17 15:01:59.303351+00:00, run_duration=5.372063, state=success, external_trigger=False, run_type=manual, data_interval_start=2024-12-16 15:01:53.931288+00:00, data_interval_end=2024-12-17 15:01:53.931288+00:00, dag_hash=None
[2024-12-17T20:31:59.312+0530] {processor.py:925} INFO - DAG(s) 'movie_pipeline' retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T20:31:59.325+0530] {logging_mixin.py:190} INFO - [2024-12-17T20:31:59.324+0530] {dag.py:3239} INFO - Sync 1 DAGs
[2024-12-17T20:31:59.342+0530] {logging_mixin.py:190} INFO - [2024-12-17T20:31:59.342+0530] {dag.py:4180} INFO - Setting next_dagrun for movie_pipeline to 2024-12-17 01:30:00+00:00, run_after=2024-12-18 01:30:00+00:00
[2024-12-17T20:31:59.362+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 5.479 seconds
[2024-12-17T20:34:48.025+0530] {processor.py:186} INFO - Started process (PID=91704) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T20:34:48.026+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-17T20:34:48.027+0530] {logging_mixin.py:190} INFO - [2024-12-17T20:34:48.027+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T20:34:48.196+0530] {logging_mixin.py:190} INFO - [2024-12-17T20:34:48.196+0530] {dag.py:4435} INFO - dagrun id: movie_pipeline
[2024-12-17T20:34:48.215+0530] {logging_mixin.py:190} INFO - [2024-12-17T20:34:48.214+0530] {dag.py:4451} INFO - created dagrun <DagRun movie_pipeline @ 2024-12-17 15:04:48.067100+00:00: manual__2024-12-17T15:04:48.067100+00:00, state:running, queued_at: None. externally triggered: False>
[2024-12-17T20:34:48.242+0530] {logging_mixin.py:190} INFO - [2024-12-17T20:34:48.242+0530] {dag.py:4396} INFO - [DAG TEST] starting task_id=scrape_collection_page map_index=-1
[2024-12-17T20:34:48.243+0530] {logging_mixin.py:190} INFO - [2024-12-17T20:34:48.243+0530] {dag.py:4399} INFO - [DAG TEST] running task <TaskInstance: movie_pipeline.scrape_collection_page manual__2024-12-17T15:04:48.067100+00:00 [scheduled]>
[2024-12-17T20:34:53.342+0530] {logging_mixin.py:190} INFO - [2024-12-17T20:34:53.342+0530] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-12-17T20:34:53.436+0530] {logging_mixin.py:190} INFO - [2024-12-17 20:34:53,436] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T15:04:48.067100+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T15:04:48.067100+00:00'
[2024-12-17T20:34:53.436+0530] {logging_mixin.py:190} INFO - [2024-12-17T20:34:53.436+0530] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T15:04:48.067100+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T15:04:48.067100+00:00'
[2024-12-17T20:34:53.441+0530] {logging_mixin.py:190} INFO - Task instance is in running state
[2024-12-17T20:34:53.441+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: queued
[2024-12-17T20:34:53.441+0530] {logging_mixin.py:190} INFO - Current task name:scrape_collection_page state:scheduled start_date:None
[2024-12-17T20:34:53.441+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline and current dag run status:running
[2024-12-17T20:34:53.442+0530] {logging_mixin.py:190} INFO - [2024-12-17T20:34:53.442+0530] {taskinstance.py:731} INFO - ::endgroup::
[2024-12-17T20:34:53.444+0530] {logging_mixin.py:190} INFO - 2024-12-17
[2024-12-17T20:34:53.445+0530] {logging_mixin.py:190} INFO - [2024-12-17 20:34:53,444] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T20:34:53.445+0530] {logging_mixin.py:190} INFO - [2024-12-17T20:34:53.444+0530] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T20:34:53.452+0530] {logging_mixin.py:190} INFO - [2024-12-17T20:34:53.452+0530] {taskinstance.py:340} INFO - ::group::Post task execution logs
[2024-12-17T20:34:53.453+0530] {logging_mixin.py:190} INFO - [2024-12-17T20:34:53.453+0530] {taskinstance.py:352} INFO - Marking task as SUCCESS. dag_id=movie_pipeline, task_id=scrape_collection_page, run_id=manual__2024-12-17T15:04:48.067100+00:00, execution_date=20241217T150448, start_date=, end_date=20241217T150453
[2024-12-17T20:34:53.471+0530] {logging_mixin.py:190} INFO - Task instance in success state
[2024-12-17T20:34:53.471+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: running
[2024-12-17T20:34:53.471+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline queued_at:None
[2024-12-17T20:34:53.472+0530] {logging_mixin.py:190} INFO - Task hostname:Jayeshs-MacBook-Air.local operator:PythonOperator
[2024-12-17T20:34:53.473+0530] {logging_mixin.py:190} INFO - [2024-12-17T20:34:53.473+0530] {dag.py:4410} INFO - [DAG TEST] end task task_id=scrape_collection_page map_index=-1
[2024-12-17T20:34:53.478+0530] {logging_mixin.py:190} INFO - [2024-12-17T20:34:53.478+0530] {dagrun.py:854} INFO - Marking run <DagRun movie_pipeline @ 2024-12-17 15:04:48.067100+00:00: manual__2024-12-17T15:04:48.067100+00:00, state:running, queued_at: None. externally triggered: False> successful
[2024-12-17T20:34:53.479+0530] {logging_mixin.py:190} INFO - Dag run in success state
[2024-12-17T20:34:53.479+0530] {logging_mixin.py:190} INFO - Dag run start:2024-12-17 15:04:48.067100+00:00 end:2024-12-17 15:04:53.478949+00:00
[2024-12-17T20:34:53.479+0530] {logging_mixin.py:190} INFO - [2024-12-17T20:34:53.479+0530] {dagrun.py:905} INFO - DagRun Finished: dag_id=movie_pipeline, execution_date=2024-12-17 15:04:48.067100+00:00, run_id=manual__2024-12-17T15:04:48.067100+00:00, run_start_date=2024-12-17 15:04:48.067100+00:00, run_end_date=2024-12-17 15:04:53.478949+00:00, run_duration=5.411849, state=success, external_trigger=False, run_type=manual, data_interval_start=2024-12-16 15:04:48.067100+00:00, data_interval_end=2024-12-17 15:04:48.067100+00:00, dag_hash=None
[2024-12-17T20:34:53.489+0530] {processor.py:925} INFO - DAG(s) 'movie_pipeline' retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T20:34:53.505+0530] {logging_mixin.py:190} INFO - [2024-12-17T20:34:53.505+0530] {dag.py:3239} INFO - Sync 1 DAGs
[2024-12-17T20:34:53.532+0530] {logging_mixin.py:190} INFO - [2024-12-17T20:34:53.531+0530] {dag.py:4180} INFO - Setting next_dagrun for movie_pipeline to 2024-12-17 01:30:00+00:00, run_after=2024-12-18 01:30:00+00:00
[2024-12-17T20:34:53.554+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 5.533 seconds
[2024-12-17T20:37:46.139+0530] {processor.py:186} INFO - Started process (PID=91906) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T20:37:46.142+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-17T20:37:46.143+0530] {logging_mixin.py:190} INFO - [2024-12-17T20:37:46.143+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T20:37:46.309+0530] {logging_mixin.py:190} INFO - [2024-12-17T20:37:46.309+0530] {dag.py:4435} INFO - dagrun id: movie_pipeline
[2024-12-17T20:37:46.323+0530] {logging_mixin.py:190} INFO - [2024-12-17T20:37:46.322+0530] {dag.py:4451} INFO - created dagrun <DagRun movie_pipeline @ 2024-12-17 15:07:46.183005+00:00: manual__2024-12-17T15:07:46.183005+00:00, state:running, queued_at: None. externally triggered: False>
[2024-12-17T20:37:46.353+0530] {logging_mixin.py:190} INFO - [2024-12-17T20:37:46.352+0530] {dag.py:4396} INFO - [DAG TEST] starting task_id=scrape_collection_page map_index=-1
[2024-12-17T20:37:46.353+0530] {logging_mixin.py:190} INFO - [2024-12-17T20:37:46.353+0530] {dag.py:4399} INFO - [DAG TEST] running task <TaskInstance: movie_pipeline.scrape_collection_page manual__2024-12-17T15:07:46.183005+00:00 [scheduled]>
[2024-12-17T20:37:51.446+0530] {logging_mixin.py:190} INFO - [2024-12-17T20:37:51.446+0530] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-12-17T20:37:51.509+0530] {logging_mixin.py:190} INFO - [2024-12-17 20:37:51,508] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T15:07:46.183005+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T15:07:46.183005+00:00'
[2024-12-17T20:37:51.509+0530] {logging_mixin.py:190} INFO - [2024-12-17T20:37:51.508+0530] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T15:07:46.183005+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T15:07:46.183005+00:00'
[2024-12-17T20:37:51.521+0530] {logging_mixin.py:190} INFO - Task instance is in running state
[2024-12-17T20:37:51.522+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: queued
[2024-12-17T20:37:51.522+0530] {logging_mixin.py:190} INFO - Current task name:scrape_collection_page state:scheduled start_date:None
[2024-12-17T20:37:51.522+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline and current dag run status:running
[2024-12-17T20:37:51.523+0530] {logging_mixin.py:190} INFO - [2024-12-17T20:37:51.523+0530] {taskinstance.py:731} INFO - ::endgroup::
[2024-12-17T20:37:51.525+0530] {logging_mixin.py:190} INFO - 2024-12-17
[2024-12-17T20:37:51.525+0530] {logging_mixin.py:190} INFO - [2024-12-17 20:37:51,525] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T20:37:51.526+0530] {logging_mixin.py:190} INFO - [2024-12-17T20:37:51.525+0530] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T20:37:51.531+0530] {logging_mixin.py:190} INFO - [2024-12-17T20:37:51.531+0530] {taskinstance.py:340} INFO - ::group::Post task execution logs
[2024-12-17T20:37:51.532+0530] {logging_mixin.py:190} INFO - [2024-12-17T20:37:51.532+0530] {taskinstance.py:352} INFO - Marking task as SUCCESS. dag_id=movie_pipeline, task_id=scrape_collection_page, run_id=manual__2024-12-17T15:07:46.183005+00:00, execution_date=20241217T150746, start_date=, end_date=20241217T150751
[2024-12-17T20:37:51.541+0530] {logging_mixin.py:190} INFO - Task instance in success state
[2024-12-17T20:37:51.542+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: running
[2024-12-17T20:37:51.542+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline queued_at:None
[2024-12-17T20:37:51.542+0530] {logging_mixin.py:190} INFO - Task hostname:Jayeshs-MacBook-Air.local operator:PythonOperator
[2024-12-17T20:37:51.542+0530] {logging_mixin.py:190} INFO - [2024-12-17T20:37:51.542+0530] {dag.py:4410} INFO - [DAG TEST] end task task_id=scrape_collection_page map_index=-1
[2024-12-17T20:37:51.546+0530] {logging_mixin.py:190} INFO - [2024-12-17T20:37:51.546+0530] {dagrun.py:854} INFO - Marking run <DagRun movie_pipeline @ 2024-12-17 15:07:46.183005+00:00: manual__2024-12-17T15:07:46.183005+00:00, state:running, queued_at: None. externally triggered: False> successful
[2024-12-17T20:37:51.546+0530] {logging_mixin.py:190} INFO - Dag run in success state
[2024-12-17T20:37:51.547+0530] {logging_mixin.py:190} INFO - Dag run start:2024-12-17 15:07:46.183005+00:00 end:2024-12-17 15:07:51.546765+00:00
[2024-12-17T20:37:51.547+0530] {logging_mixin.py:190} INFO - [2024-12-17T20:37:51.547+0530] {dagrun.py:905} INFO - DagRun Finished: dag_id=movie_pipeline, execution_date=2024-12-17 15:07:46.183005+00:00, run_id=manual__2024-12-17T15:07:46.183005+00:00, run_start_date=2024-12-17 15:07:46.183005+00:00, run_end_date=2024-12-17 15:07:51.546765+00:00, run_duration=5.36376, state=success, external_trigger=False, run_type=manual, data_interval_start=2024-12-16 15:07:46.183005+00:00, data_interval_end=2024-12-17 15:07:46.183005+00:00, dag_hash=None
[2024-12-17T20:37:51.556+0530] {processor.py:925} INFO - DAG(s) 'movie_pipeline' retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T20:37:51.568+0530] {logging_mixin.py:190} INFO - [2024-12-17T20:37:51.567+0530] {dag.py:3239} INFO - Sync 1 DAGs
[2024-12-17T20:37:51.584+0530] {logging_mixin.py:190} INFO - [2024-12-17T20:37:51.584+0530] {dag.py:4180} INFO - Setting next_dagrun for movie_pipeline to 2024-12-17 01:30:00+00:00, run_after=2024-12-18 01:30:00+00:00
[2024-12-17T20:37:51.605+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 5.469 seconds
[2024-12-17T20:40:33.715+0530] {processor.py:186} INFO - Started process (PID=92112) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T20:40:33.716+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-17T20:40:33.717+0530] {logging_mixin.py:190} INFO - [2024-12-17T20:40:33.717+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T20:40:33.879+0530] {logging_mixin.py:190} INFO - [2024-12-17T20:40:33.878+0530] {dag.py:4435} INFO - dagrun id: movie_pipeline
[2024-12-17T20:40:33.892+0530] {logging_mixin.py:190} INFO - [2024-12-17T20:40:33.892+0530] {dag.py:4451} INFO - created dagrun <DagRun movie_pipeline @ 2024-12-17 15:10:33.756214+00:00: manual__2024-12-17T15:10:33.756214+00:00, state:running, queued_at: None. externally triggered: False>
[2024-12-17T20:40:33.918+0530] {logging_mixin.py:190} INFO - [2024-12-17T20:40:33.917+0530] {dag.py:4396} INFO - [DAG TEST] starting task_id=scrape_collection_page map_index=-1
[2024-12-17T20:40:33.918+0530] {logging_mixin.py:190} INFO - [2024-12-17T20:40:33.918+0530] {dag.py:4399} INFO - [DAG TEST] running task <TaskInstance: movie_pipeline.scrape_collection_page manual__2024-12-17T15:10:33.756214+00:00 [scheduled]>
[2024-12-17T20:40:39.011+0530] {logging_mixin.py:190} INFO - [2024-12-17T20:40:39.011+0530] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-12-17T20:40:39.069+0530] {logging_mixin.py:190} INFO - [2024-12-17 20:40:39,068] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T15:10:33.756214+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T15:10:33.756214+00:00'
[2024-12-17T20:40:39.069+0530] {logging_mixin.py:190} INFO - [2024-12-17T20:40:39.068+0530] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T15:10:33.756214+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T15:10:33.756214+00:00'
[2024-12-17T20:40:39.072+0530] {logging_mixin.py:190} INFO - Task instance is in running state
[2024-12-17T20:40:39.072+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: queued
[2024-12-17T20:40:39.073+0530] {logging_mixin.py:190} INFO - Current task name:scrape_collection_page state:scheduled start_date:None
[2024-12-17T20:40:39.073+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline and current dag run status:running
[2024-12-17T20:40:39.073+0530] {logging_mixin.py:190} INFO - [2024-12-17T20:40:39.073+0530] {taskinstance.py:731} INFO - ::endgroup::
[2024-12-17T20:40:39.075+0530] {logging_mixin.py:190} INFO - 2024-12-17
[2024-12-17T20:40:39.075+0530] {logging_mixin.py:190} INFO - [2024-12-17 20:40:39,075] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T20:40:39.076+0530] {logging_mixin.py:190} INFO - [2024-12-17T20:40:39.075+0530] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T20:40:39.080+0530] {logging_mixin.py:190} INFO - [2024-12-17T20:40:39.080+0530] {taskinstance.py:340} INFO - ::group::Post task execution logs
[2024-12-17T20:40:39.081+0530] {logging_mixin.py:190} INFO - [2024-12-17T20:40:39.081+0530] {taskinstance.py:352} INFO - Marking task as SUCCESS. dag_id=movie_pipeline, task_id=scrape_collection_page, run_id=manual__2024-12-17T15:10:33.756214+00:00, execution_date=20241217T151033, start_date=, end_date=20241217T151039
[2024-12-17T20:40:39.090+0530] {logging_mixin.py:190} INFO - Task instance in success state
[2024-12-17T20:40:39.091+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: running
[2024-12-17T20:40:39.091+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline queued_at:None
[2024-12-17T20:40:39.091+0530] {logging_mixin.py:190} INFO - Task hostname:Jayeshs-MacBook-Air.local operator:PythonOperator
[2024-12-17T20:40:39.091+0530] {logging_mixin.py:190} INFO - [2024-12-17T20:40:39.091+0530] {dag.py:4410} INFO - [DAG TEST] end task task_id=scrape_collection_page map_index=-1
[2024-12-17T20:40:39.095+0530] {logging_mixin.py:190} INFO - [2024-12-17T20:40:39.095+0530] {dagrun.py:854} INFO - Marking run <DagRun movie_pipeline @ 2024-12-17 15:10:33.756214+00:00: manual__2024-12-17T15:10:33.756214+00:00, state:running, queued_at: None. externally triggered: False> successful
[2024-12-17T20:40:39.095+0530] {logging_mixin.py:190} INFO - Dag run in success state
[2024-12-17T20:40:39.096+0530] {logging_mixin.py:190} INFO - Dag run start:2024-12-17 15:10:33.756214+00:00 end:2024-12-17 15:10:39.095728+00:00
[2024-12-17T20:40:39.096+0530] {logging_mixin.py:190} INFO - [2024-12-17T20:40:39.096+0530] {dagrun.py:905} INFO - DagRun Finished: dag_id=movie_pipeline, execution_date=2024-12-17 15:10:33.756214+00:00, run_id=manual__2024-12-17T15:10:33.756214+00:00, run_start_date=2024-12-17 15:10:33.756214+00:00, run_end_date=2024-12-17 15:10:39.095728+00:00, run_duration=5.339514, state=success, external_trigger=False, run_type=manual, data_interval_start=2024-12-16 15:10:33.756214+00:00, data_interval_end=2024-12-17 15:10:33.756214+00:00, dag_hash=None
[2024-12-17T20:40:39.106+0530] {processor.py:925} INFO - DAG(s) 'movie_pipeline' retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T20:40:39.118+0530] {logging_mixin.py:190} INFO - [2024-12-17T20:40:39.118+0530] {dag.py:3239} INFO - Sync 1 DAGs
[2024-12-17T20:40:39.136+0530] {logging_mixin.py:190} INFO - [2024-12-17T20:40:39.135+0530] {dag.py:4180} INFO - Setting next_dagrun for movie_pipeline to 2024-12-17 01:30:00+00:00, run_after=2024-12-18 01:30:00+00:00
[2024-12-17T20:40:39.153+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 5.442 seconds
[2024-12-17T20:43:15.939+0530] {processor.py:186} INFO - Started process (PID=92300) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T20:43:15.940+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-17T20:43:15.941+0530] {logging_mixin.py:190} INFO - [2024-12-17T20:43:15.941+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T20:43:16.102+0530] {logging_mixin.py:190} INFO - [2024-12-17T20:43:16.102+0530] {dag.py:4435} INFO - dagrun id: movie_pipeline
[2024-12-17T20:43:16.117+0530] {logging_mixin.py:190} INFO - [2024-12-17T20:43:16.117+0530] {dag.py:4451} INFO - created dagrun <DagRun movie_pipeline @ 2024-12-17 15:13:15.979670+00:00: manual__2024-12-17T15:13:15.979670+00:00, state:running, queued_at: None. externally triggered: False>
[2024-12-17T20:43:16.142+0530] {logging_mixin.py:190} INFO - [2024-12-17T20:43:16.142+0530] {dag.py:4396} INFO - [DAG TEST] starting task_id=scrape_collection_page map_index=-1
[2024-12-17T20:43:16.143+0530] {logging_mixin.py:190} INFO - [2024-12-17T20:43:16.142+0530] {dag.py:4399} INFO - [DAG TEST] running task <TaskInstance: movie_pipeline.scrape_collection_page manual__2024-12-17T15:13:15.979670+00:00 [scheduled]>
[2024-12-17T20:43:21.240+0530] {logging_mixin.py:190} INFO - [2024-12-17T20:43:21.240+0530] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-12-17T20:43:21.299+0530] {logging_mixin.py:190} INFO - [2024-12-17 20:43:21,299] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T15:13:15.979670+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T15:13:15.979670+00:00'
[2024-12-17T20:43:21.300+0530] {logging_mixin.py:190} INFO - [2024-12-17T20:43:21.299+0530] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T15:13:15.979670+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T15:13:15.979670+00:00'
[2024-12-17T20:43:21.302+0530] {logging_mixin.py:190} INFO - Task instance is in running state
[2024-12-17T20:43:21.302+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: queued
[2024-12-17T20:43:21.303+0530] {logging_mixin.py:190} INFO - Current task name:scrape_collection_page state:scheduled start_date:None
[2024-12-17T20:43:21.303+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline and current dag run status:running
[2024-12-17T20:43:21.304+0530] {logging_mixin.py:190} INFO - [2024-12-17T20:43:21.303+0530] {taskinstance.py:731} INFO - ::endgroup::
[2024-12-17T20:43:21.306+0530] {logging_mixin.py:190} INFO - 2024-12-17
[2024-12-17T20:43:21.306+0530] {logging_mixin.py:190} INFO - [2024-12-17 20:43:21,306] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T20:43:21.306+0530] {logging_mixin.py:190} INFO - [2024-12-17T20:43:21.306+0530] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T20:43:21.312+0530] {logging_mixin.py:190} INFO - [2024-12-17T20:43:21.311+0530] {taskinstance.py:340} INFO - ::group::Post task execution logs
[2024-12-17T20:43:21.312+0530] {logging_mixin.py:190} INFO - [2024-12-17T20:43:21.312+0530] {taskinstance.py:352} INFO - Marking task as SUCCESS. dag_id=movie_pipeline, task_id=scrape_collection_page, run_id=manual__2024-12-17T15:13:15.979670+00:00, execution_date=20241217T151315, start_date=, end_date=20241217T151321
[2024-12-17T20:43:21.321+0530] {logging_mixin.py:190} INFO - Task instance in success state
[2024-12-17T20:43:21.321+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: running
[2024-12-17T20:43:21.322+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline queued_at:None
[2024-12-17T20:43:21.322+0530] {logging_mixin.py:190} INFO - Task hostname:Jayeshs-MacBook-Air.local operator:PythonOperator
[2024-12-17T20:43:21.322+0530] {logging_mixin.py:190} INFO - [2024-12-17T20:43:21.322+0530] {dag.py:4410} INFO - [DAG TEST] end task task_id=scrape_collection_page map_index=-1
[2024-12-17T20:43:21.326+0530] {logging_mixin.py:190} INFO - [2024-12-17T20:43:21.326+0530] {dagrun.py:854} INFO - Marking run <DagRun movie_pipeline @ 2024-12-17 15:13:15.979670+00:00: manual__2024-12-17T15:13:15.979670+00:00, state:running, queued_at: None. externally triggered: False> successful
[2024-12-17T20:43:21.326+0530] {logging_mixin.py:190} INFO - Dag run in success state
[2024-12-17T20:43:21.326+0530] {logging_mixin.py:190} INFO - Dag run start:2024-12-17 15:13:15.979670+00:00 end:2024-12-17 15:13:21.326528+00:00
[2024-12-17T20:43:21.327+0530] {logging_mixin.py:190} INFO - [2024-12-17T20:43:21.327+0530] {dagrun.py:905} INFO - DagRun Finished: dag_id=movie_pipeline, execution_date=2024-12-17 15:13:15.979670+00:00, run_id=manual__2024-12-17T15:13:15.979670+00:00, run_start_date=2024-12-17 15:13:15.979670+00:00, run_end_date=2024-12-17 15:13:21.326528+00:00, run_duration=5.346858, state=success, external_trigger=False, run_type=manual, data_interval_start=2024-12-16 15:13:15.979670+00:00, data_interval_end=2024-12-17 15:13:15.979670+00:00, dag_hash=None
[2024-12-17T20:43:21.335+0530] {processor.py:925} INFO - DAG(s) 'movie_pipeline' retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T20:43:21.346+0530] {logging_mixin.py:190} INFO - [2024-12-17T20:43:21.346+0530] {dag.py:3239} INFO - Sync 1 DAGs
[2024-12-17T20:43:21.364+0530] {logging_mixin.py:190} INFO - [2024-12-17T20:43:21.364+0530] {dag.py:4180} INFO - Setting next_dagrun for movie_pipeline to 2024-12-17 01:30:00+00:00, run_after=2024-12-18 01:30:00+00:00
[2024-12-17T20:43:21.383+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 5.447 seconds
[2024-12-17T20:46:04.121+0530] {processor.py:186} INFO - Started process (PID=92500) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T20:46:04.122+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-17T20:46:04.124+0530] {logging_mixin.py:190} INFO - [2024-12-17T20:46:04.124+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T20:46:04.289+0530] {logging_mixin.py:190} INFO - [2024-12-17T20:46:04.289+0530] {dag.py:4435} INFO - dagrun id: movie_pipeline
[2024-12-17T20:46:04.303+0530] {logging_mixin.py:190} INFO - [2024-12-17T20:46:04.303+0530] {dag.py:4451} INFO - created dagrun <DagRun movie_pipeline @ 2024-12-17 15:16:04.164869+00:00: manual__2024-12-17T15:16:04.164869+00:00, state:running, queued_at: None. externally triggered: False>
[2024-12-17T20:46:04.328+0530] {logging_mixin.py:190} INFO - [2024-12-17T20:46:04.328+0530] {dag.py:4396} INFO - [DAG TEST] starting task_id=scrape_collection_page map_index=-1
[2024-12-17T20:46:04.329+0530] {logging_mixin.py:190} INFO - [2024-12-17T20:46:04.328+0530] {dag.py:4399} INFO - [DAG TEST] running task <TaskInstance: movie_pipeline.scrape_collection_page manual__2024-12-17T15:16:04.164869+00:00 [scheduled]>
[2024-12-17T20:46:09.424+0530] {logging_mixin.py:190} INFO - [2024-12-17T20:46:09.423+0530] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-12-17T20:46:09.484+0530] {logging_mixin.py:190} INFO - [2024-12-17 20:46:09,484] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T15:16:04.164869+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T15:16:04.164869+00:00'
[2024-12-17T20:46:09.484+0530] {logging_mixin.py:190} INFO - [2024-12-17T20:46:09.484+0530] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T15:16:04.164869+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T15:16:04.164869+00:00'
[2024-12-17T20:46:09.487+0530] {logging_mixin.py:190} INFO - Task instance is in running state
[2024-12-17T20:46:09.487+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: queued
[2024-12-17T20:46:09.488+0530] {logging_mixin.py:190} INFO - Current task name:scrape_collection_page state:scheduled start_date:None
[2024-12-17T20:46:09.488+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline and current dag run status:running
[2024-12-17T20:46:09.488+0530] {logging_mixin.py:190} INFO - [2024-12-17T20:46:09.488+0530] {taskinstance.py:731} INFO - ::endgroup::
[2024-12-17T20:46:09.490+0530] {logging_mixin.py:190} INFO - 2024-12-17
[2024-12-17T20:46:09.490+0530] {logging_mixin.py:190} INFO - [2024-12-17 20:46:09,490] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T20:46:09.491+0530] {logging_mixin.py:190} INFO - [2024-12-17T20:46:09.490+0530] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T20:46:09.495+0530] {logging_mixin.py:190} INFO - [2024-12-17T20:46:09.495+0530] {taskinstance.py:340} INFO - ::group::Post task execution logs
[2024-12-17T20:46:09.496+0530] {logging_mixin.py:190} INFO - [2024-12-17T20:46:09.496+0530] {taskinstance.py:352} INFO - Marking task as SUCCESS. dag_id=movie_pipeline, task_id=scrape_collection_page, run_id=manual__2024-12-17T15:16:04.164869+00:00, execution_date=20241217T151604, start_date=, end_date=20241217T151609
[2024-12-17T20:46:09.505+0530] {logging_mixin.py:190} INFO - Task instance in success state
[2024-12-17T20:46:09.505+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: running
[2024-12-17T20:46:09.506+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline queued_at:None
[2024-12-17T20:46:09.506+0530] {logging_mixin.py:190} INFO - Task hostname:Jayeshs-MacBook-Air.local operator:PythonOperator
[2024-12-17T20:46:09.506+0530] {logging_mixin.py:190} INFO - [2024-12-17T20:46:09.506+0530] {dag.py:4410} INFO - [DAG TEST] end task task_id=scrape_collection_page map_index=-1
[2024-12-17T20:46:09.510+0530] {logging_mixin.py:190} INFO - [2024-12-17T20:46:09.510+0530] {dagrun.py:854} INFO - Marking run <DagRun movie_pipeline @ 2024-12-17 15:16:04.164869+00:00: manual__2024-12-17T15:16:04.164869+00:00, state:running, queued_at: None. externally triggered: False> successful
[2024-12-17T20:46:09.510+0530] {logging_mixin.py:190} INFO - Dag run in success state
[2024-12-17T20:46:09.511+0530] {logging_mixin.py:190} INFO - Dag run start:2024-12-17 15:16:04.164869+00:00 end:2024-12-17 15:16:09.510801+00:00
[2024-12-17T20:46:09.511+0530] {logging_mixin.py:190} INFO - [2024-12-17T20:46:09.511+0530] {dagrun.py:905} INFO - DagRun Finished: dag_id=movie_pipeline, execution_date=2024-12-17 15:16:04.164869+00:00, run_id=manual__2024-12-17T15:16:04.164869+00:00, run_start_date=2024-12-17 15:16:04.164869+00:00, run_end_date=2024-12-17 15:16:09.510801+00:00, run_duration=5.345932, state=success, external_trigger=False, run_type=manual, data_interval_start=2024-12-16 15:16:04.164869+00:00, data_interval_end=2024-12-17 15:16:04.164869+00:00, dag_hash=None
[2024-12-17T20:46:09.520+0530] {processor.py:925} INFO - DAG(s) 'movie_pipeline' retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T20:46:09.532+0530] {logging_mixin.py:190} INFO - [2024-12-17T20:46:09.531+0530] {dag.py:3239} INFO - Sync 1 DAGs
[2024-12-17T20:46:09.549+0530] {logging_mixin.py:190} INFO - [2024-12-17T20:46:09.548+0530] {dag.py:4180} INFO - Setting next_dagrun for movie_pipeline to 2024-12-17 01:30:00+00:00, run_after=2024-12-18 01:30:00+00:00
[2024-12-17T20:46:09.568+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 5.451 seconds
[2024-12-17T20:48:47.678+0530] {processor.py:186} INFO - Started process (PID=92699) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T20:48:47.679+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-17T20:48:47.680+0530] {logging_mixin.py:190} INFO - [2024-12-17T20:48:47.680+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T20:48:47.842+0530] {logging_mixin.py:190} INFO - [2024-12-17T20:48:47.842+0530] {dag.py:4435} INFO - dagrun id: movie_pipeline
[2024-12-17T20:48:47.856+0530] {logging_mixin.py:190} INFO - [2024-12-17T20:48:47.855+0530] {dag.py:4451} INFO - created dagrun <DagRun movie_pipeline @ 2024-12-17 15:18:47.719154+00:00: manual__2024-12-17T15:18:47.719154+00:00, state:running, queued_at: None. externally triggered: False>
[2024-12-17T20:48:47.881+0530] {logging_mixin.py:190} INFO - [2024-12-17T20:48:47.881+0530] {dag.py:4396} INFO - [DAG TEST] starting task_id=scrape_collection_page map_index=-1
[2024-12-17T20:48:47.882+0530] {logging_mixin.py:190} INFO - [2024-12-17T20:48:47.881+0530] {dag.py:4399} INFO - [DAG TEST] running task <TaskInstance: movie_pipeline.scrape_collection_page manual__2024-12-17T15:18:47.719154+00:00 [scheduled]>
[2024-12-17T20:48:52.976+0530] {logging_mixin.py:190} INFO - [2024-12-17T20:48:52.976+0530] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-12-17T20:48:53.034+0530] {logging_mixin.py:190} INFO - [2024-12-17 20:48:53,034] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T15:18:47.719154+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T15:18:47.719154+00:00'
[2024-12-17T20:48:53.035+0530] {logging_mixin.py:190} INFO - [2024-12-17T20:48:53.034+0530] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T15:18:47.719154+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T15:18:47.719154+00:00'
[2024-12-17T20:48:53.038+0530] {logging_mixin.py:190} INFO - Task instance is in running state
[2024-12-17T20:48:53.038+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: queued
[2024-12-17T20:48:53.039+0530] {logging_mixin.py:190} INFO - Current task name:scrape_collection_page state:scheduled start_date:None
[2024-12-17T20:48:53.039+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline and current dag run status:running
[2024-12-17T20:48:53.040+0530] {logging_mixin.py:190} INFO - [2024-12-17T20:48:53.039+0530] {taskinstance.py:731} INFO - ::endgroup::
[2024-12-17T20:48:53.042+0530] {logging_mixin.py:190} INFO - 2024-12-17
[2024-12-17T20:48:53.042+0530] {logging_mixin.py:190} INFO - [2024-12-17 20:48:53,042] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T20:48:53.043+0530] {logging_mixin.py:190} INFO - [2024-12-17T20:48:53.042+0530] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T20:48:53.048+0530] {logging_mixin.py:190} INFO - [2024-12-17T20:48:53.047+0530] {taskinstance.py:340} INFO - ::group::Post task execution logs
[2024-12-17T20:48:53.048+0530] {logging_mixin.py:190} INFO - [2024-12-17T20:48:53.048+0530] {taskinstance.py:352} INFO - Marking task as SUCCESS. dag_id=movie_pipeline, task_id=scrape_collection_page, run_id=manual__2024-12-17T15:18:47.719154+00:00, execution_date=20241217T151847, start_date=, end_date=20241217T151853
[2024-12-17T20:48:53.057+0530] {logging_mixin.py:190} INFO - Task instance in success state
[2024-12-17T20:48:53.058+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: running
[2024-12-17T20:48:53.058+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline queued_at:None
[2024-12-17T20:48:53.058+0530] {logging_mixin.py:190} INFO - Task hostname:Jayeshs-MacBook-Air.local operator:PythonOperator
[2024-12-17T20:48:53.059+0530] {logging_mixin.py:190} INFO - [2024-12-17T20:48:53.059+0530] {dag.py:4410} INFO - [DAG TEST] end task task_id=scrape_collection_page map_index=-1
[2024-12-17T20:48:53.063+0530] {logging_mixin.py:190} INFO - [2024-12-17T20:48:53.063+0530] {dagrun.py:854} INFO - Marking run <DagRun movie_pipeline @ 2024-12-17 15:18:47.719154+00:00: manual__2024-12-17T15:18:47.719154+00:00, state:running, queued_at: None. externally triggered: False> successful
[2024-12-17T20:48:53.063+0530] {logging_mixin.py:190} INFO - Dag run in success state
[2024-12-17T20:48:53.064+0530] {logging_mixin.py:190} INFO - Dag run start:2024-12-17 15:18:47.719154+00:00 end:2024-12-17 15:18:53.063878+00:00
[2024-12-17T20:48:53.064+0530] {logging_mixin.py:190} INFO - [2024-12-17T20:48:53.064+0530] {dagrun.py:905} INFO - DagRun Finished: dag_id=movie_pipeline, execution_date=2024-12-17 15:18:47.719154+00:00, run_id=manual__2024-12-17T15:18:47.719154+00:00, run_start_date=2024-12-17 15:18:47.719154+00:00, run_end_date=2024-12-17 15:18:53.063878+00:00, run_duration=5.344724, state=success, external_trigger=False, run_type=manual, data_interval_start=2024-12-16 15:18:47.719154+00:00, data_interval_end=2024-12-17 15:18:47.719154+00:00, dag_hash=None
[2024-12-17T20:48:53.073+0530] {processor.py:925} INFO - DAG(s) 'movie_pipeline' retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T20:48:53.085+0530] {logging_mixin.py:190} INFO - [2024-12-17T20:48:53.085+0530] {dag.py:3239} INFO - Sync 1 DAGs
[2024-12-17T20:48:53.102+0530] {logging_mixin.py:190} INFO - [2024-12-17T20:48:53.102+0530] {dag.py:4180} INFO - Setting next_dagrun for movie_pipeline to 2024-12-17 01:30:00+00:00, run_after=2024-12-18 01:30:00+00:00
[2024-12-17T20:48:53.122+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 5.447 seconds
[2024-12-17T20:51:30.793+0530] {processor.py:186} INFO - Started process (PID=92892) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T20:51:30.794+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-17T20:51:30.795+0530] {logging_mixin.py:190} INFO - [2024-12-17T20:51:30.795+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T20:51:30.956+0530] {logging_mixin.py:190} INFO - [2024-12-17T20:51:30.956+0530] {dag.py:4435} INFO - dagrun id: movie_pipeline
[2024-12-17T20:51:30.969+0530] {logging_mixin.py:190} INFO - [2024-12-17T20:51:30.969+0530] {dag.py:4451} INFO - created dagrun <DagRun movie_pipeline @ 2024-12-17 15:21:30.833880+00:00: manual__2024-12-17T15:21:30.833880+00:00, state:running, queued_at: None. externally triggered: False>
[2024-12-17T20:51:30.995+0530] {logging_mixin.py:190} INFO - [2024-12-17T20:51:30.994+0530] {dag.py:4396} INFO - [DAG TEST] starting task_id=scrape_collection_page map_index=-1
[2024-12-17T20:51:30.995+0530] {logging_mixin.py:190} INFO - [2024-12-17T20:51:30.995+0530] {dag.py:4399} INFO - [DAG TEST] running task <TaskInstance: movie_pipeline.scrape_collection_page manual__2024-12-17T15:21:30.833880+00:00 [scheduled]>
[2024-12-17T20:51:36.090+0530] {logging_mixin.py:190} INFO - [2024-12-17T20:51:36.089+0530] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-12-17T20:51:36.148+0530] {logging_mixin.py:190} INFO - [2024-12-17 20:51:36,148] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T15:21:30.833880+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T15:21:30.833880+00:00'
[2024-12-17T20:51:36.149+0530] {logging_mixin.py:190} INFO - [2024-12-17T20:51:36.148+0530] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T15:21:30.833880+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T15:21:30.833880+00:00'
[2024-12-17T20:51:36.151+0530] {logging_mixin.py:190} INFO - Task instance is in running state
[2024-12-17T20:51:36.152+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: queued
[2024-12-17T20:51:36.152+0530] {logging_mixin.py:190} INFO - Current task name:scrape_collection_page state:scheduled start_date:None
[2024-12-17T20:51:36.152+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline and current dag run status:running
[2024-12-17T20:51:36.153+0530] {logging_mixin.py:190} INFO - [2024-12-17T20:51:36.153+0530] {taskinstance.py:731} INFO - ::endgroup::
[2024-12-17T20:51:36.155+0530] {logging_mixin.py:190} INFO - 2024-12-17
[2024-12-17T20:51:36.155+0530] {logging_mixin.py:190} INFO - [2024-12-17 20:51:36,155] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T20:51:36.156+0530] {logging_mixin.py:190} INFO - [2024-12-17T20:51:36.155+0530] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T20:51:36.161+0530] {logging_mixin.py:190} INFO - [2024-12-17T20:51:36.160+0530] {taskinstance.py:340} INFO - ::group::Post task execution logs
[2024-12-17T20:51:36.161+0530] {logging_mixin.py:190} INFO - [2024-12-17T20:51:36.161+0530] {taskinstance.py:352} INFO - Marking task as SUCCESS. dag_id=movie_pipeline, task_id=scrape_collection_page, run_id=manual__2024-12-17T15:21:30.833880+00:00, execution_date=20241217T152130, start_date=, end_date=20241217T152136
[2024-12-17T20:51:36.170+0530] {logging_mixin.py:190} INFO - Task instance in success state
[2024-12-17T20:51:36.170+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: running
[2024-12-17T20:51:36.171+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline queued_at:None
[2024-12-17T20:51:36.171+0530] {logging_mixin.py:190} INFO - Task hostname:Jayeshs-MacBook-Air.local operator:PythonOperator
[2024-12-17T20:51:36.171+0530] {logging_mixin.py:190} INFO - [2024-12-17T20:51:36.171+0530] {dag.py:4410} INFO - [DAG TEST] end task task_id=scrape_collection_page map_index=-1
[2024-12-17T20:51:36.175+0530] {logging_mixin.py:190} INFO - [2024-12-17T20:51:36.175+0530] {dagrun.py:854} INFO - Marking run <DagRun movie_pipeline @ 2024-12-17 15:21:30.833880+00:00: manual__2024-12-17T15:21:30.833880+00:00, state:running, queued_at: None. externally triggered: False> successful
[2024-12-17T20:51:36.175+0530] {logging_mixin.py:190} INFO - Dag run in success state
[2024-12-17T20:51:36.176+0530] {logging_mixin.py:190} INFO - Dag run start:2024-12-17 15:21:30.833880+00:00 end:2024-12-17 15:21:36.175865+00:00
[2024-12-17T20:51:36.176+0530] {logging_mixin.py:190} INFO - [2024-12-17T20:51:36.176+0530] {dagrun.py:905} INFO - DagRun Finished: dag_id=movie_pipeline, execution_date=2024-12-17 15:21:30.833880+00:00, run_id=manual__2024-12-17T15:21:30.833880+00:00, run_start_date=2024-12-17 15:21:30.833880+00:00, run_end_date=2024-12-17 15:21:36.175865+00:00, run_duration=5.341985, state=success, external_trigger=False, run_type=manual, data_interval_start=2024-12-16 15:21:30.833880+00:00, data_interval_end=2024-12-17 15:21:30.833880+00:00, dag_hash=None
[2024-12-17T20:51:36.186+0530] {processor.py:925} INFO - DAG(s) 'movie_pipeline' retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T20:51:36.198+0530] {logging_mixin.py:190} INFO - [2024-12-17T20:51:36.198+0530] {dag.py:3239} INFO - Sync 1 DAGs
[2024-12-17T20:51:36.215+0530] {logging_mixin.py:190} INFO - [2024-12-17T20:51:36.214+0530] {dag.py:4180} INFO - Setting next_dagrun for movie_pipeline to 2024-12-17 01:30:00+00:00, run_after=2024-12-18 01:30:00+00:00
[2024-12-17T20:51:36.232+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 5.443 seconds
[2024-12-17T20:54:15.850+0530] {processor.py:186} INFO - Started process (PID=93102) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T20:54:15.852+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-17T20:54:15.854+0530] {logging_mixin.py:190} INFO - [2024-12-17T20:54:15.853+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T20:54:16.026+0530] {logging_mixin.py:190} INFO - [2024-12-17T20:54:16.026+0530] {dag.py:4435} INFO - dagrun id: movie_pipeline
[2024-12-17T20:54:16.041+0530] {logging_mixin.py:190} INFO - [2024-12-17T20:54:16.040+0530] {dag.py:4451} INFO - created dagrun <DagRun movie_pipeline @ 2024-12-17 15:24:15.896838+00:00: manual__2024-12-17T15:24:15.896838+00:00, state:running, queued_at: None. externally triggered: False>
[2024-12-17T20:54:16.075+0530] {logging_mixin.py:190} INFO - [2024-12-17T20:54:16.075+0530] {dag.py:4396} INFO - [DAG TEST] starting task_id=scrape_collection_page map_index=-1
[2024-12-17T20:54:16.076+0530] {logging_mixin.py:190} INFO - [2024-12-17T20:54:16.076+0530] {dag.py:4399} INFO - [DAG TEST] running task <TaskInstance: movie_pipeline.scrape_collection_page manual__2024-12-17T15:24:15.896838+00:00 [scheduled]>
[2024-12-17T20:54:21.181+0530] {logging_mixin.py:190} INFO - [2024-12-17T20:54:21.181+0530] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-12-17T20:54:21.255+0530] {logging_mixin.py:190} INFO - [2024-12-17 20:54:21,255] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T15:24:15.896838+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T15:24:15.896838+00:00'
[2024-12-17T20:54:21.256+0530] {logging_mixin.py:190} INFO - [2024-12-17T20:54:21.255+0530] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T15:24:15.896838+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T15:24:15.896838+00:00'
[2024-12-17T20:54:21.268+0530] {logging_mixin.py:190} INFO - Task instance is in running state
[2024-12-17T20:54:21.269+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: queued
[2024-12-17T20:54:21.269+0530] {logging_mixin.py:190} INFO - Current task name:scrape_collection_page state:scheduled start_date:None
[2024-12-17T20:54:21.269+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline and current dag run status:running
[2024-12-17T20:54:21.270+0530] {logging_mixin.py:190} INFO - [2024-12-17T20:54:21.270+0530] {taskinstance.py:731} INFO - ::endgroup::
[2024-12-17T20:54:21.273+0530] {logging_mixin.py:190} INFO - 2024-12-17
[2024-12-17T20:54:21.274+0530] {logging_mixin.py:190} INFO - [2024-12-17 20:54:21,274] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T20:54:21.274+0530] {logging_mixin.py:190} INFO - [2024-12-17T20:54:21.274+0530] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T20:54:21.280+0530] {logging_mixin.py:190} INFO - [2024-12-17T20:54:21.279+0530] {taskinstance.py:340} INFO - ::group::Post task execution logs
[2024-12-17T20:54:21.280+0530] {logging_mixin.py:190} INFO - [2024-12-17T20:54:21.280+0530] {taskinstance.py:352} INFO - Marking task as SUCCESS. dag_id=movie_pipeline, task_id=scrape_collection_page, run_id=manual__2024-12-17T15:24:15.896838+00:00, execution_date=20241217T152415, start_date=, end_date=20241217T152421
[2024-12-17T20:54:21.289+0530] {logging_mixin.py:190} INFO - Task instance in success state
[2024-12-17T20:54:21.289+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: running
[2024-12-17T20:54:21.290+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline queued_at:None
[2024-12-17T20:54:21.290+0530] {logging_mixin.py:190} INFO - Task hostname:Jayeshs-MacBook-Air.local operator:PythonOperator
[2024-12-17T20:54:21.290+0530] {logging_mixin.py:190} INFO - [2024-12-17T20:54:21.290+0530] {dag.py:4410} INFO - [DAG TEST] end task task_id=scrape_collection_page map_index=-1
[2024-12-17T20:54:21.294+0530] {logging_mixin.py:190} INFO - [2024-12-17T20:54:21.294+0530] {dagrun.py:854} INFO - Marking run <DagRun movie_pipeline @ 2024-12-17 15:24:15.896838+00:00: manual__2024-12-17T15:24:15.896838+00:00, state:running, queued_at: None. externally triggered: False> successful
[2024-12-17T20:54:21.294+0530] {logging_mixin.py:190} INFO - Dag run in success state
[2024-12-17T20:54:21.294+0530] {logging_mixin.py:190} INFO - Dag run start:2024-12-17 15:24:15.896838+00:00 end:2024-12-17 15:24:21.294497+00:00
[2024-12-17T20:54:21.295+0530] {logging_mixin.py:190} INFO - [2024-12-17T20:54:21.295+0530] {dagrun.py:905} INFO - DagRun Finished: dag_id=movie_pipeline, execution_date=2024-12-17 15:24:15.896838+00:00, run_id=manual__2024-12-17T15:24:15.896838+00:00, run_start_date=2024-12-17 15:24:15.896838+00:00, run_end_date=2024-12-17 15:24:21.294497+00:00, run_duration=5.397659, state=success, external_trigger=False, run_type=manual, data_interval_start=2024-12-16 15:24:15.896838+00:00, data_interval_end=2024-12-17 15:24:15.896838+00:00, dag_hash=None
[2024-12-17T20:54:21.304+0530] {processor.py:925} INFO - DAG(s) 'movie_pipeline' retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T20:54:21.316+0530] {logging_mixin.py:190} INFO - [2024-12-17T20:54:21.316+0530] {dag.py:3239} INFO - Sync 1 DAGs
[2024-12-17T20:54:21.335+0530] {logging_mixin.py:190} INFO - [2024-12-17T20:54:21.334+0530] {dag.py:4180} INFO - Setting next_dagrun for movie_pipeline to 2024-12-17 01:30:00+00:00, run_after=2024-12-18 01:30:00+00:00
[2024-12-17T20:54:21.354+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 5.507 seconds
[2024-12-17T20:57:37.341+0530] {processor.py:186} INFO - Started process (PID=93392) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T20:57:37.344+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-17T20:57:37.345+0530] {logging_mixin.py:190} INFO - [2024-12-17T20:57:37.345+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T20:57:37.600+0530] {logging_mixin.py:190} INFO - [2024-12-17T20:57:37.600+0530] {dag.py:4435} INFO - dagrun id: movie_pipeline
[2024-12-17T20:57:37.615+0530] {logging_mixin.py:190} INFO - [2024-12-17T20:57:37.614+0530] {dag.py:4451} INFO - created dagrun <DagRun movie_pipeline @ 2024-12-17 15:27:37.443061+00:00: manual__2024-12-17T15:27:37.443061+00:00, state:running, queued_at: None. externally triggered: False>
[2024-12-17T20:57:37.659+0530] {logging_mixin.py:190} INFO - [2024-12-17T20:57:37.659+0530] {dag.py:4396} INFO - [DAG TEST] starting task_id=scrape_collection_page map_index=-1
[2024-12-17T20:57:37.660+0530] {logging_mixin.py:190} INFO - [2024-12-17T20:57:37.660+0530] {dag.py:4399} INFO - [DAG TEST] running task <TaskInstance: movie_pipeline.scrape_collection_page manual__2024-12-17T15:27:37.443061+00:00 [scheduled]>
[2024-12-17T20:57:42.772+0530] {logging_mixin.py:190} INFO - [2024-12-17T20:57:42.771+0530] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-12-17T20:57:42.842+0530] {logging_mixin.py:190} INFO - [2024-12-17 20:57:42,841] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T15:27:37.443061+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T15:27:37.443061+00:00'
[2024-12-17T20:57:42.842+0530] {logging_mixin.py:190} INFO - [2024-12-17T20:57:42.841+0530] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T15:27:37.443061+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T15:27:37.443061+00:00'
[2024-12-17T20:57:42.856+0530] {logging_mixin.py:190} INFO - Task instance is in running state
[2024-12-17T20:57:42.857+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: queued
[2024-12-17T20:57:42.857+0530] {logging_mixin.py:190} INFO - Current task name:scrape_collection_page state:scheduled start_date:None
[2024-12-17T20:57:42.857+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline and current dag run status:running
[2024-12-17T20:57:42.858+0530] {logging_mixin.py:190} INFO - [2024-12-17T20:57:42.857+0530] {taskinstance.py:731} INFO - ::endgroup::
[2024-12-17T20:57:42.861+0530] {logging_mixin.py:190} INFO - 2024-12-17
[2024-12-17T20:57:42.862+0530] {logging_mixin.py:190} INFO - [2024-12-17 20:57:42,862] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T20:57:42.862+0530] {logging_mixin.py:190} INFO - [2024-12-17T20:57:42.862+0530] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T20:57:42.867+0530] {logging_mixin.py:190} INFO - [2024-12-17T20:57:42.867+0530] {taskinstance.py:340} INFO - ::group::Post task execution logs
[2024-12-17T20:57:42.868+0530] {logging_mixin.py:190} INFO - [2024-12-17T20:57:42.867+0530] {taskinstance.py:352} INFO - Marking task as SUCCESS. dag_id=movie_pipeline, task_id=scrape_collection_page, run_id=manual__2024-12-17T15:27:37.443061+00:00, execution_date=20241217T152737, start_date=, end_date=20241217T152742
[2024-12-17T20:57:42.877+0530] {logging_mixin.py:190} INFO - Task instance in success state
[2024-12-17T20:57:42.878+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: running
[2024-12-17T20:57:42.878+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline queued_at:None
[2024-12-17T20:57:42.878+0530] {logging_mixin.py:190} INFO - Task hostname:Jayeshs-MacBook-Air.local operator:PythonOperator
[2024-12-17T20:57:42.879+0530] {logging_mixin.py:190} INFO - [2024-12-17T20:57:42.878+0530] {dag.py:4410} INFO - [DAG TEST] end task task_id=scrape_collection_page map_index=-1
[2024-12-17T20:57:42.882+0530] {logging_mixin.py:190} INFO - [2024-12-17T20:57:42.882+0530] {dagrun.py:854} INFO - Marking run <DagRun movie_pipeline @ 2024-12-17 15:27:37.443061+00:00: manual__2024-12-17T15:27:37.443061+00:00, state:running, queued_at: None. externally triggered: False> successful
[2024-12-17T20:57:42.883+0530] {logging_mixin.py:190} INFO - Dag run in success state
[2024-12-17T20:57:42.883+0530] {logging_mixin.py:190} INFO - Dag run start:2024-12-17 15:27:37.443061+00:00 end:2024-12-17 15:27:42.883265+00:00
[2024-12-17T20:57:42.884+0530] {logging_mixin.py:190} INFO - [2024-12-17T20:57:42.883+0530] {dagrun.py:905} INFO - DagRun Finished: dag_id=movie_pipeline, execution_date=2024-12-17 15:27:37.443061+00:00, run_id=manual__2024-12-17T15:27:37.443061+00:00, run_start_date=2024-12-17 15:27:37.443061+00:00, run_end_date=2024-12-17 15:27:42.883265+00:00, run_duration=5.440204, state=success, external_trigger=False, run_type=manual, data_interval_start=2024-12-16 15:27:37.443061+00:00, data_interval_end=2024-12-17 15:27:37.443061+00:00, dag_hash=None
[2024-12-17T20:57:42.893+0530] {processor.py:925} INFO - DAG(s) 'movie_pipeline' retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T20:57:42.905+0530] {logging_mixin.py:190} INFO - [2024-12-17T20:57:42.905+0530] {dag.py:3239} INFO - Sync 1 DAGs
[2024-12-17T20:57:42.923+0530] {logging_mixin.py:190} INFO - [2024-12-17T20:57:42.922+0530] {dag.py:4180} INFO - Setting next_dagrun for movie_pipeline to 2024-12-17 01:30:00+00:00, run_after=2024-12-18 01:30:00+00:00
[2024-12-17T20:57:42.941+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 5.603 seconds
[2024-12-17T21:00:37.760+0530] {processor.py:186} INFO - Started process (PID=93611) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T21:00:37.762+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-17T21:00:37.764+0530] {logging_mixin.py:190} INFO - [2024-12-17T21:00:37.764+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T21:00:37.970+0530] {logging_mixin.py:190} INFO - [2024-12-17T21:00:37.970+0530] {dag.py:4435} INFO - dagrun id: movie_pipeline
[2024-12-17T21:00:37.984+0530] {logging_mixin.py:190} INFO - [2024-12-17T21:00:37.984+0530] {dag.py:4451} INFO - created dagrun <DagRun movie_pipeline @ 2024-12-17 15:30:37.839497+00:00: manual__2024-12-17T15:30:37.839497+00:00, state:running, queued_at: None. externally triggered: False>
[2024-12-17T21:00:38.023+0530] {logging_mixin.py:190} INFO - [2024-12-17T21:00:38.022+0530] {dag.py:4396} INFO - [DAG TEST] starting task_id=scrape_collection_page map_index=-1
[2024-12-17T21:00:38.024+0530] {logging_mixin.py:190} INFO - [2024-12-17T21:00:38.023+0530] {dag.py:4399} INFO - [DAG TEST] running task <TaskInstance: movie_pipeline.scrape_collection_page manual__2024-12-17T15:30:37.839497+00:00 [scheduled]>
[2024-12-17T21:00:43.134+0530] {logging_mixin.py:190} INFO - [2024-12-17T21:00:43.134+0530] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-12-17T21:00:43.211+0530] {logging_mixin.py:190} INFO - [2024-12-17 21:00:43,210] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T15:30:37.839497+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T15:30:37.839497+00:00'
[2024-12-17T21:00:43.211+0530] {logging_mixin.py:190} INFO - [2024-12-17T21:00:43.210+0530] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T15:30:37.839497+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T15:30:37.839497+00:00'
[2024-12-17T21:00:43.224+0530] {logging_mixin.py:190} INFO - Task instance is in running state
[2024-12-17T21:00:43.225+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: queued
[2024-12-17T21:00:43.225+0530] {logging_mixin.py:190} INFO - Current task name:scrape_collection_page state:scheduled start_date:None
[2024-12-17T21:00:43.225+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline and current dag run status:running
[2024-12-17T21:00:43.226+0530] {logging_mixin.py:190} INFO - [2024-12-17T21:00:43.226+0530] {taskinstance.py:731} INFO - ::endgroup::
[2024-12-17T21:00:43.230+0530] {logging_mixin.py:190} INFO - 2024-12-17
[2024-12-17T21:00:43.231+0530] {logging_mixin.py:190} INFO - [2024-12-17 21:00:43,231] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T21:00:43.232+0530] {logging_mixin.py:190} INFO - [2024-12-17T21:00:43.231+0530] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T21:00:43.239+0530] {logging_mixin.py:190} INFO - [2024-12-17T21:00:43.238+0530] {taskinstance.py:340} INFO - ::group::Post task execution logs
[2024-12-17T21:00:43.239+0530] {logging_mixin.py:190} INFO - [2024-12-17T21:00:43.239+0530] {taskinstance.py:352} INFO - Marking task as SUCCESS. dag_id=movie_pipeline, task_id=scrape_collection_page, run_id=manual__2024-12-17T15:30:37.839497+00:00, execution_date=20241217T153037, start_date=, end_date=20241217T153043
[2024-12-17T21:00:43.249+0530] {logging_mixin.py:190} INFO - Task instance in success state
[2024-12-17T21:00:43.249+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: running
[2024-12-17T21:00:43.249+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline queued_at:None
[2024-12-17T21:00:43.249+0530] {logging_mixin.py:190} INFO - Task hostname:Jayeshs-MacBook-Air.local operator:PythonOperator
[2024-12-17T21:00:43.250+0530] {logging_mixin.py:190} INFO - [2024-12-17T21:00:43.250+0530] {dag.py:4410} INFO - [DAG TEST] end task task_id=scrape_collection_page map_index=-1
[2024-12-17T21:00:43.254+0530] {logging_mixin.py:190} INFO - [2024-12-17T21:00:43.254+0530] {dagrun.py:854} INFO - Marking run <DagRun movie_pipeline @ 2024-12-17 15:30:37.839497+00:00: manual__2024-12-17T15:30:37.839497+00:00, state:running, queued_at: None. externally triggered: False> successful
[2024-12-17T21:00:43.254+0530] {logging_mixin.py:190} INFO - Dag run in success state
[2024-12-17T21:00:43.254+0530] {logging_mixin.py:190} INFO - Dag run start:2024-12-17 15:30:37.839497+00:00 end:2024-12-17 15:30:43.254631+00:00
[2024-12-17T21:00:43.255+0530] {logging_mixin.py:190} INFO - [2024-12-17T21:00:43.255+0530] {dagrun.py:905} INFO - DagRun Finished: dag_id=movie_pipeline, execution_date=2024-12-17 15:30:37.839497+00:00, run_id=manual__2024-12-17T15:30:37.839497+00:00, run_start_date=2024-12-17 15:30:37.839497+00:00, run_end_date=2024-12-17 15:30:43.254631+00:00, run_duration=5.415134, state=success, external_trigger=False, run_type=manual, data_interval_start=2024-12-16 15:30:37.839497+00:00, data_interval_end=2024-12-17 15:30:37.839497+00:00, dag_hash=None
[2024-12-17T21:00:43.263+0530] {processor.py:925} INFO - DAG(s) 'movie_pipeline' retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T21:00:43.276+0530] {logging_mixin.py:190} INFO - [2024-12-17T21:00:43.275+0530] {dag.py:3239} INFO - Sync 1 DAGs
[2024-12-17T21:00:43.293+0530] {logging_mixin.py:190} INFO - [2024-12-17T21:00:43.293+0530] {dag.py:4180} INFO - Setting next_dagrun for movie_pipeline to 2024-12-17 01:30:00+00:00, run_after=2024-12-18 01:30:00+00:00
[2024-12-17T21:00:43.311+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 5.557 seconds
[2024-12-17T21:03:36.280+0530] {processor.py:186} INFO - Started process (PID=93811) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T21:03:36.281+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-17T21:03:36.282+0530] {logging_mixin.py:190} INFO - [2024-12-17T21:03:36.282+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T21:03:36.448+0530] {logging_mixin.py:190} INFO - [2024-12-17T21:03:36.448+0530] {dag.py:4435} INFO - dagrun id: movie_pipeline
[2024-12-17T21:03:36.462+0530] {logging_mixin.py:190} INFO - [2024-12-17T21:03:36.461+0530] {dag.py:4451} INFO - created dagrun <DagRun movie_pipeline @ 2024-12-17 15:33:36.321742+00:00: manual__2024-12-17T15:33:36.321742+00:00, state:running, queued_at: None. externally triggered: False>
[2024-12-17T21:03:36.488+0530] {logging_mixin.py:190} INFO - [2024-12-17T21:03:36.487+0530] {dag.py:4396} INFO - [DAG TEST] starting task_id=scrape_collection_page map_index=-1
[2024-12-17T21:03:36.489+0530] {logging_mixin.py:190} INFO - [2024-12-17T21:03:36.488+0530] {dag.py:4399} INFO - [DAG TEST] running task <TaskInstance: movie_pipeline.scrape_collection_page manual__2024-12-17T15:33:36.321742+00:00 [scheduled]>
[2024-12-17T21:03:41.592+0530] {logging_mixin.py:190} INFO - [2024-12-17T21:03:41.592+0530] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-12-17T21:03:41.653+0530] {logging_mixin.py:190} INFO - [2024-12-17 21:03:41,653] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T15:33:36.321742+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T15:33:36.321742+00:00'
[2024-12-17T21:03:41.654+0530] {logging_mixin.py:190} INFO - [2024-12-17T21:03:41.653+0530] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T15:33:36.321742+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T15:33:36.321742+00:00'
[2024-12-17T21:03:41.657+0530] {logging_mixin.py:190} INFO - Task instance is in running state
[2024-12-17T21:03:41.658+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: queued
[2024-12-17T21:03:41.658+0530] {logging_mixin.py:190} INFO - Current task name:scrape_collection_page state:scheduled start_date:None
[2024-12-17T21:03:41.658+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline and current dag run status:running
[2024-12-17T21:03:41.658+0530] {logging_mixin.py:190} INFO - [2024-12-17T21:03:41.658+0530] {taskinstance.py:731} INFO - ::endgroup::
[2024-12-17T21:03:41.660+0530] {logging_mixin.py:190} INFO - 2024-12-17
[2024-12-17T21:03:41.661+0530] {logging_mixin.py:190} INFO - [2024-12-17 21:03:41,660] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T21:03:41.661+0530] {logging_mixin.py:190} INFO - [2024-12-17T21:03:41.660+0530] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T21:03:41.666+0530] {logging_mixin.py:190} INFO - [2024-12-17T21:03:41.666+0530] {taskinstance.py:340} INFO - ::group::Post task execution logs
[2024-12-17T21:03:41.667+0530] {logging_mixin.py:190} INFO - [2024-12-17T21:03:41.667+0530] {taskinstance.py:352} INFO - Marking task as SUCCESS. dag_id=movie_pipeline, task_id=scrape_collection_page, run_id=manual__2024-12-17T15:33:36.321742+00:00, execution_date=20241217T153336, start_date=, end_date=20241217T153341
[2024-12-17T21:03:41.676+0530] {logging_mixin.py:190} INFO - Task instance in success state
[2024-12-17T21:03:41.677+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: running
[2024-12-17T21:03:41.677+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline queued_at:None
[2024-12-17T21:03:41.677+0530] {logging_mixin.py:190} INFO - Task hostname:Jayeshs-MacBook-Air.local operator:PythonOperator
[2024-12-17T21:03:41.678+0530] {logging_mixin.py:190} INFO - [2024-12-17T21:03:41.678+0530] {dag.py:4410} INFO - [DAG TEST] end task task_id=scrape_collection_page map_index=-1
[2024-12-17T21:03:41.684+0530] {logging_mixin.py:190} INFO - [2024-12-17T21:03:41.684+0530] {dagrun.py:854} INFO - Marking run <DagRun movie_pipeline @ 2024-12-17 15:33:36.321742+00:00: manual__2024-12-17T15:33:36.321742+00:00, state:running, queued_at: None. externally triggered: False> successful
[2024-12-17T21:03:41.684+0530] {logging_mixin.py:190} INFO - Dag run in success state
[2024-12-17T21:03:41.685+0530] {logging_mixin.py:190} INFO - Dag run start:2024-12-17 15:33:36.321742+00:00 end:2024-12-17 15:33:41.684739+00:00
[2024-12-17T21:03:41.685+0530] {logging_mixin.py:190} INFO - [2024-12-17T21:03:41.685+0530] {dagrun.py:905} INFO - DagRun Finished: dag_id=movie_pipeline, execution_date=2024-12-17 15:33:36.321742+00:00, run_id=manual__2024-12-17T15:33:36.321742+00:00, run_start_date=2024-12-17 15:33:36.321742+00:00, run_end_date=2024-12-17 15:33:41.684739+00:00, run_duration=5.362997, state=success, external_trigger=False, run_type=manual, data_interval_start=2024-12-16 15:33:36.321742+00:00, data_interval_end=2024-12-17 15:33:36.321742+00:00, dag_hash=None
[2024-12-17T21:03:41.694+0530] {processor.py:925} INFO - DAG(s) 'movie_pipeline' retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T21:03:41.706+0530] {logging_mixin.py:190} INFO - [2024-12-17T21:03:41.705+0530] {dag.py:3239} INFO - Sync 1 DAGs
[2024-12-17T21:03:41.724+0530] {logging_mixin.py:190} INFO - [2024-12-17T21:03:41.724+0530] {dag.py:4180} INFO - Setting next_dagrun for movie_pipeline to 2024-12-17 01:30:00+00:00, run_after=2024-12-18 01:30:00+00:00
[2024-12-17T21:03:41.743+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 5.467 seconds
[2024-12-17T21:06:31.079+0530] {processor.py:186} INFO - Started process (PID=94013) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T21:06:31.080+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-17T21:06:31.081+0530] {logging_mixin.py:190} INFO - [2024-12-17T21:06:31.081+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T21:06:31.248+0530] {logging_mixin.py:190} INFO - [2024-12-17T21:06:31.248+0530] {dag.py:4435} INFO - dagrun id: movie_pipeline
[2024-12-17T21:06:31.263+0530] {logging_mixin.py:190} INFO - [2024-12-17T21:06:31.262+0530] {dag.py:4451} INFO - created dagrun <DagRun movie_pipeline @ 2024-12-17 15:36:31.120946+00:00: manual__2024-12-17T15:36:31.120946+00:00, state:running, queued_at: None. externally triggered: False>
[2024-12-17T21:06:31.287+0530] {logging_mixin.py:190} INFO - [2024-12-17T21:06:31.287+0530] {dag.py:4396} INFO - [DAG TEST] starting task_id=scrape_collection_page map_index=-1
[2024-12-17T21:06:31.288+0530] {logging_mixin.py:190} INFO - [2024-12-17T21:06:31.288+0530] {dag.py:4399} INFO - [DAG TEST] running task <TaskInstance: movie_pipeline.scrape_collection_page manual__2024-12-17T15:36:31.120946+00:00 [scheduled]>
[2024-12-17T21:06:36.388+0530] {logging_mixin.py:190} INFO - [2024-12-17T21:06:36.388+0530] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-12-17T21:06:36.448+0530] {logging_mixin.py:190} INFO - [2024-12-17 21:06:36,448] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T15:36:31.120946+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T15:36:31.120946+00:00'
[2024-12-17T21:06:36.449+0530] {logging_mixin.py:190} INFO - [2024-12-17T21:06:36.448+0530] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T15:36:31.120946+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T15:36:31.120946+00:00'
[2024-12-17T21:06:36.451+0530] {logging_mixin.py:190} INFO - Task instance is in running state
[2024-12-17T21:06:36.452+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: queued
[2024-12-17T21:06:36.452+0530] {logging_mixin.py:190} INFO - Current task name:scrape_collection_page state:scheduled start_date:None
[2024-12-17T21:06:36.452+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline and current dag run status:running
[2024-12-17T21:06:36.453+0530] {logging_mixin.py:190} INFO - [2024-12-17T21:06:36.453+0530] {taskinstance.py:731} INFO - ::endgroup::
[2024-12-17T21:06:36.454+0530] {logging_mixin.py:190} INFO - 2024-12-17
[2024-12-17T21:06:36.455+0530] {logging_mixin.py:190} INFO - [2024-12-17 21:06:36,455] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T21:06:36.455+0530] {logging_mixin.py:190} INFO - [2024-12-17T21:06:36.455+0530] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T21:06:36.461+0530] {logging_mixin.py:190} INFO - [2024-12-17T21:06:36.460+0530] {taskinstance.py:340} INFO - ::group::Post task execution logs
[2024-12-17T21:06:36.461+0530] {logging_mixin.py:190} INFO - [2024-12-17T21:06:36.461+0530] {taskinstance.py:352} INFO - Marking task as SUCCESS. dag_id=movie_pipeline, task_id=scrape_collection_page, run_id=manual__2024-12-17T15:36:31.120946+00:00, execution_date=20241217T153631, start_date=, end_date=20241217T153636
[2024-12-17T21:06:36.471+0530] {logging_mixin.py:190} INFO - Task instance in success state
[2024-12-17T21:06:36.471+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: running
[2024-12-17T21:06:36.471+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline queued_at:None
[2024-12-17T21:06:36.471+0530] {logging_mixin.py:190} INFO - Task hostname:Jayeshs-MacBook-Air.local operator:PythonOperator
[2024-12-17T21:06:36.472+0530] {logging_mixin.py:190} INFO - [2024-12-17T21:06:36.472+0530] {dag.py:4410} INFO - [DAG TEST] end task task_id=scrape_collection_page map_index=-1
[2024-12-17T21:06:36.476+0530] {logging_mixin.py:190} INFO - [2024-12-17T21:06:36.475+0530] {dagrun.py:854} INFO - Marking run <DagRun movie_pipeline @ 2024-12-17 15:36:31.120946+00:00: manual__2024-12-17T15:36:31.120946+00:00, state:running, queued_at: None. externally triggered: False> successful
[2024-12-17T21:06:36.476+0530] {logging_mixin.py:190} INFO - Dag run in success state
[2024-12-17T21:06:36.476+0530] {logging_mixin.py:190} INFO - Dag run start:2024-12-17 15:36:31.120946+00:00 end:2024-12-17 15:36:36.476388+00:00
[2024-12-17T21:06:36.477+0530] {logging_mixin.py:190} INFO - [2024-12-17T21:06:36.476+0530] {dagrun.py:905} INFO - DagRun Finished: dag_id=movie_pipeline, execution_date=2024-12-17 15:36:31.120946+00:00, run_id=manual__2024-12-17T15:36:31.120946+00:00, run_start_date=2024-12-17 15:36:31.120946+00:00, run_end_date=2024-12-17 15:36:36.476388+00:00, run_duration=5.355442, state=success, external_trigger=False, run_type=manual, data_interval_start=2024-12-16 15:36:31.120946+00:00, data_interval_end=2024-12-17 15:36:31.120946+00:00, dag_hash=None
[2024-12-17T21:06:36.488+0530] {processor.py:925} INFO - DAG(s) 'movie_pipeline' retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T21:06:36.501+0530] {logging_mixin.py:190} INFO - [2024-12-17T21:06:36.500+0530] {dag.py:3239} INFO - Sync 1 DAGs
[2024-12-17T21:06:36.519+0530] {logging_mixin.py:190} INFO - [2024-12-17T21:06:36.518+0530] {dag.py:4180} INFO - Setting next_dagrun for movie_pipeline to 2024-12-17 01:30:00+00:00, run_after=2024-12-18 01:30:00+00:00
[2024-12-17T21:06:36.541+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 5.466 seconds
[2024-12-17T21:09:25.294+0530] {processor.py:186} INFO - Started process (PID=94208) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T21:09:25.295+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-17T21:09:25.296+0530] {logging_mixin.py:190} INFO - [2024-12-17T21:09:25.296+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T21:09:25.463+0530] {logging_mixin.py:190} INFO - [2024-12-17T21:09:25.463+0530] {dag.py:4435} INFO - dagrun id: movie_pipeline
[2024-12-17T21:09:25.478+0530] {logging_mixin.py:190} INFO - [2024-12-17T21:09:25.477+0530] {dag.py:4451} INFO - created dagrun <DagRun movie_pipeline @ 2024-12-17 15:39:25.335603+00:00: manual__2024-12-17T15:39:25.335603+00:00, state:running, queued_at: None. externally triggered: False>
[2024-12-17T21:09:25.503+0530] {logging_mixin.py:190} INFO - [2024-12-17T21:09:25.503+0530] {dag.py:4396} INFO - [DAG TEST] starting task_id=scrape_collection_page map_index=-1
[2024-12-17T21:09:25.504+0530] {logging_mixin.py:190} INFO - [2024-12-17T21:09:25.504+0530] {dag.py:4399} INFO - [DAG TEST] running task <TaskInstance: movie_pipeline.scrape_collection_page manual__2024-12-17T15:39:25.335603+00:00 [scheduled]>
[2024-12-17T21:09:30.605+0530] {logging_mixin.py:190} INFO - [2024-12-17T21:09:30.605+0530] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-12-17T21:09:30.665+0530] {logging_mixin.py:190} INFO - [2024-12-17 21:09:30,665] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T15:39:25.335603+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T15:39:25.335603+00:00'
[2024-12-17T21:09:30.666+0530] {logging_mixin.py:190} INFO - [2024-12-17T21:09:30.665+0530] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T15:39:25.335603+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T15:39:25.335603+00:00'
[2024-12-17T21:09:30.669+0530] {logging_mixin.py:190} INFO - Task instance is in running state
[2024-12-17T21:09:30.669+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: queued
[2024-12-17T21:09:30.669+0530] {logging_mixin.py:190} INFO - Current task name:scrape_collection_page state:scheduled start_date:None
[2024-12-17T21:09:30.669+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline and current dag run status:running
[2024-12-17T21:09:30.670+0530] {logging_mixin.py:190} INFO - [2024-12-17T21:09:30.670+0530] {taskinstance.py:731} INFO - ::endgroup::
[2024-12-17T21:09:30.672+0530] {logging_mixin.py:190} INFO - 2024-12-17
[2024-12-17T21:09:30.672+0530] {logging_mixin.py:190} INFO - [2024-12-17 21:09:30,672] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T21:09:30.673+0530] {logging_mixin.py:190} INFO - [2024-12-17T21:09:30.672+0530] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T21:09:30.677+0530] {logging_mixin.py:190} INFO - [2024-12-17T21:09:30.677+0530] {taskinstance.py:340} INFO - ::group::Post task execution logs
[2024-12-17T21:09:30.678+0530] {logging_mixin.py:190} INFO - [2024-12-17T21:09:30.678+0530] {taskinstance.py:352} INFO - Marking task as SUCCESS. dag_id=movie_pipeline, task_id=scrape_collection_page, run_id=manual__2024-12-17T15:39:25.335603+00:00, execution_date=20241217T153925, start_date=, end_date=20241217T153930
[2024-12-17T21:09:30.688+0530] {logging_mixin.py:190} INFO - Task instance in success state
[2024-12-17T21:09:30.688+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: running
[2024-12-17T21:09:30.688+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline queued_at:None
[2024-12-17T21:09:30.688+0530] {logging_mixin.py:190} INFO - Task hostname:Jayeshs-MacBook-Air.local operator:PythonOperator
[2024-12-17T21:09:30.689+0530] {logging_mixin.py:190} INFO - [2024-12-17T21:09:30.689+0530] {dag.py:4410} INFO - [DAG TEST] end task task_id=scrape_collection_page map_index=-1
[2024-12-17T21:09:30.692+0530] {logging_mixin.py:190} INFO - [2024-12-17T21:09:30.692+0530] {dagrun.py:854} INFO - Marking run <DagRun movie_pipeline @ 2024-12-17 15:39:25.335603+00:00: manual__2024-12-17T15:39:25.335603+00:00, state:running, queued_at: None. externally triggered: False> successful
[2024-12-17T21:09:30.693+0530] {logging_mixin.py:190} INFO - Dag run in success state
[2024-12-17T21:09:30.693+0530] {logging_mixin.py:190} INFO - Dag run start:2024-12-17 15:39:25.335603+00:00 end:2024-12-17 15:39:30.693171+00:00
[2024-12-17T21:09:30.693+0530] {logging_mixin.py:190} INFO - [2024-12-17T21:09:30.693+0530] {dagrun.py:905} INFO - DagRun Finished: dag_id=movie_pipeline, execution_date=2024-12-17 15:39:25.335603+00:00, run_id=manual__2024-12-17T15:39:25.335603+00:00, run_start_date=2024-12-17 15:39:25.335603+00:00, run_end_date=2024-12-17 15:39:30.693171+00:00, run_duration=5.357568, state=success, external_trigger=False, run_type=manual, data_interval_start=2024-12-16 15:39:25.335603+00:00, data_interval_end=2024-12-17 15:39:25.335603+00:00, dag_hash=None
[2024-12-17T21:09:30.703+0530] {processor.py:925} INFO - DAG(s) 'movie_pipeline' retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T21:09:30.715+0530] {logging_mixin.py:190} INFO - [2024-12-17T21:09:30.714+0530] {dag.py:3239} INFO - Sync 1 DAGs
[2024-12-17T21:09:30.732+0530] {logging_mixin.py:190} INFO - [2024-12-17T21:09:30.732+0530] {dag.py:4180} INFO - Setting next_dagrun for movie_pipeline to 2024-12-17 01:30:00+00:00, run_after=2024-12-18 01:30:00+00:00
[2024-12-17T21:09:30.751+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 5.460 seconds
[2024-12-17T21:12:21.333+0530] {processor.py:186} INFO - Started process (PID=94418) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T21:12:21.336+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-17T21:12:21.338+0530] {logging_mixin.py:190} INFO - [2024-12-17T21:12:21.337+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T21:12:21.505+0530] {logging_mixin.py:190} INFO - [2024-12-17T21:12:21.504+0530] {dag.py:4435} INFO - dagrun id: movie_pipeline
[2024-12-17T21:12:21.519+0530] {logging_mixin.py:190} INFO - [2024-12-17T21:12:21.518+0530] {dag.py:4451} INFO - created dagrun <DagRun movie_pipeline @ 2024-12-17 15:42:21.378854+00:00: manual__2024-12-17T15:42:21.378854+00:00, state:running, queued_at: None. externally triggered: False>
[2024-12-17T21:12:21.551+0530] {logging_mixin.py:190} INFO - [2024-12-17T21:12:21.551+0530] {dag.py:4396} INFO - [DAG TEST] starting task_id=scrape_collection_page map_index=-1
[2024-12-17T21:12:21.552+0530] {logging_mixin.py:190} INFO - [2024-12-17T21:12:21.551+0530] {dag.py:4399} INFO - [DAG TEST] running task <TaskInstance: movie_pipeline.scrape_collection_page manual__2024-12-17T15:42:21.378854+00:00 [scheduled]>
[2024-12-17T21:12:26.650+0530] {logging_mixin.py:190} INFO - [2024-12-17T21:12:26.650+0530] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-12-17T21:12:26.714+0530] {logging_mixin.py:190} INFO - [2024-12-17 21:12:26,714] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T15:42:21.378854+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T15:42:21.378854+00:00'
[2024-12-17T21:12:26.715+0530] {logging_mixin.py:190} INFO - [2024-12-17T21:12:26.714+0530] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T15:42:21.378854+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T15:42:21.378854+00:00'
[2024-12-17T21:12:26.727+0530] {logging_mixin.py:190} INFO - Task instance is in running state
[2024-12-17T21:12:26.728+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: queued
[2024-12-17T21:12:26.728+0530] {logging_mixin.py:190} INFO - Current task name:scrape_collection_page state:scheduled start_date:None
[2024-12-17T21:12:26.729+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline and current dag run status:running
[2024-12-17T21:12:26.729+0530] {logging_mixin.py:190} INFO - [2024-12-17T21:12:26.729+0530] {taskinstance.py:731} INFO - ::endgroup::
[2024-12-17T21:12:26.731+0530] {logging_mixin.py:190} INFO - 2024-12-17
[2024-12-17T21:12:26.732+0530] {logging_mixin.py:190} INFO - [2024-12-17 21:12:26,732] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T21:12:26.732+0530] {logging_mixin.py:190} INFO - [2024-12-17T21:12:26.732+0530] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T21:12:26.738+0530] {logging_mixin.py:190} INFO - [2024-12-17T21:12:26.737+0530] {taskinstance.py:340} INFO - ::group::Post task execution logs
[2024-12-17T21:12:26.738+0530] {logging_mixin.py:190} INFO - [2024-12-17T21:12:26.738+0530] {taskinstance.py:352} INFO - Marking task as SUCCESS. dag_id=movie_pipeline, task_id=scrape_collection_page, run_id=manual__2024-12-17T15:42:21.378854+00:00, execution_date=20241217T154221, start_date=, end_date=20241217T154226
[2024-12-17T21:12:26.748+0530] {logging_mixin.py:190} INFO - Task instance in success state
[2024-12-17T21:12:26.748+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: running
[2024-12-17T21:12:26.749+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline queued_at:None
[2024-12-17T21:12:26.749+0530] {logging_mixin.py:190} INFO - Task hostname:Jayeshs-MacBook-Air.local operator:PythonOperator
[2024-12-17T21:12:26.749+0530] {logging_mixin.py:190} INFO - [2024-12-17T21:12:26.749+0530] {dag.py:4410} INFO - [DAG TEST] end task task_id=scrape_collection_page map_index=-1
[2024-12-17T21:12:26.753+0530] {logging_mixin.py:190} INFO - [2024-12-17T21:12:26.753+0530] {dagrun.py:854} INFO - Marking run <DagRun movie_pipeline @ 2024-12-17 15:42:21.378854+00:00: manual__2024-12-17T15:42:21.378854+00:00, state:running, queued_at: None. externally triggered: False> successful
[2024-12-17T21:12:26.754+0530] {logging_mixin.py:190} INFO - Dag run in success state
[2024-12-17T21:12:26.754+0530] {logging_mixin.py:190} INFO - Dag run start:2024-12-17 15:42:21.378854+00:00 end:2024-12-17 15:42:26.754176+00:00
[2024-12-17T21:12:26.755+0530] {logging_mixin.py:190} INFO - [2024-12-17T21:12:26.755+0530] {dagrun.py:905} INFO - DagRun Finished: dag_id=movie_pipeline, execution_date=2024-12-17 15:42:21.378854+00:00, run_id=manual__2024-12-17T15:42:21.378854+00:00, run_start_date=2024-12-17 15:42:21.378854+00:00, run_end_date=2024-12-17 15:42:26.754176+00:00, run_duration=5.375322, state=success, external_trigger=False, run_type=manual, data_interval_start=2024-12-16 15:42:21.378854+00:00, data_interval_end=2024-12-17 15:42:21.378854+00:00, dag_hash=None
[2024-12-17T21:12:26.764+0530] {processor.py:925} INFO - DAG(s) 'movie_pipeline' retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T21:12:26.776+0530] {logging_mixin.py:190} INFO - [2024-12-17T21:12:26.775+0530] {dag.py:3239} INFO - Sync 1 DAGs
[2024-12-17T21:12:26.793+0530] {logging_mixin.py:190} INFO - [2024-12-17T21:12:26.793+0530] {dag.py:4180} INFO - Setting next_dagrun for movie_pipeline to 2024-12-17 01:30:00+00:00, run_after=2024-12-18 01:30:00+00:00
[2024-12-17T21:12:26.812+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 5.482 seconds
[2024-12-17T21:15:14.213+0530] {processor.py:186} INFO - Started process (PID=94611) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T21:15:14.214+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-17T21:15:14.215+0530] {logging_mixin.py:190} INFO - [2024-12-17T21:15:14.215+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T21:15:14.383+0530] {logging_mixin.py:190} INFO - [2024-12-17T21:15:14.383+0530] {dag.py:4435} INFO - dagrun id: movie_pipeline
[2024-12-17T21:15:14.398+0530] {logging_mixin.py:190} INFO - [2024-12-17T21:15:14.397+0530] {dag.py:4451} INFO - created dagrun <DagRun movie_pipeline @ 2024-12-17 15:45:14.254274+00:00: manual__2024-12-17T15:45:14.254274+00:00, state:running, queued_at: None. externally triggered: False>
[2024-12-17T21:15:14.426+0530] {logging_mixin.py:190} INFO - [2024-12-17T21:15:14.426+0530] {dag.py:4396} INFO - [DAG TEST] starting task_id=scrape_collection_page map_index=-1
[2024-12-17T21:15:14.427+0530] {logging_mixin.py:190} INFO - [2024-12-17T21:15:14.426+0530] {dag.py:4399} INFO - [DAG TEST] running task <TaskInstance: movie_pipeline.scrape_collection_page manual__2024-12-17T15:45:14.254274+00:00 [scheduled]>
[2024-12-17T21:15:19.528+0530] {logging_mixin.py:190} INFO - [2024-12-17T21:15:19.528+0530] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-12-17T21:15:19.598+0530] {logging_mixin.py:190} INFO - [2024-12-17 21:15:19,597] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T15:45:14.254274+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T15:45:14.254274+00:00'
[2024-12-17T21:15:19.598+0530] {logging_mixin.py:190} INFO - [2024-12-17T21:15:19.597+0530] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T15:45:14.254274+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T15:45:14.254274+00:00'
[2024-12-17T21:15:19.601+0530] {logging_mixin.py:190} INFO - Task instance is in running state
[2024-12-17T21:15:19.602+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: queued
[2024-12-17T21:15:19.602+0530] {logging_mixin.py:190} INFO - Current task name:scrape_collection_page state:scheduled start_date:None
[2024-12-17T21:15:19.602+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline and current dag run status:running
[2024-12-17T21:15:19.602+0530] {logging_mixin.py:190} INFO - [2024-12-17T21:15:19.602+0530] {taskinstance.py:731} INFO - ::endgroup::
[2024-12-17T21:15:19.604+0530] {logging_mixin.py:190} INFO - 2024-12-17
[2024-12-17T21:15:19.605+0530] {logging_mixin.py:190} INFO - [2024-12-17 21:15:19,604] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T21:15:19.605+0530] {logging_mixin.py:190} INFO - [2024-12-17T21:15:19.604+0530] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T21:15:19.610+0530] {logging_mixin.py:190} INFO - [2024-12-17T21:15:19.610+0530] {taskinstance.py:340} INFO - ::group::Post task execution logs
[2024-12-17T21:15:19.611+0530] {logging_mixin.py:190} INFO - [2024-12-17T21:15:19.611+0530] {taskinstance.py:352} INFO - Marking task as SUCCESS. dag_id=movie_pipeline, task_id=scrape_collection_page, run_id=manual__2024-12-17T15:45:14.254274+00:00, execution_date=20241217T154514, start_date=, end_date=20241217T154519
[2024-12-17T21:15:19.621+0530] {logging_mixin.py:190} INFO - Task instance in success state
[2024-12-17T21:15:19.621+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: running
[2024-12-17T21:15:19.621+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline queued_at:None
[2024-12-17T21:15:19.622+0530] {logging_mixin.py:190} INFO - Task hostname:Jayeshs-MacBook-Air.local operator:PythonOperator
[2024-12-17T21:15:19.622+0530] {logging_mixin.py:190} INFO - [2024-12-17T21:15:19.622+0530] {dag.py:4410} INFO - [DAG TEST] end task task_id=scrape_collection_page map_index=-1
[2024-12-17T21:15:19.627+0530] {logging_mixin.py:190} INFO - [2024-12-17T21:15:19.626+0530] {dagrun.py:854} INFO - Marking run <DagRun movie_pipeline @ 2024-12-17 15:45:14.254274+00:00: manual__2024-12-17T15:45:14.254274+00:00, state:running, queued_at: None. externally triggered: False> successful
[2024-12-17T21:15:19.627+0530] {logging_mixin.py:190} INFO - Dag run in success state
[2024-12-17T21:15:19.627+0530] {logging_mixin.py:190} INFO - Dag run start:2024-12-17 15:45:14.254274+00:00 end:2024-12-17 15:45:19.627239+00:00
[2024-12-17T21:15:19.628+0530] {logging_mixin.py:190} INFO - [2024-12-17T21:15:19.627+0530] {dagrun.py:905} INFO - DagRun Finished: dag_id=movie_pipeline, execution_date=2024-12-17 15:45:14.254274+00:00, run_id=manual__2024-12-17T15:45:14.254274+00:00, run_start_date=2024-12-17 15:45:14.254274+00:00, run_end_date=2024-12-17 15:45:19.627239+00:00, run_duration=5.372965, state=success, external_trigger=False, run_type=manual, data_interval_start=2024-12-16 15:45:14.254274+00:00, data_interval_end=2024-12-17 15:45:14.254274+00:00, dag_hash=None
[2024-12-17T21:15:19.636+0530] {processor.py:925} INFO - DAG(s) 'movie_pipeline' retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T21:15:19.648+0530] {logging_mixin.py:190} INFO - [2024-12-17T21:15:19.648+0530] {dag.py:3239} INFO - Sync 1 DAGs
[2024-12-17T21:15:19.666+0530] {logging_mixin.py:190} INFO - [2024-12-17T21:15:19.666+0530] {dag.py:4180} INFO - Setting next_dagrun for movie_pipeline to 2024-12-17 01:30:00+00:00, run_after=2024-12-18 01:30:00+00:00
[2024-12-17T21:15:19.684+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 5.475 seconds
[2024-12-17T21:18:08.427+0530] {processor.py:186} INFO - Started process (PID=94818) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T21:18:08.429+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-17T21:18:08.430+0530] {logging_mixin.py:190} INFO - [2024-12-17T21:18:08.430+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T21:18:08.606+0530] {logging_mixin.py:190} INFO - [2024-12-17T21:18:08.606+0530] {dag.py:4435} INFO - dagrun id: movie_pipeline
[2024-12-17T21:18:08.620+0530] {logging_mixin.py:190} INFO - [2024-12-17T21:18:08.620+0530] {dag.py:4451} INFO - created dagrun <DagRun movie_pipeline @ 2024-12-17 15:48:08.471283+00:00: manual__2024-12-17T15:48:08.471283+00:00, state:running, queued_at: None. externally triggered: False>
[2024-12-17T21:18:08.646+0530] {logging_mixin.py:190} INFO - [2024-12-17T21:18:08.645+0530] {dag.py:4396} INFO - [DAG TEST] starting task_id=scrape_collection_page map_index=-1
[2024-12-17T21:18:08.646+0530] {logging_mixin.py:190} INFO - [2024-12-17T21:18:08.646+0530] {dag.py:4399} INFO - [DAG TEST] running task <TaskInstance: movie_pipeline.scrape_collection_page manual__2024-12-17T15:48:08.471283+00:00 [scheduled]>
[2024-12-17T21:18:13.744+0530] {logging_mixin.py:190} INFO - [2024-12-17T21:18:13.744+0530] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-12-17T21:18:13.804+0530] {logging_mixin.py:190} INFO - [2024-12-17 21:18:13,804] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T15:48:08.471283+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T15:48:08.471283+00:00'
[2024-12-17T21:18:13.805+0530] {logging_mixin.py:190} INFO - [2024-12-17T21:18:13.804+0530] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T15:48:08.471283+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T15:48:08.471283+00:00'
[2024-12-17T21:18:13.807+0530] {logging_mixin.py:190} INFO - Task instance is in running state
[2024-12-17T21:18:13.808+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: queued
[2024-12-17T21:18:13.808+0530] {logging_mixin.py:190} INFO - Current task name:scrape_collection_page state:scheduled start_date:None
[2024-12-17T21:18:13.808+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline and current dag run status:running
[2024-12-17T21:18:13.809+0530] {logging_mixin.py:190} INFO - [2024-12-17T21:18:13.809+0530] {taskinstance.py:731} INFO - ::endgroup::
[2024-12-17T21:18:13.810+0530] {logging_mixin.py:190} INFO - 2024-12-17
[2024-12-17T21:18:13.811+0530] {logging_mixin.py:190} INFO - [2024-12-17 21:18:13,811] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T21:18:13.811+0530] {logging_mixin.py:190} INFO - [2024-12-17T21:18:13.811+0530] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T21:18:13.817+0530] {logging_mixin.py:190} INFO - [2024-12-17T21:18:13.816+0530] {taskinstance.py:340} INFO - ::group::Post task execution logs
[2024-12-17T21:18:13.817+0530] {logging_mixin.py:190} INFO - [2024-12-17T21:18:13.817+0530] {taskinstance.py:352} INFO - Marking task as SUCCESS. dag_id=movie_pipeline, task_id=scrape_collection_page, run_id=manual__2024-12-17T15:48:08.471283+00:00, execution_date=20241217T154808, start_date=, end_date=20241217T154813
[2024-12-17T21:18:13.826+0530] {logging_mixin.py:190} INFO - Task instance in success state
[2024-12-17T21:18:13.827+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: running
[2024-12-17T21:18:13.827+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline queued_at:None
[2024-12-17T21:18:13.827+0530] {logging_mixin.py:190} INFO - Task hostname:Jayeshs-MacBook-Air.local operator:PythonOperator
[2024-12-17T21:18:13.827+0530] {logging_mixin.py:190} INFO - [2024-12-17T21:18:13.827+0530] {dag.py:4410} INFO - [DAG TEST] end task task_id=scrape_collection_page map_index=-1
[2024-12-17T21:18:13.832+0530] {logging_mixin.py:190} INFO - [2024-12-17T21:18:13.831+0530] {dagrun.py:854} INFO - Marking run <DagRun movie_pipeline @ 2024-12-17 15:48:08.471283+00:00: manual__2024-12-17T15:48:08.471283+00:00, state:running, queued_at: None. externally triggered: False> successful
[2024-12-17T21:18:13.832+0530] {logging_mixin.py:190} INFO - Dag run in success state
[2024-12-17T21:18:13.832+0530] {logging_mixin.py:190} INFO - Dag run start:2024-12-17 15:48:08.471283+00:00 end:2024-12-17 15:48:13.832515+00:00
[2024-12-17T21:18:13.833+0530] {logging_mixin.py:190} INFO - [2024-12-17T21:18:13.833+0530] {dagrun.py:905} INFO - DagRun Finished: dag_id=movie_pipeline, execution_date=2024-12-17 15:48:08.471283+00:00, run_id=manual__2024-12-17T15:48:08.471283+00:00, run_start_date=2024-12-17 15:48:08.471283+00:00, run_end_date=2024-12-17 15:48:13.832515+00:00, run_duration=5.361232, state=success, external_trigger=False, run_type=manual, data_interval_start=2024-12-16 15:48:08.471283+00:00, data_interval_end=2024-12-17 15:48:08.471283+00:00, dag_hash=None
[2024-12-17T21:18:13.841+0530] {processor.py:925} INFO - DAG(s) 'movie_pipeline' retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T21:18:13.853+0530] {logging_mixin.py:190} INFO - [2024-12-17T21:18:13.853+0530] {dag.py:3239} INFO - Sync 1 DAGs
[2024-12-17T21:18:13.871+0530] {logging_mixin.py:190} INFO - [2024-12-17T21:18:13.870+0530] {dag.py:4180} INFO - Setting next_dagrun for movie_pipeline to 2024-12-17 01:30:00+00:00, run_after=2024-12-18 01:30:00+00:00
[2024-12-17T21:18:13.889+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 5.465 seconds
[2024-12-17T21:21:07.952+0530] {processor.py:186} INFO - Started process (PID=95017) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T21:21:07.955+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-17T21:21:07.956+0530] {logging_mixin.py:190} INFO - [2024-12-17T21:21:07.956+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T21:21:08.132+0530] {logging_mixin.py:190} INFO - [2024-12-17T21:21:08.132+0530] {dag.py:4435} INFO - dagrun id: movie_pipeline
[2024-12-17T21:21:08.146+0530] {logging_mixin.py:190} INFO - [2024-12-17T21:21:08.146+0530] {dag.py:4451} INFO - created dagrun <DagRun movie_pipeline @ 2024-12-17 15:51:07.999449+00:00: manual__2024-12-17T15:51:07.999449+00:00, state:running, queued_at: None. externally triggered: False>
[2024-12-17T21:21:08.178+0530] {logging_mixin.py:190} INFO - [2024-12-17T21:21:08.178+0530] {dag.py:4396} INFO - [DAG TEST] starting task_id=scrape_collection_page map_index=-1
[2024-12-17T21:21:08.179+0530] {logging_mixin.py:190} INFO - [2024-12-17T21:21:08.178+0530] {dag.py:4399} INFO - [DAG TEST] running task <TaskInstance: movie_pipeline.scrape_collection_page manual__2024-12-17T15:51:07.999449+00:00 [scheduled]>
[2024-12-17T21:21:13.276+0530] {logging_mixin.py:190} INFO - [2024-12-17T21:21:13.275+0530] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-12-17T21:21:13.341+0530] {logging_mixin.py:190} INFO - [2024-12-17 21:21:13,341] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T15:51:07.999449+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T15:51:07.999449+00:00'
[2024-12-17T21:21:13.342+0530] {logging_mixin.py:190} INFO - [2024-12-17T21:21:13.341+0530] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T15:51:07.999449+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T15:51:07.999449+00:00'
[2024-12-17T21:21:13.353+0530] {logging_mixin.py:190} INFO - Task instance is in running state
[2024-12-17T21:21:13.354+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: queued
[2024-12-17T21:21:13.354+0530] {logging_mixin.py:190} INFO - Current task name:scrape_collection_page state:scheduled start_date:None
[2024-12-17T21:21:13.354+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline and current dag run status:running
[2024-12-17T21:21:13.355+0530] {logging_mixin.py:190} INFO - [2024-12-17T21:21:13.354+0530] {taskinstance.py:731} INFO - ::endgroup::
[2024-12-17T21:21:13.359+0530] {logging_mixin.py:190} INFO - 2024-12-17
[2024-12-17T21:21:13.359+0530] {logging_mixin.py:190} INFO - [2024-12-17 21:21:13,359] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T21:21:13.360+0530] {logging_mixin.py:190} INFO - [2024-12-17T21:21:13.359+0530] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T21:21:13.365+0530] {logging_mixin.py:190} INFO - [2024-12-17T21:21:13.365+0530] {taskinstance.py:340} INFO - ::group::Post task execution logs
[2024-12-17T21:21:13.366+0530] {logging_mixin.py:190} INFO - [2024-12-17T21:21:13.366+0530] {taskinstance.py:352} INFO - Marking task as SUCCESS. dag_id=movie_pipeline, task_id=scrape_collection_page, run_id=manual__2024-12-17T15:51:07.999449+00:00, execution_date=20241217T155107, start_date=, end_date=20241217T155113
[2024-12-17T21:21:13.375+0530] {logging_mixin.py:190} INFO - Task instance in success state
[2024-12-17T21:21:13.375+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: running
[2024-12-17T21:21:13.376+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline queued_at:None
[2024-12-17T21:21:13.376+0530] {logging_mixin.py:190} INFO - Task hostname:Jayeshs-MacBook-Air.local operator:PythonOperator
[2024-12-17T21:21:13.376+0530] {logging_mixin.py:190} INFO - [2024-12-17T21:21:13.376+0530] {dag.py:4410} INFO - [DAG TEST] end task task_id=scrape_collection_page map_index=-1
[2024-12-17T21:21:13.381+0530] {logging_mixin.py:190} INFO - [2024-12-17T21:21:13.380+0530] {dagrun.py:854} INFO - Marking run <DagRun movie_pipeline @ 2024-12-17 15:51:07.999449+00:00: manual__2024-12-17T15:51:07.999449+00:00, state:running, queued_at: None. externally triggered: False> successful
[2024-12-17T21:21:13.381+0530] {logging_mixin.py:190} INFO - Dag run in success state
[2024-12-17T21:21:13.381+0530] {logging_mixin.py:190} INFO - Dag run start:2024-12-17 15:51:07.999449+00:00 end:2024-12-17 15:51:13.381315+00:00
[2024-12-17T21:21:13.382+0530] {logging_mixin.py:190} INFO - [2024-12-17T21:21:13.382+0530] {dagrun.py:905} INFO - DagRun Finished: dag_id=movie_pipeline, execution_date=2024-12-17 15:51:07.999449+00:00, run_id=manual__2024-12-17T15:51:07.999449+00:00, run_start_date=2024-12-17 15:51:07.999449+00:00, run_end_date=2024-12-17 15:51:13.381315+00:00, run_duration=5.381866, state=success, external_trigger=False, run_type=manual, data_interval_start=2024-12-16 15:51:07.999449+00:00, data_interval_end=2024-12-17 15:51:07.999449+00:00, dag_hash=None
[2024-12-17T21:21:13.390+0530] {processor.py:925} INFO - DAG(s) 'movie_pipeline' retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T21:21:13.409+0530] {logging_mixin.py:190} INFO - [2024-12-17T21:21:13.408+0530] {dag.py:3239} INFO - Sync 1 DAGs
[2024-12-17T21:21:13.437+0530] {logging_mixin.py:190} INFO - [2024-12-17T21:21:13.437+0530] {dag.py:4180} INFO - Setting next_dagrun for movie_pipeline to 2024-12-17 01:30:00+00:00, run_after=2024-12-18 01:30:00+00:00
[2024-12-17T21:21:13.466+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 5.517 seconds
[2024-12-17T21:24:15.361+0530] {processor.py:186} INFO - Started process (PID=95265) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T21:24:15.365+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-17T21:24:15.368+0530] {logging_mixin.py:190} INFO - [2024-12-17T21:24:15.367+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T21:24:15.548+0530] {logging_mixin.py:190} INFO - [2024-12-17T21:24:15.547+0530] {dag.py:4435} INFO - dagrun id: movie_pipeline
[2024-12-17T21:24:15.562+0530] {logging_mixin.py:190} INFO - [2024-12-17T21:24:15.562+0530] {dag.py:4451} INFO - created dagrun <DagRun movie_pipeline @ 2024-12-17 15:54:15.415497+00:00: manual__2024-12-17T15:54:15.415497+00:00, state:running, queued_at: None. externally triggered: False>
[2024-12-17T21:24:15.593+0530] {logging_mixin.py:190} INFO - [2024-12-17T21:24:15.593+0530] {dag.py:4396} INFO - [DAG TEST] starting task_id=scrape_collection_page map_index=-1
[2024-12-17T21:24:15.594+0530] {logging_mixin.py:190} INFO - [2024-12-17T21:24:15.593+0530] {dag.py:4399} INFO - [DAG TEST] running task <TaskInstance: movie_pipeline.scrape_collection_page manual__2024-12-17T15:54:15.415497+00:00 [scheduled]>
[2024-12-17T21:24:20.698+0530] {logging_mixin.py:190} INFO - [2024-12-17T21:24:20.698+0530] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-12-17T21:24:20.766+0530] {logging_mixin.py:190} INFO - [2024-12-17 21:24:20,766] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T15:54:15.415497+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T15:54:15.415497+00:00'
[2024-12-17T21:24:20.767+0530] {logging_mixin.py:190} INFO - [2024-12-17T21:24:20.766+0530] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T15:54:15.415497+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T15:54:15.415497+00:00'
[2024-12-17T21:24:20.780+0530] {logging_mixin.py:190} INFO - Task instance is in running state
[2024-12-17T21:24:20.781+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: queued
[2024-12-17T21:24:20.781+0530] {logging_mixin.py:190} INFO - Current task name:scrape_collection_page state:scheduled start_date:None
[2024-12-17T21:24:20.781+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline and current dag run status:running
[2024-12-17T21:24:20.782+0530] {logging_mixin.py:190} INFO - [2024-12-17T21:24:20.782+0530] {taskinstance.py:731} INFO - ::endgroup::
[2024-12-17T21:24:20.785+0530] {logging_mixin.py:190} INFO - 2024-12-17
[2024-12-17T21:24:20.786+0530] {logging_mixin.py:190} INFO - [2024-12-17 21:24:20,786] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T21:24:20.786+0530] {logging_mixin.py:190} INFO - [2024-12-17T21:24:20.786+0530] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T21:24:20.791+0530] {logging_mixin.py:190} INFO - [2024-12-17T21:24:20.791+0530] {taskinstance.py:340} INFO - ::group::Post task execution logs
[2024-12-17T21:24:20.792+0530] {logging_mixin.py:190} INFO - [2024-12-17T21:24:20.792+0530] {taskinstance.py:352} INFO - Marking task as SUCCESS. dag_id=movie_pipeline, task_id=scrape_collection_page, run_id=manual__2024-12-17T15:54:15.415497+00:00, execution_date=20241217T155415, start_date=, end_date=20241217T155420
[2024-12-17T21:24:20.801+0530] {logging_mixin.py:190} INFO - Task instance in success state
[2024-12-17T21:24:20.802+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: running
[2024-12-17T21:24:20.802+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline queued_at:None
[2024-12-17T21:24:20.802+0530] {logging_mixin.py:190} INFO - Task hostname:Jayeshs-MacBook-Air.local operator:PythonOperator
[2024-12-17T21:24:20.802+0530] {logging_mixin.py:190} INFO - [2024-12-17T21:24:20.802+0530] {dag.py:4410} INFO - [DAG TEST] end task task_id=scrape_collection_page map_index=-1
[2024-12-17T21:24:20.806+0530] {logging_mixin.py:190} INFO - [2024-12-17T21:24:20.806+0530] {dagrun.py:854} INFO - Marking run <DagRun movie_pipeline @ 2024-12-17 15:54:15.415497+00:00: manual__2024-12-17T15:54:15.415497+00:00, state:running, queued_at: None. externally triggered: False> successful
[2024-12-17T21:24:20.806+0530] {logging_mixin.py:190} INFO - Dag run in success state
[2024-12-17T21:24:20.807+0530] {logging_mixin.py:190} INFO - Dag run start:2024-12-17 15:54:15.415497+00:00 end:2024-12-17 15:54:20.806873+00:00
[2024-12-17T21:24:20.807+0530] {logging_mixin.py:190} INFO - [2024-12-17T21:24:20.807+0530] {dagrun.py:905} INFO - DagRun Finished: dag_id=movie_pipeline, execution_date=2024-12-17 15:54:15.415497+00:00, run_id=manual__2024-12-17T15:54:15.415497+00:00, run_start_date=2024-12-17 15:54:15.415497+00:00, run_end_date=2024-12-17 15:54:20.806873+00:00, run_duration=5.391376, state=success, external_trigger=False, run_type=manual, data_interval_start=2024-12-16 15:54:15.415497+00:00, data_interval_end=2024-12-17 15:54:15.415497+00:00, dag_hash=None
[2024-12-17T21:24:20.817+0530] {processor.py:925} INFO - DAG(s) 'movie_pipeline' retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T21:24:20.828+0530] {logging_mixin.py:190} INFO - [2024-12-17T21:24:20.828+0530] {dag.py:3239} INFO - Sync 1 DAGs
[2024-12-17T21:24:20.846+0530] {logging_mixin.py:190} INFO - [2024-12-17T21:24:20.846+0530] {dag.py:4180} INFO - Setting next_dagrun for movie_pipeline to 2024-12-17 01:30:00+00:00, run_after=2024-12-18 01:30:00+00:00
[2024-12-17T21:24:20.865+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 5.507 seconds
[2024-12-17T21:27:12.739+0530] {processor.py:186} INFO - Started process (PID=95462) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T21:27:12.742+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-17T21:27:12.744+0530] {logging_mixin.py:190} INFO - [2024-12-17T21:27:12.743+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T21:27:12.997+0530] {logging_mixin.py:190} INFO - [2024-12-17T21:27:12.996+0530] {dag.py:4435} INFO - dagrun id: movie_pipeline
[2024-12-17T21:27:13.012+0530] {logging_mixin.py:190} INFO - [2024-12-17T21:27:13.012+0530] {dag.py:4451} INFO - created dagrun <DagRun movie_pipeline @ 2024-12-17 15:57:12.839923+00:00: manual__2024-12-17T15:57:12.839923+00:00, state:running, queued_at: None. externally triggered: False>
[2024-12-17T21:27:13.051+0530] {logging_mixin.py:190} INFO - [2024-12-17T21:27:13.050+0530] {dag.py:4396} INFO - [DAG TEST] starting task_id=scrape_collection_page map_index=-1
[2024-12-17T21:27:13.051+0530] {logging_mixin.py:190} INFO - [2024-12-17T21:27:13.051+0530] {dag.py:4399} INFO - [DAG TEST] running task <TaskInstance: movie_pipeline.scrape_collection_page manual__2024-12-17T15:57:12.839923+00:00 [scheduled]>
[2024-12-17T21:27:18.160+0530] {logging_mixin.py:190} INFO - [2024-12-17T21:27:18.160+0530] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-12-17T21:27:18.244+0530] {logging_mixin.py:190} INFO - [2024-12-17 21:27:18,244] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T15:57:12.839923+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T15:57:12.839923+00:00'
[2024-12-17T21:27:18.244+0530] {logging_mixin.py:190} INFO - [2024-12-17T21:27:18.244+0530] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T15:57:12.839923+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T15:57:12.839923+00:00'
[2024-12-17T21:27:18.257+0530] {logging_mixin.py:190} INFO - Task instance is in running state
[2024-12-17T21:27:18.258+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: queued
[2024-12-17T21:27:18.258+0530] {logging_mixin.py:190} INFO - Current task name:scrape_collection_page state:scheduled start_date:None
[2024-12-17T21:27:18.258+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline and current dag run status:running
[2024-12-17T21:27:18.259+0530] {logging_mixin.py:190} INFO - [2024-12-17T21:27:18.258+0530] {taskinstance.py:731} INFO - ::endgroup::
[2024-12-17T21:27:18.262+0530] {logging_mixin.py:190} INFO - 2024-12-17
[2024-12-17T21:27:18.263+0530] {logging_mixin.py:190} INFO - [2024-12-17 21:27:18,262] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T21:27:18.263+0530] {logging_mixin.py:190} INFO - [2024-12-17T21:27:18.262+0530] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T21:27:18.268+0530] {logging_mixin.py:190} INFO - [2024-12-17T21:27:18.268+0530] {taskinstance.py:340} INFO - ::group::Post task execution logs
[2024-12-17T21:27:18.268+0530] {logging_mixin.py:190} INFO - [2024-12-17T21:27:18.268+0530] {taskinstance.py:352} INFO - Marking task as SUCCESS. dag_id=movie_pipeline, task_id=scrape_collection_page, run_id=manual__2024-12-17T15:57:12.839923+00:00, execution_date=20241217T155712, start_date=, end_date=20241217T155718
[2024-12-17T21:27:18.280+0530] {logging_mixin.py:190} INFO - Task instance in success state
[2024-12-17T21:27:18.280+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: running
[2024-12-17T21:27:18.281+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline queued_at:None
[2024-12-17T21:27:18.281+0530] {logging_mixin.py:190} INFO - Task hostname:Jayeshs-MacBook-Air.local operator:PythonOperator
[2024-12-17T21:27:18.281+0530] {logging_mixin.py:190} INFO - [2024-12-17T21:27:18.281+0530] {dag.py:4410} INFO - [DAG TEST] end task task_id=scrape_collection_page map_index=-1
[2024-12-17T21:27:18.285+0530] {logging_mixin.py:190} INFO - [2024-12-17T21:27:18.285+0530] {dagrun.py:854} INFO - Marking run <DagRun movie_pipeline @ 2024-12-17 15:57:12.839923+00:00: manual__2024-12-17T15:57:12.839923+00:00, state:running, queued_at: None. externally triggered: False> successful
[2024-12-17T21:27:18.285+0530] {logging_mixin.py:190} INFO - Dag run in success state
[2024-12-17T21:27:18.286+0530] {logging_mixin.py:190} INFO - Dag run start:2024-12-17 15:57:12.839923+00:00 end:2024-12-17 15:57:18.285802+00:00
[2024-12-17T21:27:18.286+0530] {logging_mixin.py:190} INFO - [2024-12-17T21:27:18.286+0530] {dagrun.py:905} INFO - DagRun Finished: dag_id=movie_pipeline, execution_date=2024-12-17 15:57:12.839923+00:00, run_id=manual__2024-12-17T15:57:12.839923+00:00, run_start_date=2024-12-17 15:57:12.839923+00:00, run_end_date=2024-12-17 15:57:18.285802+00:00, run_duration=5.445879, state=success, external_trigger=False, run_type=manual, data_interval_start=2024-12-16 15:57:12.839923+00:00, data_interval_end=2024-12-17 15:57:12.839923+00:00, dag_hash=None
[2024-12-17T21:27:18.295+0530] {processor.py:925} INFO - DAG(s) 'movie_pipeline' retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T21:27:18.308+0530] {logging_mixin.py:190} INFO - [2024-12-17T21:27:18.307+0530] {dag.py:3239} INFO - Sync 1 DAGs
[2024-12-17T21:27:18.325+0530] {logging_mixin.py:190} INFO - [2024-12-17T21:27:18.324+0530] {dag.py:4180} INFO - Setting next_dagrun for movie_pipeline to 2024-12-17 01:30:00+00:00, run_after=2024-12-18 01:30:00+00:00
[2024-12-17T21:27:18.344+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 5.612 seconds
[2024-12-17T21:30:10.341+0530] {processor.py:186} INFO - Started process (PID=95674) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T21:30:10.344+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-17T21:30:10.346+0530] {logging_mixin.py:190} INFO - [2024-12-17T21:30:10.346+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T21:30:10.523+0530] {logging_mixin.py:190} INFO - [2024-12-17T21:30:10.522+0530] {dag.py:4435} INFO - dagrun id: movie_pipeline
[2024-12-17T21:30:10.537+0530] {logging_mixin.py:190} INFO - [2024-12-17T21:30:10.537+0530] {dag.py:4451} INFO - created dagrun <DagRun movie_pipeline @ 2024-12-17 16:00:10.388026+00:00: manual__2024-12-17T16:00:10.388026+00:00, state:running, queued_at: None. externally triggered: False>
[2024-12-17T21:30:10.567+0530] {logging_mixin.py:190} INFO - [2024-12-17T21:30:10.567+0530] {dag.py:4396} INFO - [DAG TEST] starting task_id=scrape_collection_page map_index=-1
[2024-12-17T21:30:10.568+0530] {logging_mixin.py:190} INFO - [2024-12-17T21:30:10.568+0530] {dag.py:4399} INFO - [DAG TEST] running task <TaskInstance: movie_pipeline.scrape_collection_page manual__2024-12-17T16:00:10.388026+00:00 [scheduled]>
[2024-12-17T21:30:15.663+0530] {logging_mixin.py:190} INFO - [2024-12-17T21:30:15.663+0530] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-12-17T21:30:15.725+0530] {logging_mixin.py:190} INFO - [2024-12-17 21:30:15,725] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T16:00:10.388026+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T16:00:10.388026+00:00'
[2024-12-17T21:30:15.726+0530] {logging_mixin.py:190} INFO - [2024-12-17T21:30:15.725+0530] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T16:00:10.388026+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T16:00:10.388026+00:00'
[2024-12-17T21:30:15.738+0530] {logging_mixin.py:190} INFO - Task instance is in running state
[2024-12-17T21:30:15.738+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: queued
[2024-12-17T21:30:15.739+0530] {logging_mixin.py:190} INFO - Current task name:scrape_collection_page state:scheduled start_date:None
[2024-12-17T21:30:15.739+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline and current dag run status:running
[2024-12-17T21:30:15.739+0530] {logging_mixin.py:190} INFO - [2024-12-17T21:30:15.739+0530] {taskinstance.py:731} INFO - ::endgroup::
[2024-12-17T21:30:15.742+0530] {logging_mixin.py:190} INFO - 2024-12-17
[2024-12-17T21:30:15.742+0530] {logging_mixin.py:190} INFO - [2024-12-17 21:30:15,742] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T21:30:15.743+0530] {logging_mixin.py:190} INFO - [2024-12-17T21:30:15.742+0530] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T21:30:15.750+0530] {logging_mixin.py:190} INFO - [2024-12-17T21:30:15.750+0530] {taskinstance.py:340} INFO - ::group::Post task execution logs
[2024-12-17T21:30:15.750+0530] {logging_mixin.py:190} INFO - [2024-12-17T21:30:15.750+0530] {taskinstance.py:352} INFO - Marking task as SUCCESS. dag_id=movie_pipeline, task_id=scrape_collection_page, run_id=manual__2024-12-17T16:00:10.388026+00:00, execution_date=20241217T160010, start_date=, end_date=20241217T160015
[2024-12-17T21:30:15.762+0530] {logging_mixin.py:190} INFO - Task instance in success state
[2024-12-17T21:30:15.762+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: running
[2024-12-17T21:30:15.762+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline queued_at:None
[2024-12-17T21:30:15.763+0530] {logging_mixin.py:190} INFO - Task hostname:Jayeshs-MacBook-Air.local operator:PythonOperator
[2024-12-17T21:30:15.763+0530] {logging_mixin.py:190} INFO - [2024-12-17T21:30:15.763+0530] {dag.py:4410} INFO - [DAG TEST] end task task_id=scrape_collection_page map_index=-1
[2024-12-17T21:30:15.767+0530] {logging_mixin.py:190} INFO - [2024-12-17T21:30:15.767+0530] {dagrun.py:854} INFO - Marking run <DagRun movie_pipeline @ 2024-12-17 16:00:10.388026+00:00: manual__2024-12-17T16:00:10.388026+00:00, state:running, queued_at: None. externally triggered: False> successful
[2024-12-17T21:30:15.767+0530] {logging_mixin.py:190} INFO - Dag run in success state
[2024-12-17T21:30:15.768+0530] {logging_mixin.py:190} INFO - Dag run start:2024-12-17 16:00:10.388026+00:00 end:2024-12-17 16:00:15.767915+00:00
[2024-12-17T21:30:15.768+0530] {logging_mixin.py:190} INFO - [2024-12-17T21:30:15.768+0530] {dagrun.py:905} INFO - DagRun Finished: dag_id=movie_pipeline, execution_date=2024-12-17 16:00:10.388026+00:00, run_id=manual__2024-12-17T16:00:10.388026+00:00, run_start_date=2024-12-17 16:00:10.388026+00:00, run_end_date=2024-12-17 16:00:15.767915+00:00, run_duration=5.379889, state=success, external_trigger=False, run_type=manual, data_interval_start=2024-12-16 16:00:10.388026+00:00, data_interval_end=2024-12-17 16:00:10.388026+00:00, dag_hash=None
[2024-12-17T21:30:15.777+0530] {processor.py:925} INFO - DAG(s) 'movie_pipeline' retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T21:30:15.789+0530] {logging_mixin.py:190} INFO - [2024-12-17T21:30:15.788+0530] {dag.py:3239} INFO - Sync 1 DAGs
[2024-12-17T21:30:15.807+0530] {logging_mixin.py:190} INFO - [2024-12-17T21:30:15.806+0530] {dag.py:4180} INFO - Setting next_dagrun for movie_pipeline to 2024-12-17 01:30:00+00:00, run_after=2024-12-18 01:30:00+00:00
[2024-12-17T21:30:15.825+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 5.487 seconds
[2024-12-17T21:33:15.180+0530] {processor.py:186} INFO - Started process (PID=95881) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T21:33:15.183+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-17T21:33:15.184+0530] {logging_mixin.py:190} INFO - [2024-12-17T21:33:15.184+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T21:33:15.355+0530] {logging_mixin.py:190} INFO - [2024-12-17T21:33:15.355+0530] {dag.py:4435} INFO - dagrun id: movie_pipeline
[2024-12-17T21:33:15.369+0530] {logging_mixin.py:190} INFO - [2024-12-17T21:33:15.369+0530] {dag.py:4451} INFO - created dagrun <DagRun movie_pipeline @ 2024-12-17 16:03:15.225526+00:00: manual__2024-12-17T16:03:15.225526+00:00, state:running, queued_at: None. externally triggered: False>
[2024-12-17T21:33:15.401+0530] {logging_mixin.py:190} INFO - [2024-12-17T21:33:15.401+0530] {dag.py:4396} INFO - [DAG TEST] starting task_id=scrape_collection_page map_index=-1
[2024-12-17T21:33:15.402+0530] {logging_mixin.py:190} INFO - [2024-12-17T21:33:15.401+0530] {dag.py:4399} INFO - [DAG TEST] running task <TaskInstance: movie_pipeline.scrape_collection_page manual__2024-12-17T16:03:15.225526+00:00 [scheduled]>
[2024-12-17T21:33:20.500+0530] {logging_mixin.py:190} INFO - [2024-12-17T21:33:20.499+0530] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-12-17T21:33:20.562+0530] {logging_mixin.py:190} INFO - [2024-12-17 21:33:20,562] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T16:03:15.225526+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T16:03:15.225526+00:00'
[2024-12-17T21:33:20.563+0530] {logging_mixin.py:190} INFO - [2024-12-17T21:33:20.562+0530] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T16:03:15.225526+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T16:03:15.225526+00:00'
[2024-12-17T21:33:20.576+0530] {logging_mixin.py:190} INFO - Task instance is in running state
[2024-12-17T21:33:20.576+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: queued
[2024-12-17T21:33:20.577+0530] {logging_mixin.py:190} INFO - Current task name:scrape_collection_page state:scheduled start_date:None
[2024-12-17T21:33:20.577+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline and current dag run status:running
[2024-12-17T21:33:20.578+0530] {logging_mixin.py:190} INFO - [2024-12-17T21:33:20.577+0530] {taskinstance.py:731} INFO - ::endgroup::
[2024-12-17T21:33:20.580+0530] {logging_mixin.py:190} INFO - 2024-12-17
[2024-12-17T21:33:20.580+0530] {logging_mixin.py:190} INFO - [2024-12-17 21:33:20,580] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T21:33:20.580+0530] {logging_mixin.py:190} INFO - [2024-12-17T21:33:20.580+0530] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T21:33:20.586+0530] {logging_mixin.py:190} INFO - [2024-12-17T21:33:20.586+0530] {taskinstance.py:340} INFO - ::group::Post task execution logs
[2024-12-17T21:33:20.586+0530] {logging_mixin.py:190} INFO - [2024-12-17T21:33:20.586+0530] {taskinstance.py:352} INFO - Marking task as SUCCESS. dag_id=movie_pipeline, task_id=scrape_collection_page, run_id=manual__2024-12-17T16:03:15.225526+00:00, execution_date=20241217T160315, start_date=, end_date=20241217T160320
[2024-12-17T21:33:20.596+0530] {logging_mixin.py:190} INFO - Task instance in success state
[2024-12-17T21:33:20.596+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: running
[2024-12-17T21:33:20.596+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline queued_at:None
[2024-12-17T21:33:20.596+0530] {logging_mixin.py:190} INFO - Task hostname:Jayeshs-MacBook-Air.local operator:PythonOperator
[2024-12-17T21:33:20.597+0530] {logging_mixin.py:190} INFO - [2024-12-17T21:33:20.597+0530] {dag.py:4410} INFO - [DAG TEST] end task task_id=scrape_collection_page map_index=-1
[2024-12-17T21:33:20.601+0530] {logging_mixin.py:190} INFO - [2024-12-17T21:33:20.601+0530] {dagrun.py:854} INFO - Marking run <DagRun movie_pipeline @ 2024-12-17 16:03:15.225526+00:00: manual__2024-12-17T16:03:15.225526+00:00, state:running, queued_at: None. externally triggered: False> successful
[2024-12-17T21:33:20.601+0530] {logging_mixin.py:190} INFO - Dag run in success state
[2024-12-17T21:33:20.602+0530] {logging_mixin.py:190} INFO - Dag run start:2024-12-17 16:03:15.225526+00:00 end:2024-12-17 16:03:20.601873+00:00
[2024-12-17T21:33:20.602+0530] {logging_mixin.py:190} INFO - [2024-12-17T21:33:20.602+0530] {dagrun.py:905} INFO - DagRun Finished: dag_id=movie_pipeline, execution_date=2024-12-17 16:03:15.225526+00:00, run_id=manual__2024-12-17T16:03:15.225526+00:00, run_start_date=2024-12-17 16:03:15.225526+00:00, run_end_date=2024-12-17 16:03:20.601873+00:00, run_duration=5.376347, state=success, external_trigger=False, run_type=manual, data_interval_start=2024-12-16 16:03:15.225526+00:00, data_interval_end=2024-12-17 16:03:15.225526+00:00, dag_hash=None
[2024-12-17T21:33:20.611+0530] {processor.py:925} INFO - DAG(s) 'movie_pipeline' retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T21:33:20.623+0530] {logging_mixin.py:190} INFO - [2024-12-17T21:33:20.623+0530] {dag.py:3239} INFO - Sync 1 DAGs
[2024-12-17T21:33:20.642+0530] {logging_mixin.py:190} INFO - [2024-12-17T21:33:20.641+0530] {dag.py:4180} INFO - Setting next_dagrun for movie_pipeline to 2024-12-17 01:30:00+00:00, run_after=2024-12-18 01:30:00+00:00
[2024-12-17T21:33:20.659+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 5.483 seconds
[2024-12-17T21:36:07.877+0530] {processor.py:186} INFO - Started process (PID=96085) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T21:36:07.879+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-17T21:36:07.880+0530] {logging_mixin.py:190} INFO - [2024-12-17T21:36:07.880+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T21:36:08.055+0530] {logging_mixin.py:190} INFO - [2024-12-17T21:36:08.055+0530] {dag.py:4435} INFO - dagrun id: movie_pipeline
[2024-12-17T21:36:08.069+0530] {logging_mixin.py:190} INFO - [2024-12-17T21:36:08.069+0530] {dag.py:4451} INFO - created dagrun <DagRun movie_pipeline @ 2024-12-17 16:06:07.924622+00:00: manual__2024-12-17T16:06:07.924622+00:00, state:running, queued_at: None. externally triggered: False>
[2024-12-17T21:36:08.094+0530] {logging_mixin.py:190} INFO - [2024-12-17T21:36:08.093+0530] {dag.py:4396} INFO - [DAG TEST] starting task_id=scrape_collection_page map_index=-1
[2024-12-17T21:36:08.095+0530] {logging_mixin.py:190} INFO - [2024-12-17T21:36:08.094+0530] {dag.py:4399} INFO - [DAG TEST] running task <TaskInstance: movie_pipeline.scrape_collection_page manual__2024-12-17T16:06:07.924622+00:00 [scheduled]>
[2024-12-17T21:36:13.185+0530] {logging_mixin.py:190} INFO - [2024-12-17T21:36:13.184+0530] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-12-17T21:36:13.244+0530] {logging_mixin.py:190} INFO - [2024-12-17 21:36:13,244] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T16:06:07.924622+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T16:06:07.924622+00:00'
[2024-12-17T21:36:13.244+0530] {logging_mixin.py:190} INFO - [2024-12-17T21:36:13.244+0530] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T16:06:07.924622+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T16:06:07.924622+00:00'
[2024-12-17T21:36:13.247+0530] {logging_mixin.py:190} INFO - Task instance is in running state
[2024-12-17T21:36:13.248+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: queued
[2024-12-17T21:36:13.248+0530] {logging_mixin.py:190} INFO - Current task name:scrape_collection_page state:scheduled start_date:None
[2024-12-17T21:36:13.248+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline and current dag run status:running
[2024-12-17T21:36:13.249+0530] {logging_mixin.py:190} INFO - [2024-12-17T21:36:13.248+0530] {taskinstance.py:731} INFO - ::endgroup::
[2024-12-17T21:36:13.250+0530] {logging_mixin.py:190} INFO - 2024-12-17
[2024-12-17T21:36:13.251+0530] {logging_mixin.py:190} INFO - [2024-12-17 21:36:13,251] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T21:36:13.252+0530] {logging_mixin.py:190} INFO - [2024-12-17T21:36:13.251+0530] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T21:36:13.257+0530] {logging_mixin.py:190} INFO - [2024-12-17T21:36:13.257+0530] {taskinstance.py:340} INFO - ::group::Post task execution logs
[2024-12-17T21:36:13.257+0530] {logging_mixin.py:190} INFO - [2024-12-17T21:36:13.257+0530] {taskinstance.py:352} INFO - Marking task as SUCCESS. dag_id=movie_pipeline, task_id=scrape_collection_page, run_id=manual__2024-12-17T16:06:07.924622+00:00, execution_date=20241217T160607, start_date=, end_date=20241217T160613
[2024-12-17T21:36:13.267+0530] {logging_mixin.py:190} INFO - Task instance in success state
[2024-12-17T21:36:13.267+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: running
[2024-12-17T21:36:13.267+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline queued_at:None
[2024-12-17T21:36:13.268+0530] {logging_mixin.py:190} INFO - Task hostname:Jayeshs-MacBook-Air.local operator:PythonOperator
[2024-12-17T21:36:13.269+0530] {logging_mixin.py:190} INFO - [2024-12-17T21:36:13.268+0530] {dag.py:4410} INFO - [DAG TEST] end task task_id=scrape_collection_page map_index=-1
[2024-12-17T21:36:13.273+0530] {logging_mixin.py:190} INFO - [2024-12-17T21:36:13.273+0530] {dagrun.py:854} INFO - Marking run <DagRun movie_pipeline @ 2024-12-17 16:06:07.924622+00:00: manual__2024-12-17T16:06:07.924622+00:00, state:running, queued_at: None. externally triggered: False> successful
[2024-12-17T21:36:13.273+0530] {logging_mixin.py:190} INFO - Dag run in success state
[2024-12-17T21:36:13.274+0530] {logging_mixin.py:190} INFO - Dag run start:2024-12-17 16:06:07.924622+00:00 end:2024-12-17 16:06:13.273816+00:00
[2024-12-17T21:36:13.274+0530] {logging_mixin.py:190} INFO - [2024-12-17T21:36:13.274+0530] {dagrun.py:905} INFO - DagRun Finished: dag_id=movie_pipeline, execution_date=2024-12-17 16:06:07.924622+00:00, run_id=manual__2024-12-17T16:06:07.924622+00:00, run_start_date=2024-12-17 16:06:07.924622+00:00, run_end_date=2024-12-17 16:06:13.273816+00:00, run_duration=5.349194, state=success, external_trigger=False, run_type=manual, data_interval_start=2024-12-16 16:06:07.924622+00:00, data_interval_end=2024-12-17 16:06:07.924622+00:00, dag_hash=None
[2024-12-17T21:36:13.283+0530] {processor.py:925} INFO - DAG(s) 'movie_pipeline' retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T21:36:13.295+0530] {logging_mixin.py:190} INFO - [2024-12-17T21:36:13.295+0530] {dag.py:3239} INFO - Sync 1 DAGs
[2024-12-17T21:36:13.313+0530] {logging_mixin.py:190} INFO - [2024-12-17T21:36:13.312+0530] {dag.py:4180} INFO - Setting next_dagrun for movie_pipeline to 2024-12-17 01:30:00+00:00, run_after=2024-12-18 01:30:00+00:00
[2024-12-17T21:36:13.332+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 5.458 seconds
[2024-12-17T21:39:11.338+0530] {processor.py:186} INFO - Started process (PID=96301) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T21:39:11.341+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-17T21:39:11.342+0530] {logging_mixin.py:190} INFO - [2024-12-17T21:39:11.342+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T21:39:11.514+0530] {logging_mixin.py:190} INFO - [2024-12-17T21:39:11.514+0530] {dag.py:4435} INFO - dagrun id: movie_pipeline
[2024-12-17T21:39:11.529+0530] {logging_mixin.py:190} INFO - [2024-12-17T21:39:11.529+0530] {dag.py:4451} INFO - created dagrun <DagRun movie_pipeline @ 2024-12-17 16:09:11.380861+00:00: manual__2024-12-17T16:09:11.380861+00:00, state:running, queued_at: None. externally triggered: False>
[2024-12-17T21:39:11.560+0530] {logging_mixin.py:190} INFO - [2024-12-17T21:39:11.559+0530] {dag.py:4396} INFO - [DAG TEST] starting task_id=scrape_collection_page map_index=-1
[2024-12-17T21:39:11.560+0530] {logging_mixin.py:190} INFO - [2024-12-17T21:39:11.560+0530] {dag.py:4399} INFO - [DAG TEST] running task <TaskInstance: movie_pipeline.scrape_collection_page manual__2024-12-17T16:09:11.380861+00:00 [scheduled]>
[2024-12-17T21:39:16.657+0530] {logging_mixin.py:190} INFO - [2024-12-17T21:39:16.657+0530] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-12-17T21:39:16.719+0530] {logging_mixin.py:190} INFO - [2024-12-17 21:39:16,719] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T16:09:11.380861+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T16:09:11.380861+00:00'
[2024-12-17T21:39:16.720+0530] {logging_mixin.py:190} INFO - [2024-12-17T21:39:16.719+0530] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T16:09:11.380861+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T16:09:11.380861+00:00'
[2024-12-17T21:39:16.727+0530] {logging_mixin.py:190} INFO - Task instance is in running state
[2024-12-17T21:39:16.727+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: queued
[2024-12-17T21:39:16.727+0530] {logging_mixin.py:190} INFO - Current task name:scrape_collection_page state:scheduled start_date:None
[2024-12-17T21:39:16.727+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline and current dag run status:running
[2024-12-17T21:39:16.728+0530] {logging_mixin.py:190} INFO - [2024-12-17T21:39:16.728+0530] {taskinstance.py:731} INFO - ::endgroup::
[2024-12-17T21:39:16.730+0530] {logging_mixin.py:190} INFO - 2024-12-17
[2024-12-17T21:39:16.730+0530] {logging_mixin.py:190} INFO - [2024-12-17 21:39:16,730] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T21:39:16.731+0530] {logging_mixin.py:190} INFO - [2024-12-17T21:39:16.730+0530] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T21:39:16.736+0530] {logging_mixin.py:190} INFO - [2024-12-17T21:39:16.736+0530] {taskinstance.py:340} INFO - ::group::Post task execution logs
[2024-12-17T21:39:16.736+0530] {logging_mixin.py:190} INFO - [2024-12-17T21:39:16.736+0530] {taskinstance.py:352} INFO - Marking task as SUCCESS. dag_id=movie_pipeline, task_id=scrape_collection_page, run_id=manual__2024-12-17T16:09:11.380861+00:00, execution_date=20241217T160911, start_date=, end_date=20241217T160916
[2024-12-17T21:39:16.748+0530] {logging_mixin.py:190} INFO - Task instance in success state
[2024-12-17T21:39:16.748+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: running
[2024-12-17T21:39:16.748+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline queued_at:None
[2024-12-17T21:39:16.748+0530] {logging_mixin.py:190} INFO - Task hostname:Jayeshs-MacBook-Air.local operator:PythonOperator
[2024-12-17T21:39:16.749+0530] {logging_mixin.py:190} INFO - [2024-12-17T21:39:16.749+0530] {dag.py:4410} INFO - [DAG TEST] end task task_id=scrape_collection_page map_index=-1
[2024-12-17T21:39:16.753+0530] {logging_mixin.py:190} INFO - [2024-12-17T21:39:16.752+0530] {dagrun.py:854} INFO - Marking run <DagRun movie_pipeline @ 2024-12-17 16:09:11.380861+00:00: manual__2024-12-17T16:09:11.380861+00:00, state:running, queued_at: None. externally triggered: False> successful
[2024-12-17T21:39:16.753+0530] {logging_mixin.py:190} INFO - Dag run in success state
[2024-12-17T21:39:16.753+0530] {logging_mixin.py:190} INFO - Dag run start:2024-12-17 16:09:11.380861+00:00 end:2024-12-17 16:09:16.753394+00:00
[2024-12-17T21:39:16.754+0530] {logging_mixin.py:190} INFO - [2024-12-17T21:39:16.754+0530] {dagrun.py:905} INFO - DagRun Finished: dag_id=movie_pipeline, execution_date=2024-12-17 16:09:11.380861+00:00, run_id=manual__2024-12-17T16:09:11.380861+00:00, run_start_date=2024-12-17 16:09:11.380861+00:00, run_end_date=2024-12-17 16:09:16.753394+00:00, run_duration=5.372533, state=success, external_trigger=False, run_type=manual, data_interval_start=2024-12-16 16:09:11.380861+00:00, data_interval_end=2024-12-17 16:09:11.380861+00:00, dag_hash=None
[2024-12-17T21:39:16.763+0530] {processor.py:925} INFO - DAG(s) 'movie_pipeline' retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T21:39:16.775+0530] {logging_mixin.py:190} INFO - [2024-12-17T21:39:16.775+0530] {dag.py:3239} INFO - Sync 1 DAGs
[2024-12-17T21:39:16.792+0530] {logging_mixin.py:190} INFO - [2024-12-17T21:39:16.792+0530] {dag.py:4180} INFO - Setting next_dagrun for movie_pipeline to 2024-12-17 01:30:00+00:00, run_after=2024-12-18 01:30:00+00:00
[2024-12-17T21:39:16.811+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 5.476 seconds
[2024-12-17T21:42:05.714+0530] {processor.py:186} INFO - Started process (PID=96511) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T21:42:05.716+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-17T21:42:05.718+0530] {logging_mixin.py:190} INFO - [2024-12-17T21:42:05.718+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T21:42:05.885+0530] {logging_mixin.py:190} INFO - [2024-12-17T21:42:05.885+0530] {dag.py:4435} INFO - dagrun id: movie_pipeline
[2024-12-17T21:42:05.900+0530] {logging_mixin.py:190} INFO - [2024-12-17T21:42:05.899+0530] {dag.py:4451} INFO - created dagrun <DagRun movie_pipeline @ 2024-12-17 16:12:05.758779+00:00: manual__2024-12-17T16:12:05.758779+00:00, state:running, queued_at: None. externally triggered: False>
[2024-12-17T21:42:05.930+0530] {logging_mixin.py:190} INFO - [2024-12-17T21:42:05.930+0530] {dag.py:4396} INFO - [DAG TEST] starting task_id=scrape_collection_page map_index=-1
[2024-12-17T21:42:05.931+0530] {logging_mixin.py:190} INFO - [2024-12-17T21:42:05.930+0530] {dag.py:4399} INFO - [DAG TEST] running task <TaskInstance: movie_pipeline.scrape_collection_page manual__2024-12-17T16:12:05.758779+00:00 [scheduled]>
[2024-12-17T21:42:11.035+0530] {logging_mixin.py:190} INFO - [2024-12-17T21:42:11.034+0530] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-12-17T21:42:11.103+0530] {logging_mixin.py:190} INFO - [2024-12-17 21:42:11,103] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T16:12:05.758779+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T16:12:05.758779+00:00'
[2024-12-17T21:42:11.103+0530] {logging_mixin.py:190} INFO - [2024-12-17T21:42:11.103+0530] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T16:12:05.758779+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T16:12:05.758779+00:00'
[2024-12-17T21:42:11.117+0530] {logging_mixin.py:190} INFO - Task instance is in running state
[2024-12-17T21:42:11.117+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: queued
[2024-12-17T21:42:11.117+0530] {logging_mixin.py:190} INFO - Current task name:scrape_collection_page state:scheduled start_date:None
[2024-12-17T21:42:11.117+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline and current dag run status:running
[2024-12-17T21:42:11.118+0530] {logging_mixin.py:190} INFO - [2024-12-17T21:42:11.118+0530] {taskinstance.py:731} INFO - ::endgroup::
[2024-12-17T21:42:11.121+0530] {logging_mixin.py:190} INFO - 2024-12-17
[2024-12-17T21:42:11.121+0530] {logging_mixin.py:190} INFO - [2024-12-17 21:42:11,121] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T21:42:11.122+0530] {logging_mixin.py:190} INFO - [2024-12-17T21:42:11.121+0530] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T21:42:11.127+0530] {logging_mixin.py:190} INFO - [2024-12-17T21:42:11.126+0530] {taskinstance.py:340} INFO - ::group::Post task execution logs
[2024-12-17T21:42:11.127+0530] {logging_mixin.py:190} INFO - [2024-12-17T21:42:11.127+0530] {taskinstance.py:352} INFO - Marking task as SUCCESS. dag_id=movie_pipeline, task_id=scrape_collection_page, run_id=manual__2024-12-17T16:12:05.758779+00:00, execution_date=20241217T161205, start_date=, end_date=20241217T161211
[2024-12-17T21:42:11.137+0530] {logging_mixin.py:190} INFO - Task instance in success state
[2024-12-17T21:42:11.137+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: running
[2024-12-17T21:42:11.138+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline queued_at:None
[2024-12-17T21:42:11.138+0530] {logging_mixin.py:190} INFO - Task hostname:Jayeshs-MacBook-Air.local operator:PythonOperator
[2024-12-17T21:42:11.138+0530] {logging_mixin.py:190} INFO - [2024-12-17T21:42:11.138+0530] {dag.py:4410} INFO - [DAG TEST] end task task_id=scrape_collection_page map_index=-1
[2024-12-17T21:42:11.142+0530] {logging_mixin.py:190} INFO - [2024-12-17T21:42:11.142+0530] {dagrun.py:854} INFO - Marking run <DagRun movie_pipeline @ 2024-12-17 16:12:05.758779+00:00: manual__2024-12-17T16:12:05.758779+00:00, state:running, queued_at: None. externally triggered: False> successful
[2024-12-17T21:42:11.143+0530] {logging_mixin.py:190} INFO - Dag run in success state
[2024-12-17T21:42:11.143+0530] {logging_mixin.py:190} INFO - Dag run start:2024-12-17 16:12:05.758779+00:00 end:2024-12-17 16:12:11.143012+00:00
[2024-12-17T21:42:11.143+0530] {logging_mixin.py:190} INFO - [2024-12-17T21:42:11.143+0530] {dagrun.py:905} INFO - DagRun Finished: dag_id=movie_pipeline, execution_date=2024-12-17 16:12:05.758779+00:00, run_id=manual__2024-12-17T16:12:05.758779+00:00, run_start_date=2024-12-17 16:12:05.758779+00:00, run_end_date=2024-12-17 16:12:11.143012+00:00, run_duration=5.384233, state=success, external_trigger=False, run_type=manual, data_interval_start=2024-12-16 16:12:05.758779+00:00, data_interval_end=2024-12-17 16:12:05.758779+00:00, dag_hash=None
[2024-12-17T21:42:11.152+0530] {processor.py:925} INFO - DAG(s) 'movie_pipeline' retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T21:42:11.165+0530] {logging_mixin.py:190} INFO - [2024-12-17T21:42:11.164+0530] {dag.py:3239} INFO - Sync 1 DAGs
[2024-12-17T21:42:11.182+0530] {logging_mixin.py:190} INFO - [2024-12-17T21:42:11.181+0530] {dag.py:4180} INFO - Setting next_dagrun for movie_pipeline to 2024-12-17 01:30:00+00:00, run_after=2024-12-18 01:30:00+00:00
[2024-12-17T21:42:11.200+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 5.491 seconds
[2024-12-17T21:45:00.878+0530] {processor.py:186} INFO - Started process (PID=96712) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T21:45:00.880+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-17T21:45:00.883+0530] {logging_mixin.py:190} INFO - [2024-12-17T21:45:00.882+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T21:45:01.066+0530] {logging_mixin.py:190} INFO - [2024-12-17T21:45:01.066+0530] {dag.py:4435} INFO - dagrun id: movie_pipeline
[2024-12-17T21:45:01.080+0530] {logging_mixin.py:190} INFO - [2024-12-17T21:45:01.080+0530] {dag.py:4451} INFO - created dagrun <DagRun movie_pipeline @ 2024-12-17 16:15:00.930350+00:00: manual__2024-12-17T16:15:00.930350+00:00, state:running, queued_at: None. externally triggered: False>
[2024-12-17T21:45:01.109+0530] {logging_mixin.py:190} INFO - [2024-12-17T21:45:01.109+0530] {dag.py:4396} INFO - [DAG TEST] starting task_id=scrape_collection_page map_index=-1
[2024-12-17T21:45:01.109+0530] {logging_mixin.py:190} INFO - [2024-12-17T21:45:01.109+0530] {dag.py:4399} INFO - [DAG TEST] running task <TaskInstance: movie_pipeline.scrape_collection_page manual__2024-12-17T16:15:00.930350+00:00 [scheduled]>
[2024-12-17T21:45:06.208+0530] {logging_mixin.py:190} INFO - [2024-12-17T21:45:06.207+0530] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-12-17T21:45:06.278+0530] {logging_mixin.py:190} INFO - [2024-12-17 21:45:06,278] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T16:15:00.930350+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T16:15:00.930350+00:00'
[2024-12-17T21:45:06.279+0530] {logging_mixin.py:190} INFO - [2024-12-17T21:45:06.278+0530] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T16:15:00.930350+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T16:15:00.930350+00:00'
[2024-12-17T21:45:06.293+0530] {logging_mixin.py:190} INFO - Task instance is in running state
[2024-12-17T21:45:06.294+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: queued
[2024-12-17T21:45:06.294+0530] {logging_mixin.py:190} INFO - Current task name:scrape_collection_page state:scheduled start_date:None
[2024-12-17T21:45:06.295+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline and current dag run status:running
[2024-12-17T21:45:06.295+0530] {logging_mixin.py:190} INFO - [2024-12-17T21:45:06.295+0530] {taskinstance.py:731} INFO - ::endgroup::
[2024-12-17T21:45:06.298+0530] {logging_mixin.py:190} INFO - 2024-12-17
[2024-12-17T21:45:06.299+0530] {logging_mixin.py:190} INFO - [2024-12-17 21:45:06,298] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T21:45:06.299+0530] {logging_mixin.py:190} INFO - [2024-12-17T21:45:06.298+0530] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T21:45:06.305+0530] {logging_mixin.py:190} INFO - [2024-12-17T21:45:06.304+0530] {taskinstance.py:340} INFO - ::group::Post task execution logs
[2024-12-17T21:45:06.305+0530] {logging_mixin.py:190} INFO - [2024-12-17T21:45:06.305+0530] {taskinstance.py:352} INFO - Marking task as SUCCESS. dag_id=movie_pipeline, task_id=scrape_collection_page, run_id=manual__2024-12-17T16:15:00.930350+00:00, execution_date=20241217T161500, start_date=, end_date=20241217T161506
[2024-12-17T21:45:06.315+0530] {logging_mixin.py:190} INFO - Task instance in success state
[2024-12-17T21:45:06.316+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: running
[2024-12-17T21:45:06.316+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline queued_at:None
[2024-12-17T21:45:06.316+0530] {logging_mixin.py:190} INFO - Task hostname:Jayeshs-MacBook-Air.local operator:PythonOperator
[2024-12-17T21:45:06.316+0530] {logging_mixin.py:190} INFO - [2024-12-17T21:45:06.316+0530] {dag.py:4410} INFO - [DAG TEST] end task task_id=scrape_collection_page map_index=-1
[2024-12-17T21:45:06.322+0530] {logging_mixin.py:190} INFO - [2024-12-17T21:45:06.322+0530] {dagrun.py:854} INFO - Marking run <DagRun movie_pipeline @ 2024-12-17 16:15:00.930350+00:00: manual__2024-12-17T16:15:00.930350+00:00, state:running, queued_at: None. externally triggered: False> successful
[2024-12-17T21:45:06.322+0530] {logging_mixin.py:190} INFO - Dag run in success state
[2024-12-17T21:45:06.323+0530] {logging_mixin.py:190} INFO - Dag run start:2024-12-17 16:15:00.930350+00:00 end:2024-12-17 16:15:06.322681+00:00
[2024-12-17T21:45:06.323+0530] {logging_mixin.py:190} INFO - [2024-12-17T21:45:06.323+0530] {dagrun.py:905} INFO - DagRun Finished: dag_id=movie_pipeline, execution_date=2024-12-17 16:15:00.930350+00:00, run_id=manual__2024-12-17T16:15:00.930350+00:00, run_start_date=2024-12-17 16:15:00.930350+00:00, run_end_date=2024-12-17 16:15:06.322681+00:00, run_duration=5.392331, state=success, external_trigger=False, run_type=manual, data_interval_start=2024-12-16 16:15:00.930350+00:00, data_interval_end=2024-12-17 16:15:00.930350+00:00, dag_hash=None
[2024-12-17T21:45:06.332+0530] {processor.py:925} INFO - DAG(s) 'movie_pipeline' retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T21:45:06.344+0530] {logging_mixin.py:190} INFO - [2024-12-17T21:45:06.344+0530] {dag.py:3239} INFO - Sync 1 DAGs
[2024-12-17T21:45:06.361+0530] {logging_mixin.py:190} INFO - [2024-12-17T21:45:06.361+0530] {dag.py:4180} INFO - Setting next_dagrun for movie_pipeline to 2024-12-17 01:30:00+00:00, run_after=2024-12-18 01:30:00+00:00
[2024-12-17T21:45:06.379+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 5.505 seconds
[2024-12-17T21:47:56.292+0530] {processor.py:186} INFO - Started process (PID=96906) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T21:47:56.295+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-17T21:47:56.296+0530] {logging_mixin.py:190} INFO - [2024-12-17T21:47:56.296+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T21:47:56.464+0530] {logging_mixin.py:190} INFO - [2024-12-17T21:47:56.464+0530] {dag.py:4435} INFO - dagrun id: movie_pipeline
[2024-12-17T21:47:56.478+0530] {logging_mixin.py:190} INFO - [2024-12-17T21:47:56.478+0530] {dag.py:4451} INFO - created dagrun <DagRun movie_pipeline @ 2024-12-17 16:17:56.337223+00:00: manual__2024-12-17T16:17:56.337223+00:00, state:running, queued_at: None. externally triggered: False>
[2024-12-17T21:47:56.509+0530] {logging_mixin.py:190} INFO - [2024-12-17T21:47:56.509+0530] {dag.py:4396} INFO - [DAG TEST] starting task_id=scrape_collection_page map_index=-1
[2024-12-17T21:47:56.510+0530] {logging_mixin.py:190} INFO - [2024-12-17T21:47:56.509+0530] {dag.py:4399} INFO - [DAG TEST] running task <TaskInstance: movie_pipeline.scrape_collection_page manual__2024-12-17T16:17:56.337223+00:00 [scheduled]>
[2024-12-17T21:48:01.601+0530] {logging_mixin.py:190} INFO - [2024-12-17T21:48:01.600+0530] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-12-17T21:48:01.667+0530] {logging_mixin.py:190} INFO - [2024-12-17 21:48:01,667] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T16:17:56.337223+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T16:17:56.337223+00:00'
[2024-12-17T21:48:01.668+0530] {logging_mixin.py:190} INFO - [2024-12-17T21:48:01.667+0530] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T16:17:56.337223+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T16:17:56.337223+00:00'
[2024-12-17T21:48:01.680+0530] {logging_mixin.py:190} INFO - Task instance is in running state
[2024-12-17T21:48:01.680+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: queued
[2024-12-17T21:48:01.681+0530] {logging_mixin.py:190} INFO - Current task name:scrape_collection_page state:scheduled start_date:None
[2024-12-17T21:48:01.681+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline and current dag run status:running
[2024-12-17T21:48:01.681+0530] {logging_mixin.py:190} INFO - [2024-12-17T21:48:01.681+0530] {taskinstance.py:731} INFO - ::endgroup::
[2024-12-17T21:48:01.686+0530] {logging_mixin.py:190} INFO - 2024-12-17
[2024-12-17T21:48:01.687+0530] {logging_mixin.py:190} INFO - [2024-12-17 21:48:01,687] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T21:48:01.687+0530] {logging_mixin.py:190} INFO - [2024-12-17T21:48:01.687+0530] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T21:48:01.693+0530] {logging_mixin.py:190} INFO - [2024-12-17T21:48:01.693+0530] {taskinstance.py:340} INFO - ::group::Post task execution logs
[2024-12-17T21:48:01.693+0530] {logging_mixin.py:190} INFO - [2024-12-17T21:48:01.693+0530] {taskinstance.py:352} INFO - Marking task as SUCCESS. dag_id=movie_pipeline, task_id=scrape_collection_page, run_id=manual__2024-12-17T16:17:56.337223+00:00, execution_date=20241217T161756, start_date=, end_date=20241217T161801
[2024-12-17T21:48:01.702+0530] {logging_mixin.py:190} INFO - Task instance in success state
[2024-12-17T21:48:01.703+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: running
[2024-12-17T21:48:01.703+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline queued_at:None
[2024-12-17T21:48:01.703+0530] {logging_mixin.py:190} INFO - Task hostname:Jayeshs-MacBook-Air.local operator:PythonOperator
[2024-12-17T21:48:01.704+0530] {logging_mixin.py:190} INFO - [2024-12-17T21:48:01.703+0530] {dag.py:4410} INFO - [DAG TEST] end task task_id=scrape_collection_page map_index=-1
[2024-12-17T21:48:01.708+0530] {logging_mixin.py:190} INFO - [2024-12-17T21:48:01.707+0530] {dagrun.py:854} INFO - Marking run <DagRun movie_pipeline @ 2024-12-17 16:17:56.337223+00:00: manual__2024-12-17T16:17:56.337223+00:00, state:running, queued_at: None. externally triggered: False> successful
[2024-12-17T21:48:01.708+0530] {logging_mixin.py:190} INFO - Dag run in success state
[2024-12-17T21:48:01.708+0530] {logging_mixin.py:190} INFO - Dag run start:2024-12-17 16:17:56.337223+00:00 end:2024-12-17 16:18:01.708380+00:00
[2024-12-17T21:48:01.709+0530] {logging_mixin.py:190} INFO - [2024-12-17T21:48:01.708+0530] {dagrun.py:905} INFO - DagRun Finished: dag_id=movie_pipeline, execution_date=2024-12-17 16:17:56.337223+00:00, run_id=manual__2024-12-17T16:17:56.337223+00:00, run_start_date=2024-12-17 16:17:56.337223+00:00, run_end_date=2024-12-17 16:18:01.708380+00:00, run_duration=5.371157, state=success, external_trigger=False, run_type=manual, data_interval_start=2024-12-16 16:17:56.337223+00:00, data_interval_end=2024-12-17 16:17:56.337223+00:00, dag_hash=None
[2024-12-17T21:48:01.717+0530] {processor.py:925} INFO - DAG(s) 'movie_pipeline' retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T21:48:01.730+0530] {logging_mixin.py:190} INFO - [2024-12-17T21:48:01.729+0530] {dag.py:3239} INFO - Sync 1 DAGs
[2024-12-17T21:48:01.754+0530] {logging_mixin.py:190} INFO - [2024-12-17T21:48:01.753+0530] {dag.py:4180} INFO - Setting next_dagrun for movie_pipeline to 2024-12-17 01:30:00+00:00, run_after=2024-12-18 01:30:00+00:00
[2024-12-17T21:48:01.774+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 5.485 seconds
[2024-12-17T21:50:52.538+0530] {processor.py:186} INFO - Started process (PID=97116) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T21:50:52.541+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-17T21:50:52.543+0530] {logging_mixin.py:190} INFO - [2024-12-17T21:50:52.542+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T21:50:52.712+0530] {logging_mixin.py:190} INFO - [2024-12-17T21:50:52.711+0530] {dag.py:4435} INFO - dagrun id: movie_pipeline
[2024-12-17T21:50:52.726+0530] {logging_mixin.py:190} INFO - [2024-12-17T21:50:52.725+0530] {dag.py:4451} INFO - created dagrun <DagRun movie_pipeline @ 2024-12-17 16:20:52.584151+00:00: manual__2024-12-17T16:20:52.584151+00:00, state:running, queued_at: None. externally triggered: False>
[2024-12-17T21:50:52.756+0530] {logging_mixin.py:190} INFO - [2024-12-17T21:50:52.755+0530] {dag.py:4396} INFO - [DAG TEST] starting task_id=scrape_collection_page map_index=-1
[2024-12-17T21:50:52.756+0530] {logging_mixin.py:190} INFO - [2024-12-17T21:50:52.756+0530] {dag.py:4399} INFO - [DAG TEST] running task <TaskInstance: movie_pipeline.scrape_collection_page manual__2024-12-17T16:20:52.584151+00:00 [scheduled]>
[2024-12-17T21:50:57.858+0530] {logging_mixin.py:190} INFO - [2024-12-17T21:50:57.858+0530] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-12-17T21:50:57.934+0530] {logging_mixin.py:190} INFO - [2024-12-17 21:50:57,934] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T16:20:52.584151+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T16:20:52.584151+00:00'
[2024-12-17T21:50:57.934+0530] {logging_mixin.py:190} INFO - [2024-12-17T21:50:57.934+0530] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T16:20:52.584151+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T16:20:52.584151+00:00'
[2024-12-17T21:50:57.947+0530] {logging_mixin.py:190} INFO - Task instance is in running state
[2024-12-17T21:50:57.947+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: queued
[2024-12-17T21:50:57.948+0530] {logging_mixin.py:190} INFO - Current task name:scrape_collection_page state:scheduled start_date:None
[2024-12-17T21:50:57.948+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline and current dag run status:running
[2024-12-17T21:50:57.948+0530] {logging_mixin.py:190} INFO - [2024-12-17T21:50:57.948+0530] {taskinstance.py:731} INFO - ::endgroup::
[2024-12-17T21:50:57.952+0530] {logging_mixin.py:190} INFO - 2024-12-17
[2024-12-17T21:50:57.952+0530] {logging_mixin.py:190} INFO - [2024-12-17 21:50:57,952] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T21:50:57.953+0530] {logging_mixin.py:190} INFO - [2024-12-17T21:50:57.952+0530] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T21:50:57.958+0530] {logging_mixin.py:190} INFO - [2024-12-17T21:50:57.958+0530] {taskinstance.py:340} INFO - ::group::Post task execution logs
[2024-12-17T21:50:57.959+0530] {logging_mixin.py:190} INFO - [2024-12-17T21:50:57.959+0530] {taskinstance.py:352} INFO - Marking task as SUCCESS. dag_id=movie_pipeline, task_id=scrape_collection_page, run_id=manual__2024-12-17T16:20:52.584151+00:00, execution_date=20241217T162052, start_date=, end_date=20241217T162057
[2024-12-17T21:50:57.967+0530] {logging_mixin.py:190} INFO - Task instance in success state
[2024-12-17T21:50:57.968+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: running
[2024-12-17T21:50:57.968+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline queued_at:None
[2024-12-17T21:50:57.968+0530] {logging_mixin.py:190} INFO - Task hostname:Jayeshs-MacBook-Air.local operator:PythonOperator
[2024-12-17T21:50:57.968+0530] {logging_mixin.py:190} INFO - [2024-12-17T21:50:57.968+0530] {dag.py:4410} INFO - [DAG TEST] end task task_id=scrape_collection_page map_index=-1
[2024-12-17T21:50:57.976+0530] {logging_mixin.py:190} INFO - [2024-12-17T21:50:57.974+0530] {dagrun.py:854} INFO - Marking run <DagRun movie_pipeline @ 2024-12-17 16:20:52.584151+00:00: manual__2024-12-17T16:20:52.584151+00:00, state:running, queued_at: None. externally triggered: False> successful
[2024-12-17T21:50:57.977+0530] {logging_mixin.py:190} INFO - Dag run in success state
[2024-12-17T21:50:57.977+0530] {logging_mixin.py:190} INFO - Dag run start:2024-12-17 16:20:52.584151+00:00 end:2024-12-17 16:20:57.976940+00:00
[2024-12-17T21:50:57.977+0530] {logging_mixin.py:190} INFO - [2024-12-17T21:50:57.977+0530] {dagrun.py:905} INFO - DagRun Finished: dag_id=movie_pipeline, execution_date=2024-12-17 16:20:52.584151+00:00, run_id=manual__2024-12-17T16:20:52.584151+00:00, run_start_date=2024-12-17 16:20:52.584151+00:00, run_end_date=2024-12-17 16:20:57.976940+00:00, run_duration=5.392789, state=success, external_trigger=False, run_type=manual, data_interval_start=2024-12-16 16:20:52.584151+00:00, data_interval_end=2024-12-17 16:20:52.584151+00:00, dag_hash=None
[2024-12-17T21:50:57.987+0530] {processor.py:925} INFO - DAG(s) 'movie_pipeline' retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T21:50:58.003+0530] {logging_mixin.py:190} INFO - [2024-12-17T21:50:58.002+0530] {dag.py:3239} INFO - Sync 1 DAGs
[2024-12-17T21:50:58.021+0530] {logging_mixin.py:190} INFO - [2024-12-17T21:50:58.020+0530] {dag.py:4180} INFO - Setting next_dagrun for movie_pipeline to 2024-12-17 01:30:00+00:00, run_after=2024-12-18 01:30:00+00:00
[2024-12-17T21:50:58.040+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 5.507 seconds
[2024-12-17T21:53:47.037+0530] {processor.py:186} INFO - Started process (PID=97317) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T21:53:47.039+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-17T21:53:47.041+0530] {logging_mixin.py:190} INFO - [2024-12-17T21:53:47.041+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T21:53:47.210+0530] {logging_mixin.py:190} INFO - [2024-12-17T21:53:47.210+0530] {dag.py:4435} INFO - dagrun id: movie_pipeline
[2024-12-17T21:53:47.224+0530] {logging_mixin.py:190} INFO - [2024-12-17T21:53:47.224+0530] {dag.py:4451} INFO - created dagrun <DagRun movie_pipeline @ 2024-12-17 16:23:47.081466+00:00: manual__2024-12-17T16:23:47.081466+00:00, state:running, queued_at: None. externally triggered: False>
[2024-12-17T21:53:47.258+0530] {logging_mixin.py:190} INFO - [2024-12-17T21:53:47.257+0530] {dag.py:4396} INFO - [DAG TEST] starting task_id=scrape_collection_page map_index=-1
[2024-12-17T21:53:47.258+0530] {logging_mixin.py:190} INFO - [2024-12-17T21:53:47.258+0530] {dag.py:4399} INFO - [DAG TEST] running task <TaskInstance: movie_pipeline.scrape_collection_page manual__2024-12-17T16:23:47.081466+00:00 [scheduled]>
[2024-12-17T21:53:52.356+0530] {logging_mixin.py:190} INFO - [2024-12-17T21:53:52.355+0530] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-12-17T21:53:52.417+0530] {logging_mixin.py:190} INFO - [2024-12-17 21:53:52,417] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T16:23:47.081466+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T16:23:47.081466+00:00'
[2024-12-17T21:53:52.418+0530] {logging_mixin.py:190} INFO - [2024-12-17T21:53:52.417+0530] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T16:23:47.081466+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T16:23:47.081466+00:00'
[2024-12-17T21:53:52.431+0530] {logging_mixin.py:190} INFO - Task instance is in running state
[2024-12-17T21:53:52.432+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: queued
[2024-12-17T21:53:52.432+0530] {logging_mixin.py:190} INFO - Current task name:scrape_collection_page state:scheduled start_date:None
[2024-12-17T21:53:52.432+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline and current dag run status:running
[2024-12-17T21:53:52.434+0530] {logging_mixin.py:190} INFO - [2024-12-17T21:53:52.433+0530] {taskinstance.py:731} INFO - ::endgroup::
[2024-12-17T21:53:52.436+0530] {logging_mixin.py:190} INFO - 2024-12-17
[2024-12-17T21:53:52.436+0530] {logging_mixin.py:190} INFO - [2024-12-17 21:53:52,436] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T21:53:52.437+0530] {logging_mixin.py:190} INFO - [2024-12-17T21:53:52.436+0530] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T21:53:52.442+0530] {logging_mixin.py:190} INFO - [2024-12-17T21:53:52.442+0530] {taskinstance.py:340} INFO - ::group::Post task execution logs
[2024-12-17T21:53:52.443+0530] {logging_mixin.py:190} INFO - [2024-12-17T21:53:52.443+0530] {taskinstance.py:352} INFO - Marking task as SUCCESS. dag_id=movie_pipeline, task_id=scrape_collection_page, run_id=manual__2024-12-17T16:23:47.081466+00:00, execution_date=20241217T162347, start_date=, end_date=20241217T162352
[2024-12-17T21:53:52.452+0530] {logging_mixin.py:190} INFO - Task instance in success state
[2024-12-17T21:53:52.452+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: running
[2024-12-17T21:53:52.453+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline queued_at:None
[2024-12-17T21:53:52.453+0530] {logging_mixin.py:190} INFO - Task hostname:Jayeshs-MacBook-Air.local operator:PythonOperator
[2024-12-17T21:53:52.453+0530] {logging_mixin.py:190} INFO - [2024-12-17T21:53:52.453+0530] {dag.py:4410} INFO - [DAG TEST] end task task_id=scrape_collection_page map_index=-1
[2024-12-17T21:53:52.457+0530] {logging_mixin.py:190} INFO - [2024-12-17T21:53:52.457+0530] {dagrun.py:854} INFO - Marking run <DagRun movie_pipeline @ 2024-12-17 16:23:47.081466+00:00: manual__2024-12-17T16:23:47.081466+00:00, state:running, queued_at: None. externally triggered: False> successful
[2024-12-17T21:53:52.457+0530] {logging_mixin.py:190} INFO - Dag run in success state
[2024-12-17T21:53:52.458+0530] {logging_mixin.py:190} INFO - Dag run start:2024-12-17 16:23:47.081466+00:00 end:2024-12-17 16:23:52.457805+00:00
[2024-12-17T21:53:52.458+0530] {logging_mixin.py:190} INFO - [2024-12-17T21:53:52.458+0530] {dagrun.py:905} INFO - DagRun Finished: dag_id=movie_pipeline, execution_date=2024-12-17 16:23:47.081466+00:00, run_id=manual__2024-12-17T16:23:47.081466+00:00, run_start_date=2024-12-17 16:23:47.081466+00:00, run_end_date=2024-12-17 16:23:52.457805+00:00, run_duration=5.376339, state=success, external_trigger=False, run_type=manual, data_interval_start=2024-12-16 16:23:47.081466+00:00, data_interval_end=2024-12-17 16:23:47.081466+00:00, dag_hash=None
[2024-12-17T21:53:52.467+0530] {processor.py:925} INFO - DAG(s) 'movie_pipeline' retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T21:53:52.483+0530] {logging_mixin.py:190} INFO - [2024-12-17T21:53:52.482+0530] {dag.py:3239} INFO - Sync 1 DAGs
[2024-12-17T21:53:52.504+0530] {logging_mixin.py:190} INFO - [2024-12-17T21:53:52.503+0530] {dag.py:4180} INFO - Setting next_dagrun for movie_pipeline to 2024-12-17 01:30:00+00:00, run_after=2024-12-18 01:30:00+00:00
[2024-12-17T21:53:52.523+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 5.490 seconds
[2024-12-17T21:56:40.543+0530] {processor.py:186} INFO - Started process (PID=97538) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T21:56:40.546+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-17T21:56:40.547+0530] {logging_mixin.py:190} INFO - [2024-12-17T21:56:40.547+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T21:56:40.715+0530] {logging_mixin.py:190} INFO - [2024-12-17T21:56:40.715+0530] {dag.py:4435} INFO - dagrun id: movie_pipeline
[2024-12-17T21:56:40.731+0530] {logging_mixin.py:190} INFO - [2024-12-17T21:56:40.730+0530] {dag.py:4451} INFO - created dagrun <DagRun movie_pipeline @ 2024-12-17 16:26:40.589350+00:00: manual__2024-12-17T16:26:40.589350+00:00, state:running, queued_at: None. externally triggered: False>
[2024-12-17T21:56:40.762+0530] {logging_mixin.py:190} INFO - [2024-12-17T21:56:40.761+0530] {dag.py:4396} INFO - [DAG TEST] starting task_id=scrape_collection_page map_index=-1
[2024-12-17T21:56:40.762+0530] {logging_mixin.py:190} INFO - [2024-12-17T21:56:40.762+0530] {dag.py:4399} INFO - [DAG TEST] running task <TaskInstance: movie_pipeline.scrape_collection_page manual__2024-12-17T16:26:40.589350+00:00 [scheduled]>
[2024-12-17T21:56:45.906+0530] {logging_mixin.py:190} INFO - [2024-12-17T21:56:45.905+0530] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-12-17T21:56:45.996+0530] {logging_mixin.py:190} INFO - [2024-12-17 21:56:45,996] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T16:26:40.589350+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T16:26:40.589350+00:00'
[2024-12-17T21:56:45.996+0530] {logging_mixin.py:190} INFO - [2024-12-17T21:56:45.996+0530] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T16:26:40.589350+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T16:26:40.589350+00:00'
[2024-12-17T21:56:46.010+0530] {logging_mixin.py:190} INFO - Task instance is in running state
[2024-12-17T21:56:46.010+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: queued
[2024-12-17T21:56:46.011+0530] {logging_mixin.py:190} INFO - Current task name:scrape_collection_page state:scheduled start_date:None
[2024-12-17T21:56:46.011+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline and current dag run status:running
[2024-12-17T21:56:46.011+0530] {logging_mixin.py:190} INFO - [2024-12-17T21:56:46.011+0530] {taskinstance.py:731} INFO - ::endgroup::
[2024-12-17T21:56:46.015+0530] {logging_mixin.py:190} INFO - 2024-12-17
[2024-12-17T21:56:46.016+0530] {logging_mixin.py:190} INFO - [2024-12-17 21:56:46,015] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T21:56:46.016+0530] {logging_mixin.py:190} INFO - [2024-12-17T21:56:46.015+0530] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T21:56:46.022+0530] {logging_mixin.py:190} INFO - [2024-12-17T21:56:46.022+0530] {taskinstance.py:340} INFO - ::group::Post task execution logs
[2024-12-17T21:56:46.022+0530] {logging_mixin.py:190} INFO - [2024-12-17T21:56:46.022+0530] {taskinstance.py:352} INFO - Marking task as SUCCESS. dag_id=movie_pipeline, task_id=scrape_collection_page, run_id=manual__2024-12-17T16:26:40.589350+00:00, execution_date=20241217T162640, start_date=, end_date=20241217T162646
[2024-12-17T21:56:46.033+0530] {logging_mixin.py:190} INFO - Task instance in success state
[2024-12-17T21:56:46.033+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: running
[2024-12-17T21:56:46.034+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline queued_at:None
[2024-12-17T21:56:46.034+0530] {logging_mixin.py:190} INFO - Task hostname:Jayeshs-MacBook-Air.local operator:PythonOperator
[2024-12-17T21:56:46.034+0530] {logging_mixin.py:190} INFO - [2024-12-17T21:56:46.034+0530] {dag.py:4410} INFO - [DAG TEST] end task task_id=scrape_collection_page map_index=-1
[2024-12-17T21:56:46.038+0530] {logging_mixin.py:190} INFO - [2024-12-17T21:56:46.038+0530] {dagrun.py:854} INFO - Marking run <DagRun movie_pipeline @ 2024-12-17 16:26:40.589350+00:00: manual__2024-12-17T16:26:40.589350+00:00, state:running, queued_at: None. externally triggered: False> successful
[2024-12-17T21:56:46.039+0530] {logging_mixin.py:190} INFO - Dag run in success state
[2024-12-17T21:56:46.039+0530] {logging_mixin.py:190} INFO - Dag run start:2024-12-17 16:26:40.589350+00:00 end:2024-12-17 16:26:46.039178+00:00
[2024-12-17T21:56:46.039+0530] {logging_mixin.py:190} INFO - [2024-12-17T21:56:46.039+0530] {dagrun.py:905} INFO - DagRun Finished: dag_id=movie_pipeline, execution_date=2024-12-17 16:26:40.589350+00:00, run_id=manual__2024-12-17T16:26:40.589350+00:00, run_start_date=2024-12-17 16:26:40.589350+00:00, run_end_date=2024-12-17 16:26:46.039178+00:00, run_duration=5.449828, state=success, external_trigger=False, run_type=manual, data_interval_start=2024-12-16 16:26:40.589350+00:00, data_interval_end=2024-12-17 16:26:40.589350+00:00, dag_hash=None
[2024-12-17T21:56:46.049+0530] {processor.py:925} INFO - DAG(s) 'movie_pipeline' retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T21:56:46.061+0530] {logging_mixin.py:190} INFO - [2024-12-17T21:56:46.061+0530] {dag.py:3239} INFO - Sync 1 DAGs
[2024-12-17T21:56:46.079+0530] {logging_mixin.py:190} INFO - [2024-12-17T21:56:46.078+0530] {dag.py:4180} INFO - Setting next_dagrun for movie_pipeline to 2024-12-17 01:30:00+00:00, run_after=2024-12-18 01:30:00+00:00
[2024-12-17T21:56:46.096+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 5.556 seconds
[2024-12-17T21:59:42.408+0530] {processor.py:186} INFO - Started process (PID=97741) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T21:59:42.420+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-17T21:59:42.423+0530] {logging_mixin.py:190} INFO - [2024-12-17T21:59:42.422+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T21:59:42.850+0530] {logging_mixin.py:190} INFO - [2024-12-17T21:59:42.850+0530] {dag.py:4435} INFO - dagrun id: movie_pipeline
[2024-12-17T21:59:42.866+0530] {logging_mixin.py:190} INFO - [2024-12-17T21:59:42.865+0530] {dag.py:4451} INFO - created dagrun <DagRun movie_pipeline @ 2024-12-17 16:29:42.691247+00:00: manual__2024-12-17T16:29:42.691247+00:00, state:running, queued_at: None. externally triggered: False>
[2024-12-17T21:59:42.897+0530] {logging_mixin.py:190} INFO - [2024-12-17T21:59:42.897+0530] {dag.py:4396} INFO - [DAG TEST] starting task_id=scrape_collection_page map_index=-1
[2024-12-17T21:59:42.898+0530] {logging_mixin.py:190} INFO - [2024-12-17T21:59:42.898+0530] {dag.py:4399} INFO - [DAG TEST] running task <TaskInstance: movie_pipeline.scrape_collection_page manual__2024-12-17T16:29:42.691247+00:00 [scheduled]>
[2024-12-17T21:59:48.157+0530] {logging_mixin.py:190} INFO - [2024-12-17T21:59:48.157+0530] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-12-17T21:59:48.334+0530] {logging_mixin.py:190} INFO - [2024-12-17 21:59:48,334] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T16:29:42.691247+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T16:29:42.691247+00:00'
[2024-12-17T21:59:48.343+0530] {logging_mixin.py:190} INFO - [2024-12-17T21:59:48.334+0530] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T16:29:42.691247+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T16:29:42.691247+00:00'
[2024-12-17T21:59:48.366+0530] {logging_mixin.py:190} INFO - Task instance is in running state
[2024-12-17T21:59:48.366+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: queued
[2024-12-17T21:59:48.367+0530] {logging_mixin.py:190} INFO - Current task name:scrape_collection_page state:scheduled start_date:None
[2024-12-17T21:59:48.374+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline and current dag run status:running
[2024-12-17T21:59:48.375+0530] {logging_mixin.py:190} INFO - [2024-12-17T21:59:48.375+0530] {taskinstance.py:731} INFO - ::endgroup::
[2024-12-17T21:59:48.379+0530] {logging_mixin.py:190} INFO - 2024-12-17
[2024-12-17T21:59:48.381+0530] {logging_mixin.py:190} INFO - [2024-12-17 21:59:48,380] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T21:59:48.381+0530] {logging_mixin.py:190} INFO - [2024-12-17T21:59:48.380+0530] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T21:59:48.395+0530] {logging_mixin.py:190} INFO - [2024-12-17T21:59:48.394+0530] {taskinstance.py:340} INFO - ::group::Post task execution logs
[2024-12-17T21:59:48.396+0530] {logging_mixin.py:190} INFO - [2024-12-17T21:59:48.396+0530] {taskinstance.py:352} INFO - Marking task as SUCCESS. dag_id=movie_pipeline, task_id=scrape_collection_page, run_id=manual__2024-12-17T16:29:42.691247+00:00, execution_date=20241217T162942, start_date=, end_date=20241217T162948
[2024-12-17T21:59:48.416+0530] {logging_mixin.py:190} INFO - Task instance in success state
[2024-12-17T21:59:48.416+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: running
[2024-12-17T21:59:48.417+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline queued_at:None
[2024-12-17T21:59:48.417+0530] {logging_mixin.py:190} INFO - Task hostname:Jayeshs-MacBook-Air.local operator:PythonOperator
[2024-12-17T21:59:48.417+0530] {logging_mixin.py:190} INFO - [2024-12-17T21:59:48.417+0530] {dag.py:4410} INFO - [DAG TEST] end task task_id=scrape_collection_page map_index=-1
[2024-12-17T21:59:48.431+0530] {logging_mixin.py:190} INFO - [2024-12-17T21:59:48.431+0530] {dagrun.py:854} INFO - Marking run <DagRun movie_pipeline @ 2024-12-17 16:29:42.691247+00:00: manual__2024-12-17T16:29:42.691247+00:00, state:running, queued_at: None. externally triggered: False> successful
[2024-12-17T21:59:48.432+0530] {logging_mixin.py:190} INFO - Dag run in success state
[2024-12-17T21:59:48.432+0530] {logging_mixin.py:190} INFO - Dag run start:2024-12-17 16:29:42.691247+00:00 end:2024-12-17 16:29:48.432333+00:00
[2024-12-17T21:59:48.433+0530] {logging_mixin.py:190} INFO - [2024-12-17T21:59:48.433+0530] {dagrun.py:905} INFO - DagRun Finished: dag_id=movie_pipeline, execution_date=2024-12-17 16:29:42.691247+00:00, run_id=manual__2024-12-17T16:29:42.691247+00:00, run_start_date=2024-12-17 16:29:42.691247+00:00, run_end_date=2024-12-17 16:29:48.432333+00:00, run_duration=5.741086, state=success, external_trigger=False, run_type=manual, data_interval_start=2024-12-16 16:29:42.691247+00:00, data_interval_end=2024-12-17 16:29:42.691247+00:00, dag_hash=None
[2024-12-17T21:59:48.450+0530] {processor.py:925} INFO - DAG(s) 'movie_pipeline' retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T21:59:48.483+0530] {logging_mixin.py:190} INFO - [2024-12-17T21:59:48.482+0530] {dag.py:3239} INFO - Sync 1 DAGs
[2024-12-17T21:59:48.531+0530] {logging_mixin.py:190} INFO - [2024-12-17T21:59:48.530+0530] {dag.py:4180} INFO - Setting next_dagrun for movie_pipeline to 2024-12-17 01:30:00+00:00, run_after=2024-12-18 01:30:00+00:00
[2024-12-17T21:59:48.577+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 6.173 seconds
[2024-12-17T22:03:13.663+0530] {processor.py:186} INFO - Started process (PID=97972) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T22:03:13.665+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-17T22:03:13.666+0530] {logging_mixin.py:190} INFO - [2024-12-17T22:03:13.666+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T22:03:13.829+0530] {logging_mixin.py:190} INFO - [2024-12-17T22:03:13.829+0530] {dag.py:4435} INFO - dagrun id: movie_pipeline
[2024-12-17T22:03:13.843+0530] {logging_mixin.py:190} INFO - [2024-12-17T22:03:13.842+0530] {dag.py:4451} INFO - created dagrun <DagRun movie_pipeline @ 2024-12-17 16:33:13.705691+00:00: manual__2024-12-17T16:33:13.705691+00:00, state:running, queued_at: None. externally triggered: False>
[2024-12-17T22:03:13.874+0530] {logging_mixin.py:190} INFO - [2024-12-17T22:03:13.873+0530] {dag.py:4396} INFO - [DAG TEST] starting task_id=scrape_collection_page map_index=-1
[2024-12-17T22:03:13.874+0530] {logging_mixin.py:190} INFO - [2024-12-17T22:03:13.874+0530] {dag.py:4399} INFO - [DAG TEST] running task <TaskInstance: movie_pipeline.scrape_collection_page manual__2024-12-17T16:33:13.705691+00:00 [scheduled]>
[2024-12-17T22:03:18.968+0530] {logging_mixin.py:190} INFO - [2024-12-17T22:03:18.968+0530] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-12-17T22:03:19.031+0530] {logging_mixin.py:190} INFO - [2024-12-17 22:03:19,031] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T16:33:13.705691+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T16:33:13.705691+00:00'
[2024-12-17T22:03:19.031+0530] {logging_mixin.py:190} INFO - [2024-12-17T22:03:19.031+0530] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T16:33:13.705691+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T16:33:13.705691+00:00'
[2024-12-17T22:03:19.044+0530] {logging_mixin.py:190} INFO - Task instance is in running state
[2024-12-17T22:03:19.045+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: queued
[2024-12-17T22:03:19.045+0530] {logging_mixin.py:190} INFO - Current task name:scrape_collection_page state:scheduled start_date:None
[2024-12-17T22:03:19.045+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline and current dag run status:running
[2024-12-17T22:03:19.046+0530] {logging_mixin.py:190} INFO - [2024-12-17T22:03:19.046+0530] {taskinstance.py:731} INFO - ::endgroup::
[2024-12-17T22:03:19.049+0530] {logging_mixin.py:190} INFO - 2024-12-17
[2024-12-17T22:03:19.049+0530] {logging_mixin.py:190} INFO - [2024-12-17 22:03:19,049] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T22:03:19.050+0530] {logging_mixin.py:190} INFO - [2024-12-17T22:03:19.049+0530] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T22:03:19.055+0530] {logging_mixin.py:190} INFO - [2024-12-17T22:03:19.055+0530] {taskinstance.py:340} INFO - ::group::Post task execution logs
[2024-12-17T22:03:19.056+0530] {logging_mixin.py:190} INFO - [2024-12-17T22:03:19.056+0530] {taskinstance.py:352} INFO - Marking task as SUCCESS. dag_id=movie_pipeline, task_id=scrape_collection_page, run_id=manual__2024-12-17T16:33:13.705691+00:00, execution_date=20241217T163313, start_date=, end_date=20241217T163319
[2024-12-17T22:03:19.065+0530] {logging_mixin.py:190} INFO - Task instance in success state
[2024-12-17T22:03:19.065+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: running
[2024-12-17T22:03:19.066+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline queued_at:None
[2024-12-17T22:03:19.066+0530] {logging_mixin.py:190} INFO - Task hostname:Jayeshs-MacBook-Air.local operator:PythonOperator
[2024-12-17T22:03:19.066+0530] {logging_mixin.py:190} INFO - [2024-12-17T22:03:19.066+0530] {dag.py:4410} INFO - [DAG TEST] end task task_id=scrape_collection_page map_index=-1
[2024-12-17T22:03:19.070+0530] {logging_mixin.py:190} INFO - [2024-12-17T22:03:19.070+0530] {dagrun.py:854} INFO - Marking run <DagRun movie_pipeline @ 2024-12-17 16:33:13.705691+00:00: manual__2024-12-17T16:33:13.705691+00:00, state:running, queued_at: None. externally triggered: False> successful
[2024-12-17T22:03:19.070+0530] {logging_mixin.py:190} INFO - Dag run in success state
[2024-12-17T22:03:19.071+0530] {logging_mixin.py:190} INFO - Dag run start:2024-12-17 16:33:13.705691+00:00 end:2024-12-17 16:33:19.070535+00:00
[2024-12-17T22:03:19.071+0530] {logging_mixin.py:190} INFO - [2024-12-17T22:03:19.071+0530] {dagrun.py:905} INFO - DagRun Finished: dag_id=movie_pipeline, execution_date=2024-12-17 16:33:13.705691+00:00, run_id=manual__2024-12-17T16:33:13.705691+00:00, run_start_date=2024-12-17 16:33:13.705691+00:00, run_end_date=2024-12-17 16:33:19.070535+00:00, run_duration=5.364844, state=success, external_trigger=False, run_type=manual, data_interval_start=2024-12-16 16:33:13.705691+00:00, data_interval_end=2024-12-17 16:33:13.705691+00:00, dag_hash=None
[2024-12-17T22:03:19.080+0530] {processor.py:925} INFO - DAG(s) 'movie_pipeline' retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T22:03:19.092+0530] {logging_mixin.py:190} INFO - [2024-12-17T22:03:19.091+0530] {dag.py:3239} INFO - Sync 1 DAGs
[2024-12-17T22:03:19.108+0530] {logging_mixin.py:190} INFO - [2024-12-17T22:03:19.108+0530] {dag.py:4180} INFO - Setting next_dagrun for movie_pipeline to 2024-12-17 01:30:00+00:00, run_after=2024-12-18 01:30:00+00:00
[2024-12-17T22:03:19.126+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 5.467 seconds
[2024-12-17T22:05:58.965+0530] {processor.py:186} INFO - Started process (PID=98165) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T22:05:58.966+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-17T22:05:58.967+0530] {logging_mixin.py:190} INFO - [2024-12-17T22:05:58.967+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T22:05:59.130+0530] {logging_mixin.py:190} INFO - [2024-12-17T22:05:59.130+0530] {dag.py:4435} INFO - dagrun id: movie_pipeline
[2024-12-17T22:05:59.144+0530] {logging_mixin.py:190} INFO - [2024-12-17T22:05:59.143+0530] {dag.py:4451} INFO - created dagrun <DagRun movie_pipeline @ 2024-12-17 16:35:59.006517+00:00: manual__2024-12-17T16:35:59.006517+00:00, state:running, queued_at: None. externally triggered: False>
[2024-12-17T22:05:59.169+0530] {logging_mixin.py:190} INFO - [2024-12-17T22:05:59.168+0530] {dag.py:4396} INFO - [DAG TEST] starting task_id=scrape_collection_page map_index=-1
[2024-12-17T22:05:59.169+0530] {logging_mixin.py:190} INFO - [2024-12-17T22:05:59.169+0530] {dag.py:4399} INFO - [DAG TEST] running task <TaskInstance: movie_pipeline.scrape_collection_page manual__2024-12-17T16:35:59.006517+00:00 [scheduled]>
[2024-12-17T22:06:04.266+0530] {logging_mixin.py:190} INFO - [2024-12-17T22:06:04.266+0530] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-12-17T22:06:04.326+0530] {logging_mixin.py:190} INFO - [2024-12-17 22:06:04,326] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T16:35:59.006517+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T16:35:59.006517+00:00'
[2024-12-17T22:06:04.326+0530] {logging_mixin.py:190} INFO - [2024-12-17T22:06:04.326+0530] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T16:35:59.006517+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T16:35:59.006517+00:00'
[2024-12-17T22:06:04.329+0530] {logging_mixin.py:190} INFO - Task instance is in running state
[2024-12-17T22:06:04.330+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: queued
[2024-12-17T22:06:04.330+0530] {logging_mixin.py:190} INFO - Current task name:scrape_collection_page state:scheduled start_date:None
[2024-12-17T22:06:04.330+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline and current dag run status:running
[2024-12-17T22:06:04.331+0530] {logging_mixin.py:190} INFO - [2024-12-17T22:06:04.331+0530] {taskinstance.py:731} INFO - ::endgroup::
[2024-12-17T22:06:04.332+0530] {logging_mixin.py:190} INFO - 2024-12-17
[2024-12-17T22:06:04.333+0530] {logging_mixin.py:190} INFO - [2024-12-17 22:06:04,333] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T22:06:04.333+0530] {logging_mixin.py:190} INFO - [2024-12-17T22:06:04.333+0530] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T22:06:04.339+0530] {logging_mixin.py:190} INFO - [2024-12-17T22:06:04.338+0530] {taskinstance.py:340} INFO - ::group::Post task execution logs
[2024-12-17T22:06:04.339+0530] {logging_mixin.py:190} INFO - [2024-12-17T22:06:04.339+0530] {taskinstance.py:352} INFO - Marking task as SUCCESS. dag_id=movie_pipeline, task_id=scrape_collection_page, run_id=manual__2024-12-17T16:35:59.006517+00:00, execution_date=20241217T163559, start_date=, end_date=20241217T163604
[2024-12-17T22:06:04.349+0530] {logging_mixin.py:190} INFO - Task instance in success state
[2024-12-17T22:06:04.349+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: running
[2024-12-17T22:06:04.349+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline queued_at:None
[2024-12-17T22:06:04.349+0530] {logging_mixin.py:190} INFO - Task hostname:Jayeshs-MacBook-Air.local operator:PythonOperator
[2024-12-17T22:06:04.350+0530] {logging_mixin.py:190} INFO - [2024-12-17T22:06:04.349+0530] {dag.py:4410} INFO - [DAG TEST] end task task_id=scrape_collection_page map_index=-1
[2024-12-17T22:06:04.356+0530] {logging_mixin.py:190} INFO - [2024-12-17T22:06:04.355+0530] {dagrun.py:854} INFO - Marking run <DagRun movie_pipeline @ 2024-12-17 16:35:59.006517+00:00: manual__2024-12-17T16:35:59.006517+00:00, state:running, queued_at: None. externally triggered: False> successful
[2024-12-17T22:06:04.356+0530] {logging_mixin.py:190} INFO - Dag run in success state
[2024-12-17T22:06:04.356+0530] {logging_mixin.py:190} INFO - Dag run start:2024-12-17 16:35:59.006517+00:00 end:2024-12-17 16:36:04.356649+00:00
[2024-12-17T22:06:04.357+0530] {logging_mixin.py:190} INFO - [2024-12-17T22:06:04.357+0530] {dagrun.py:905} INFO - DagRun Finished: dag_id=movie_pipeline, execution_date=2024-12-17 16:35:59.006517+00:00, run_id=manual__2024-12-17T16:35:59.006517+00:00, run_start_date=2024-12-17 16:35:59.006517+00:00, run_end_date=2024-12-17 16:36:04.356649+00:00, run_duration=5.350132, state=success, external_trigger=False, run_type=manual, data_interval_start=2024-12-16 16:35:59.006517+00:00, data_interval_end=2024-12-17 16:35:59.006517+00:00, dag_hash=None
[2024-12-17T22:06:04.366+0530] {processor.py:925} INFO - DAG(s) 'movie_pipeline' retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T22:06:04.377+0530] {logging_mixin.py:190} INFO - [2024-12-17T22:06:04.377+0530] {dag.py:3239} INFO - Sync 1 DAGs
[2024-12-17T22:06:04.394+0530] {logging_mixin.py:190} INFO - [2024-12-17T22:06:04.394+0530] {dag.py:4180} INFO - Setting next_dagrun for movie_pipeline to 2024-12-17 01:30:00+00:00, run_after=2024-12-18 01:30:00+00:00
[2024-12-17T22:06:04.412+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 5.450 seconds
[2024-12-17T22:08:46.272+0530] {processor.py:186} INFO - Started process (PID=98368) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T22:08:46.273+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-17T22:08:46.274+0530] {logging_mixin.py:190} INFO - [2024-12-17T22:08:46.274+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T22:08:46.435+0530] {logging_mixin.py:190} INFO - [2024-12-17T22:08:46.434+0530] {dag.py:4435} INFO - dagrun id: movie_pipeline
[2024-12-17T22:08:46.448+0530] {logging_mixin.py:190} INFO - [2024-12-17T22:08:46.448+0530] {dag.py:4451} INFO - created dagrun <DagRun movie_pipeline @ 2024-12-17 16:38:46.311843+00:00: manual__2024-12-17T16:38:46.311843+00:00, state:running, queued_at: None. externally triggered: False>
[2024-12-17T22:08:46.481+0530] {logging_mixin.py:190} INFO - [2024-12-17T22:08:46.480+0530] {dag.py:4396} INFO - [DAG TEST] starting task_id=scrape_collection_page map_index=-1
[2024-12-17T22:08:46.481+0530] {logging_mixin.py:190} INFO - [2024-12-17T22:08:46.481+0530] {dag.py:4399} INFO - [DAG TEST] running task <TaskInstance: movie_pipeline.scrape_collection_page manual__2024-12-17T16:38:46.311843+00:00 [scheduled]>
[2024-12-17T22:08:51.586+0530] {logging_mixin.py:190} INFO - [2024-12-17T22:08:51.586+0530] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-12-17T22:08:51.645+0530] {logging_mixin.py:190} INFO - [2024-12-17 22:08:51,645] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T16:38:46.311843+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T16:38:46.311843+00:00'
[2024-12-17T22:08:51.646+0530] {logging_mixin.py:190} INFO - [2024-12-17T22:08:51.645+0530] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T16:38:46.311843+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T16:38:46.311843+00:00'
[2024-12-17T22:08:51.648+0530] {logging_mixin.py:190} INFO - Task instance is in running state
[2024-12-17T22:08:51.648+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: queued
[2024-12-17T22:08:51.649+0530] {logging_mixin.py:190} INFO - Current task name:scrape_collection_page state:scheduled start_date:None
[2024-12-17T22:08:51.649+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline and current dag run status:running
[2024-12-17T22:08:51.650+0530] {logging_mixin.py:190} INFO - [2024-12-17T22:08:51.649+0530] {taskinstance.py:731} INFO - ::endgroup::
[2024-12-17T22:08:51.651+0530] {logging_mixin.py:190} INFO - 2024-12-17
[2024-12-17T22:08:51.652+0530] {logging_mixin.py:190} INFO - [2024-12-17 22:08:51,652] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T22:08:51.652+0530] {logging_mixin.py:190} INFO - [2024-12-17T22:08:51.652+0530] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T22:08:51.658+0530] {logging_mixin.py:190} INFO - [2024-12-17T22:08:51.658+0530] {taskinstance.py:340} INFO - ::group::Post task execution logs
[2024-12-17T22:08:51.659+0530] {logging_mixin.py:190} INFO - [2024-12-17T22:08:51.658+0530] {taskinstance.py:352} INFO - Marking task as SUCCESS. dag_id=movie_pipeline, task_id=scrape_collection_page, run_id=manual__2024-12-17T16:38:46.311843+00:00, execution_date=20241217T163846, start_date=, end_date=20241217T163851
[2024-12-17T22:08:51.669+0530] {logging_mixin.py:190} INFO - Task instance in success state
[2024-12-17T22:08:51.670+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: running
[2024-12-17T22:08:51.670+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline queued_at:None
[2024-12-17T22:08:51.670+0530] {logging_mixin.py:190} INFO - Task hostname:Jayeshs-MacBook-Air.local operator:PythonOperator
[2024-12-17T22:08:51.671+0530] {logging_mixin.py:190} INFO - [2024-12-17T22:08:51.670+0530] {dag.py:4410} INFO - [DAG TEST] end task task_id=scrape_collection_page map_index=-1
[2024-12-17T22:08:51.675+0530] {logging_mixin.py:190} INFO - [2024-12-17T22:08:51.674+0530] {dagrun.py:854} INFO - Marking run <DagRun movie_pipeline @ 2024-12-17 16:38:46.311843+00:00: manual__2024-12-17T16:38:46.311843+00:00, state:running, queued_at: None. externally triggered: False> successful
[2024-12-17T22:08:51.675+0530] {logging_mixin.py:190} INFO - Dag run in success state
[2024-12-17T22:08:51.675+0530] {logging_mixin.py:190} INFO - Dag run start:2024-12-17 16:38:46.311843+00:00 end:2024-12-17 16:38:51.675359+00:00
[2024-12-17T22:08:51.676+0530] {logging_mixin.py:190} INFO - [2024-12-17T22:08:51.675+0530] {dagrun.py:905} INFO - DagRun Finished: dag_id=movie_pipeline, execution_date=2024-12-17 16:38:46.311843+00:00, run_id=manual__2024-12-17T16:38:46.311843+00:00, run_start_date=2024-12-17 16:38:46.311843+00:00, run_end_date=2024-12-17 16:38:51.675359+00:00, run_duration=5.363516, state=success, external_trigger=False, run_type=manual, data_interval_start=2024-12-16 16:38:46.311843+00:00, data_interval_end=2024-12-17 16:38:46.311843+00:00, dag_hash=None
[2024-12-17T22:08:51.684+0530] {processor.py:925} INFO - DAG(s) 'movie_pipeline' retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T22:08:51.696+0530] {logging_mixin.py:190} INFO - [2024-12-17T22:08:51.695+0530] {dag.py:3239} INFO - Sync 1 DAGs
[2024-12-17T22:08:51.713+0530] {logging_mixin.py:190} INFO - [2024-12-17T22:08:51.712+0530] {dag.py:4180} INFO - Setting next_dagrun for movie_pipeline to 2024-12-17 01:30:00+00:00, run_after=2024-12-18 01:30:00+00:00
[2024-12-17T22:08:51.732+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 5.464 seconds
[2024-12-17T22:11:34.623+0530] {processor.py:186} INFO - Started process (PID=98559) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T22:11:34.626+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-17T22:11:34.628+0530] {logging_mixin.py:190} INFO - [2024-12-17T22:11:34.628+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T22:11:34.810+0530] {logging_mixin.py:190} INFO - [2024-12-17T22:11:34.810+0530] {dag.py:4435} INFO - dagrun id: movie_pipeline
[2024-12-17T22:11:34.824+0530] {logging_mixin.py:190} INFO - [2024-12-17T22:11:34.823+0530] {dag.py:4451} INFO - created dagrun <DagRun movie_pipeline @ 2024-12-17 16:41:34.678141+00:00: manual__2024-12-17T16:41:34.678141+00:00, state:running, queued_at: None. externally triggered: False>
[2024-12-17T22:11:34.853+0530] {logging_mixin.py:190} INFO - [2024-12-17T22:11:34.852+0530] {dag.py:4396} INFO - [DAG TEST] starting task_id=scrape_collection_page map_index=-1
[2024-12-17T22:11:34.853+0530] {logging_mixin.py:190} INFO - [2024-12-17T22:11:34.853+0530] {dag.py:4399} INFO - [DAG TEST] running task <TaskInstance: movie_pipeline.scrape_collection_page manual__2024-12-17T16:41:34.678141+00:00 [scheduled]>
[2024-12-17T22:11:39.958+0530] {logging_mixin.py:190} INFO - [2024-12-17T22:11:39.958+0530] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-12-17T22:11:40.027+0530] {logging_mixin.py:190} INFO - [2024-12-17 22:11:40,027] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T16:41:34.678141+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T16:41:34.678141+00:00'
[2024-12-17T22:11:40.027+0530] {logging_mixin.py:190} INFO - [2024-12-17T22:11:40.027+0530] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T16:41:34.678141+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T16:41:34.678141+00:00'
[2024-12-17T22:11:40.040+0530] {logging_mixin.py:190} INFO - Task instance is in running state
[2024-12-17T22:11:40.041+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: queued
[2024-12-17T22:11:40.041+0530] {logging_mixin.py:190} INFO - Current task name:scrape_collection_page state:scheduled start_date:None
[2024-12-17T22:11:40.042+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline and current dag run status:running
[2024-12-17T22:11:40.042+0530] {logging_mixin.py:190} INFO - [2024-12-17T22:11:40.042+0530] {taskinstance.py:731} INFO - ::endgroup::
[2024-12-17T22:11:40.045+0530] {logging_mixin.py:190} INFO - 2024-12-17
[2024-12-17T22:11:40.046+0530] {logging_mixin.py:190} INFO - [2024-12-17 22:11:40,046] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T22:11:40.046+0530] {logging_mixin.py:190} INFO - [2024-12-17T22:11:40.046+0530] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T22:11:40.051+0530] {logging_mixin.py:190} INFO - [2024-12-17T22:11:40.051+0530] {taskinstance.py:340} INFO - ::group::Post task execution logs
[2024-12-17T22:11:40.052+0530] {logging_mixin.py:190} INFO - [2024-12-17T22:11:40.052+0530] {taskinstance.py:352} INFO - Marking task as SUCCESS. dag_id=movie_pipeline, task_id=scrape_collection_page, run_id=manual__2024-12-17T16:41:34.678141+00:00, execution_date=20241217T164134, start_date=, end_date=20241217T164140
[2024-12-17T22:11:40.061+0530] {logging_mixin.py:190} INFO - Task instance in success state
[2024-12-17T22:11:40.061+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: running
[2024-12-17T22:11:40.061+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline queued_at:None
[2024-12-17T22:11:40.062+0530] {logging_mixin.py:190} INFO - Task hostname:Jayeshs-MacBook-Air.local operator:PythonOperator
[2024-12-17T22:11:40.062+0530] {logging_mixin.py:190} INFO - [2024-12-17T22:11:40.062+0530] {dag.py:4410} INFO - [DAG TEST] end task task_id=scrape_collection_page map_index=-1
[2024-12-17T22:11:40.066+0530] {logging_mixin.py:190} INFO - [2024-12-17T22:11:40.065+0530] {dagrun.py:854} INFO - Marking run <DagRun movie_pipeline @ 2024-12-17 16:41:34.678141+00:00: manual__2024-12-17T16:41:34.678141+00:00, state:running, queued_at: None. externally triggered: False> successful
[2024-12-17T22:11:40.066+0530] {logging_mixin.py:190} INFO - Dag run in success state
[2024-12-17T22:11:40.066+0530] {logging_mixin.py:190} INFO - Dag run start:2024-12-17 16:41:34.678141+00:00 end:2024-12-17 16:41:40.066421+00:00
[2024-12-17T22:11:40.067+0530] {logging_mixin.py:190} INFO - [2024-12-17T22:11:40.066+0530] {dagrun.py:905} INFO - DagRun Finished: dag_id=movie_pipeline, execution_date=2024-12-17 16:41:34.678141+00:00, run_id=manual__2024-12-17T16:41:34.678141+00:00, run_start_date=2024-12-17 16:41:34.678141+00:00, run_end_date=2024-12-17 16:41:40.066421+00:00, run_duration=5.38828, state=success, external_trigger=False, run_type=manual, data_interval_start=2024-12-16 16:41:34.678141+00:00, data_interval_end=2024-12-17 16:41:34.678141+00:00, dag_hash=None
[2024-12-17T22:11:40.075+0530] {processor.py:925} INFO - DAG(s) 'movie_pipeline' retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T22:11:40.087+0530] {logging_mixin.py:190} INFO - [2024-12-17T22:11:40.086+0530] {dag.py:3239} INFO - Sync 1 DAGs
[2024-12-17T22:11:40.103+0530] {logging_mixin.py:190} INFO - [2024-12-17T22:11:40.103+0530] {dag.py:4180} INFO - Setting next_dagrun for movie_pipeline to 2024-12-17 01:30:00+00:00, run_after=2024-12-18 01:30:00+00:00
[2024-12-17T22:11:40.121+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 5.501 seconds
[2024-12-17T22:14:46.981+0530] {processor.py:186} INFO - Started process (PID=98780) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T22:14:46.984+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-17T22:14:46.986+0530] {logging_mixin.py:190} INFO - [2024-12-17T22:14:46.985+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T22:14:47.154+0530] {logging_mixin.py:190} INFO - [2024-12-17T22:14:47.154+0530] {dag.py:4435} INFO - dagrun id: movie_pipeline
[2024-12-17T22:14:47.168+0530] {logging_mixin.py:190} INFO - [2024-12-17T22:14:47.168+0530] {dag.py:4451} INFO - created dagrun <DagRun movie_pipeline @ 2024-12-17 16:44:47.025845+00:00: manual__2024-12-17T16:44:47.025845+00:00, state:running, queued_at: None. externally triggered: False>
[2024-12-17T22:14:47.201+0530] {logging_mixin.py:190} INFO - [2024-12-17T22:14:47.201+0530] {dag.py:4396} INFO - [DAG TEST] starting task_id=scrape_collection_page map_index=-1
[2024-12-17T22:14:47.201+0530] {logging_mixin.py:190} INFO - [2024-12-17T22:14:47.201+0530] {dag.py:4399} INFO - [DAG TEST] running task <TaskInstance: movie_pipeline.scrape_collection_page manual__2024-12-17T16:44:47.025845+00:00 [scheduled]>
[2024-12-17T22:14:52.308+0530] {logging_mixin.py:190} INFO - [2024-12-17T22:14:52.308+0530] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-12-17T22:14:52.384+0530] {logging_mixin.py:190} INFO - [2024-12-17 22:14:52,384] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T16:44:47.025845+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T16:44:47.025845+00:00'
[2024-12-17T22:14:52.385+0530] {logging_mixin.py:190} INFO - [2024-12-17T22:14:52.384+0530] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T16:44:47.025845+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T16:44:47.025845+00:00'
[2024-12-17T22:14:52.402+0530] {logging_mixin.py:190} INFO - Task instance is in running state
[2024-12-17T22:14:52.402+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: queued
[2024-12-17T22:14:52.403+0530] {logging_mixin.py:190} INFO - Current task name:scrape_collection_page state:scheduled start_date:None
[2024-12-17T22:14:52.403+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline and current dag run status:running
[2024-12-17T22:14:52.404+0530] {logging_mixin.py:190} INFO - [2024-12-17T22:14:52.403+0530] {taskinstance.py:731} INFO - ::endgroup::
[2024-12-17T22:14:52.407+0530] {logging_mixin.py:190} INFO - 2024-12-17
[2024-12-17T22:14:52.408+0530] {logging_mixin.py:190} INFO - [2024-12-17 22:14:52,407] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T22:14:52.408+0530] {logging_mixin.py:190} INFO - [2024-12-17T22:14:52.407+0530] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T22:14:52.413+0530] {logging_mixin.py:190} INFO - [2024-12-17T22:14:52.412+0530] {taskinstance.py:340} INFO - ::group::Post task execution logs
[2024-12-17T22:14:52.413+0530] {logging_mixin.py:190} INFO - [2024-12-17T22:14:52.413+0530] {taskinstance.py:352} INFO - Marking task as SUCCESS. dag_id=movie_pipeline, task_id=scrape_collection_page, run_id=manual__2024-12-17T16:44:47.025845+00:00, execution_date=20241217T164447, start_date=, end_date=20241217T164452
[2024-12-17T22:14:52.423+0530] {logging_mixin.py:190} INFO - Task instance in success state
[2024-12-17T22:14:52.423+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: running
[2024-12-17T22:14:52.423+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline queued_at:None
[2024-12-17T22:14:52.423+0530] {logging_mixin.py:190} INFO - Task hostname:Jayeshs-MacBook-Air.local operator:PythonOperator
[2024-12-17T22:14:52.424+0530] {logging_mixin.py:190} INFO - [2024-12-17T22:14:52.424+0530] {dag.py:4410} INFO - [DAG TEST] end task task_id=scrape_collection_page map_index=-1
[2024-12-17T22:14:52.428+0530] {logging_mixin.py:190} INFO - [2024-12-17T22:14:52.427+0530] {dagrun.py:854} INFO - Marking run <DagRun movie_pipeline @ 2024-12-17 16:44:47.025845+00:00: manual__2024-12-17T16:44:47.025845+00:00, state:running, queued_at: None. externally triggered: False> successful
[2024-12-17T22:14:52.428+0530] {logging_mixin.py:190} INFO - Dag run in success state
[2024-12-17T22:14:52.428+0530] {logging_mixin.py:190} INFO - Dag run start:2024-12-17 16:44:47.025845+00:00 end:2024-12-17 16:44:52.428208+00:00
[2024-12-17T22:14:52.429+0530] {logging_mixin.py:190} INFO - [2024-12-17T22:14:52.428+0530] {dagrun.py:905} INFO - DagRun Finished: dag_id=movie_pipeline, execution_date=2024-12-17 16:44:47.025845+00:00, run_id=manual__2024-12-17T16:44:47.025845+00:00, run_start_date=2024-12-17 16:44:47.025845+00:00, run_end_date=2024-12-17 16:44:52.428208+00:00, run_duration=5.402363, state=success, external_trigger=False, run_type=manual, data_interval_start=2024-12-16 16:44:47.025845+00:00, data_interval_end=2024-12-17 16:44:47.025845+00:00, dag_hash=None
[2024-12-17T22:14:52.438+0530] {processor.py:925} INFO - DAG(s) 'movie_pipeline' retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T22:14:52.449+0530] {logging_mixin.py:190} INFO - [2024-12-17T22:14:52.449+0530] {dag.py:3239} INFO - Sync 1 DAGs
[2024-12-17T22:14:52.466+0530] {logging_mixin.py:190} INFO - [2024-12-17T22:14:52.466+0530] {dag.py:4180} INFO - Setting next_dagrun for movie_pipeline to 2024-12-17 01:30:00+00:00, run_after=2024-12-18 01:30:00+00:00
[2024-12-17T22:14:52.489+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 5.511 seconds
[2024-12-17T22:18:03.919+0530] {processor.py:186} INFO - Started process (PID=98999) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T22:18:03.922+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-17T22:18:03.924+0530] {logging_mixin.py:190} INFO - [2024-12-17T22:18:03.923+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T22:18:04.262+0530] {logging_mixin.py:190} INFO - [2024-12-17T22:18:04.262+0530] {dag.py:4435} INFO - dagrun id: movie_pipeline
[2024-12-17T22:18:04.292+0530] {logging_mixin.py:190} INFO - [2024-12-17T22:18:04.292+0530] {dag.py:4451} INFO - created dagrun <DagRun movie_pipeline @ 2024-12-17 16:48:04.025888+00:00: manual__2024-12-17T16:48:04.025888+00:00, state:running, queued_at: None. externally triggered: False>
[2024-12-17T22:18:04.337+0530] {logging_mixin.py:190} INFO - [2024-12-17T22:18:04.337+0530] {dag.py:4396} INFO - [DAG TEST] starting task_id=scrape_collection_page map_index=-1
[2024-12-17T22:18:04.338+0530] {logging_mixin.py:190} INFO - [2024-12-17T22:18:04.338+0530] {dag.py:4399} INFO - [DAG TEST] running task <TaskInstance: movie_pipeline.scrape_collection_page manual__2024-12-17T16:48:04.025888+00:00 [scheduled]>
[2024-12-17T22:18:09.502+0530] {logging_mixin.py:190} INFO - [2024-12-17T22:18:09.502+0530] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-12-17T22:18:09.620+0530] {logging_mixin.py:190} INFO - [2024-12-17 22:18:09,619] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T16:48:04.025888+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T16:48:04.025888+00:00'
[2024-12-17T22:18:09.620+0530] {logging_mixin.py:190} INFO - [2024-12-17T22:18:09.619+0530] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T16:48:04.025888+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T16:48:04.025888+00:00'
[2024-12-17T22:18:09.629+0530] {logging_mixin.py:190} INFO - Task instance is in running state
[2024-12-17T22:18:09.629+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: queued
[2024-12-17T22:18:09.629+0530] {logging_mixin.py:190} INFO - Current task name:scrape_collection_page state:scheduled start_date:None
[2024-12-17T22:18:09.630+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline and current dag run status:running
[2024-12-17T22:18:09.630+0530] {logging_mixin.py:190} INFO - [2024-12-17T22:18:09.630+0530] {taskinstance.py:731} INFO - ::endgroup::
[2024-12-17T22:18:09.634+0530] {logging_mixin.py:190} INFO - 2024-12-17
[2024-12-17T22:18:09.635+0530] {logging_mixin.py:190} INFO - [2024-12-17 22:18:09,635] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T22:18:09.636+0530] {logging_mixin.py:190} INFO - [2024-12-17T22:18:09.635+0530] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T22:18:09.644+0530] {logging_mixin.py:190} INFO - [2024-12-17T22:18:09.644+0530] {taskinstance.py:340} INFO - ::group::Post task execution logs
[2024-12-17T22:18:09.645+0530] {logging_mixin.py:190} INFO - [2024-12-17T22:18:09.644+0530] {taskinstance.py:352} INFO - Marking task as SUCCESS. dag_id=movie_pipeline, task_id=scrape_collection_page, run_id=manual__2024-12-17T16:48:04.025888+00:00, execution_date=20241217T164804, start_date=, end_date=20241217T164809
[2024-12-17T22:18:09.654+0530] {logging_mixin.py:190} INFO - Task instance in success state
[2024-12-17T22:18:09.655+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: running
[2024-12-17T22:18:09.656+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline queued_at:None
[2024-12-17T22:18:09.657+0530] {logging_mixin.py:190} INFO - Task hostname:Jayeshs-MacBook-Air.local operator:PythonOperator
[2024-12-17T22:18:09.657+0530] {logging_mixin.py:190} INFO - [2024-12-17T22:18:09.657+0530] {dag.py:4410} INFO - [DAG TEST] end task task_id=scrape_collection_page map_index=-1
[2024-12-17T22:18:09.662+0530] {logging_mixin.py:190} INFO - [2024-12-17T22:18:09.662+0530] {dagrun.py:854} INFO - Marking run <DagRun movie_pipeline @ 2024-12-17 16:48:04.025888+00:00: manual__2024-12-17T16:48:04.025888+00:00, state:running, queued_at: None. externally triggered: False> successful
[2024-12-17T22:18:09.662+0530] {logging_mixin.py:190} INFO - Dag run in success state
[2024-12-17T22:18:09.663+0530] {logging_mixin.py:190} INFO - Dag run start:2024-12-17 16:48:04.025888+00:00 end:2024-12-17 16:48:09.662701+00:00
[2024-12-17T22:18:09.663+0530] {logging_mixin.py:190} INFO - [2024-12-17T22:18:09.663+0530] {dagrun.py:905} INFO - DagRun Finished: dag_id=movie_pipeline, execution_date=2024-12-17 16:48:04.025888+00:00, run_id=manual__2024-12-17T16:48:04.025888+00:00, run_start_date=2024-12-17 16:48:04.025888+00:00, run_end_date=2024-12-17 16:48:09.662701+00:00, run_duration=5.636813, state=success, external_trigger=False, run_type=manual, data_interval_start=2024-12-16 16:48:04.025888+00:00, data_interval_end=2024-12-17 16:48:04.025888+00:00, dag_hash=None
[2024-12-17T22:18:09.671+0530] {processor.py:925} INFO - DAG(s) 'movie_pipeline' retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T22:18:09.695+0530] {logging_mixin.py:190} INFO - [2024-12-17T22:18:09.694+0530] {dag.py:3239} INFO - Sync 1 DAGs
[2024-12-17T22:18:09.716+0530] {logging_mixin.py:190} INFO - [2024-12-17T22:18:09.715+0530] {dag.py:4180} INFO - Setting next_dagrun for movie_pipeline to 2024-12-17 01:30:00+00:00, run_after=2024-12-18 01:30:00+00:00
[2024-12-17T22:18:09.739+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 5.824 seconds
[2024-12-17T22:21:34.453+0530] {processor.py:186} INFO - Started process (PID=99242) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T22:21:34.458+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-17T22:21:34.462+0530] {logging_mixin.py:190} INFO - [2024-12-17T22:21:34.461+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T22:21:34.889+0530] {logging_mixin.py:190} INFO - [2024-12-17T22:21:34.889+0530] {dag.py:4435} INFO - dagrun id: movie_pipeline
[2024-12-17T22:21:34.922+0530] {logging_mixin.py:190} INFO - [2024-12-17T22:21:34.921+0530] {dag.py:4451} INFO - created dagrun <DagRun movie_pipeline @ 2024-12-17 16:51:34.577905+00:00: manual__2024-12-17T16:51:34.577905+00:00, state:running, queued_at: None. externally triggered: False>
[2024-12-17T22:21:34.993+0530] {logging_mixin.py:190} INFO - [2024-12-17T22:21:34.991+0530] {dag.py:4396} INFO - [DAG TEST] starting task_id=scrape_collection_page map_index=-1
[2024-12-17T22:21:34.994+0530] {logging_mixin.py:190} INFO - [2024-12-17T22:21:34.994+0530] {dag.py:4399} INFO - [DAG TEST] running task <TaskInstance: movie_pipeline.scrape_collection_page manual__2024-12-17T16:51:34.577905+00:00 [scheduled]>
[2024-12-17T22:21:40.117+0530] {logging_mixin.py:190} INFO - [2024-12-17T22:21:40.116+0530] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-12-17T22:21:40.208+0530] {logging_mixin.py:190} INFO - [2024-12-17 22:21:40,208] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T16:51:34.577905+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T16:51:34.577905+00:00'
[2024-12-17T22:21:40.209+0530] {logging_mixin.py:190} INFO - [2024-12-17T22:21:40.208+0530] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T16:51:34.577905+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T16:51:34.577905+00:00'
[2024-12-17T22:21:40.216+0530] {logging_mixin.py:190} INFO - Task instance is in running state
[2024-12-17T22:21:40.217+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: queued
[2024-12-17T22:21:40.217+0530] {logging_mixin.py:190} INFO - Current task name:scrape_collection_page state:scheduled start_date:None
[2024-12-17T22:21:40.218+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline and current dag run status:running
[2024-12-17T22:21:40.218+0530] {logging_mixin.py:190} INFO - [2024-12-17T22:21:40.218+0530] {taskinstance.py:731} INFO - ::endgroup::
[2024-12-17T22:21:40.224+0530] {logging_mixin.py:190} INFO - 2024-12-17
[2024-12-17T22:21:40.224+0530] {logging_mixin.py:190} INFO - [2024-12-17 22:21:40,224] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T22:21:40.225+0530] {logging_mixin.py:190} INFO - [2024-12-17T22:21:40.224+0530] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T22:21:40.233+0530] {logging_mixin.py:190} INFO - [2024-12-17T22:21:40.233+0530] {taskinstance.py:340} INFO - ::group::Post task execution logs
[2024-12-17T22:21:40.234+0530] {logging_mixin.py:190} INFO - [2024-12-17T22:21:40.234+0530] {taskinstance.py:352} INFO - Marking task as SUCCESS. dag_id=movie_pipeline, task_id=scrape_collection_page, run_id=manual__2024-12-17T16:51:34.577905+00:00, execution_date=20241217T165134, start_date=, end_date=20241217T165140
[2024-12-17T22:21:40.244+0530] {logging_mixin.py:190} INFO - Task instance in success state
[2024-12-17T22:21:40.244+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: running
[2024-12-17T22:21:40.245+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline queued_at:None
[2024-12-17T22:21:40.245+0530] {logging_mixin.py:190} INFO - Task hostname:Jayeshs-MacBook-Air.local operator:PythonOperator
[2024-12-17T22:21:40.245+0530] {logging_mixin.py:190} INFO - [2024-12-17T22:21:40.245+0530] {dag.py:4410} INFO - [DAG TEST] end task task_id=scrape_collection_page map_index=-1
[2024-12-17T22:21:40.252+0530] {logging_mixin.py:190} INFO - [2024-12-17T22:21:40.251+0530] {dagrun.py:854} INFO - Marking run <DagRun movie_pipeline @ 2024-12-17 16:51:34.577905+00:00: manual__2024-12-17T16:51:34.577905+00:00, state:running, queued_at: None. externally triggered: False> successful
[2024-12-17T22:21:40.252+0530] {logging_mixin.py:190} INFO - Dag run in success state
[2024-12-17T22:21:40.252+0530] {logging_mixin.py:190} INFO - Dag run start:2024-12-17 16:51:34.577905+00:00 end:2024-12-17 16:51:40.252512+00:00
[2024-12-17T22:21:40.253+0530] {logging_mixin.py:190} INFO - [2024-12-17T22:21:40.253+0530] {dagrun.py:905} INFO - DagRun Finished: dag_id=movie_pipeline, execution_date=2024-12-17 16:51:34.577905+00:00, run_id=manual__2024-12-17T16:51:34.577905+00:00, run_start_date=2024-12-17 16:51:34.577905+00:00, run_end_date=2024-12-17 16:51:40.252512+00:00, run_duration=5.674607, state=success, external_trigger=False, run_type=manual, data_interval_start=2024-12-16 16:51:34.577905+00:00, data_interval_end=2024-12-17 16:51:34.577905+00:00, dag_hash=None
[2024-12-17T22:21:40.264+0530] {processor.py:925} INFO - DAG(s) 'movie_pipeline' retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T22:21:40.286+0530] {logging_mixin.py:190} INFO - [2024-12-17T22:21:40.285+0530] {dag.py:3239} INFO - Sync 1 DAGs
[2024-12-17T22:21:40.313+0530] {logging_mixin.py:190} INFO - [2024-12-17T22:21:40.313+0530] {dag.py:4180} INFO - Setting next_dagrun for movie_pipeline to 2024-12-17 01:30:00+00:00, run_after=2024-12-18 01:30:00+00:00
[2024-12-17T22:21:40.340+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 5.886 seconds
[2024-12-17T22:25:26.074+0530] {processor.py:186} INFO - Started process (PID=99497) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T22:25:26.079+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-17T22:25:26.083+0530] {logging_mixin.py:190} INFO - [2024-12-17T22:25:26.080+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T22:25:26.393+0530] {logging_mixin.py:190} INFO - [2024-12-17T22:25:26.393+0530] {dag.py:4435} INFO - dagrun id: movie_pipeline
[2024-12-17T22:25:26.411+0530] {logging_mixin.py:190} INFO - [2024-12-17T22:25:26.410+0530] {dag.py:4451} INFO - created dagrun <DagRun movie_pipeline @ 2024-12-17 16:55:26.137652+00:00: manual__2024-12-17T16:55:26.137652+00:00, state:running, queued_at: None. externally triggered: False>
[2024-12-17T22:25:26.462+0530] {logging_mixin.py:190} INFO - [2024-12-17T22:25:26.461+0530] {dag.py:4396} INFO - [DAG TEST] starting task_id=scrape_collection_page map_index=-1
[2024-12-17T22:25:26.462+0530] {logging_mixin.py:190} INFO - [2024-12-17T22:25:26.462+0530] {dag.py:4399} INFO - [DAG TEST] running task <TaskInstance: movie_pipeline.scrape_collection_page manual__2024-12-17T16:55:26.137652+00:00 [scheduled]>
[2024-12-17T22:25:31.591+0530] {logging_mixin.py:190} INFO - [2024-12-17T22:25:31.591+0530] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-12-17T22:25:31.696+0530] {logging_mixin.py:190} INFO - [2024-12-17 22:25:31,695] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T16:55:26.137652+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T16:55:26.137652+00:00'
[2024-12-17T22:25:31.696+0530] {logging_mixin.py:190} INFO - [2024-12-17T22:25:31.695+0530] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T16:55:26.137652+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T16:55:26.137652+00:00'
[2024-12-17T22:25:31.710+0530] {logging_mixin.py:190} INFO - Task instance is in running state
[2024-12-17T22:25:31.711+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: queued
[2024-12-17T22:25:31.712+0530] {logging_mixin.py:190} INFO - Current task name:scrape_collection_page state:scheduled start_date:None
[2024-12-17T22:25:31.712+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline and current dag run status:running
[2024-12-17T22:25:31.713+0530] {logging_mixin.py:190} INFO - [2024-12-17T22:25:31.712+0530] {taskinstance.py:731} INFO - ::endgroup::
[2024-12-17T22:25:31.719+0530] {logging_mixin.py:190} INFO - 2024-12-17
[2024-12-17T22:25:31.719+0530] {logging_mixin.py:190} INFO - [2024-12-17 22:25:31,719] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T22:25:31.720+0530] {logging_mixin.py:190} INFO - [2024-12-17T22:25:31.719+0530] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T22:25:31.725+0530] {logging_mixin.py:190} INFO - [2024-12-17T22:25:31.725+0530] {taskinstance.py:340} INFO - ::group::Post task execution logs
[2024-12-17T22:25:31.726+0530] {logging_mixin.py:190} INFO - [2024-12-17T22:25:31.725+0530] {taskinstance.py:352} INFO - Marking task as SUCCESS. dag_id=movie_pipeline, task_id=scrape_collection_page, run_id=manual__2024-12-17T16:55:26.137652+00:00, execution_date=20241217T165526, start_date=, end_date=20241217T165531
[2024-12-17T22:25:31.736+0530] {logging_mixin.py:190} INFO - Task instance in success state
[2024-12-17T22:25:31.736+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: running
[2024-12-17T22:25:31.737+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline queued_at:None
[2024-12-17T22:25:31.737+0530] {logging_mixin.py:190} INFO - Task hostname:Jayeshs-MacBook-Air.local operator:PythonOperator
[2024-12-17T22:25:31.737+0530] {logging_mixin.py:190} INFO - [2024-12-17T22:25:31.737+0530] {dag.py:4410} INFO - [DAG TEST] end task task_id=scrape_collection_page map_index=-1
[2024-12-17T22:25:31.744+0530] {logging_mixin.py:190} INFO - [2024-12-17T22:25:31.743+0530] {dagrun.py:854} INFO - Marking run <DagRun movie_pipeline @ 2024-12-17 16:55:26.137652+00:00: manual__2024-12-17T16:55:26.137652+00:00, state:running, queued_at: None. externally triggered: False> successful
[2024-12-17T22:25:31.744+0530] {logging_mixin.py:190} INFO - Dag run in success state
[2024-12-17T22:25:31.745+0530] {logging_mixin.py:190} INFO - Dag run start:2024-12-17 16:55:26.137652+00:00 end:2024-12-17 16:55:31.744558+00:00
[2024-12-17T22:25:31.746+0530] {logging_mixin.py:190} INFO - [2024-12-17T22:25:31.746+0530] {dagrun.py:905} INFO - DagRun Finished: dag_id=movie_pipeline, execution_date=2024-12-17 16:55:26.137652+00:00, run_id=manual__2024-12-17T16:55:26.137652+00:00, run_start_date=2024-12-17 16:55:26.137652+00:00, run_end_date=2024-12-17 16:55:31.744558+00:00, run_duration=5.606906, state=success, external_trigger=False, run_type=manual, data_interval_start=2024-12-16 16:55:26.137652+00:00, data_interval_end=2024-12-17 16:55:26.137652+00:00, dag_hash=None
[2024-12-17T22:25:31.757+0530] {processor.py:925} INFO - DAG(s) 'movie_pipeline' retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T22:25:31.770+0530] {logging_mixin.py:190} INFO - [2024-12-17T22:25:31.769+0530] {dag.py:3239} INFO - Sync 1 DAGs
[2024-12-17T22:25:31.801+0530] {logging_mixin.py:190} INFO - [2024-12-17T22:25:31.801+0530] {dag.py:4180} INFO - Setting next_dagrun for movie_pipeline to 2024-12-17 01:30:00+00:00, run_after=2024-12-18 01:30:00+00:00
[2024-12-17T22:25:31.835+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 5.765 seconds
[2024-12-17T22:29:06.629+0530] {processor.py:186} INFO - Started process (PID=99751) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T22:29:06.632+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-17T22:29:06.634+0530] {logging_mixin.py:190} INFO - [2024-12-17T22:29:06.633+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T22:29:06.916+0530] {logging_mixin.py:190} INFO - [2024-12-17T22:29:06.915+0530] {dag.py:4435} INFO - dagrun id: movie_pipeline
[2024-12-17T22:29:06.934+0530] {logging_mixin.py:190} INFO - [2024-12-17T22:29:06.934+0530] {dag.py:4451} INFO - created dagrun <DagRun movie_pipeline @ 2024-12-17 16:59:06.748189+00:00: manual__2024-12-17T16:59:06.748189+00:00, state:running, queued_at: None. externally triggered: False>
[2024-12-17T22:29:06.968+0530] {logging_mixin.py:190} INFO - [2024-12-17T22:29:06.968+0530] {dag.py:4396} INFO - [DAG TEST] starting task_id=scrape_collection_page map_index=-1
[2024-12-17T22:29:06.969+0530] {logging_mixin.py:190} INFO - [2024-12-17T22:29:06.969+0530] {dag.py:4399} INFO - [DAG TEST] running task <TaskInstance: movie_pipeline.scrape_collection_page manual__2024-12-17T16:59:06.748189+00:00 [scheduled]>
[2024-12-17T22:29:12.097+0530] {logging_mixin.py:190} INFO - [2024-12-17T22:29:12.097+0530] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-12-17T22:29:12.242+0530] {logging_mixin.py:190} INFO - [2024-12-17 22:29:12,241] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T16:59:06.748189+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T16:59:06.748189+00:00'
[2024-12-17T22:29:12.243+0530] {logging_mixin.py:190} INFO - [2024-12-17T22:29:12.241+0530] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T16:59:06.748189+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T16:59:06.748189+00:00'
[2024-12-17T22:29:12.259+0530] {logging_mixin.py:190} INFO - Task instance is in running state
[2024-12-17T22:29:12.259+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: queued
[2024-12-17T22:29:12.260+0530] {logging_mixin.py:190} INFO - Current task name:scrape_collection_page state:scheduled start_date:None
[2024-12-17T22:29:12.260+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline and current dag run status:running
[2024-12-17T22:29:12.260+0530] {logging_mixin.py:190} INFO - [2024-12-17T22:29:12.260+0530] {taskinstance.py:731} INFO - ::endgroup::
[2024-12-17T22:29:12.266+0530] {logging_mixin.py:190} INFO - 2024-12-17
[2024-12-17T22:29:12.267+0530] {logging_mixin.py:190} INFO - [2024-12-17 22:29:12,267] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T22:29:12.268+0530] {logging_mixin.py:190} INFO - [2024-12-17T22:29:12.267+0530] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T22:29:12.277+0530] {logging_mixin.py:190} INFO - [2024-12-17T22:29:12.277+0530] {taskinstance.py:340} INFO - ::group::Post task execution logs
[2024-12-17T22:29:12.278+0530] {logging_mixin.py:190} INFO - [2024-12-17T22:29:12.278+0530] {taskinstance.py:352} INFO - Marking task as SUCCESS. dag_id=movie_pipeline, task_id=scrape_collection_page, run_id=manual__2024-12-17T16:59:06.748189+00:00, execution_date=20241217T165906, start_date=, end_date=20241217T165912
[2024-12-17T22:29:12.294+0530] {logging_mixin.py:190} INFO - Task instance in success state
[2024-12-17T22:29:12.294+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: running
[2024-12-17T22:29:12.294+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline queued_at:None
[2024-12-17T22:29:12.294+0530] {logging_mixin.py:190} INFO - Task hostname:Jayeshs-MacBook-Air.local operator:PythonOperator
[2024-12-17T22:29:12.295+0530] {logging_mixin.py:190} INFO - [2024-12-17T22:29:12.295+0530] {dag.py:4410} INFO - [DAG TEST] end task task_id=scrape_collection_page map_index=-1
[2024-12-17T22:29:12.301+0530] {logging_mixin.py:190} INFO - [2024-12-17T22:29:12.301+0530] {dagrun.py:854} INFO - Marking run <DagRun movie_pipeline @ 2024-12-17 16:59:06.748189+00:00: manual__2024-12-17T16:59:06.748189+00:00, state:running, queued_at: None. externally triggered: False> successful
[2024-12-17T22:29:12.302+0530] {logging_mixin.py:190} INFO - Dag run in success state
[2024-12-17T22:29:12.302+0530] {logging_mixin.py:190} INFO - Dag run start:2024-12-17 16:59:06.748189+00:00 end:2024-12-17 16:59:12.301938+00:00
[2024-12-17T22:29:12.302+0530] {logging_mixin.py:190} INFO - [2024-12-17T22:29:12.302+0530] {dagrun.py:905} INFO - DagRun Finished: dag_id=movie_pipeline, execution_date=2024-12-17 16:59:06.748189+00:00, run_id=manual__2024-12-17T16:59:06.748189+00:00, run_start_date=2024-12-17 16:59:06.748189+00:00, run_end_date=2024-12-17 16:59:12.301938+00:00, run_duration=5.553749, state=success, external_trigger=False, run_type=manual, data_interval_start=2024-12-16 16:59:06.748189+00:00, data_interval_end=2024-12-17 16:59:06.748189+00:00, dag_hash=None
[2024-12-17T22:29:12.314+0530] {processor.py:925} INFO - DAG(s) 'movie_pipeline' retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T22:29:12.329+0530] {logging_mixin.py:190} INFO - [2024-12-17T22:29:12.329+0530] {dag.py:3239} INFO - Sync 1 DAGs
[2024-12-17T22:29:12.360+0530] {logging_mixin.py:190} INFO - [2024-12-17T22:29:12.360+0530] {dag.py:4180} INFO - Setting next_dagrun for movie_pipeline to 2024-12-17 01:30:00+00:00, run_after=2024-12-18 01:30:00+00:00
[2024-12-17T22:29:12.394+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 5.768 seconds
[2024-12-17T22:33:17.083+0530] {processor.py:186} INFO - Started process (PID=138) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T22:33:17.085+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-17T22:33:17.088+0530] {logging_mixin.py:190} INFO - [2024-12-17T22:33:17.087+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T22:33:17.416+0530] {logging_mixin.py:190} INFO - [2024-12-17T22:33:17.416+0530] {dag.py:4435} INFO - dagrun id: movie_pipeline
[2024-12-17T22:33:17.437+0530] {logging_mixin.py:190} INFO - [2024-12-17T22:33:17.436+0530] {dag.py:4451} INFO - created dagrun <DagRun movie_pipeline @ 2024-12-17 17:03:17.188207+00:00: manual__2024-12-17T17:03:17.188207+00:00, state:running, queued_at: None. externally triggered: False>
[2024-12-17T22:33:17.483+0530] {logging_mixin.py:190} INFO - [2024-12-17T22:33:17.483+0530] {dag.py:4396} INFO - [DAG TEST] starting task_id=scrape_collection_page map_index=-1
[2024-12-17T22:33:17.483+0530] {logging_mixin.py:190} INFO - [2024-12-17T22:33:17.483+0530] {dag.py:4399} INFO - [DAG TEST] running task <TaskInstance: movie_pipeline.scrape_collection_page manual__2024-12-17T17:03:17.188207+00:00 [scheduled]>
[2024-12-17T22:33:22.668+0530] {logging_mixin.py:190} INFO - [2024-12-17T22:33:22.668+0530] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-12-17T22:33:22.816+0530] {logging_mixin.py:190} INFO - [2024-12-17 22:33:22,816] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T17:03:17.188207+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T17:03:17.188207+00:00'
[2024-12-17T22:33:22.816+0530] {logging_mixin.py:190} INFO - [2024-12-17T22:33:22.816+0530] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T17:03:17.188207+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T17:03:17.188207+00:00'
[2024-12-17T22:33:22.831+0530] {logging_mixin.py:190} INFO - Task instance is in running state
[2024-12-17T22:33:22.832+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: queued
[2024-12-17T22:33:22.832+0530] {logging_mixin.py:190} INFO - Current task name:scrape_collection_page state:scheduled start_date:None
[2024-12-17T22:33:22.832+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline and current dag run status:running
[2024-12-17T22:33:22.832+0530] {logging_mixin.py:190} INFO - [2024-12-17T22:33:22.832+0530] {taskinstance.py:731} INFO - ::endgroup::
[2024-12-17T22:33:22.836+0530] {logging_mixin.py:190} INFO - 2024-12-17
[2024-12-17T22:33:22.836+0530] {logging_mixin.py:190} INFO - [2024-12-17 22:33:22,836] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T22:33:22.838+0530] {logging_mixin.py:190} INFO - [2024-12-17T22:33:22.836+0530] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T22:33:22.843+0530] {logging_mixin.py:190} INFO - [2024-12-17T22:33:22.843+0530] {taskinstance.py:340} INFO - ::group::Post task execution logs
[2024-12-17T22:33:22.844+0530] {logging_mixin.py:190} INFO - [2024-12-17T22:33:22.844+0530] {taskinstance.py:352} INFO - Marking task as SUCCESS. dag_id=movie_pipeline, task_id=scrape_collection_page, run_id=manual__2024-12-17T17:03:17.188207+00:00, execution_date=20241217T170317, start_date=, end_date=20241217T170322
[2024-12-17T22:33:22.856+0530] {logging_mixin.py:190} INFO - Task instance in success state
[2024-12-17T22:33:22.856+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: running
[2024-12-17T22:33:22.856+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline queued_at:None
[2024-12-17T22:33:22.857+0530] {logging_mixin.py:190} INFO - Task hostname:Jayeshs-MacBook-Air.local operator:PythonOperator
[2024-12-17T22:33:22.857+0530] {logging_mixin.py:190} INFO - [2024-12-17T22:33:22.857+0530] {dag.py:4410} INFO - [DAG TEST] end task task_id=scrape_collection_page map_index=-1
[2024-12-17T22:33:22.863+0530] {logging_mixin.py:190} INFO - [2024-12-17T22:33:22.862+0530] {dagrun.py:854} INFO - Marking run <DagRun movie_pipeline @ 2024-12-17 17:03:17.188207+00:00: manual__2024-12-17T17:03:17.188207+00:00, state:running, queued_at: None. externally triggered: False> successful
[2024-12-17T22:33:22.864+0530] {logging_mixin.py:190} INFO - Dag run in success state
[2024-12-17T22:33:22.865+0530] {logging_mixin.py:190} INFO - Dag run start:2024-12-17 17:03:17.188207+00:00 end:2024-12-17 17:03:22.864654+00:00
[2024-12-17T22:33:22.865+0530] {logging_mixin.py:190} INFO - [2024-12-17T22:33:22.865+0530] {dagrun.py:905} INFO - DagRun Finished: dag_id=movie_pipeline, execution_date=2024-12-17 17:03:17.188207+00:00, run_id=manual__2024-12-17T17:03:17.188207+00:00, run_start_date=2024-12-17 17:03:17.188207+00:00, run_end_date=2024-12-17 17:03:22.864654+00:00, run_duration=5.676447, state=success, external_trigger=False, run_type=manual, data_interval_start=2024-12-16 17:03:17.188207+00:00, data_interval_end=2024-12-17 17:03:17.188207+00:00, dag_hash=None
[2024-12-17T22:33:22.875+0530] {processor.py:925} INFO - DAG(s) 'movie_pipeline' retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T22:33:22.888+0530] {logging_mixin.py:190} INFO - [2024-12-17T22:33:22.888+0530] {dag.py:3239} INFO - Sync 1 DAGs
[2024-12-17T22:33:22.910+0530] {logging_mixin.py:190} INFO - [2024-12-17T22:33:22.909+0530] {dag.py:4180} INFO - Setting next_dagrun for movie_pipeline to 2024-12-17 01:30:00+00:00, run_after=2024-12-18 01:30:00+00:00
[2024-12-17T22:33:22.936+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 5.857 seconds
[2024-12-17T22:37:26.050+0530] {processor.py:186} INFO - Started process (PID=682) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T22:37:26.053+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-17T22:37:26.055+0530] {logging_mixin.py:190} INFO - [2024-12-17T22:37:26.054+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T22:37:26.221+0530] {logging_mixin.py:190} INFO - [2024-12-17T22:37:26.221+0530] {dag.py:4435} INFO - dagrun id: movie_pipeline
[2024-12-17T22:37:26.235+0530] {logging_mixin.py:190} INFO - [2024-12-17T22:37:26.234+0530] {dag.py:4451} INFO - created dagrun <DagRun movie_pipeline @ 2024-12-17 17:07:26.094921+00:00: manual__2024-12-17T17:07:26.094921+00:00, state:running, queued_at: None. externally triggered: False>
[2024-12-17T22:37:26.264+0530] {logging_mixin.py:190} INFO - [2024-12-17T22:37:26.264+0530] {dag.py:4396} INFO - [DAG TEST] starting task_id=scrape_collection_page map_index=-1
[2024-12-17T22:37:26.265+0530] {logging_mixin.py:190} INFO - [2024-12-17T22:37:26.265+0530] {dag.py:4399} INFO - [DAG TEST] running task <TaskInstance: movie_pipeline.scrape_collection_page manual__2024-12-17T17:07:26.094921+00:00 [scheduled]>
[2024-12-17T22:37:31.360+0530] {logging_mixin.py:190} INFO - [2024-12-17T22:37:31.359+0530] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-12-17T22:37:31.421+0530] {logging_mixin.py:190} INFO - [2024-12-17 22:37:31,421] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T17:07:26.094921+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T17:07:26.094921+00:00'
[2024-12-17T22:37:31.421+0530] {logging_mixin.py:190} INFO - [2024-12-17T22:37:31.421+0530] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T17:07:26.094921+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T17:07:26.094921+00:00'
[2024-12-17T22:37:31.435+0530] {logging_mixin.py:190} INFO - Task instance is in running state
[2024-12-17T22:37:31.435+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: queued
[2024-12-17T22:37:31.435+0530] {logging_mixin.py:190} INFO - Current task name:scrape_collection_page state:scheduled start_date:None
[2024-12-17T22:37:31.436+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline and current dag run status:running
[2024-12-17T22:37:31.436+0530] {logging_mixin.py:190} INFO - [2024-12-17T22:37:31.436+0530] {taskinstance.py:731} INFO - ::endgroup::
[2024-12-17T22:37:31.440+0530] {logging_mixin.py:190} INFO - 2024-12-17
[2024-12-17T22:37:31.440+0530] {logging_mixin.py:190} INFO - [2024-12-17 22:37:31,440] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T22:37:31.441+0530] {logging_mixin.py:190} INFO - [2024-12-17T22:37:31.440+0530] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T22:37:31.446+0530] {logging_mixin.py:190} INFO - [2024-12-17T22:37:31.445+0530] {taskinstance.py:340} INFO - ::group::Post task execution logs
[2024-12-17T22:37:31.446+0530] {logging_mixin.py:190} INFO - [2024-12-17T22:37:31.446+0530] {taskinstance.py:352} INFO - Marking task as SUCCESS. dag_id=movie_pipeline, task_id=scrape_collection_page, run_id=manual__2024-12-17T17:07:26.094921+00:00, execution_date=20241217T170726, start_date=, end_date=20241217T170731
[2024-12-17T22:37:31.457+0530] {logging_mixin.py:190} INFO - Task instance in success state
[2024-12-17T22:37:31.457+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: running
[2024-12-17T22:37:31.457+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline queued_at:None
[2024-12-17T22:37:31.457+0530] {logging_mixin.py:190} INFO - Task hostname:Jayeshs-MacBook-Air.local operator:PythonOperator
[2024-12-17T22:37:31.458+0530] {logging_mixin.py:190} INFO - [2024-12-17T22:37:31.458+0530] {dag.py:4410} INFO - [DAG TEST] end task task_id=scrape_collection_page map_index=-1
[2024-12-17T22:37:31.462+0530] {logging_mixin.py:190} INFO - [2024-12-17T22:37:31.461+0530] {dagrun.py:854} INFO - Marking run <DagRun movie_pipeline @ 2024-12-17 17:07:26.094921+00:00: manual__2024-12-17T17:07:26.094921+00:00, state:running, queued_at: None. externally triggered: False> successful
[2024-12-17T22:37:31.462+0530] {logging_mixin.py:190} INFO - Dag run in success state
[2024-12-17T22:37:31.463+0530] {logging_mixin.py:190} INFO - Dag run start:2024-12-17 17:07:26.094921+00:00 end:2024-12-17 17:07:31.462649+00:00
[2024-12-17T22:37:31.463+0530] {logging_mixin.py:190} INFO - [2024-12-17T22:37:31.463+0530] {dagrun.py:905} INFO - DagRun Finished: dag_id=movie_pipeline, execution_date=2024-12-17 17:07:26.094921+00:00, run_id=manual__2024-12-17T17:07:26.094921+00:00, run_start_date=2024-12-17 17:07:26.094921+00:00, run_end_date=2024-12-17 17:07:31.462649+00:00, run_duration=5.367728, state=success, external_trigger=False, run_type=manual, data_interval_start=2024-12-16 17:07:26.094921+00:00, data_interval_end=2024-12-17 17:07:26.094921+00:00, dag_hash=None
[2024-12-17T22:37:31.472+0530] {processor.py:925} INFO - DAG(s) 'movie_pipeline' retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T22:37:31.483+0530] {logging_mixin.py:190} INFO - [2024-12-17T22:37:31.483+0530] {dag.py:3239} INFO - Sync 1 DAGs
[2024-12-17T22:37:31.501+0530] {logging_mixin.py:190} INFO - [2024-12-17T22:37:31.501+0530] {dag.py:4180} INFO - Setting next_dagrun for movie_pipeline to 2024-12-17 01:30:00+00:00, run_after=2024-12-18 01:30:00+00:00
[2024-12-17T22:37:31.518+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 5.471 seconds
[2024-12-17T22:40:21.791+0530] {processor.py:186} INFO - Started process (PID=944) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T22:40:21.794+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-17T22:40:21.795+0530] {logging_mixin.py:190} INFO - [2024-12-17T22:40:21.795+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T22:40:21.982+0530] {logging_mixin.py:190} INFO - [2024-12-17T22:40:21.982+0530] {dag.py:4435} INFO - dagrun id: movie_pipeline
[2024-12-17T22:40:21.996+0530] {logging_mixin.py:190} INFO - [2024-12-17T22:40:21.995+0530] {dag.py:4451} INFO - created dagrun <DagRun movie_pipeline @ 2024-12-17 17:10:21.842352+00:00: manual__2024-12-17T17:10:21.842352+00:00, state:running, queued_at: None. externally triggered: False>
[2024-12-17T22:40:22.027+0530] {logging_mixin.py:190} INFO - [2024-12-17T22:40:22.026+0530] {dag.py:4396} INFO - [DAG TEST] starting task_id=scrape_collection_page map_index=-1
[2024-12-17T22:40:22.027+0530] {logging_mixin.py:190} INFO - [2024-12-17T22:40:22.027+0530] {dag.py:4399} INFO - [DAG TEST] running task <TaskInstance: movie_pipeline.scrape_collection_page manual__2024-12-17T17:10:21.842352+00:00 [scheduled]>
[2024-12-17T22:40:27.129+0530] {logging_mixin.py:190} INFO - [2024-12-17T22:40:27.128+0530] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-12-17T22:40:27.191+0530] {logging_mixin.py:190} INFO - [2024-12-17 22:40:27,191] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T17:10:21.842352+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T17:10:21.842352+00:00'
[2024-12-17T22:40:27.192+0530] {logging_mixin.py:190} INFO - [2024-12-17T22:40:27.191+0530] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T17:10:21.842352+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T17:10:21.842352+00:00'
[2024-12-17T22:40:27.204+0530] {logging_mixin.py:190} INFO - Task instance is in running state
[2024-12-17T22:40:27.204+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: queued
[2024-12-17T22:40:27.204+0530] {logging_mixin.py:190} INFO - Current task name:scrape_collection_page state:scheduled start_date:None
[2024-12-17T22:40:27.205+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline and current dag run status:running
[2024-12-17T22:40:27.205+0530] {logging_mixin.py:190} INFO - [2024-12-17T22:40:27.205+0530] {taskinstance.py:731} INFO - ::endgroup::
[2024-12-17T22:40:27.207+0530] {logging_mixin.py:190} INFO - 2024-12-17
[2024-12-17T22:40:27.208+0530] {logging_mixin.py:190} INFO - [2024-12-17 22:40:27,208] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T22:40:27.208+0530] {logging_mixin.py:190} INFO - [2024-12-17T22:40:27.208+0530] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T22:40:27.214+0530] {logging_mixin.py:190} INFO - [2024-12-17T22:40:27.213+0530] {taskinstance.py:340} INFO - ::group::Post task execution logs
[2024-12-17T22:40:27.214+0530] {logging_mixin.py:190} INFO - [2024-12-17T22:40:27.214+0530] {taskinstance.py:352} INFO - Marking task as SUCCESS. dag_id=movie_pipeline, task_id=scrape_collection_page, run_id=manual__2024-12-17T17:10:21.842352+00:00, execution_date=20241217T171021, start_date=, end_date=20241217T171027
[2024-12-17T22:40:27.224+0530] {logging_mixin.py:190} INFO - Task instance in success state
[2024-12-17T22:40:27.224+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: running
[2024-12-17T22:40:27.224+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline queued_at:None
[2024-12-17T22:40:27.225+0530] {logging_mixin.py:190} INFO - Task hostname:Jayeshs-MacBook-Air.local operator:PythonOperator
[2024-12-17T22:40:27.225+0530] {logging_mixin.py:190} INFO - [2024-12-17T22:40:27.225+0530] {dag.py:4410} INFO - [DAG TEST] end task task_id=scrape_collection_page map_index=-1
[2024-12-17T22:40:27.229+0530] {logging_mixin.py:190} INFO - [2024-12-17T22:40:27.228+0530] {dagrun.py:854} INFO - Marking run <DagRun movie_pipeline @ 2024-12-17 17:10:21.842352+00:00: manual__2024-12-17T17:10:21.842352+00:00, state:running, queued_at: None. externally triggered: False> successful
[2024-12-17T22:40:27.229+0530] {logging_mixin.py:190} INFO - Dag run in success state
[2024-12-17T22:40:27.229+0530] {logging_mixin.py:190} INFO - Dag run start:2024-12-17 17:10:21.842352+00:00 end:2024-12-17 17:10:27.229322+00:00
[2024-12-17T22:40:27.230+0530] {logging_mixin.py:190} INFO - [2024-12-17T22:40:27.229+0530] {dagrun.py:905} INFO - DagRun Finished: dag_id=movie_pipeline, execution_date=2024-12-17 17:10:21.842352+00:00, run_id=manual__2024-12-17T17:10:21.842352+00:00, run_start_date=2024-12-17 17:10:21.842352+00:00, run_end_date=2024-12-17 17:10:27.229322+00:00, run_duration=5.38697, state=success, external_trigger=False, run_type=manual, data_interval_start=2024-12-16 17:10:21.842352+00:00, data_interval_end=2024-12-17 17:10:21.842352+00:00, dag_hash=None
[2024-12-17T22:40:27.238+0530] {processor.py:925} INFO - DAG(s) 'movie_pipeline' retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T22:40:27.250+0530] {logging_mixin.py:190} INFO - [2024-12-17T22:40:27.249+0530] {dag.py:3239} INFO - Sync 1 DAGs
[2024-12-17T22:40:27.267+0530] {logging_mixin.py:190} INFO - [2024-12-17T22:40:27.267+0530] {dag.py:4180} INFO - Setting next_dagrun for movie_pipeline to 2024-12-17 01:30:00+00:00, run_after=2024-12-18 01:30:00+00:00
[2024-12-17T22:40:27.285+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 5.497 seconds
[2024-12-17T22:43:01.635+0530] {processor.py:186} INFO - Started process (PID=1171) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T22:43:01.637+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-17T22:43:01.638+0530] {logging_mixin.py:190} INFO - [2024-12-17T22:43:01.638+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T22:43:01.799+0530] {logging_mixin.py:190} INFO - [2024-12-17T22:43:01.799+0530] {dag.py:4435} INFO - dagrun id: movie_pipeline
[2024-12-17T22:43:01.813+0530] {logging_mixin.py:190} INFO - [2024-12-17T22:43:01.812+0530] {dag.py:4451} INFO - created dagrun <DagRun movie_pipeline @ 2024-12-17 17:13:01.676246+00:00: manual__2024-12-17T17:13:01.676246+00:00, state:running, queued_at: None. externally triggered: False>
[2024-12-17T22:43:01.837+0530] {logging_mixin.py:190} INFO - [2024-12-17T22:43:01.837+0530] {dag.py:4396} INFO - [DAG TEST] starting task_id=scrape_collection_page map_index=-1
[2024-12-17T22:43:01.838+0530] {logging_mixin.py:190} INFO - [2024-12-17T22:43:01.837+0530] {dag.py:4399} INFO - [DAG TEST] running task <TaskInstance: movie_pipeline.scrape_collection_page manual__2024-12-17T17:13:01.676246+00:00 [scheduled]>
[2024-12-17T22:43:06.930+0530] {logging_mixin.py:190} INFO - [2024-12-17T22:43:06.930+0530] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-12-17T22:43:06.988+0530] {logging_mixin.py:190} INFO - [2024-12-17 22:43:06,988] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T17:13:01.676246+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T17:13:01.676246+00:00'
[2024-12-17T22:43:06.989+0530] {logging_mixin.py:190} INFO - [2024-12-17T22:43:06.988+0530] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T17:13:01.676246+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T17:13:01.676246+00:00'
[2024-12-17T22:43:06.992+0530] {logging_mixin.py:190} INFO - Task instance is in running state
[2024-12-17T22:43:06.992+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: queued
[2024-12-17T22:43:06.992+0530] {logging_mixin.py:190} INFO - Current task name:scrape_collection_page state:scheduled start_date:None
[2024-12-17T22:43:06.992+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline and current dag run status:running
[2024-12-17T22:43:06.993+0530] {logging_mixin.py:190} INFO - [2024-12-17T22:43:06.993+0530] {taskinstance.py:731} INFO - ::endgroup::
[2024-12-17T22:43:06.994+0530] {logging_mixin.py:190} INFO - 2024-12-17
[2024-12-17T22:43:06.995+0530] {logging_mixin.py:190} INFO - [2024-12-17 22:43:06,995] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T22:43:06.995+0530] {logging_mixin.py:190} INFO - [2024-12-17T22:43:06.995+0530] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T22:43:07.000+0530] {logging_mixin.py:190} INFO - [2024-12-17T22:43:07.000+0530] {taskinstance.py:340} INFO - ::group::Post task execution logs
[2024-12-17T22:43:07.001+0530] {logging_mixin.py:190} INFO - [2024-12-17T22:43:07.000+0530] {taskinstance.py:352} INFO - Marking task as SUCCESS. dag_id=movie_pipeline, task_id=scrape_collection_page, run_id=manual__2024-12-17T17:13:01.676246+00:00, execution_date=20241217T171301, start_date=, end_date=20241217T171307
[2024-12-17T22:43:07.011+0530] {logging_mixin.py:190} INFO - Task instance in success state
[2024-12-17T22:43:07.011+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: running
[2024-12-17T22:43:07.011+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline queued_at:None
[2024-12-17T22:43:07.012+0530] {logging_mixin.py:190} INFO - Task hostname:Jayeshs-MacBook-Air.local operator:PythonOperator
[2024-12-17T22:43:07.012+0530] {logging_mixin.py:190} INFO - [2024-12-17T22:43:07.012+0530] {dag.py:4410} INFO - [DAG TEST] end task task_id=scrape_collection_page map_index=-1
[2024-12-17T22:43:07.018+0530] {logging_mixin.py:190} INFO - [2024-12-17T22:43:07.018+0530] {dagrun.py:854} INFO - Marking run <DagRun movie_pipeline @ 2024-12-17 17:13:01.676246+00:00: manual__2024-12-17T17:13:01.676246+00:00, state:running, queued_at: None. externally triggered: False> successful
[2024-12-17T22:43:07.018+0530] {logging_mixin.py:190} INFO - Dag run in success state
[2024-12-17T22:43:07.018+0530] {logging_mixin.py:190} INFO - Dag run start:2024-12-17 17:13:01.676246+00:00 end:2024-12-17 17:13:07.018613+00:00
[2024-12-17T22:43:07.019+0530] {logging_mixin.py:190} INFO - [2024-12-17T22:43:07.019+0530] {dagrun.py:905} INFO - DagRun Finished: dag_id=movie_pipeline, execution_date=2024-12-17 17:13:01.676246+00:00, run_id=manual__2024-12-17T17:13:01.676246+00:00, run_start_date=2024-12-17 17:13:01.676246+00:00, run_end_date=2024-12-17 17:13:07.018613+00:00, run_duration=5.342367, state=success, external_trigger=False, run_type=manual, data_interval_start=2024-12-16 17:13:01.676246+00:00, data_interval_end=2024-12-17 17:13:01.676246+00:00, dag_hash=None
[2024-12-17T22:43:07.028+0530] {processor.py:925} INFO - DAG(s) 'movie_pipeline' retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T22:43:07.040+0530] {logging_mixin.py:190} INFO - [2024-12-17T22:43:07.039+0530] {dag.py:3239} INFO - Sync 1 DAGs
[2024-12-17T22:43:07.056+0530] {logging_mixin.py:190} INFO - [2024-12-17T22:43:07.056+0530] {dag.py:4180} INFO - Setting next_dagrun for movie_pipeline to 2024-12-17 01:30:00+00:00, run_after=2024-12-18 01:30:00+00:00
[2024-12-17T22:43:07.074+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 5.442 seconds
[2024-12-17T22:45:54.483+0530] {processor.py:186} INFO - Started process (PID=1382) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T22:45:54.484+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-17T22:45:54.485+0530] {logging_mixin.py:190} INFO - [2024-12-17T22:45:54.485+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T22:45:54.659+0530] {logging_mixin.py:190} INFO - [2024-12-17T22:45:54.659+0530] {dag.py:4435} INFO - dagrun id: movie_pipeline
[2024-12-17T22:45:54.673+0530] {logging_mixin.py:190} INFO - [2024-12-17T22:45:54.672+0530] {dag.py:4451} INFO - created dagrun <DagRun movie_pipeline @ 2024-12-17 17:15:54.525247+00:00: manual__2024-12-17T17:15:54.525247+00:00, state:running, queued_at: None. externally triggered: False>
[2024-12-17T22:45:54.698+0530] {logging_mixin.py:190} INFO - [2024-12-17T22:45:54.698+0530] {dag.py:4396} INFO - [DAG TEST] starting task_id=scrape_collection_page map_index=-1
[2024-12-17T22:45:54.699+0530] {logging_mixin.py:190} INFO - [2024-12-17T22:45:54.698+0530] {dag.py:4399} INFO - [DAG TEST] running task <TaskInstance: movie_pipeline.scrape_collection_page manual__2024-12-17T17:15:54.525247+00:00 [scheduled]>
[2024-12-17T22:45:59.796+0530] {logging_mixin.py:190} INFO - [2024-12-17T22:45:59.796+0530] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-12-17T22:45:59.855+0530] {logging_mixin.py:190} INFO - [2024-12-17 22:45:59,855] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T17:15:54.525247+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T17:15:54.525247+00:00'
[2024-12-17T22:45:59.856+0530] {logging_mixin.py:190} INFO - [2024-12-17T22:45:59.855+0530] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T17:15:54.525247+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T17:15:54.525247+00:00'
[2024-12-17T22:45:59.858+0530] {logging_mixin.py:190} INFO - Task instance is in running state
[2024-12-17T22:45:59.858+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: queued
[2024-12-17T22:45:59.859+0530] {logging_mixin.py:190} INFO - Current task name:scrape_collection_page state:scheduled start_date:None
[2024-12-17T22:45:59.859+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline and current dag run status:running
[2024-12-17T22:45:59.860+0530] {logging_mixin.py:190} INFO - [2024-12-17T22:45:59.859+0530] {taskinstance.py:731} INFO - ::endgroup::
[2024-12-17T22:45:59.861+0530] {logging_mixin.py:190} INFO - 2024-12-17
[2024-12-17T22:45:59.862+0530] {logging_mixin.py:190} INFO - [2024-12-17 22:45:59,862] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T22:45:59.863+0530] {logging_mixin.py:190} INFO - [2024-12-17T22:45:59.862+0530] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T22:45:59.868+0530] {logging_mixin.py:190} INFO - [2024-12-17T22:45:59.867+0530] {taskinstance.py:340} INFO - ::group::Post task execution logs
[2024-12-17T22:45:59.868+0530] {logging_mixin.py:190} INFO - [2024-12-17T22:45:59.868+0530] {taskinstance.py:352} INFO - Marking task as SUCCESS. dag_id=movie_pipeline, task_id=scrape_collection_page, run_id=manual__2024-12-17T17:15:54.525247+00:00, execution_date=20241217T171554, start_date=, end_date=20241217T171559
[2024-12-17T22:45:59.878+0530] {logging_mixin.py:190} INFO - Task instance in success state
[2024-12-17T22:45:59.878+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: running
[2024-12-17T22:45:59.878+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline queued_at:None
[2024-12-17T22:45:59.879+0530] {logging_mixin.py:190} INFO - Task hostname:Jayeshs-MacBook-Air.local operator:PythonOperator
[2024-12-17T22:45:59.879+0530] {logging_mixin.py:190} INFO - [2024-12-17T22:45:59.879+0530] {dag.py:4410} INFO - [DAG TEST] end task task_id=scrape_collection_page map_index=-1
[2024-12-17T22:45:59.883+0530] {logging_mixin.py:190} INFO - [2024-12-17T22:45:59.883+0530] {dagrun.py:854} INFO - Marking run <DagRun movie_pipeline @ 2024-12-17 17:15:54.525247+00:00: manual__2024-12-17T17:15:54.525247+00:00, state:running, queued_at: None. externally triggered: False> successful
[2024-12-17T22:45:59.884+0530] {logging_mixin.py:190} INFO - Dag run in success state
[2024-12-17T22:45:59.885+0530] {logging_mixin.py:190} INFO - Dag run start:2024-12-17 17:15:54.525247+00:00 end:2024-12-17 17:15:59.884148+00:00
[2024-12-17T22:45:59.885+0530] {logging_mixin.py:190} INFO - [2024-12-17T22:45:59.885+0530] {dagrun.py:905} INFO - DagRun Finished: dag_id=movie_pipeline, execution_date=2024-12-17 17:15:54.525247+00:00, run_id=manual__2024-12-17T17:15:54.525247+00:00, run_start_date=2024-12-17 17:15:54.525247+00:00, run_end_date=2024-12-17 17:15:59.884148+00:00, run_duration=5.358901, state=success, external_trigger=False, run_type=manual, data_interval_start=2024-12-16 17:15:54.525247+00:00, data_interval_end=2024-12-17 17:15:54.525247+00:00, dag_hash=None
[2024-12-17T22:45:59.895+0530] {processor.py:925} INFO - DAG(s) 'movie_pipeline' retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T22:45:59.907+0530] {logging_mixin.py:190} INFO - [2024-12-17T22:45:59.906+0530] {dag.py:3239} INFO - Sync 1 DAGs
[2024-12-17T22:45:59.924+0530] {logging_mixin.py:190} INFO - [2024-12-17T22:45:59.924+0530] {dag.py:4180} INFO - Setting next_dagrun for movie_pipeline to 2024-12-17 01:30:00+00:00, run_after=2024-12-18 01:30:00+00:00
[2024-12-17T22:45:59.946+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 5.467 seconds
[2024-12-17T22:48:41.433+0530] {processor.py:186} INFO - Started process (PID=1600) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T22:48:41.434+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-17T22:48:41.435+0530] {logging_mixin.py:190} INFO - [2024-12-17T22:48:41.435+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T22:48:41.616+0530] {logging_mixin.py:190} INFO - [2024-12-17T22:48:41.615+0530] {dag.py:4435} INFO - dagrun id: movie_pipeline
[2024-12-17T22:48:41.629+0530] {logging_mixin.py:190} INFO - [2024-12-17T22:48:41.628+0530] {dag.py:4451} INFO - created dagrun <DagRun movie_pipeline @ 2024-12-17 17:18:41.481637+00:00: manual__2024-12-17T17:18:41.481637+00:00, state:running, queued_at: None. externally triggered: False>
[2024-12-17T22:48:41.656+0530] {logging_mixin.py:190} INFO - [2024-12-17T22:48:41.656+0530] {dag.py:4396} INFO - [DAG TEST] starting task_id=scrape_collection_page map_index=-1
[2024-12-17T22:48:41.657+0530] {logging_mixin.py:190} INFO - [2024-12-17T22:48:41.656+0530] {dag.py:4399} INFO - [DAG TEST] running task <TaskInstance: movie_pipeline.scrape_collection_page manual__2024-12-17T17:18:41.481637+00:00 [scheduled]>
[2024-12-17T22:48:46.755+0530] {logging_mixin.py:190} INFO - [2024-12-17T22:48:46.755+0530] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-12-17T22:48:46.815+0530] {logging_mixin.py:190} INFO - [2024-12-17 22:48:46,815] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T17:18:41.481637+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T17:18:41.481637+00:00'
[2024-12-17T22:48:46.816+0530] {logging_mixin.py:190} INFO - [2024-12-17T22:48:46.815+0530] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T17:18:41.481637+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T17:18:41.481637+00:00'
[2024-12-17T22:48:46.818+0530] {logging_mixin.py:190} INFO - Task instance is in running state
[2024-12-17T22:48:46.819+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: queued
[2024-12-17T22:48:46.819+0530] {logging_mixin.py:190} INFO - Current task name:scrape_collection_page state:scheduled start_date:None
[2024-12-17T22:48:46.819+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline and current dag run status:running
[2024-12-17T22:48:46.820+0530] {logging_mixin.py:190} INFO - [2024-12-17T22:48:46.819+0530] {taskinstance.py:731} INFO - ::endgroup::
[2024-12-17T22:48:46.821+0530] {logging_mixin.py:190} INFO - 2024-12-17
[2024-12-17T22:48:46.822+0530] {logging_mixin.py:190} INFO - [2024-12-17 22:48:46,822] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T22:48:46.822+0530] {logging_mixin.py:190} INFO - [2024-12-17T22:48:46.822+0530] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T22:48:46.827+0530] {logging_mixin.py:190} INFO - [2024-12-17T22:48:46.827+0530] {taskinstance.py:340} INFO - ::group::Post task execution logs
[2024-12-17T22:48:46.828+0530] {logging_mixin.py:190} INFO - [2024-12-17T22:48:46.827+0530] {taskinstance.py:352} INFO - Marking task as SUCCESS. dag_id=movie_pipeline, task_id=scrape_collection_page, run_id=manual__2024-12-17T17:18:41.481637+00:00, execution_date=20241217T171841, start_date=, end_date=20241217T171846
[2024-12-17T22:48:46.837+0530] {logging_mixin.py:190} INFO - Task instance in success state
[2024-12-17T22:48:46.837+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: running
[2024-12-17T22:48:46.838+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline queued_at:None
[2024-12-17T22:48:46.838+0530] {logging_mixin.py:190} INFO - Task hostname:Jayeshs-MacBook-Air.local operator:PythonOperator
[2024-12-17T22:48:46.838+0530] {logging_mixin.py:190} INFO - [2024-12-17T22:48:46.838+0530] {dag.py:4410} INFO - [DAG TEST] end task task_id=scrape_collection_page map_index=-1
[2024-12-17T22:48:46.842+0530] {logging_mixin.py:190} INFO - [2024-12-17T22:48:46.842+0530] {dagrun.py:854} INFO - Marking run <DagRun movie_pipeline @ 2024-12-17 17:18:41.481637+00:00: manual__2024-12-17T17:18:41.481637+00:00, state:running, queued_at: None. externally triggered: False> successful
[2024-12-17T22:48:46.842+0530] {logging_mixin.py:190} INFO - Dag run in success state
[2024-12-17T22:48:46.843+0530] {logging_mixin.py:190} INFO - Dag run start:2024-12-17 17:18:41.481637+00:00 end:2024-12-17 17:18:46.842757+00:00
[2024-12-17T22:48:46.843+0530] {logging_mixin.py:190} INFO - [2024-12-17T22:48:46.843+0530] {dagrun.py:905} INFO - DagRun Finished: dag_id=movie_pipeline, execution_date=2024-12-17 17:18:41.481637+00:00, run_id=manual__2024-12-17T17:18:41.481637+00:00, run_start_date=2024-12-17 17:18:41.481637+00:00, run_end_date=2024-12-17 17:18:46.842757+00:00, run_duration=5.36112, state=success, external_trigger=False, run_type=manual, data_interval_start=2024-12-16 17:18:41.481637+00:00, data_interval_end=2024-12-17 17:18:41.481637+00:00, dag_hash=None
[2024-12-17T22:48:46.853+0530] {processor.py:925} INFO - DAG(s) 'movie_pipeline' retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T22:48:46.865+0530] {logging_mixin.py:190} INFO - [2024-12-17T22:48:46.864+0530] {dag.py:3239} INFO - Sync 1 DAGs
[2024-12-17T22:48:46.881+0530] {logging_mixin.py:190} INFO - [2024-12-17T22:48:46.881+0530] {dag.py:4180} INFO - Setting next_dagrun for movie_pipeline to 2024-12-17 01:30:00+00:00, run_after=2024-12-18 01:30:00+00:00
[2024-12-17T22:48:46.899+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 5.470 seconds
[2024-12-17T22:51:22.216+0530] {processor.py:186} INFO - Started process (PID=1785) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T22:51:22.217+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-17T22:51:22.219+0530] {logging_mixin.py:190} INFO - [2024-12-17T22:51:22.218+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T22:51:22.391+0530] {logging_mixin.py:190} INFO - [2024-12-17T22:51:22.390+0530] {dag.py:4435} INFO - dagrun id: movie_pipeline
[2024-12-17T22:51:22.405+0530] {logging_mixin.py:190} INFO - [2024-12-17T22:51:22.405+0530] {dag.py:4451} INFO - created dagrun <DagRun movie_pipeline @ 2024-12-17 17:21:22.260185+00:00: manual__2024-12-17T17:21:22.260185+00:00, state:running, queued_at: None. externally triggered: False>
[2024-12-17T22:51:22.431+0530] {logging_mixin.py:190} INFO - [2024-12-17T22:51:22.431+0530] {dag.py:4396} INFO - [DAG TEST] starting task_id=scrape_collection_page map_index=-1
[2024-12-17T22:51:22.431+0530] {logging_mixin.py:190} INFO - [2024-12-17T22:51:22.431+0530] {dag.py:4399} INFO - [DAG TEST] running task <TaskInstance: movie_pipeline.scrape_collection_page manual__2024-12-17T17:21:22.260185+00:00 [scheduled]>
[2024-12-17T22:51:27.530+0530] {logging_mixin.py:190} INFO - [2024-12-17T22:51:27.530+0530] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-12-17T22:51:27.597+0530] {logging_mixin.py:190} INFO - [2024-12-17 22:51:27,597] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T17:21:22.260185+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T17:21:22.260185+00:00'
[2024-12-17T22:51:27.598+0530] {logging_mixin.py:190} INFO - [2024-12-17T22:51:27.597+0530] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T17:21:22.260185+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T17:21:22.260185+00:00'
[2024-12-17T22:51:27.601+0530] {logging_mixin.py:190} INFO - Task instance is in running state
[2024-12-17T22:51:27.601+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: queued
[2024-12-17T22:51:27.602+0530] {logging_mixin.py:190} INFO - Current task name:scrape_collection_page state:scheduled start_date:None
[2024-12-17T22:51:27.602+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline and current dag run status:running
[2024-12-17T22:51:27.602+0530] {logging_mixin.py:190} INFO - [2024-12-17T22:51:27.602+0530] {taskinstance.py:731} INFO - ::endgroup::
[2024-12-17T22:51:27.604+0530] {logging_mixin.py:190} INFO - 2024-12-17
[2024-12-17T22:51:27.605+0530] {logging_mixin.py:190} INFO - [2024-12-17 22:51:27,604] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T22:51:27.605+0530] {logging_mixin.py:190} INFO - [2024-12-17T22:51:27.604+0530] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T22:51:27.610+0530] {logging_mixin.py:190} INFO - [2024-12-17T22:51:27.610+0530] {taskinstance.py:340} INFO - ::group::Post task execution logs
[2024-12-17T22:51:27.611+0530] {logging_mixin.py:190} INFO - [2024-12-17T22:51:27.611+0530] {taskinstance.py:352} INFO - Marking task as SUCCESS. dag_id=movie_pipeline, task_id=scrape_collection_page, run_id=manual__2024-12-17T17:21:22.260185+00:00, execution_date=20241217T172122, start_date=, end_date=20241217T172127
[2024-12-17T22:51:27.621+0530] {logging_mixin.py:190} INFO - Task instance in success state
[2024-12-17T22:51:27.621+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: running
[2024-12-17T22:51:27.622+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline queued_at:None
[2024-12-17T22:51:27.622+0530] {logging_mixin.py:190} INFO - Task hostname:Jayeshs-MacBook-Air.local operator:PythonOperator
[2024-12-17T22:51:27.622+0530] {logging_mixin.py:190} INFO - [2024-12-17T22:51:27.622+0530] {dag.py:4410} INFO - [DAG TEST] end task task_id=scrape_collection_page map_index=-1
[2024-12-17T22:51:27.626+0530] {logging_mixin.py:190} INFO - [2024-12-17T22:51:27.626+0530] {dagrun.py:854} INFO - Marking run <DagRun movie_pipeline @ 2024-12-17 17:21:22.260185+00:00: manual__2024-12-17T17:21:22.260185+00:00, state:running, queued_at: None. externally triggered: False> successful
[2024-12-17T22:51:27.626+0530] {logging_mixin.py:190} INFO - Dag run in success state
[2024-12-17T22:51:27.627+0530] {logging_mixin.py:190} INFO - Dag run start:2024-12-17 17:21:22.260185+00:00 end:2024-12-17 17:21:27.626893+00:00
[2024-12-17T22:51:27.627+0530] {logging_mixin.py:190} INFO - [2024-12-17T22:51:27.627+0530] {dagrun.py:905} INFO - DagRun Finished: dag_id=movie_pipeline, execution_date=2024-12-17 17:21:22.260185+00:00, run_id=manual__2024-12-17T17:21:22.260185+00:00, run_start_date=2024-12-17 17:21:22.260185+00:00, run_end_date=2024-12-17 17:21:27.626893+00:00, run_duration=5.366708, state=success, external_trigger=False, run_type=manual, data_interval_start=2024-12-16 17:21:22.260185+00:00, data_interval_end=2024-12-17 17:21:22.260185+00:00, dag_hash=None
[2024-12-17T22:51:27.637+0530] {processor.py:925} INFO - DAG(s) 'movie_pipeline' retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T22:51:27.650+0530] {logging_mixin.py:190} INFO - [2024-12-17T22:51:27.649+0530] {dag.py:3239} INFO - Sync 1 DAGs
[2024-12-17T22:51:27.668+0530] {logging_mixin.py:190} INFO - [2024-12-17T22:51:27.668+0530] {dag.py:4180} INFO - Setting next_dagrun for movie_pipeline to 2024-12-17 01:30:00+00:00, run_after=2024-12-18 01:30:00+00:00
[2024-12-17T22:51:27.688+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 5.476 seconds
[2024-12-17T22:54:11.676+0530] {processor.py:186} INFO - Started process (PID=1997) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T22:54:11.679+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-17T22:54:11.681+0530] {logging_mixin.py:190} INFO - [2024-12-17T22:54:11.680+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T22:54:12.030+0530] {logging_mixin.py:190} INFO - [2024-12-17T22:54:12.030+0530] {dag.py:4435} INFO - dagrun id: movie_pipeline
[2024-12-17T22:54:12.085+0530] {logging_mixin.py:190} INFO - [2024-12-17T22:54:12.085+0530] {dag.py:4451} INFO - created dagrun <DagRun movie_pipeline @ 2024-12-17 17:24:11.750141+00:00: manual__2024-12-17T17:24:11.750141+00:00, state:running, queued_at: None. externally triggered: False>
[2024-12-17T22:54:12.211+0530] {logging_mixin.py:190} INFO - [2024-12-17T22:54:12.211+0530] {dag.py:4396} INFO - [DAG TEST] starting task_id=scrape_collection_page map_index=-1
[2024-12-17T22:54:12.212+0530] {logging_mixin.py:190} INFO - [2024-12-17T22:54:12.211+0530] {dag.py:4399} INFO - [DAG TEST] running task <TaskInstance: movie_pipeline.scrape_collection_page manual__2024-12-17T17:24:11.750141+00:00 [scheduled]>
[2024-12-17T22:54:17.374+0530] {logging_mixin.py:190} INFO - [2024-12-17T22:54:17.373+0530] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-12-17T22:54:17.450+0530] {logging_mixin.py:190} INFO - [2024-12-17 22:54:17,450] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T17:24:11.750141+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T17:24:11.750141+00:00'
[2024-12-17T22:54:17.450+0530] {logging_mixin.py:190} INFO - [2024-12-17T22:54:17.450+0530] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T17:24:11.750141+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T17:24:11.750141+00:00'
[2024-12-17T22:54:17.463+0530] {logging_mixin.py:190} INFO - Task instance is in running state
[2024-12-17T22:54:17.464+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: queued
[2024-12-17T22:54:17.464+0530] {logging_mixin.py:190} INFO - Current task name:scrape_collection_page state:scheduled start_date:None
[2024-12-17T22:54:17.464+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline and current dag run status:running
[2024-12-17T22:54:17.465+0530] {logging_mixin.py:190} INFO - [2024-12-17T22:54:17.465+0530] {taskinstance.py:731} INFO - ::endgroup::
[2024-12-17T22:54:17.469+0530] {logging_mixin.py:190} INFO - 2024-12-17
[2024-12-17T22:54:17.469+0530] {logging_mixin.py:190} INFO - [2024-12-17 22:54:17,469] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T22:54:17.470+0530] {logging_mixin.py:190} INFO - [2024-12-17T22:54:17.469+0530] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T22:54:17.475+0530] {logging_mixin.py:190} INFO - [2024-12-17T22:54:17.475+0530] {taskinstance.py:340} INFO - ::group::Post task execution logs
[2024-12-17T22:54:17.476+0530] {logging_mixin.py:190} INFO - [2024-12-17T22:54:17.476+0530] {taskinstance.py:352} INFO - Marking task as SUCCESS. dag_id=movie_pipeline, task_id=scrape_collection_page, run_id=manual__2024-12-17T17:24:11.750141+00:00, execution_date=20241217T172411, start_date=, end_date=20241217T172417
[2024-12-17T22:54:17.485+0530] {logging_mixin.py:190} INFO - Task instance in success state
[2024-12-17T22:54:17.486+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: running
[2024-12-17T22:54:17.486+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline queued_at:None
[2024-12-17T22:54:17.486+0530] {logging_mixin.py:190} INFO - Task hostname:Jayeshs-MacBook-Air.local operator:PythonOperator
[2024-12-17T22:54:17.486+0530] {logging_mixin.py:190} INFO - [2024-12-17T22:54:17.486+0530] {dag.py:4410} INFO - [DAG TEST] end task task_id=scrape_collection_page map_index=-1
[2024-12-17T22:54:17.490+0530] {logging_mixin.py:190} INFO - [2024-12-17T22:54:17.490+0530] {dagrun.py:854} INFO - Marking run <DagRun movie_pipeline @ 2024-12-17 17:24:11.750141+00:00: manual__2024-12-17T17:24:11.750141+00:00, state:running, queued_at: None. externally triggered: False> successful
[2024-12-17T22:54:17.491+0530] {logging_mixin.py:190} INFO - Dag run in success state
[2024-12-17T22:54:17.491+0530] {logging_mixin.py:190} INFO - Dag run start:2024-12-17 17:24:11.750141+00:00 end:2024-12-17 17:24:17.490945+00:00
[2024-12-17T22:54:17.491+0530] {logging_mixin.py:190} INFO - [2024-12-17T22:54:17.491+0530] {dagrun.py:905} INFO - DagRun Finished: dag_id=movie_pipeline, execution_date=2024-12-17 17:24:11.750141+00:00, run_id=manual__2024-12-17T17:24:11.750141+00:00, run_start_date=2024-12-17 17:24:11.750141+00:00, run_end_date=2024-12-17 17:24:17.490945+00:00, run_duration=5.740804, state=success, external_trigger=False, run_type=manual, data_interval_start=2024-12-16 17:24:11.750141+00:00, data_interval_end=2024-12-17 17:24:11.750141+00:00, dag_hash=None
[2024-12-17T22:54:17.500+0530] {processor.py:925} INFO - DAG(s) 'movie_pipeline' retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T22:54:17.513+0530] {logging_mixin.py:190} INFO - [2024-12-17T22:54:17.513+0530] {dag.py:3239} INFO - Sync 1 DAGs
[2024-12-17T22:54:17.531+0530] {logging_mixin.py:190} INFO - [2024-12-17T22:54:17.531+0530] {dag.py:4180} INFO - Setting next_dagrun for movie_pipeline to 2024-12-17 01:30:00+00:00, run_after=2024-12-18 01:30:00+00:00
[2024-12-17T22:54:17.552+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 5.882 seconds
[2024-12-17T22:57:23.390+0530] {processor.py:186} INFO - Started process (PID=2254) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T22:57:23.393+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-17T22:57:23.394+0530] {logging_mixin.py:190} INFO - [2024-12-17T22:57:23.394+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T22:57:23.565+0530] {logging_mixin.py:190} INFO - [2024-12-17T22:57:23.565+0530] {dag.py:4435} INFO - dagrun id: movie_pipeline
[2024-12-17T22:57:23.580+0530] {logging_mixin.py:190} INFO - [2024-12-17T22:57:23.580+0530] {dag.py:4451} INFO - created dagrun <DagRun movie_pipeline @ 2024-12-17 17:27:23.435017+00:00: manual__2024-12-17T17:27:23.435017+00:00, state:running, queued_at: None. externally triggered: False>
[2024-12-17T22:57:23.610+0530] {logging_mixin.py:190} INFO - [2024-12-17T22:57:23.610+0530] {dag.py:4396} INFO - [DAG TEST] starting task_id=scrape_collection_page map_index=-1
[2024-12-17T22:57:23.611+0530] {logging_mixin.py:190} INFO - [2024-12-17T22:57:23.611+0530] {dag.py:4399} INFO - [DAG TEST] running task <TaskInstance: movie_pipeline.scrape_collection_page manual__2024-12-17T17:27:23.435017+00:00 [scheduled]>
[2024-12-17T22:57:28.715+0530] {logging_mixin.py:190} INFO - [2024-12-17T22:57:28.714+0530] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-12-17T22:57:28.799+0530] {logging_mixin.py:190} INFO - [2024-12-17 22:57:28,799] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T17:27:23.435017+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T17:27:23.435017+00:00'
[2024-12-17T22:57:28.800+0530] {logging_mixin.py:190} INFO - [2024-12-17T22:57:28.799+0530] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T17:27:23.435017+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T17:27:23.435017+00:00'
[2024-12-17T22:57:28.826+0530] {logging_mixin.py:190} INFO - Task instance is in running state
[2024-12-17T22:57:28.826+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: queued
[2024-12-17T22:57:28.827+0530] {logging_mixin.py:190} INFO - Current task name:scrape_collection_page state:scheduled start_date:None
[2024-12-17T22:57:28.828+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline and current dag run status:running
[2024-12-17T22:57:28.830+0530] {logging_mixin.py:190} INFO - [2024-12-17T22:57:28.829+0530] {taskinstance.py:731} INFO - ::endgroup::
[2024-12-17T22:57:28.833+0530] {logging_mixin.py:190} INFO - 2024-12-17
[2024-12-17T22:57:28.834+0530] {logging_mixin.py:190} INFO - [2024-12-17 22:57:28,834] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T22:57:28.834+0530] {logging_mixin.py:190} INFO - [2024-12-17T22:57:28.834+0530] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T22:57:28.842+0530] {logging_mixin.py:190} INFO - [2024-12-17T22:57:28.842+0530] {taskinstance.py:340} INFO - ::group::Post task execution logs
[2024-12-17T22:57:28.843+0530] {logging_mixin.py:190} INFO - [2024-12-17T22:57:28.842+0530] {taskinstance.py:352} INFO - Marking task as SUCCESS. dag_id=movie_pipeline, task_id=scrape_collection_page, run_id=manual__2024-12-17T17:27:23.435017+00:00, execution_date=20241217T172723, start_date=, end_date=20241217T172728
[2024-12-17T22:57:28.859+0530] {logging_mixin.py:190} INFO - Task instance in success state
[2024-12-17T22:57:28.862+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: running
[2024-12-17T22:57:28.862+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline queued_at:None
[2024-12-17T22:57:28.863+0530] {logging_mixin.py:190} INFO - Task hostname:Jayeshs-MacBook-Air.local operator:PythonOperator
[2024-12-17T22:57:28.863+0530] {logging_mixin.py:190} INFO - [2024-12-17T22:57:28.863+0530] {dag.py:4410} INFO - [DAG TEST] end task task_id=scrape_collection_page map_index=-1
[2024-12-17T22:57:28.867+0530] {logging_mixin.py:190} INFO - [2024-12-17T22:57:28.867+0530] {dagrun.py:854} INFO - Marking run <DagRun movie_pipeline @ 2024-12-17 17:27:23.435017+00:00: manual__2024-12-17T17:27:23.435017+00:00, state:running, queued_at: None. externally triggered: False> successful
[2024-12-17T22:57:28.868+0530] {logging_mixin.py:190} INFO - Dag run in success state
[2024-12-17T22:57:28.868+0530] {logging_mixin.py:190} INFO - Dag run start:2024-12-17 17:27:23.435017+00:00 end:2024-12-17 17:27:28.867993+00:00
[2024-12-17T22:57:28.869+0530] {logging_mixin.py:190} INFO - [2024-12-17T22:57:28.869+0530] {dagrun.py:905} INFO - DagRun Finished: dag_id=movie_pipeline, execution_date=2024-12-17 17:27:23.435017+00:00, run_id=manual__2024-12-17T17:27:23.435017+00:00, run_start_date=2024-12-17 17:27:23.435017+00:00, run_end_date=2024-12-17 17:27:28.867993+00:00, run_duration=5.432976, state=success, external_trigger=False, run_type=manual, data_interval_start=2024-12-16 17:27:23.435017+00:00, data_interval_end=2024-12-17 17:27:23.435017+00:00, dag_hash=None
[2024-12-17T22:57:28.882+0530] {processor.py:925} INFO - DAG(s) 'movie_pipeline' retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T22:57:28.899+0530] {logging_mixin.py:190} INFO - [2024-12-17T22:57:28.898+0530] {dag.py:3239} INFO - Sync 1 DAGs
[2024-12-17T22:57:28.928+0530] {logging_mixin.py:190} INFO - [2024-12-17T22:57:28.928+0530] {dag.py:4180} INFO - Setting next_dagrun for movie_pipeline to 2024-12-17 01:30:00+00:00, run_after=2024-12-18 01:30:00+00:00
[2024-12-17T22:57:28.963+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 5.576 seconds
[2024-12-17T23:00:44.539+0530] {processor.py:186} INFO - Started process (PID=2490) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T23:00:44.541+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-17T23:00:44.543+0530] {logging_mixin.py:190} INFO - [2024-12-17T23:00:44.542+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T23:00:44.790+0530] {logging_mixin.py:190} INFO - [2024-12-17T23:00:44.790+0530] {dag.py:4435} INFO - dagrun id: movie_pipeline
[2024-12-17T23:00:44.807+0530] {logging_mixin.py:190} INFO - [2024-12-17T23:00:44.807+0530] {dag.py:4451} INFO - created dagrun <DagRun movie_pipeline @ 2024-12-17 17:30:44.637270+00:00: manual__2024-12-17T17:30:44.637270+00:00, state:running, queued_at: None. externally triggered: False>
[2024-12-17T23:00:44.839+0530] {logging_mixin.py:190} INFO - [2024-12-17T23:00:44.838+0530] {dag.py:4396} INFO - [DAG TEST] starting task_id=scrape_collection_page map_index=-1
[2024-12-17T23:00:44.839+0530] {logging_mixin.py:190} INFO - [2024-12-17T23:00:44.839+0530] {dag.py:4399} INFO - [DAG TEST] running task <TaskInstance: movie_pipeline.scrape_collection_page manual__2024-12-17T17:30:44.637270+00:00 [scheduled]>
[2024-12-17T23:00:49.955+0530] {logging_mixin.py:190} INFO - [2024-12-17T23:00:49.954+0530] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-12-17T23:00:50.029+0530] {logging_mixin.py:190} INFO - [2024-12-17 23:00:50,029] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T17:30:44.637270+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T17:30:44.637270+00:00'
[2024-12-17T23:00:50.030+0530] {logging_mixin.py:190} INFO - [2024-12-17T23:00:50.029+0530] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T17:30:44.637270+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T17:30:44.637270+00:00'
[2024-12-17T23:00:50.043+0530] {logging_mixin.py:190} INFO - Task instance is in running state
[2024-12-17T23:00:50.043+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: queued
[2024-12-17T23:00:50.043+0530] {logging_mixin.py:190} INFO - Current task name:scrape_collection_page state:scheduled start_date:None
[2024-12-17T23:00:50.044+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline and current dag run status:running
[2024-12-17T23:00:50.044+0530] {logging_mixin.py:190} INFO - [2024-12-17T23:00:50.044+0530] {taskinstance.py:731} INFO - ::endgroup::
[2024-12-17T23:00:50.049+0530] {logging_mixin.py:190} INFO - 2024-12-17
[2024-12-17T23:00:50.049+0530] {logging_mixin.py:190} INFO - [2024-12-17 23:00:50,049] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T23:00:50.050+0530] {logging_mixin.py:190} INFO - [2024-12-17T23:00:50.049+0530] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T23:00:50.055+0530] {logging_mixin.py:190} INFO - [2024-12-17T23:00:50.055+0530] {taskinstance.py:340} INFO - ::group::Post task execution logs
[2024-12-17T23:00:50.056+0530] {logging_mixin.py:190} INFO - [2024-12-17T23:00:50.055+0530] {taskinstance.py:352} INFO - Marking task as SUCCESS. dag_id=movie_pipeline, task_id=scrape_collection_page, run_id=manual__2024-12-17T17:30:44.637270+00:00, execution_date=20241217T173044, start_date=, end_date=20241217T173050
[2024-12-17T23:00:50.065+0530] {logging_mixin.py:190} INFO - Task instance in success state
[2024-12-17T23:00:50.065+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: running
[2024-12-17T23:00:50.065+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline queued_at:None
[2024-12-17T23:00:50.066+0530] {logging_mixin.py:190} INFO - Task hostname:Jayeshs-MacBook-Air.local operator:PythonOperator
[2024-12-17T23:00:50.066+0530] {logging_mixin.py:190} INFO - [2024-12-17T23:00:50.066+0530] {dag.py:4410} INFO - [DAG TEST] end task task_id=scrape_collection_page map_index=-1
[2024-12-17T23:00:50.070+0530] {logging_mixin.py:190} INFO - [2024-12-17T23:00:50.069+0530] {dagrun.py:854} INFO - Marking run <DagRun movie_pipeline @ 2024-12-17 17:30:44.637270+00:00: manual__2024-12-17T17:30:44.637270+00:00, state:running, queued_at: None. externally triggered: False> successful
[2024-12-17T23:00:50.070+0530] {logging_mixin.py:190} INFO - Dag run in success state
[2024-12-17T23:00:50.070+0530] {logging_mixin.py:190} INFO - Dag run start:2024-12-17 17:30:44.637270+00:00 end:2024-12-17 17:30:50.070423+00:00
[2024-12-17T23:00:50.071+0530] {logging_mixin.py:190} INFO - [2024-12-17T23:00:50.070+0530] {dagrun.py:905} INFO - DagRun Finished: dag_id=movie_pipeline, execution_date=2024-12-17 17:30:44.637270+00:00, run_id=manual__2024-12-17T17:30:44.637270+00:00, run_start_date=2024-12-17 17:30:44.637270+00:00, run_end_date=2024-12-17 17:30:50.070423+00:00, run_duration=5.433153, state=success, external_trigger=False, run_type=manual, data_interval_start=2024-12-16 17:30:44.637270+00:00, data_interval_end=2024-12-17 17:30:44.637270+00:00, dag_hash=None
[2024-12-17T23:00:50.079+0530] {processor.py:925} INFO - DAG(s) 'movie_pipeline' retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T23:00:50.091+0530] {logging_mixin.py:190} INFO - [2024-12-17T23:00:50.091+0530] {dag.py:3239} INFO - Sync 1 DAGs
[2024-12-17T23:00:50.108+0530] {logging_mixin.py:190} INFO - [2024-12-17T23:00:50.108+0530] {dag.py:4180} INFO - Setting next_dagrun for movie_pipeline to 2024-12-17 01:30:00+00:00, run_after=2024-12-18 01:30:00+00:00
[2024-12-17T23:00:50.128+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 5.593 seconds
[2024-12-17T23:04:27.914+0530] {processor.py:186} INFO - Started process (PID=2766) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T23:04:27.917+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-17T23:04:27.918+0530] {logging_mixin.py:190} INFO - [2024-12-17T23:04:27.918+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T23:04:28.088+0530] {logging_mixin.py:190} INFO - [2024-12-17T23:04:28.088+0530] {dag.py:4435} INFO - dagrun id: movie_pipeline
[2024-12-17T23:04:28.103+0530] {logging_mixin.py:190} INFO - [2024-12-17T23:04:28.102+0530] {dag.py:4451} INFO - created dagrun <DagRun movie_pipeline @ 2024-12-17 17:34:27.957567+00:00: manual__2024-12-17T17:34:27.957567+00:00, state:running, queued_at: None. externally triggered: False>
[2024-12-17T23:04:28.131+0530] {logging_mixin.py:190} INFO - [2024-12-17T23:04:28.131+0530] {dag.py:4396} INFO - [DAG TEST] starting task_id=scrape_collection_page map_index=-1
[2024-12-17T23:04:28.131+0530] {logging_mixin.py:190} INFO - [2024-12-17T23:04:28.131+0530] {dag.py:4399} INFO - [DAG TEST] running task <TaskInstance: movie_pipeline.scrape_collection_page manual__2024-12-17T17:34:27.957567+00:00 [scheduled]>
[2024-12-17T23:04:33.226+0530] {logging_mixin.py:190} INFO - [2024-12-17T23:04:33.226+0530] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-12-17T23:04:33.288+0530] {logging_mixin.py:190} INFO - [2024-12-17 23:04:33,288] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T17:34:27.957567+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T17:34:27.957567+00:00'
[2024-12-17T23:04:33.288+0530] {logging_mixin.py:190} INFO - [2024-12-17T23:04:33.288+0530] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T17:34:27.957567+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T17:34:27.957567+00:00'
[2024-12-17T23:04:33.300+0530] {logging_mixin.py:190} INFO - Task instance is in running state
[2024-12-17T23:04:33.301+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: queued
[2024-12-17T23:04:33.301+0530] {logging_mixin.py:190} INFO - Current task name:scrape_collection_page state:scheduled start_date:None
[2024-12-17T23:04:33.301+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline and current dag run status:running
[2024-12-17T23:04:33.302+0530] {logging_mixin.py:190} INFO - [2024-12-17T23:04:33.302+0530] {taskinstance.py:731} INFO - ::endgroup::
[2024-12-17T23:04:33.304+0530] {logging_mixin.py:190} INFO - 2024-12-17
[2024-12-17T23:04:33.304+0530] {logging_mixin.py:190} INFO - [2024-12-17 23:04:33,304] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T23:04:33.304+0530] {logging_mixin.py:190} INFO - [2024-12-17T23:04:33.304+0530] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T23:04:33.309+0530] {logging_mixin.py:190} INFO - [2024-12-17T23:04:33.309+0530] {taskinstance.py:340} INFO - ::group::Post task execution logs
[2024-12-17T23:04:33.310+0530] {logging_mixin.py:190} INFO - [2024-12-17T23:04:33.310+0530] {taskinstance.py:352} INFO - Marking task as SUCCESS. dag_id=movie_pipeline, task_id=scrape_collection_page, run_id=manual__2024-12-17T17:34:27.957567+00:00, execution_date=20241217T173427, start_date=, end_date=20241217T173433
[2024-12-17T23:04:33.320+0530] {logging_mixin.py:190} INFO - Task instance in success state
[2024-12-17T23:04:33.320+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: running
[2024-12-17T23:04:33.320+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline queued_at:None
[2024-12-17T23:04:33.321+0530] {logging_mixin.py:190} INFO - Task hostname:Jayeshs-MacBook-Air.local operator:PythonOperator
[2024-12-17T23:04:33.321+0530] {logging_mixin.py:190} INFO - [2024-12-17T23:04:33.321+0530] {dag.py:4410} INFO - [DAG TEST] end task task_id=scrape_collection_page map_index=-1
[2024-12-17T23:04:33.325+0530] {logging_mixin.py:190} INFO - [2024-12-17T23:04:33.324+0530] {dagrun.py:854} INFO - Marking run <DagRun movie_pipeline @ 2024-12-17 17:34:27.957567+00:00: manual__2024-12-17T17:34:27.957567+00:00, state:running, queued_at: None. externally triggered: False> successful
[2024-12-17T23:04:33.325+0530] {logging_mixin.py:190} INFO - Dag run in success state
[2024-12-17T23:04:33.325+0530] {logging_mixin.py:190} INFO - Dag run start:2024-12-17 17:34:27.957567+00:00 end:2024-12-17 17:34:33.325276+00:00
[2024-12-17T23:04:33.325+0530] {logging_mixin.py:190} INFO - [2024-12-17T23:04:33.325+0530] {dagrun.py:905} INFO - DagRun Finished: dag_id=movie_pipeline, execution_date=2024-12-17 17:34:27.957567+00:00, run_id=manual__2024-12-17T17:34:27.957567+00:00, run_start_date=2024-12-17 17:34:27.957567+00:00, run_end_date=2024-12-17 17:34:33.325276+00:00, run_duration=5.367709, state=success, external_trigger=False, run_type=manual, data_interval_start=2024-12-16 17:34:27.957567+00:00, data_interval_end=2024-12-17 17:34:27.957567+00:00, dag_hash=None
[2024-12-17T23:04:33.335+0530] {processor.py:925} INFO - DAG(s) 'movie_pipeline' retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T23:04:33.346+0530] {logging_mixin.py:190} INFO - [2024-12-17T23:04:33.346+0530] {dag.py:3239} INFO - Sync 1 DAGs
[2024-12-17T23:04:33.364+0530] {logging_mixin.py:190} INFO - [2024-12-17T23:04:33.363+0530] {dag.py:4180} INFO - Setting next_dagrun for movie_pipeline to 2024-12-17 01:30:00+00:00, run_after=2024-12-18 01:30:00+00:00
[2024-12-17T23:04:33.382+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 5.471 seconds
[2024-12-17T23:07:19.734+0530] {processor.py:186} INFO - Started process (PID=2958) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T23:07:19.735+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-17T23:07:19.737+0530] {logging_mixin.py:190} INFO - [2024-12-17T23:07:19.737+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T23:07:19.942+0530] {logging_mixin.py:190} INFO - [2024-12-17T23:07:19.941+0530] {dag.py:4435} INFO - dagrun id: movie_pipeline
[2024-12-17T23:07:19.957+0530] {logging_mixin.py:190} INFO - [2024-12-17T23:07:19.957+0530] {dag.py:4451} INFO - created dagrun <DagRun movie_pipeline @ 2024-12-17 17:37:19.788030+00:00: manual__2024-12-17T17:37:19.788030+00:00, state:running, queued_at: None. externally triggered: False>
[2024-12-17T23:07:19.985+0530] {logging_mixin.py:190} INFO - [2024-12-17T23:07:19.984+0530] {dag.py:4396} INFO - [DAG TEST] starting task_id=scrape_collection_page map_index=-1
[2024-12-17T23:07:19.985+0530] {logging_mixin.py:190} INFO - [2024-12-17T23:07:19.985+0530] {dag.py:4399} INFO - [DAG TEST] running task <TaskInstance: movie_pipeline.scrape_collection_page manual__2024-12-17T17:37:19.788030+00:00 [scheduled]>
[2024-12-17T23:07:25.087+0530] {logging_mixin.py:190} INFO - [2024-12-17T23:07:25.087+0530] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-12-17T23:07:25.147+0530] {logging_mixin.py:190} INFO - [2024-12-17 23:07:25,146] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T17:37:19.788030+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T17:37:19.788030+00:00'
[2024-12-17T23:07:25.147+0530] {logging_mixin.py:190} INFO - [2024-12-17T23:07:25.146+0530] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T17:37:19.788030+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T17:37:19.788030+00:00'
[2024-12-17T23:07:25.150+0530] {logging_mixin.py:190} INFO - Task instance is in running state
[2024-12-17T23:07:25.151+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: queued
[2024-12-17T23:07:25.151+0530] {logging_mixin.py:190} INFO - Current task name:scrape_collection_page state:scheduled start_date:None
[2024-12-17T23:07:25.152+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline and current dag run status:running
[2024-12-17T23:07:25.152+0530] {logging_mixin.py:190} INFO - [2024-12-17T23:07:25.152+0530] {taskinstance.py:731} INFO - ::endgroup::
[2024-12-17T23:07:25.154+0530] {logging_mixin.py:190} INFO - 2024-12-17
[2024-12-17T23:07:25.155+0530] {logging_mixin.py:190} INFO - [2024-12-17 23:07:25,154] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T23:07:25.156+0530] {logging_mixin.py:190} INFO - [2024-12-17T23:07:25.154+0530] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T23:07:25.165+0530] {logging_mixin.py:190} INFO - [2024-12-17T23:07:25.164+0530] {taskinstance.py:340} INFO - ::group::Post task execution logs
[2024-12-17T23:07:25.165+0530] {logging_mixin.py:190} INFO - [2024-12-17T23:07:25.165+0530] {taskinstance.py:352} INFO - Marking task as SUCCESS. dag_id=movie_pipeline, task_id=scrape_collection_page, run_id=manual__2024-12-17T17:37:19.788030+00:00, execution_date=20241217T173719, start_date=, end_date=20241217T173725
[2024-12-17T23:07:25.174+0530] {logging_mixin.py:190} INFO - Task instance in success state
[2024-12-17T23:07:25.175+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: running
[2024-12-17T23:07:25.175+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline queued_at:None
[2024-12-17T23:07:25.175+0530] {logging_mixin.py:190} INFO - Task hostname:Jayeshs-MacBook-Air.local operator:PythonOperator
[2024-12-17T23:07:25.176+0530] {logging_mixin.py:190} INFO - [2024-12-17T23:07:25.175+0530] {dag.py:4410} INFO - [DAG TEST] end task task_id=scrape_collection_page map_index=-1
[2024-12-17T23:07:25.180+0530] {logging_mixin.py:190} INFO - [2024-12-17T23:07:25.180+0530] {dagrun.py:854} INFO - Marking run <DagRun movie_pipeline @ 2024-12-17 17:37:19.788030+00:00: manual__2024-12-17T17:37:19.788030+00:00, state:running, queued_at: None. externally triggered: False> successful
[2024-12-17T23:07:25.181+0530] {logging_mixin.py:190} INFO - Dag run in success state
[2024-12-17T23:07:25.182+0530] {logging_mixin.py:190} INFO - Dag run start:2024-12-17 17:37:19.788030+00:00 end:2024-12-17 17:37:25.181497+00:00
[2024-12-17T23:07:25.182+0530] {logging_mixin.py:190} INFO - [2024-12-17T23:07:25.182+0530] {dagrun.py:905} INFO - DagRun Finished: dag_id=movie_pipeline, execution_date=2024-12-17 17:37:19.788030+00:00, run_id=manual__2024-12-17T17:37:19.788030+00:00, run_start_date=2024-12-17 17:37:19.788030+00:00, run_end_date=2024-12-17 17:37:25.181497+00:00, run_duration=5.393467, state=success, external_trigger=False, run_type=manual, data_interval_start=2024-12-16 17:37:19.788030+00:00, data_interval_end=2024-12-17 17:37:19.788030+00:00, dag_hash=None
[2024-12-17T23:07:25.192+0530] {processor.py:925} INFO - DAG(s) 'movie_pipeline' retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T23:07:25.204+0530] {logging_mixin.py:190} INFO - [2024-12-17T23:07:25.204+0530] {dag.py:3239} INFO - Sync 1 DAGs
[2024-12-17T23:07:25.221+0530] {logging_mixin.py:190} INFO - [2024-12-17T23:07:25.221+0530] {dag.py:4180} INFO - Setting next_dagrun for movie_pipeline to 2024-12-17 01:30:00+00:00, run_after=2024-12-18 01:30:00+00:00
[2024-12-17T23:07:25.245+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 5.515 seconds
[2024-12-17T23:10:15.517+0530] {processor.py:186} INFO - Started process (PID=3167) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T23:10:15.518+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-17T23:10:15.519+0530] {logging_mixin.py:190} INFO - [2024-12-17T23:10:15.519+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T23:10:15.700+0530] {logging_mixin.py:190} INFO - [2024-12-17T23:10:15.699+0530] {dag.py:4435} INFO - dagrun id: movie_pipeline
[2024-12-17T23:10:15.715+0530] {logging_mixin.py:190} INFO - [2024-12-17T23:10:15.715+0530] {dag.py:4451} INFO - created dagrun <DagRun movie_pipeline @ 2024-12-17 17:40:15.560406+00:00: manual__2024-12-17T17:40:15.560406+00:00, state:running, queued_at: None. externally triggered: False>
[2024-12-17T23:10:15.743+0530] {logging_mixin.py:190} INFO - [2024-12-17T23:10:15.743+0530] {dag.py:4396} INFO - [DAG TEST] starting task_id=scrape_collection_page map_index=-1
[2024-12-17T23:10:15.744+0530] {logging_mixin.py:190} INFO - [2024-12-17T23:10:15.743+0530] {dag.py:4399} INFO - [DAG TEST] running task <TaskInstance: movie_pipeline.scrape_collection_page manual__2024-12-17T17:40:15.560406+00:00 [scheduled]>
[2024-12-17T23:10:20.847+0530] {logging_mixin.py:190} INFO - [2024-12-17T23:10:20.847+0530] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-12-17T23:10:20.918+0530] {logging_mixin.py:190} INFO - [2024-12-17 23:10:20,918] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T17:40:15.560406+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T17:40:15.560406+00:00'
[2024-12-17T23:10:20.919+0530] {logging_mixin.py:190} INFO - [2024-12-17T23:10:20.918+0530] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T17:40:15.560406+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T17:40:15.560406+00:00'
[2024-12-17T23:10:20.922+0530] {logging_mixin.py:190} INFO - Task instance is in running state
[2024-12-17T23:10:20.922+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: queued
[2024-12-17T23:10:20.923+0530] {logging_mixin.py:190} INFO - Current task name:scrape_collection_page state:scheduled start_date:None
[2024-12-17T23:10:20.923+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline and current dag run status:running
[2024-12-17T23:10:20.923+0530] {logging_mixin.py:190} INFO - [2024-12-17T23:10:20.923+0530] {taskinstance.py:731} INFO - ::endgroup::
[2024-12-17T23:10:20.925+0530] {logging_mixin.py:190} INFO - 2024-12-17
[2024-12-17T23:10:20.926+0530] {logging_mixin.py:190} INFO - [2024-12-17 23:10:20,926] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T23:10:20.926+0530] {logging_mixin.py:190} INFO - [2024-12-17T23:10:20.926+0530] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T23:10:20.932+0530] {logging_mixin.py:190} INFO - [2024-12-17T23:10:20.932+0530] {taskinstance.py:340} INFO - ::group::Post task execution logs
[2024-12-17T23:10:20.932+0530] {logging_mixin.py:190} INFO - [2024-12-17T23:10:20.932+0530] {taskinstance.py:352} INFO - Marking task as SUCCESS. dag_id=movie_pipeline, task_id=scrape_collection_page, run_id=manual__2024-12-17T17:40:15.560406+00:00, execution_date=20241217T174015, start_date=, end_date=20241217T174020
[2024-12-17T23:10:20.943+0530] {logging_mixin.py:190} INFO - Task instance in success state
[2024-12-17T23:10:20.943+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: running
[2024-12-17T23:10:20.943+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline queued_at:None
[2024-12-17T23:10:20.943+0530] {logging_mixin.py:190} INFO - Task hostname:Jayeshs-MacBook-Air.local operator:PythonOperator
[2024-12-17T23:10:20.944+0530] {logging_mixin.py:190} INFO - [2024-12-17T23:10:20.944+0530] {dag.py:4410} INFO - [DAG TEST] end task task_id=scrape_collection_page map_index=-1
[2024-12-17T23:10:20.948+0530] {logging_mixin.py:190} INFO - [2024-12-17T23:10:20.948+0530] {dagrun.py:854} INFO - Marking run <DagRun movie_pipeline @ 2024-12-17 17:40:15.560406+00:00: manual__2024-12-17T17:40:15.560406+00:00, state:running, queued_at: None. externally triggered: False> successful
[2024-12-17T23:10:20.948+0530] {logging_mixin.py:190} INFO - Dag run in success state
[2024-12-17T23:10:20.949+0530] {logging_mixin.py:190} INFO - Dag run start:2024-12-17 17:40:15.560406+00:00 end:2024-12-17 17:40:20.948632+00:00
[2024-12-17T23:10:20.949+0530] {logging_mixin.py:190} INFO - [2024-12-17T23:10:20.949+0530] {dagrun.py:905} INFO - DagRun Finished: dag_id=movie_pipeline, execution_date=2024-12-17 17:40:15.560406+00:00, run_id=manual__2024-12-17T17:40:15.560406+00:00, run_start_date=2024-12-17 17:40:15.560406+00:00, run_end_date=2024-12-17 17:40:20.948632+00:00, run_duration=5.388226, state=success, external_trigger=False, run_type=manual, data_interval_start=2024-12-16 17:40:15.560406+00:00, data_interval_end=2024-12-17 17:40:15.560406+00:00, dag_hash=None
[2024-12-17T23:10:20.959+0530] {processor.py:925} INFO - DAG(s) 'movie_pipeline' retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T23:10:20.971+0530] {logging_mixin.py:190} INFO - [2024-12-17T23:10:20.971+0530] {dag.py:3239} INFO - Sync 1 DAGs
[2024-12-17T23:10:20.990+0530] {logging_mixin.py:190} INFO - [2024-12-17T23:10:20.990+0530] {dag.py:4180} INFO - Setting next_dagrun for movie_pipeline to 2024-12-17 01:30:00+00:00, run_after=2024-12-18 01:30:00+00:00
[2024-12-17T23:10:21.009+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 5.496 seconds
[2024-12-17T23:13:21.756+0530] {processor.py:186} INFO - Started process (PID=3368) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T23:13:21.757+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-17T23:13:21.759+0530] {logging_mixin.py:190} INFO - [2024-12-17T23:13:21.758+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T23:13:21.939+0530] {logging_mixin.py:190} INFO - [2024-12-17T23:13:21.939+0530] {dag.py:4435} INFO - dagrun id: movie_pipeline
[2024-12-17T23:13:21.953+0530] {logging_mixin.py:190} INFO - [2024-12-17T23:13:21.953+0530] {dag.py:4451} INFO - created dagrun <DagRun movie_pipeline @ 2024-12-17 17:43:21.801698+00:00: manual__2024-12-17T17:43:21.801698+00:00, state:running, queued_at: None. externally triggered: False>
[2024-12-17T23:13:21.980+0530] {logging_mixin.py:190} INFO - [2024-12-17T23:13:21.979+0530] {dag.py:4396} INFO - [DAG TEST] starting task_id=scrape_collection_page map_index=-1
[2024-12-17T23:13:21.981+0530] {logging_mixin.py:190} INFO - [2024-12-17T23:13:21.980+0530] {dag.py:4399} INFO - [DAG TEST] running task <TaskInstance: movie_pipeline.scrape_collection_page manual__2024-12-17T17:43:21.801698+00:00 [scheduled]>
[2024-12-17T23:13:27.078+0530] {logging_mixin.py:190} INFO - [2024-12-17T23:13:27.078+0530] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-12-17T23:13:27.141+0530] {logging_mixin.py:190} INFO - [2024-12-17 23:13:27,141] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T17:43:21.801698+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T17:43:21.801698+00:00'
[2024-12-17T23:13:27.142+0530] {logging_mixin.py:190} INFO - [2024-12-17T23:13:27.141+0530] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T17:43:21.801698+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T17:43:21.801698+00:00'
[2024-12-17T23:13:27.147+0530] {logging_mixin.py:190} INFO - Task instance is in running state
[2024-12-17T23:13:27.147+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: queued
[2024-12-17T23:13:27.147+0530] {logging_mixin.py:190} INFO - Current task name:scrape_collection_page state:scheduled start_date:None
[2024-12-17T23:13:27.147+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline and current dag run status:running
[2024-12-17T23:13:27.148+0530] {logging_mixin.py:190} INFO - [2024-12-17T23:13:27.148+0530] {taskinstance.py:731} INFO - ::endgroup::
[2024-12-17T23:13:27.150+0530] {logging_mixin.py:190} INFO - 2024-12-17
[2024-12-17T23:13:27.150+0530] {logging_mixin.py:190} INFO - [2024-12-17 23:13:27,150] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T23:13:27.150+0530] {logging_mixin.py:190} INFO - [2024-12-17T23:13:27.150+0530] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T23:13:27.156+0530] {logging_mixin.py:190} INFO - [2024-12-17T23:13:27.156+0530] {taskinstance.py:340} INFO - ::group::Post task execution logs
[2024-12-17T23:13:27.157+0530] {logging_mixin.py:190} INFO - [2024-12-17T23:13:27.157+0530] {taskinstance.py:352} INFO - Marking task as SUCCESS. dag_id=movie_pipeline, task_id=scrape_collection_page, run_id=manual__2024-12-17T17:43:21.801698+00:00, execution_date=20241217T174321, start_date=, end_date=20241217T174327
[2024-12-17T23:13:27.169+0530] {logging_mixin.py:190} INFO - Task instance in success state
[2024-12-17T23:13:27.170+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: running
[2024-12-17T23:13:27.170+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline queued_at:None
[2024-12-17T23:13:27.170+0530] {logging_mixin.py:190} INFO - Task hostname:Jayeshs-MacBook-Air.local operator:PythonOperator
[2024-12-17T23:13:27.171+0530] {logging_mixin.py:190} INFO - [2024-12-17T23:13:27.170+0530] {dag.py:4410} INFO - [DAG TEST] end task task_id=scrape_collection_page map_index=-1
[2024-12-17T23:13:27.174+0530] {logging_mixin.py:190} INFO - [2024-12-17T23:13:27.174+0530] {dagrun.py:854} INFO - Marking run <DagRun movie_pipeline @ 2024-12-17 17:43:21.801698+00:00: manual__2024-12-17T17:43:21.801698+00:00, state:running, queued_at: None. externally triggered: False> successful
[2024-12-17T23:13:27.175+0530] {logging_mixin.py:190} INFO - Dag run in success state
[2024-12-17T23:13:27.175+0530] {logging_mixin.py:190} INFO - Dag run start:2024-12-17 17:43:21.801698+00:00 end:2024-12-17 17:43:27.175208+00:00
[2024-12-17T23:13:27.176+0530] {logging_mixin.py:190} INFO - [2024-12-17T23:13:27.175+0530] {dagrun.py:905} INFO - DagRun Finished: dag_id=movie_pipeline, execution_date=2024-12-17 17:43:21.801698+00:00, run_id=manual__2024-12-17T17:43:21.801698+00:00, run_start_date=2024-12-17 17:43:21.801698+00:00, run_end_date=2024-12-17 17:43:27.175208+00:00, run_duration=5.37351, state=success, external_trigger=False, run_type=manual, data_interval_start=2024-12-16 17:43:21.801698+00:00, data_interval_end=2024-12-17 17:43:21.801698+00:00, dag_hash=None
[2024-12-17T23:13:27.185+0530] {processor.py:925} INFO - DAG(s) 'movie_pipeline' retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T23:13:27.197+0530] {logging_mixin.py:190} INFO - [2024-12-17T23:13:27.197+0530] {dag.py:3239} INFO - Sync 1 DAGs
[2024-12-17T23:13:27.216+0530] {logging_mixin.py:190} INFO - [2024-12-17T23:13:27.215+0530] {dag.py:4180} INFO - Setting next_dagrun for movie_pipeline to 2024-12-17 01:30:00+00:00, run_after=2024-12-18 01:30:00+00:00
[2024-12-17T23:13:27.235+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 5.484 seconds
[2024-12-17T23:16:25.796+0530] {processor.py:186} INFO - Started process (PID=3580) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T23:16:25.797+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-17T23:16:25.799+0530] {logging_mixin.py:190} INFO - [2024-12-17T23:16:25.798+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T23:16:25.980+0530] {logging_mixin.py:190} INFO - [2024-12-17T23:16:25.980+0530] {dag.py:4435} INFO - dagrun id: movie_pipeline
[2024-12-17T23:16:26.003+0530] {logging_mixin.py:190} INFO - [2024-12-17T23:16:26.003+0530] {dag.py:4451} INFO - created dagrun <DagRun movie_pipeline @ 2024-12-17 17:46:25.840159+00:00: manual__2024-12-17T17:46:25.840159+00:00, state:running, queued_at: None. externally triggered: False>
[2024-12-17T23:16:26.032+0530] {logging_mixin.py:190} INFO - [2024-12-17T23:16:26.032+0530] {dag.py:4396} INFO - [DAG TEST] starting task_id=scrape_collection_page map_index=-1
[2024-12-17T23:16:26.033+0530] {logging_mixin.py:190} INFO - [2024-12-17T23:16:26.033+0530] {dag.py:4399} INFO - [DAG TEST] running task <TaskInstance: movie_pipeline.scrape_collection_page manual__2024-12-17T17:46:25.840159+00:00 [scheduled]>
[2024-12-17T23:16:31.133+0530] {logging_mixin.py:190} INFO - [2024-12-17T23:16:31.133+0530] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-12-17T23:16:31.224+0530] {logging_mixin.py:190} INFO - [2024-12-17 23:16:31,224] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T17:46:25.840159+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T17:46:25.840159+00:00'
[2024-12-17T23:16:31.224+0530] {logging_mixin.py:190} INFO - [2024-12-17T23:16:31.224+0530] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T17:46:25.840159+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T17:46:25.840159+00:00'
[2024-12-17T23:16:31.230+0530] {logging_mixin.py:190} INFO - Task instance is in running state
[2024-12-17T23:16:31.230+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: queued
[2024-12-17T23:16:31.230+0530] {logging_mixin.py:190} INFO - Current task name:scrape_collection_page state:scheduled start_date:None
[2024-12-17T23:16:31.231+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline and current dag run status:running
[2024-12-17T23:16:31.231+0530] {logging_mixin.py:190} INFO - [2024-12-17T23:16:31.231+0530] {taskinstance.py:731} INFO - ::endgroup::
[2024-12-17T23:16:31.233+0530] {logging_mixin.py:190} INFO - 2024-12-17
[2024-12-17T23:16:31.233+0530] {logging_mixin.py:190} INFO - [2024-12-17 23:16:31,233] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T23:16:31.234+0530] {logging_mixin.py:190} INFO - [2024-12-17T23:16:31.233+0530] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T23:16:31.241+0530] {logging_mixin.py:190} INFO - [2024-12-17T23:16:31.241+0530] {taskinstance.py:340} INFO - ::group::Post task execution logs
[2024-12-17T23:16:31.242+0530] {logging_mixin.py:190} INFO - [2024-12-17T23:16:31.241+0530] {taskinstance.py:352} INFO - Marking task as SUCCESS. dag_id=movie_pipeline, task_id=scrape_collection_page, run_id=manual__2024-12-17T17:46:25.840159+00:00, execution_date=20241217T174625, start_date=, end_date=20241217T174631
[2024-12-17T23:16:31.251+0530] {logging_mixin.py:190} INFO - Task instance in success state
[2024-12-17T23:16:31.252+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: running
[2024-12-17T23:16:31.252+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline queued_at:None
[2024-12-17T23:16:31.252+0530] {logging_mixin.py:190} INFO - Task hostname:Jayeshs-MacBook-Air.local operator:PythonOperator
[2024-12-17T23:16:31.252+0530] {logging_mixin.py:190} INFO - [2024-12-17T23:16:31.252+0530] {dag.py:4410} INFO - [DAG TEST] end task task_id=scrape_collection_page map_index=-1
[2024-12-17T23:16:31.256+0530] {logging_mixin.py:190} INFO - [2024-12-17T23:16:31.256+0530] {dagrun.py:854} INFO - Marking run <DagRun movie_pipeline @ 2024-12-17 17:46:25.840159+00:00: manual__2024-12-17T17:46:25.840159+00:00, state:running, queued_at: None. externally triggered: False> successful
[2024-12-17T23:16:31.256+0530] {logging_mixin.py:190} INFO - Dag run in success state
[2024-12-17T23:16:31.257+0530] {logging_mixin.py:190} INFO - Dag run start:2024-12-17 17:46:25.840159+00:00 end:2024-12-17 17:46:31.256847+00:00
[2024-12-17T23:16:31.257+0530] {logging_mixin.py:190} INFO - [2024-12-17T23:16:31.257+0530] {dagrun.py:905} INFO - DagRun Finished: dag_id=movie_pipeline, execution_date=2024-12-17 17:46:25.840159+00:00, run_id=manual__2024-12-17T17:46:25.840159+00:00, run_start_date=2024-12-17 17:46:25.840159+00:00, run_end_date=2024-12-17 17:46:31.256847+00:00, run_duration=5.416688, state=success, external_trigger=False, run_type=manual, data_interval_start=2024-12-16 17:46:25.840159+00:00, data_interval_end=2024-12-17 17:46:25.840159+00:00, dag_hash=None
[2024-12-17T23:16:31.266+0530] {processor.py:925} INFO - DAG(s) 'movie_pipeline' retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T23:16:31.278+0530] {logging_mixin.py:190} INFO - [2024-12-17T23:16:31.278+0530] {dag.py:3239} INFO - Sync 1 DAGs
[2024-12-17T23:16:31.297+0530] {logging_mixin.py:190} INFO - [2024-12-17T23:16:31.297+0530] {dag.py:4180} INFO - Setting next_dagrun for movie_pipeline to 2024-12-17 01:30:00+00:00, run_after=2024-12-18 01:30:00+00:00
[2024-12-17T23:16:31.323+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 5.531 seconds
[2024-12-17T23:19:32.489+0530] {processor.py:186} INFO - Started process (PID=3784) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T23:19:32.490+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-17T23:19:32.491+0530] {logging_mixin.py:190} INFO - [2024-12-17T23:19:32.491+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T23:19:32.682+0530] {logging_mixin.py:190} INFO - [2024-12-17T23:19:32.682+0530] {dag.py:4435} INFO - dagrun id: movie_pipeline
[2024-12-17T23:19:32.703+0530] {logging_mixin.py:190} INFO - [2024-12-17T23:19:32.702+0530] {dag.py:4451} INFO - created dagrun <DagRun movie_pipeline @ 2024-12-17 17:49:32.533100+00:00: manual__2024-12-17T17:49:32.533100+00:00, state:running, queued_at: None. externally triggered: False>
[2024-12-17T23:19:32.729+0530] {logging_mixin.py:190} INFO - [2024-12-17T23:19:32.729+0530] {dag.py:4396} INFO - [DAG TEST] starting task_id=scrape_collection_page map_index=-1
[2024-12-17T23:19:32.730+0530] {logging_mixin.py:190} INFO - [2024-12-17T23:19:32.729+0530] {dag.py:4399} INFO - [DAG TEST] running task <TaskInstance: movie_pipeline.scrape_collection_page manual__2024-12-17T17:49:32.533100+00:00 [scheduled]>
[2024-12-17T23:19:37.832+0530] {logging_mixin.py:190} INFO - [2024-12-17T23:19:37.831+0530] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-12-17T23:19:37.896+0530] {logging_mixin.py:190} INFO - [2024-12-17 23:19:37,896] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T17:49:32.533100+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T17:49:32.533100+00:00'
[2024-12-17T23:19:37.897+0530] {logging_mixin.py:190} INFO - [2024-12-17T23:19:37.896+0530] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T17:49:32.533100+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T17:49:32.533100+00:00'
[2024-12-17T23:19:37.900+0530] {logging_mixin.py:190} INFO - Task instance is in running state
[2024-12-17T23:19:37.900+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: queued
[2024-12-17T23:19:37.900+0530] {logging_mixin.py:190} INFO - Current task name:scrape_collection_page state:scheduled start_date:None
[2024-12-17T23:19:37.901+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline and current dag run status:running
[2024-12-17T23:19:37.901+0530] {logging_mixin.py:190} INFO - [2024-12-17T23:19:37.901+0530] {taskinstance.py:731} INFO - ::endgroup::
[2024-12-17T23:19:37.903+0530] {logging_mixin.py:190} INFO - 2024-12-17
[2024-12-17T23:19:37.903+0530] {logging_mixin.py:190} INFO - [2024-12-17 23:19:37,903] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T23:19:37.904+0530] {logging_mixin.py:190} INFO - [2024-12-17T23:19:37.903+0530] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T23:19:37.909+0530] {logging_mixin.py:190} INFO - [2024-12-17T23:19:37.909+0530] {taskinstance.py:340} INFO - ::group::Post task execution logs
[2024-12-17T23:19:37.909+0530] {logging_mixin.py:190} INFO - [2024-12-17T23:19:37.909+0530] {taskinstance.py:352} INFO - Marking task as SUCCESS. dag_id=movie_pipeline, task_id=scrape_collection_page, run_id=manual__2024-12-17T17:49:32.533100+00:00, execution_date=20241217T174932, start_date=, end_date=20241217T174937
[2024-12-17T23:19:37.923+0530] {logging_mixin.py:190} INFO - Task instance in success state
[2024-12-17T23:19:37.923+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: running
[2024-12-17T23:19:37.923+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline queued_at:None
[2024-12-17T23:19:37.923+0530] {logging_mixin.py:190} INFO - Task hostname:Jayeshs-MacBook-Air.local operator:PythonOperator
[2024-12-17T23:19:37.924+0530] {logging_mixin.py:190} INFO - [2024-12-17T23:19:37.923+0530] {dag.py:4410} INFO - [DAG TEST] end task task_id=scrape_collection_page map_index=-1
[2024-12-17T23:19:37.928+0530] {logging_mixin.py:190} INFO - [2024-12-17T23:19:37.928+0530] {dagrun.py:854} INFO - Marking run <DagRun movie_pipeline @ 2024-12-17 17:49:32.533100+00:00: manual__2024-12-17T17:49:32.533100+00:00, state:running, queued_at: None. externally triggered: False> successful
[2024-12-17T23:19:37.928+0530] {logging_mixin.py:190} INFO - Dag run in success state
[2024-12-17T23:19:37.929+0530] {logging_mixin.py:190} INFO - Dag run start:2024-12-17 17:49:32.533100+00:00 end:2024-12-17 17:49:37.928888+00:00
[2024-12-17T23:19:37.930+0530] {logging_mixin.py:190} INFO - [2024-12-17T23:19:37.929+0530] {dagrun.py:905} INFO - DagRun Finished: dag_id=movie_pipeline, execution_date=2024-12-17 17:49:32.533100+00:00, run_id=manual__2024-12-17T17:49:32.533100+00:00, run_start_date=2024-12-17 17:49:32.533100+00:00, run_end_date=2024-12-17 17:49:37.928888+00:00, run_duration=5.395788, state=success, external_trigger=False, run_type=manual, data_interval_start=2024-12-16 17:49:32.533100+00:00, data_interval_end=2024-12-17 17:49:32.533100+00:00, dag_hash=None
[2024-12-17T23:19:37.940+0530] {processor.py:925} INFO - DAG(s) 'movie_pipeline' retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T23:19:37.952+0530] {logging_mixin.py:190} INFO - [2024-12-17T23:19:37.952+0530] {dag.py:3239} INFO - Sync 1 DAGs
[2024-12-17T23:19:37.970+0530] {logging_mixin.py:190} INFO - [2024-12-17T23:19:37.969+0530] {dag.py:4180} INFO - Setting next_dagrun for movie_pipeline to 2024-12-17 01:30:00+00:00, run_after=2024-12-18 01:30:00+00:00
[2024-12-17T23:19:37.991+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 5.506 seconds
[2024-12-17T23:23:17.460+0530] {processor.py:186} INFO - Started process (PID=4036) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T23:23:17.461+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-17T23:23:17.462+0530] {logging_mixin.py:190} INFO - [2024-12-17T23:23:17.462+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T23:23:17.638+0530] {logging_mixin.py:190} INFO - [2024-12-17T23:23:17.638+0530] {dag.py:4435} INFO - dagrun id: movie_pipeline
[2024-12-17T23:23:17.652+0530] {logging_mixin.py:190} INFO - [2024-12-17T23:23:17.651+0530] {dag.py:4451} INFO - created dagrun <DagRun movie_pipeline @ 2024-12-17 17:53:17.504082+00:00: manual__2024-12-17T17:53:17.504082+00:00, state:running, queued_at: None. externally triggered: False>
[2024-12-17T23:23:17.676+0530] {logging_mixin.py:190} INFO - [2024-12-17T23:23:17.676+0530] {dag.py:4396} INFO - [DAG TEST] starting task_id=scrape_collection_page map_index=-1
[2024-12-17T23:23:17.677+0530] {logging_mixin.py:190} INFO - [2024-12-17T23:23:17.677+0530] {dag.py:4399} INFO - [DAG TEST] running task <TaskInstance: movie_pipeline.scrape_collection_page manual__2024-12-17T17:53:17.504082+00:00 [scheduled]>
[2024-12-17T23:23:22.776+0530] {logging_mixin.py:190} INFO - [2024-12-17T23:23:22.776+0530] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-12-17T23:23:22.840+0530] {logging_mixin.py:190} INFO - [2024-12-17 23:23:22,840] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T17:53:17.504082+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T17:53:17.504082+00:00'
[2024-12-17T23:23:22.841+0530] {logging_mixin.py:190} INFO - [2024-12-17T23:23:22.840+0530] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T17:53:17.504082+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T17:53:17.504082+00:00'
[2024-12-17T23:23:22.844+0530] {logging_mixin.py:190} INFO - Task instance is in running state
[2024-12-17T23:23:22.845+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: queued
[2024-12-17T23:23:22.845+0530] {logging_mixin.py:190} INFO - Current task name:scrape_collection_page state:scheduled start_date:None
[2024-12-17T23:23:22.845+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline and current dag run status:running
[2024-12-17T23:23:22.846+0530] {logging_mixin.py:190} INFO - [2024-12-17T23:23:22.846+0530] {taskinstance.py:731} INFO - ::endgroup::
[2024-12-17T23:23:22.848+0530] {logging_mixin.py:190} INFO - 2024-12-17
[2024-12-17T23:23:22.848+0530] {logging_mixin.py:190} INFO - [2024-12-17 23:23:22,848] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T23:23:22.849+0530] {logging_mixin.py:190} INFO - [2024-12-17T23:23:22.848+0530] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T23:23:22.854+0530] {logging_mixin.py:190} INFO - [2024-12-17T23:23:22.854+0530] {taskinstance.py:340} INFO - ::group::Post task execution logs
[2024-12-17T23:23:22.855+0530] {logging_mixin.py:190} INFO - [2024-12-17T23:23:22.855+0530] {taskinstance.py:352} INFO - Marking task as SUCCESS. dag_id=movie_pipeline, task_id=scrape_collection_page, run_id=manual__2024-12-17T17:53:17.504082+00:00, execution_date=20241217T175317, start_date=, end_date=20241217T175322
[2024-12-17T23:23:23.008+0530] {logging_mixin.py:190} INFO - Task instance in success state
[2024-12-17T23:23:23.009+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: running
[2024-12-17T23:23:23.009+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline queued_at:None
[2024-12-17T23:23:23.010+0530] {logging_mixin.py:190} INFO - Task hostname:Jayeshs-MacBook-Air.local operator:PythonOperator
[2024-12-17T23:23:23.010+0530] {logging_mixin.py:190} INFO - [2024-12-17T23:23:23.010+0530] {dag.py:4410} INFO - [DAG TEST] end task task_id=scrape_collection_page map_index=-1
[2024-12-17T23:23:23.030+0530] {logging_mixin.py:190} INFO - [2024-12-17T23:23:23.029+0530] {dagrun.py:854} INFO - Marking run <DagRun movie_pipeline @ 2024-12-17 17:53:17.504082+00:00: manual__2024-12-17T17:53:17.504082+00:00, state:running, queued_at: None. externally triggered: False> successful
[2024-12-17T23:23:23.030+0530] {logging_mixin.py:190} INFO - Dag run in success state
[2024-12-17T23:23:23.030+0530] {logging_mixin.py:190} INFO - Dag run start:2024-12-17 17:53:17.504082+00:00 end:2024-12-17 17:53:23.030316+00:00
[2024-12-17T23:23:23.031+0530] {logging_mixin.py:190} INFO - [2024-12-17T23:23:23.031+0530] {dagrun.py:905} INFO - DagRun Finished: dag_id=movie_pipeline, execution_date=2024-12-17 17:53:17.504082+00:00, run_id=manual__2024-12-17T17:53:17.504082+00:00, run_start_date=2024-12-17 17:53:17.504082+00:00, run_end_date=2024-12-17 17:53:23.030316+00:00, run_duration=5.526234, state=success, external_trigger=False, run_type=manual, data_interval_start=2024-12-16 17:53:17.504082+00:00, data_interval_end=2024-12-17 17:53:17.504082+00:00, dag_hash=None
[2024-12-17T23:23:23.042+0530] {processor.py:925} INFO - DAG(s) 'movie_pipeline' retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T23:23:23.064+0530] {logging_mixin.py:190} INFO - [2024-12-17T23:23:23.063+0530] {dag.py:3239} INFO - Sync 1 DAGs
[2024-12-17T23:23:23.085+0530] {logging_mixin.py:190} INFO - [2024-12-17T23:23:23.084+0530] {dag.py:4180} INFO - Setting next_dagrun for movie_pipeline to 2024-12-17 01:30:00+00:00, run_after=2024-12-18 01:30:00+00:00
[2024-12-17T23:23:23.106+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 5.650 seconds
[2024-12-17T23:26:29.579+0530] {processor.py:186} INFO - Started process (PID=4262) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T23:26:29.580+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-17T23:26:29.581+0530] {logging_mixin.py:190} INFO - [2024-12-17T23:26:29.581+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T23:26:29.772+0530] {logging_mixin.py:190} INFO - [2024-12-17T23:26:29.772+0530] {dag.py:4435} INFO - dagrun id: movie_pipeline
[2024-12-17T23:26:29.789+0530] {logging_mixin.py:190} INFO - [2024-12-17T23:26:29.789+0530] {dag.py:4451} INFO - created dagrun <DagRun movie_pipeline @ 2024-12-17 17:56:29.622815+00:00: manual__2024-12-17T17:56:29.622815+00:00, state:running, queued_at: None. externally triggered: False>
[2024-12-17T23:26:29.957+0530] {logging_mixin.py:190} INFO - [2024-12-17T23:26:29.957+0530] {dag.py:4396} INFO - [DAG TEST] starting task_id=scrape_collection_page map_index=-1
[2024-12-17T23:26:29.960+0530] {logging_mixin.py:190} INFO - [2024-12-17T23:26:29.959+0530] {dag.py:4399} INFO - [DAG TEST] running task <TaskInstance: movie_pipeline.scrape_collection_page manual__2024-12-17T17:56:29.622815+00:00 [scheduled]>
[2024-12-17T23:26:35.132+0530] {logging_mixin.py:190} INFO - [2024-12-17T23:26:35.132+0530] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-12-17T23:26:35.212+0530] {logging_mixin.py:190} INFO - [2024-12-17 23:26:35,212] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T17:56:29.622815+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T17:56:29.622815+00:00'
[2024-12-17T23:26:35.212+0530] {logging_mixin.py:190} INFO - [2024-12-17T23:26:35.212+0530] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T17:56:29.622815+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T17:56:29.622815+00:00'
[2024-12-17T23:26:35.215+0530] {logging_mixin.py:190} INFO - Task instance is in running state
[2024-12-17T23:26:35.216+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: queued
[2024-12-17T23:26:35.216+0530] {logging_mixin.py:190} INFO - Current task name:scrape_collection_page state:scheduled start_date:None
[2024-12-17T23:26:35.216+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline and current dag run status:running
[2024-12-17T23:26:35.217+0530] {logging_mixin.py:190} INFO - [2024-12-17T23:26:35.217+0530] {taskinstance.py:731} INFO - ::endgroup::
[2024-12-17T23:26:35.219+0530] {logging_mixin.py:190} INFO - 2024-12-17
[2024-12-17T23:26:35.220+0530] {logging_mixin.py:190} INFO - [2024-12-17 23:26:35,220] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T23:26:35.221+0530] {logging_mixin.py:190} INFO - [2024-12-17T23:26:35.220+0530] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T23:26:35.226+0530] {logging_mixin.py:190} INFO - [2024-12-17T23:26:35.225+0530] {taskinstance.py:340} INFO - ::group::Post task execution logs
[2024-12-17T23:26:35.226+0530] {logging_mixin.py:190} INFO - [2024-12-17T23:26:35.226+0530] {taskinstance.py:352} INFO - Marking task as SUCCESS. dag_id=movie_pipeline, task_id=scrape_collection_page, run_id=manual__2024-12-17T17:56:29.622815+00:00, execution_date=20241217T175629, start_date=, end_date=20241217T175635
[2024-12-17T23:26:35.240+0530] {logging_mixin.py:190} INFO - Task instance in success state
[2024-12-17T23:26:35.240+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: running
[2024-12-17T23:26:35.240+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline queued_at:None
[2024-12-17T23:26:35.241+0530] {logging_mixin.py:190} INFO - Task hostname:Jayeshs-MacBook-Air.local operator:PythonOperator
[2024-12-17T23:26:35.241+0530] {logging_mixin.py:190} INFO - [2024-12-17T23:26:35.241+0530] {dag.py:4410} INFO - [DAG TEST] end task task_id=scrape_collection_page map_index=-1
[2024-12-17T23:26:35.247+0530] {logging_mixin.py:190} INFO - [2024-12-17T23:26:35.247+0530] {dagrun.py:854} INFO - Marking run <DagRun movie_pipeline @ 2024-12-17 17:56:29.622815+00:00: manual__2024-12-17T17:56:29.622815+00:00, state:running, queued_at: None. externally triggered: False> successful
[2024-12-17T23:26:35.247+0530] {logging_mixin.py:190} INFO - Dag run in success state
[2024-12-17T23:26:35.247+0530] {logging_mixin.py:190} INFO - Dag run start:2024-12-17 17:56:29.622815+00:00 end:2024-12-17 17:56:35.247592+00:00
[2024-12-17T23:26:35.248+0530] {logging_mixin.py:190} INFO - [2024-12-17T23:26:35.248+0530] {dagrun.py:905} INFO - DagRun Finished: dag_id=movie_pipeline, execution_date=2024-12-17 17:56:29.622815+00:00, run_id=manual__2024-12-17T17:56:29.622815+00:00, run_start_date=2024-12-17 17:56:29.622815+00:00, run_end_date=2024-12-17 17:56:35.247592+00:00, run_duration=5.624777, state=success, external_trigger=False, run_type=manual, data_interval_start=2024-12-16 17:56:29.622815+00:00, data_interval_end=2024-12-17 17:56:29.622815+00:00, dag_hash=None
[2024-12-17T23:26:35.259+0530] {processor.py:925} INFO - DAG(s) 'movie_pipeline' retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T23:26:35.271+0530] {logging_mixin.py:190} INFO - [2024-12-17T23:26:35.271+0530] {dag.py:3239} INFO - Sync 1 DAGs
[2024-12-17T23:26:35.289+0530] {logging_mixin.py:190} INFO - [2024-12-17T23:26:35.289+0530] {dag.py:4180} INFO - Setting next_dagrun for movie_pipeline to 2024-12-17 01:30:00+00:00, run_after=2024-12-18 01:30:00+00:00
[2024-12-17T23:26:35.308+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 5.733 seconds
[2024-12-17T23:29:35.309+0530] {processor.py:186} INFO - Started process (PID=4460) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T23:29:35.310+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-17T23:29:35.312+0530] {logging_mixin.py:190} INFO - [2024-12-17T23:29:35.312+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T23:29:35.495+0530] {logging_mixin.py:190} INFO - [2024-12-17T23:29:35.495+0530] {dag.py:4435} INFO - dagrun id: movie_pipeline
[2024-12-17T23:29:35.511+0530] {logging_mixin.py:190} INFO - [2024-12-17T23:29:35.510+0530] {dag.py:4451} INFO - created dagrun <DagRun movie_pipeline @ 2024-12-17 17:59:35.360189+00:00: manual__2024-12-17T17:59:35.360189+00:00, state:running, queued_at: None. externally triggered: False>
[2024-12-17T23:29:35.537+0530] {logging_mixin.py:190} INFO - [2024-12-17T23:29:35.536+0530] {dag.py:4396} INFO - [DAG TEST] starting task_id=scrape_collection_page map_index=-1
[2024-12-17T23:29:35.537+0530] {logging_mixin.py:190} INFO - [2024-12-17T23:29:35.537+0530] {dag.py:4399} INFO - [DAG TEST] running task <TaskInstance: movie_pipeline.scrape_collection_page manual__2024-12-17T17:59:35.360189+00:00 [scheduled]>
[2024-12-17T23:29:40.642+0530] {logging_mixin.py:190} INFO - [2024-12-17T23:29:40.642+0530] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-12-17T23:29:40.721+0530] {logging_mixin.py:190} INFO - [2024-12-17 23:29:40,721] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T17:59:35.360189+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T17:59:35.360189+00:00'
[2024-12-17T23:29:40.722+0530] {logging_mixin.py:190} INFO - [2024-12-17T23:29:40.721+0530] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T17:59:35.360189+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T17:59:35.360189+00:00'
[2024-12-17T23:29:40.726+0530] {logging_mixin.py:190} INFO - Task instance is in running state
[2024-12-17T23:29:40.726+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: queued
[2024-12-17T23:29:40.726+0530] {logging_mixin.py:190} INFO - Current task name:scrape_collection_page state:scheduled start_date:None
[2024-12-17T23:29:40.726+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline and current dag run status:running
[2024-12-17T23:29:40.727+0530] {logging_mixin.py:190} INFO - [2024-12-17T23:29:40.727+0530] {taskinstance.py:731} INFO - ::endgroup::
[2024-12-17T23:29:40.730+0530] {logging_mixin.py:190} INFO - 2024-12-17
[2024-12-17T23:29:40.731+0530] {logging_mixin.py:190} INFO - [2024-12-17 23:29:40,730] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T23:29:40.731+0530] {logging_mixin.py:190} INFO - [2024-12-17T23:29:40.730+0530] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T23:29:40.741+0530] {logging_mixin.py:190} INFO - [2024-12-17T23:29:40.741+0530] {taskinstance.py:340} INFO - ::group::Post task execution logs
[2024-12-17T23:29:40.742+0530] {logging_mixin.py:190} INFO - [2024-12-17T23:29:40.741+0530] {taskinstance.py:352} INFO - Marking task as SUCCESS. dag_id=movie_pipeline, task_id=scrape_collection_page, run_id=manual__2024-12-17T17:59:35.360189+00:00, execution_date=20241217T175935, start_date=, end_date=20241217T175940
[2024-12-17T23:29:40.751+0530] {logging_mixin.py:190} INFO - Task instance in success state
[2024-12-17T23:29:40.752+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: running
[2024-12-17T23:29:40.752+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline queued_at:None
[2024-12-17T23:29:40.752+0530] {logging_mixin.py:190} INFO - Task hostname:Jayeshs-MacBook-Air.local operator:PythonOperator
[2024-12-17T23:29:40.753+0530] {logging_mixin.py:190} INFO - [2024-12-17T23:29:40.752+0530] {dag.py:4410} INFO - [DAG TEST] end task task_id=scrape_collection_page map_index=-1
[2024-12-17T23:29:40.756+0530] {logging_mixin.py:190} INFO - [2024-12-17T23:29:40.756+0530] {dagrun.py:854} INFO - Marking run <DagRun movie_pipeline @ 2024-12-17 17:59:35.360189+00:00: manual__2024-12-17T17:59:35.360189+00:00, state:running, queued_at: None. externally triggered: False> successful
[2024-12-17T23:29:40.757+0530] {logging_mixin.py:190} INFO - Dag run in success state
[2024-12-17T23:29:40.757+0530] {logging_mixin.py:190} INFO - Dag run start:2024-12-17 17:59:35.360189+00:00 end:2024-12-17 17:59:40.757157+00:00
[2024-12-17T23:29:40.758+0530] {logging_mixin.py:190} INFO - [2024-12-17T23:29:40.757+0530] {dagrun.py:905} INFO - DagRun Finished: dag_id=movie_pipeline, execution_date=2024-12-17 17:59:35.360189+00:00, run_id=manual__2024-12-17T17:59:35.360189+00:00, run_start_date=2024-12-17 17:59:35.360189+00:00, run_end_date=2024-12-17 17:59:40.757157+00:00, run_duration=5.396968, state=success, external_trigger=False, run_type=manual, data_interval_start=2024-12-16 17:59:35.360189+00:00, data_interval_end=2024-12-17 17:59:35.360189+00:00, dag_hash=None
[2024-12-17T23:29:40.766+0530] {processor.py:925} INFO - DAG(s) 'movie_pipeline' retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T23:29:40.784+0530] {logging_mixin.py:190} INFO - [2024-12-17T23:29:40.784+0530] {dag.py:3239} INFO - Sync 1 DAGs
[2024-12-17T23:29:40.805+0530] {logging_mixin.py:190} INFO - [2024-12-17T23:29:40.804+0530] {dag.py:4180} INFO - Setting next_dagrun for movie_pipeline to 2024-12-17 01:30:00+00:00, run_after=2024-12-18 01:30:00+00:00
[2024-12-17T23:29:40.828+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 5.523 seconds
[2024-12-17T23:32:52.410+0530] {processor.py:186} INFO - Started process (PID=4876) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T23:32:52.412+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-17T23:32:52.413+0530] {logging_mixin.py:190} INFO - [2024-12-17T23:32:52.413+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T23:32:52.603+0530] {logging_mixin.py:190} INFO - [2024-12-17T23:32:52.603+0530] {dag.py:4435} INFO - dagrun id: movie_pipeline
[2024-12-17T23:32:52.619+0530] {logging_mixin.py:190} INFO - [2024-12-17T23:32:52.619+0530] {dag.py:4451} INFO - created dagrun <DagRun movie_pipeline @ 2024-12-17 18:02:52.457624+00:00: manual__2024-12-17T18:02:52.457624+00:00, state:running, queued_at: None. externally triggered: False>
[2024-12-17T23:32:52.644+0530] {logging_mixin.py:190} INFO - [2024-12-17T23:32:52.644+0530] {dag.py:4396} INFO - [DAG TEST] starting task_id=scrape_collection_page map_index=-1
[2024-12-17T23:32:52.645+0530] {logging_mixin.py:190} INFO - [2024-12-17T23:32:52.644+0530] {dag.py:4399} INFO - [DAG TEST] running task <TaskInstance: movie_pipeline.scrape_collection_page manual__2024-12-17T18:02:52.457624+00:00 [scheduled]>
[2024-12-17T23:32:57.749+0530] {logging_mixin.py:190} INFO - [2024-12-17T23:32:57.749+0530] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-12-17T23:32:57.824+0530] {logging_mixin.py:190} INFO - [2024-12-17 23:32:57,823] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T18:02:52.457624+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T18:02:52.457624+00:00'
[2024-12-17T23:32:57.824+0530] {logging_mixin.py:190} INFO - [2024-12-17T23:32:57.823+0530] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='movie_pipeline' AIRFLOW_CTX_TASK_ID='scrape_collection_page' AIRFLOW_CTX_EXECUTION_DATE='2024-12-17T18:02:52.457624+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-17T18:02:52.457624+00:00'
[2024-12-17T23:32:57.828+0530] {logging_mixin.py:190} INFO - Task instance is in running state
[2024-12-17T23:32:57.829+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: queued
[2024-12-17T23:32:57.830+0530] {logging_mixin.py:190} INFO - Current task name:scrape_collection_page state:scheduled start_date:None
[2024-12-17T23:32:57.830+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline and current dag run status:running
[2024-12-17T23:32:57.830+0530] {logging_mixin.py:190} INFO - [2024-12-17T23:32:57.830+0530] {taskinstance.py:731} INFO - ::endgroup::
[2024-12-17T23:32:57.832+0530] {logging_mixin.py:190} INFO - 2024-12-17
[2024-12-17T23:32:57.832+0530] {logging_mixin.py:190} INFO - [2024-12-17 23:32:57,832] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T23:32:57.833+0530] {logging_mixin.py:190} INFO - [2024-12-17T23:32:57.832+0530] {python.py:240} INFO - Done. Returned value was: None
[2024-12-17T23:32:57.838+0530] {logging_mixin.py:190} INFO - [2024-12-17T23:32:57.838+0530] {taskinstance.py:340} INFO - ::group::Post task execution logs
[2024-12-17T23:32:57.839+0530] {logging_mixin.py:190} INFO - [2024-12-17T23:32:57.838+0530] {taskinstance.py:352} INFO - Marking task as SUCCESS. dag_id=movie_pipeline, task_id=scrape_collection_page, run_id=manual__2024-12-17T18:02:52.457624+00:00, execution_date=20241217T180252, start_date=, end_date=20241217T180257
[2024-12-17T23:32:57.848+0530] {logging_mixin.py:190} INFO - Task instance in success state
[2024-12-17T23:32:57.849+0530] {logging_mixin.py:190} INFO -  Previous state of the Task instance: running
[2024-12-17T23:32:57.849+0530] {logging_mixin.py:190} INFO - Dag name:movie_pipeline queued_at:None
[2024-12-17T23:32:57.849+0530] {logging_mixin.py:190} INFO - Task hostname:Jayeshs-MacBook-Air.local operator:PythonOperator
[2024-12-17T23:32:57.849+0530] {logging_mixin.py:190} INFO - [2024-12-17T23:32:57.849+0530] {dag.py:4410} INFO - [DAG TEST] end task task_id=scrape_collection_page map_index=-1
[2024-12-17T23:32:57.854+0530] {logging_mixin.py:190} INFO - [2024-12-17T23:32:57.853+0530] {dagrun.py:854} INFO - Marking run <DagRun movie_pipeline @ 2024-12-17 18:02:52.457624+00:00: manual__2024-12-17T18:02:52.457624+00:00, state:running, queued_at: None. externally triggered: False> successful
[2024-12-17T23:32:57.854+0530] {logging_mixin.py:190} INFO - Dag run in success state
[2024-12-17T23:32:57.854+0530] {logging_mixin.py:190} INFO - Dag run start:2024-12-17 18:02:52.457624+00:00 end:2024-12-17 18:02:57.854457+00:00
[2024-12-17T23:32:57.855+0530] {logging_mixin.py:190} INFO - [2024-12-17T23:32:57.855+0530] {dagrun.py:905} INFO - DagRun Finished: dag_id=movie_pipeline, execution_date=2024-12-17 18:02:52.457624+00:00, run_id=manual__2024-12-17T18:02:52.457624+00:00, run_start_date=2024-12-17 18:02:52.457624+00:00, run_end_date=2024-12-17 18:02:57.854457+00:00, run_duration=5.396833, state=success, external_trigger=False, run_type=manual, data_interval_start=2024-12-16 18:02:52.457624+00:00, data_interval_end=2024-12-17 18:02:52.457624+00:00, dag_hash=None
[2024-12-17T23:32:57.864+0530] {processor.py:925} INFO - DAG(s) 'movie_pipeline' retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T23:32:57.876+0530] {logging_mixin.py:190} INFO - [2024-12-17T23:32:57.876+0530] {dag.py:3239} INFO - Sync 1 DAGs
[2024-12-17T23:32:57.896+0530] {logging_mixin.py:190} INFO - [2024-12-17T23:32:57.896+0530] {dag.py:4180} INFO - Setting next_dagrun for movie_pipeline to 2024-12-17 01:30:00+00:00, run_after=2024-12-18 01:30:00+00:00
[2024-12-17T23:32:57.919+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 5.512 seconds
[2024-12-17T23:36:22.413+0530] {processor.py:186} INFO - Started process (PID=5182) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T23:36:22.416+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-17T23:36:22.417+0530] {logging_mixin.py:190} INFO - [2024-12-17T23:36:22.417+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T23:36:22.464+0530] {processor.py:925} INFO - DAG(s) 'movie_pipeline' retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T23:36:22.586+0530] {logging_mixin.py:190} INFO - [2024-12-17T23:36:22.585+0530] {dag.py:3239} INFO - Sync 1 DAGs
[2024-12-17T23:36:22.620+0530] {logging_mixin.py:190} INFO - [2024-12-17T23:36:22.619+0530] {dag.py:4180} INFO - Setting next_dagrun for movie_pipeline to 2024-12-17 01:30:00+00:00, run_after=2024-12-18 01:30:00+00:00
[2024-12-17T23:36:22.649+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 0.239 seconds
[2024-12-17T23:39:32.473+0530] {processor.py:186} INFO - Started process (PID=5565) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T23:39:32.474+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-17T23:39:32.475+0530] {logging_mixin.py:190} INFO - [2024-12-17T23:39:32.475+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T23:39:32.479+0530] {logging_mixin.py:190} INFO - [2024-12-17T23:39:32.478+0530] {dagbag.py:387} ERROR - Failed to import: /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
Traceback (most recent call last):
  File "/Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/.venv/lib/python3.12/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py", line 1, in <module>
    from extract import *
ModuleNotFoundError: No module named 'extract'
[2024-12-17T23:39:32.480+0530] {processor.py:927} WARNING - No viable dags retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T23:39:32.601+0530] {logging_mixin.py:190} INFO - [2024-12-17T23:39:32.601+0530] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-12-17T23:39:32.630+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 0.161 seconds
[2024-12-17T23:42:26.675+0530] {processor.py:186} INFO - Started process (PID=5911) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T23:42:26.676+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-17T23:42:26.678+0530] {logging_mixin.py:190} INFO - [2024-12-17T23:42:26.677+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T23:42:26.681+0530] {logging_mixin.py:190} INFO - [2024-12-17T23:42:26.679+0530] {dagbag.py:387} ERROR - Failed to import: /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
Traceback (most recent call last):
  File "/Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/.venv/lib/python3.12/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py", line 1, in <module>
    from extract import *
ModuleNotFoundError: No module named 'extract'
[2024-12-17T23:42:26.681+0530] {processor.py:927} WARNING - No viable dags retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T23:42:26.814+0530] {logging_mixin.py:190} INFO - [2024-12-17T23:42:26.814+0530] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-12-17T23:42:26.849+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 0.178 seconds
[2024-12-17T23:45:18.197+0530] {processor.py:186} INFO - Started process (PID=6248) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T23:45:18.198+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-17T23:45:18.199+0530] {logging_mixin.py:190} INFO - [2024-12-17T23:45:18.199+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T23:45:18.202+0530] {logging_mixin.py:190} INFO - [2024-12-17T23:45:18.201+0530] {dagbag.py:387} ERROR - Failed to import: /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
Traceback (most recent call last):
  File "/Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/.venv/lib/python3.12/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py", line 1, in <module>
    from extract import *
ModuleNotFoundError: No module named 'extract'
[2024-12-17T23:45:18.202+0530] {processor.py:927} WARNING - No viable dags retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T23:45:18.341+0530] {logging_mixin.py:190} INFO - [2024-12-17T23:45:18.340+0530] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-12-17T23:45:18.364+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 0.171 seconds
[2024-12-17T23:48:09.260+0530] {processor.py:186} INFO - Started process (PID=6589) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T23:48:09.262+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-17T23:48:09.263+0530] {logging_mixin.py:190} INFO - [2024-12-17T23:48:09.263+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T23:48:09.266+0530] {logging_mixin.py:190} INFO - [2024-12-17T23:48:09.265+0530] {dagbag.py:387} ERROR - Failed to import: /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
Traceback (most recent call last):
  File "/Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/.venv/lib/python3.12/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py", line 1, in <module>
    from extract import *
ModuleNotFoundError: No module named 'extract'
[2024-12-17T23:48:09.267+0530] {processor.py:927} WARNING - No viable dags retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T23:48:09.394+0530] {logging_mixin.py:190} INFO - [2024-12-17T23:48:09.394+0530] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-12-17T23:48:09.419+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 0.163 seconds
[2024-12-17T23:50:59.640+0530] {processor.py:186} INFO - Started process (PID=6919) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T23:50:59.643+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-17T23:50:59.645+0530] {logging_mixin.py:190} INFO - [2024-12-17T23:50:59.644+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T23:50:59.648+0530] {logging_mixin.py:190} INFO - [2024-12-17T23:50:59.646+0530] {dagbag.py:387} ERROR - Failed to import: /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
Traceback (most recent call last):
  File "/Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/.venv/lib/python3.12/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py", line 1, in <module>
    from extract import *
ModuleNotFoundError: No module named 'extract'
[2024-12-17T23:50:59.649+0530] {processor.py:927} WARNING - No viable dags retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T23:50:59.781+0530] {logging_mixin.py:190} INFO - [2024-12-17T23:50:59.781+0530] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-12-17T23:50:59.804+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 0.168 seconds
[2024-12-17T23:53:51.307+0530] {processor.py:186} INFO - Started process (PID=7253) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T23:53:51.308+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-17T23:53:51.310+0530] {logging_mixin.py:190} INFO - [2024-12-17T23:53:51.309+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T23:53:51.313+0530] {logging_mixin.py:190} INFO - [2024-12-17T23:53:51.311+0530] {dagbag.py:387} ERROR - Failed to import: /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
Traceback (most recent call last):
  File "/Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/.venv/lib/python3.12/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py", line 1, in <module>
    from extract import *
ModuleNotFoundError: No module named 'extract'
[2024-12-17T23:53:51.313+0530] {processor.py:927} WARNING - No viable dags retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T23:53:51.436+0530] {logging_mixin.py:190} INFO - [2024-12-17T23:53:51.436+0530] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-12-17T23:53:51.465+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 0.162 seconds
[2024-12-17T23:56:42.528+0530] {processor.py:186} INFO - Started process (PID=7607) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T23:56:42.529+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-17T23:56:42.531+0530] {logging_mixin.py:190} INFO - [2024-12-17T23:56:42.530+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T23:56:42.533+0530] {logging_mixin.py:190} INFO - [2024-12-17T23:56:42.532+0530] {dagbag.py:387} ERROR - Failed to import: /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
Traceback (most recent call last):
  File "/Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/.venv/lib/python3.12/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py", line 1, in <module>
    from extract import *
ModuleNotFoundError: No module named 'extract'
[2024-12-17T23:56:42.534+0530] {processor.py:927} WARNING - No viable dags retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T23:56:42.657+0530] {logging_mixin.py:190} INFO - [2024-12-17T23:56:42.656+0530] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-12-17T23:56:42.684+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 0.161 seconds
[2024-12-17T23:59:28.434+0530] {processor.py:186} INFO - Started process (PID=7872) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T23:59:28.435+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-17T23:59:28.436+0530] {logging_mixin.py:190} INFO - [2024-12-17T23:59:28.436+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T23:59:28.439+0530] {logging_mixin.py:190} INFO - [2024-12-17T23:59:28.438+0530] {dagbag.py:387} ERROR - Failed to import: /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
Traceback (most recent call last):
  File "/Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/.venv/lib/python3.12/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py", line 1, in <module>
    from extract import *
ModuleNotFoundError: No module named 'extract'
[2024-12-17T23:59:28.440+0530] {processor.py:927} WARNING - No viable dags retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-17T23:59:28.566+0530] {logging_mixin.py:190} INFO - [2024-12-17T23:59:28.565+0530] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-12-17T23:59:28.589+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 0.158 seconds
[2024-12-18T00:02:05.328+0530] {processor.py:186} INFO - Started process (PID=8059) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-18T00:02:05.331+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-18T00:02:05.332+0530] {logging_mixin.py:190} INFO - [2024-12-18T00:02:05.332+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-18T00:02:05.335+0530] {logging_mixin.py:190} INFO - [2024-12-18T00:02:05.334+0530] {dagbag.py:387} ERROR - Failed to import: /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
Traceback (most recent call last):
  File "/Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/.venv/lib/python3.12/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py", line 1, in <module>
    from extract import *
ModuleNotFoundError: No module named 'extract'
[2024-12-18T00:02:05.335+0530] {processor.py:927} WARNING - No viable dags retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-18T00:02:05.454+0530] {logging_mixin.py:190} INFO - [2024-12-18T00:02:05.454+0530] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-12-18T00:02:05.480+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 0.155 seconds
[2024-12-18T00:04:43.790+0530] {processor.py:186} INFO - Started process (PID=8256) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-18T00:04:43.791+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-18T00:04:43.793+0530] {logging_mixin.py:190} INFO - [2024-12-18T00:04:43.792+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-18T00:04:43.795+0530] {logging_mixin.py:190} INFO - [2024-12-18T00:04:43.794+0530] {dagbag.py:387} ERROR - Failed to import: /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
Traceback (most recent call last):
  File "/Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/.venv/lib/python3.12/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py", line 1, in <module>
    from extract import *
ModuleNotFoundError: No module named 'extract'
[2024-12-18T00:04:43.795+0530] {processor.py:927} WARNING - No viable dags retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-18T00:04:43.911+0530] {logging_mixin.py:190} INFO - [2024-12-18T00:04:43.910+0530] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-12-18T00:04:43.934+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 0.147 seconds
[2024-12-18T00:07:22.626+0530] {processor.py:186} INFO - Started process (PID=8448) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-18T00:07:22.628+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-18T00:07:22.629+0530] {logging_mixin.py:190} INFO - [2024-12-18T00:07:22.628+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-18T00:07:22.631+0530] {logging_mixin.py:190} INFO - [2024-12-18T00:07:22.630+0530] {dagbag.py:387} ERROR - Failed to import: /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
Traceback (most recent call last):
  File "/Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/.venv/lib/python3.12/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py", line 1, in <module>
    from extract import *
ModuleNotFoundError: No module named 'extract'
[2024-12-18T00:07:22.632+0530] {processor.py:927} WARNING - No viable dags retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-18T00:07:22.748+0530] {logging_mixin.py:190} INFO - [2024-12-18T00:07:22.748+0530] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-12-18T00:07:22.772+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 0.149 seconds
[2024-12-18T00:09:58.697+0530] {processor.py:186} INFO - Started process (PID=8632) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-18T00:09:58.698+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-18T00:09:58.700+0530] {logging_mixin.py:190} INFO - [2024-12-18T00:09:58.699+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-18T00:09:58.703+0530] {logging_mixin.py:190} INFO - [2024-12-18T00:09:58.701+0530] {dagbag.py:387} ERROR - Failed to import: /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
Traceback (most recent call last):
  File "/Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/.venv/lib/python3.12/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py", line 1, in <module>
    from extract import *
ModuleNotFoundError: No module named 'extract'
[2024-12-18T00:09:58.703+0530] {processor.py:927} WARNING - No viable dags retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-18T00:09:58.821+0530] {logging_mixin.py:190} INFO - [2024-12-18T00:09:58.821+0530] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-12-18T00:09:58.845+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 0.152 seconds
[2024-12-18T00:12:33.090+0530] {processor.py:186} INFO - Started process (PID=8828) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-18T00:12:33.092+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-18T00:12:33.093+0530] {logging_mixin.py:190} INFO - [2024-12-18T00:12:33.093+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-18T00:12:33.095+0530] {logging_mixin.py:190} INFO - [2024-12-18T00:12:33.094+0530] {dagbag.py:387} ERROR - Failed to import: /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
Traceback (most recent call last):
  File "/Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/.venv/lib/python3.12/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py", line 1, in <module>
    from extract import *
ModuleNotFoundError: No module named 'extract'
[2024-12-18T00:12:33.096+0530] {processor.py:927} WARNING - No viable dags retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-18T00:12:33.209+0530] {logging_mixin.py:190} INFO - [2024-12-18T00:12:33.209+0530] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-12-18T00:12:33.232+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 0.145 seconds
[2024-12-18T00:15:07.956+0530] {processor.py:186} INFO - Started process (PID=9012) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-18T00:15:07.957+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-18T00:15:07.959+0530] {logging_mixin.py:190} INFO - [2024-12-18T00:15:07.958+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-18T00:15:07.961+0530] {logging_mixin.py:190} INFO - [2024-12-18T00:15:07.960+0530] {dagbag.py:387} ERROR - Failed to import: /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
Traceback (most recent call last):
  File "/Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/.venv/lib/python3.12/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py", line 1, in <module>
    from extract import *
ModuleNotFoundError: No module named 'extract'
[2024-12-18T00:15:07.962+0530] {processor.py:927} WARNING - No viable dags retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-18T00:15:08.081+0530] {logging_mixin.py:190} INFO - [2024-12-18T00:15:08.081+0530] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-12-18T00:15:08.105+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 0.153 seconds
[2024-12-18T00:17:41.596+0530] {processor.py:186} INFO - Started process (PID=9194) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-18T00:17:41.597+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-18T00:17:41.598+0530] {logging_mixin.py:190} INFO - [2024-12-18T00:17:41.598+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-18T00:17:41.601+0530] {logging_mixin.py:190} INFO - [2024-12-18T00:17:41.600+0530] {dagbag.py:387} ERROR - Failed to import: /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
Traceback (most recent call last):
  File "/Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/.venv/lib/python3.12/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py", line 1, in <module>
    from extract import *
ModuleNotFoundError: No module named 'extract'
[2024-12-18T00:17:41.601+0530] {processor.py:927} WARNING - No viable dags retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-18T00:17:41.721+0530] {logging_mixin.py:190} INFO - [2024-12-18T00:17:41.720+0530] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-12-18T00:17:41.744+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 0.152 seconds
[2024-12-18T00:20:19.517+0530] {processor.py:186} INFO - Started process (PID=9391) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-18T00:20:19.518+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-18T00:20:19.519+0530] {logging_mixin.py:190} INFO - [2024-12-18T00:20:19.519+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-18T00:20:19.522+0530] {logging_mixin.py:190} INFO - [2024-12-18T00:20:19.521+0530] {dagbag.py:387} ERROR - Failed to import: /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
Traceback (most recent call last):
  File "/Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/.venv/lib/python3.12/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py", line 1, in <module>
    from extract import *
ModuleNotFoundError: No module named 'extract'
[2024-12-18T00:20:19.522+0530] {processor.py:927} WARNING - No viable dags retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-18T00:20:19.641+0530] {logging_mixin.py:190} INFO - [2024-12-18T00:20:19.640+0530] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-12-18T00:20:19.665+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 0.151 seconds
[2024-12-18T00:22:57.223+0530] {processor.py:186} INFO - Started process (PID=9584) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-18T00:22:57.226+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-18T00:22:57.227+0530] {logging_mixin.py:190} INFO - [2024-12-18T00:22:57.227+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-18T00:22:57.230+0530] {logging_mixin.py:190} INFO - [2024-12-18T00:22:57.229+0530] {dagbag.py:387} ERROR - Failed to import: /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
Traceback (most recent call last):
  File "/Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/.venv/lib/python3.12/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py", line 1, in <module>
    from extract import *
ModuleNotFoundError: No module named 'extract'
[2024-12-18T00:22:57.230+0530] {processor.py:927} WARNING - No viable dags retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-18T00:22:57.347+0530] {logging_mixin.py:190} INFO - [2024-12-18T00:22:57.347+0530] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-12-18T00:22:57.372+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 0.153 seconds
[2024-12-18T00:25:37.782+0530] {processor.py:186} INFO - Started process (PID=9776) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-18T00:25:37.783+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-18T00:25:37.784+0530] {logging_mixin.py:190} INFO - [2024-12-18T00:25:37.784+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-18T00:25:37.787+0530] {logging_mixin.py:190} INFO - [2024-12-18T00:25:37.786+0530] {dagbag.py:387} ERROR - Failed to import: /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
Traceback (most recent call last):
  File "/Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/.venv/lib/python3.12/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py", line 1, in <module>
    from extract import *
ModuleNotFoundError: No module named 'extract'
[2024-12-18T00:25:37.788+0530] {processor.py:927} WARNING - No viable dags retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-18T00:25:37.904+0530] {logging_mixin.py:190} INFO - [2024-12-18T00:25:37.903+0530] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-12-18T00:25:37.927+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 0.148 seconds
[2024-12-18T00:28:16.894+0530] {processor.py:186} INFO - Started process (PID=9974) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-18T00:28:16.895+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-18T00:28:16.896+0530] {logging_mixin.py:190} INFO - [2024-12-18T00:28:16.896+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-18T00:28:16.899+0530] {logging_mixin.py:190} INFO - [2024-12-18T00:28:16.898+0530] {dagbag.py:387} ERROR - Failed to import: /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
Traceback (most recent call last):
  File "/Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/.venv/lib/python3.12/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py", line 1, in <module>
    from extract import *
ModuleNotFoundError: No module named 'extract'
[2024-12-18T00:28:16.899+0530] {processor.py:927} WARNING - No viable dags retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-18T00:28:17.017+0530] {logging_mixin.py:190} INFO - [2024-12-18T00:28:17.017+0530] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-12-18T00:28:17.041+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 0.151 seconds
[2024-12-18T00:30:57.780+0530] {processor.py:186} INFO - Started process (PID=10161) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-18T00:30:57.781+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-18T00:30:57.782+0530] {logging_mixin.py:190} INFO - [2024-12-18T00:30:57.782+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-18T00:30:57.785+0530] {logging_mixin.py:190} INFO - [2024-12-18T00:30:57.783+0530] {dagbag.py:387} ERROR - Failed to import: /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
Traceback (most recent call last):
  File "/Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/.venv/lib/python3.12/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py", line 1, in <module>
    from extract import *
ModuleNotFoundError: No module named 'extract'
[2024-12-18T00:30:57.785+0530] {processor.py:927} WARNING - No viable dags retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-18T00:30:57.900+0530] {logging_mixin.py:190} INFO - [2024-12-18T00:30:57.900+0530] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-12-18T00:30:57.923+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 0.147 seconds
[2024-12-18T00:33:35.043+0530] {processor.py:186} INFO - Started process (PID=10345) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-18T00:33:35.044+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-18T00:33:35.045+0530] {logging_mixin.py:190} INFO - [2024-12-18T00:33:35.045+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-18T00:33:35.048+0530] {logging_mixin.py:190} INFO - [2024-12-18T00:33:35.047+0530] {dagbag.py:387} ERROR - Failed to import: /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
Traceback (most recent call last):
  File "/Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/.venv/lib/python3.12/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py", line 1, in <module>
    from extract import *
ModuleNotFoundError: No module named 'extract'
[2024-12-18T00:33:35.048+0530] {processor.py:927} WARNING - No viable dags retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-18T00:33:35.163+0530] {logging_mixin.py:190} INFO - [2024-12-18T00:33:35.163+0530] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-12-18T00:33:35.187+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 0.148 seconds
[2024-12-18T00:36:15.396+0530] {processor.py:186} INFO - Started process (PID=10544) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-18T00:36:15.397+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-18T00:36:15.398+0530] {logging_mixin.py:190} INFO - [2024-12-18T00:36:15.398+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-18T00:36:15.401+0530] {logging_mixin.py:190} INFO - [2024-12-18T00:36:15.400+0530] {dagbag.py:387} ERROR - Failed to import: /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
Traceback (most recent call last):
  File "/Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/.venv/lib/python3.12/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py", line 1, in <module>
    from extract import *
ModuleNotFoundError: No module named 'extract'
[2024-12-18T00:36:15.401+0530] {processor.py:927} WARNING - No viable dags retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-18T00:36:15.517+0530] {logging_mixin.py:190} INFO - [2024-12-18T00:36:15.517+0530] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-12-18T00:36:15.540+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 0.148 seconds
[2024-12-18T00:38:54.894+0530] {processor.py:186} INFO - Started process (PID=10730) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-18T00:38:54.895+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-18T00:38:54.897+0530] {logging_mixin.py:190} INFO - [2024-12-18T00:38:54.896+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-18T00:38:54.899+0530] {logging_mixin.py:190} INFO - [2024-12-18T00:38:54.898+0530] {dagbag.py:387} ERROR - Failed to import: /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
Traceback (most recent call last):
  File "/Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/.venv/lib/python3.12/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py", line 1, in <module>
    from extract import *
ModuleNotFoundError: No module named 'extract'
[2024-12-18T00:38:54.900+0530] {processor.py:927} WARNING - No viable dags retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-18T00:38:55.016+0530] {logging_mixin.py:190} INFO - [2024-12-18T00:38:55.016+0530] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-12-18T00:38:55.040+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 0.149 seconds
[2024-12-18T00:41:29.652+0530] {processor.py:186} INFO - Started process (PID=10914) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-18T00:41:29.653+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-18T00:41:29.655+0530] {logging_mixin.py:190} INFO - [2024-12-18T00:41:29.654+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-18T00:41:29.658+0530] {logging_mixin.py:190} INFO - [2024-12-18T00:41:29.657+0530] {dagbag.py:387} ERROR - Failed to import: /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
Traceback (most recent call last):
  File "/Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/.venv/lib/python3.12/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py", line 1, in <module>
    from extract import *
ModuleNotFoundError: No module named 'extract'
[2024-12-18T00:41:29.658+0530] {processor.py:927} WARNING - No viable dags retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-18T00:41:29.775+0530] {logging_mixin.py:190} INFO - [2024-12-18T00:41:29.775+0530] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-12-18T00:41:29.798+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 0.150 seconds
[2024-12-18T00:44:09.338+0530] {processor.py:186} INFO - Started process (PID=11115) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-18T00:44:09.341+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-18T00:44:09.343+0530] {logging_mixin.py:190} INFO - [2024-12-18T00:44:09.342+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-18T00:44:09.345+0530] {logging_mixin.py:190} INFO - [2024-12-18T00:44:09.344+0530] {dagbag.py:387} ERROR - Failed to import: /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
Traceback (most recent call last):
  File "/Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/.venv/lib/python3.12/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py", line 1, in <module>
    from extract import *
ModuleNotFoundError: No module named 'extract'
[2024-12-18T00:44:09.346+0530] {processor.py:927} WARNING - No viable dags retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-18T00:44:09.471+0530] {logging_mixin.py:190} INFO - [2024-12-18T00:44:09.471+0530] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-12-18T00:44:09.499+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 0.164 seconds
[2024-12-18T00:46:56.445+0530] {processor.py:186} INFO - Started process (PID=11314) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-18T00:46:56.447+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-18T00:46:56.449+0530] {logging_mixin.py:190} INFO - [2024-12-18T00:46:56.449+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-18T00:46:56.455+0530] {logging_mixin.py:190} INFO - [2024-12-18T00:46:56.452+0530] {dagbag.py:387} ERROR - Failed to import: /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
Traceback (most recent call last):
  File "/Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/.venv/lib/python3.12/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py", line 1, in <module>
    from extract import *
ModuleNotFoundError: No module named 'extract'
[2024-12-18T00:46:56.456+0530] {processor.py:927} WARNING - No viable dags retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-18T00:46:56.605+0530] {logging_mixin.py:190} INFO - [2024-12-18T00:46:56.605+0530] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-12-18T00:46:56.632+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 0.191 seconds
[2024-12-18T00:49:46.775+0530] {processor.py:186} INFO - Started process (PID=11507) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-18T00:49:46.778+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-18T00:49:46.779+0530] {logging_mixin.py:190} INFO - [2024-12-18T00:49:46.779+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-18T00:49:46.787+0530] {logging_mixin.py:190} INFO - [2024-12-18T00:49:46.783+0530] {dagbag.py:387} ERROR - Failed to import: /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
Traceback (most recent call last):
  File "/Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/.venv/lib/python3.12/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py", line 1, in <module>
    from extract import *
ModuleNotFoundError: No module named 'extract'
[2024-12-18T00:49:46.787+0530] {processor.py:927} WARNING - No viable dags retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-18T00:49:46.937+0530] {logging_mixin.py:190} INFO - [2024-12-18T00:49:46.936+0530] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-12-18T00:49:46.971+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 0.200 seconds
[2024-12-18T00:52:38.861+0530] {processor.py:186} INFO - Started process (PID=11730) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-18T00:52:38.864+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-18T00:52:38.865+0530] {logging_mixin.py:190} INFO - [2024-12-18T00:52:38.865+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-18T00:52:38.869+0530] {logging_mixin.py:190} INFO - [2024-12-18T00:52:38.868+0530] {dagbag.py:387} ERROR - Failed to import: /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
Traceback (most recent call last):
  File "/Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/.venv/lib/python3.12/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py", line 1, in <module>
    from extract import *
ModuleNotFoundError: No module named 'extract'
[2024-12-18T00:52:38.870+0530] {processor.py:927} WARNING - No viable dags retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-18T00:52:39.003+0530] {logging_mixin.py:190} INFO - [2024-12-18T00:52:39.003+0530] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-12-18T00:52:39.034+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 0.177 seconds
[2024-12-18T00:55:28.099+0530] {processor.py:186} INFO - Started process (PID=11946) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-18T00:55:28.101+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-18T00:55:28.103+0530] {logging_mixin.py:190} INFO - [2024-12-18T00:55:28.103+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-18T00:55:28.108+0530] {logging_mixin.py:190} INFO - [2024-12-18T00:55:28.106+0530] {dagbag.py:387} ERROR - Failed to import: /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
Traceback (most recent call last):
  File "/Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/.venv/lib/python3.12/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py", line 1, in <module>
    from extract import *
ModuleNotFoundError: No module named 'extract'
[2024-12-18T00:55:28.109+0530] {processor.py:927} WARNING - No viable dags retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-18T00:55:28.245+0530] {logging_mixin.py:190} INFO - [2024-12-18T00:55:28.245+0530] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-12-18T00:55:28.275+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 0.181 seconds
[2024-12-18T00:58:14.424+0530] {processor.py:186} INFO - Started process (PID=12155) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-18T00:58:14.427+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-18T00:58:14.429+0530] {logging_mixin.py:190} INFO - [2024-12-18T00:58:14.428+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-18T00:58:14.434+0530] {logging_mixin.py:190} INFO - [2024-12-18T00:58:14.432+0530] {dagbag.py:387} ERROR - Failed to import: /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
Traceback (most recent call last):
  File "/Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/.venv/lib/python3.12/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py", line 1, in <module>
    from extract import *
ModuleNotFoundError: No module named 'extract'
[2024-12-18T00:58:14.435+0530] {processor.py:927} WARNING - No viable dags retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-18T00:58:14.561+0530] {logging_mixin.py:190} INFO - [2024-12-18T00:58:14.561+0530] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-12-18T00:58:14.584+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 0.164 seconds
[2024-12-18T01:00:54.342+0530] {processor.py:186} INFO - Started process (PID=12341) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-18T01:00:54.343+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-18T01:00:54.344+0530] {logging_mixin.py:190} INFO - [2024-12-18T01:00:54.344+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-18T01:00:54.346+0530] {logging_mixin.py:190} INFO - [2024-12-18T01:00:54.345+0530] {dagbag.py:387} ERROR - Failed to import: /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
Traceback (most recent call last):
  File "/Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/.venv/lib/python3.12/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py", line 1, in <module>
    from extract import *
ModuleNotFoundError: No module named 'extract'
[2024-12-18T01:00:54.347+0530] {processor.py:927} WARNING - No viable dags retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-18T01:00:54.462+0530] {logging_mixin.py:190} INFO - [2024-12-18T01:00:54.462+0530] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-12-18T01:00:54.487+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 0.149 seconds
[2024-12-18T01:03:32.008+0530] {processor.py:186} INFO - Started process (PID=12528) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-18T01:03:32.009+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-18T01:03:32.010+0530] {logging_mixin.py:190} INFO - [2024-12-18T01:03:32.010+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-18T01:03:32.013+0530] {logging_mixin.py:190} INFO - [2024-12-18T01:03:32.012+0530] {dagbag.py:387} ERROR - Failed to import: /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
Traceback (most recent call last):
  File "/Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/.venv/lib/python3.12/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py", line 1, in <module>
    from extract import *
ModuleNotFoundError: No module named 'extract'
[2024-12-18T01:03:32.013+0530] {processor.py:927} WARNING - No viable dags retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-18T01:03:32.131+0530] {logging_mixin.py:190} INFO - [2024-12-18T01:03:32.131+0530] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-12-18T01:03:32.154+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 0.150 seconds
[2024-12-18T01:06:07.908+0530] {processor.py:186} INFO - Started process (PID=12724) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-18T01:06:07.909+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-18T01:06:07.911+0530] {logging_mixin.py:190} INFO - [2024-12-18T01:06:07.910+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-18T01:06:07.913+0530] {logging_mixin.py:190} INFO - [2024-12-18T01:06:07.912+0530] {dagbag.py:387} ERROR - Failed to import: /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
Traceback (most recent call last):
  File "/Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/.venv/lib/python3.12/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py", line 1, in <module>
    from extract import *
ModuleNotFoundError: No module named 'extract'
[2024-12-18T01:06:07.914+0530] {processor.py:927} WARNING - No viable dags retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-18T01:06:08.044+0530] {logging_mixin.py:190} INFO - [2024-12-18T01:06:08.044+0530] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-12-18T01:06:08.068+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 0.163 seconds
[2024-12-18T01:08:42.869+0530] {processor.py:186} INFO - Started process (PID=12908) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-18T01:08:42.871+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-18T01:08:42.872+0530] {logging_mixin.py:190} INFO - [2024-12-18T01:08:42.872+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-18T01:08:42.874+0530] {logging_mixin.py:190} INFO - [2024-12-18T01:08:42.873+0530] {dagbag.py:387} ERROR - Failed to import: /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
Traceback (most recent call last):
  File "/Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/.venv/lib/python3.12/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py", line 1, in <module>
    from extract import *
ModuleNotFoundError: No module named 'extract'
[2024-12-18T01:08:42.875+0530] {processor.py:927} WARNING - No viable dags retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-18T01:08:42.990+0530] {logging_mixin.py:190} INFO - [2024-12-18T01:08:42.990+0530] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-12-18T01:08:43.013+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 0.147 seconds
[2024-12-18T01:11:22.953+0530] {processor.py:186} INFO - Started process (PID=13097) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-18T01:11:22.954+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-18T01:11:22.955+0530] {logging_mixin.py:190} INFO - [2024-12-18T01:11:22.955+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-18T01:11:22.958+0530] {logging_mixin.py:190} INFO - [2024-12-18T01:11:22.957+0530] {dagbag.py:387} ERROR - Failed to import: /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
Traceback (most recent call last):
  File "/Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/.venv/lib/python3.12/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py", line 1, in <module>
    from extract import *
ModuleNotFoundError: No module named 'extract'
[2024-12-18T01:11:22.958+0530] {processor.py:927} WARNING - No viable dags retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-18T01:11:23.072+0530] {logging_mixin.py:190} INFO - [2024-12-18T01:11:23.071+0530] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-12-18T01:11:23.097+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 0.148 seconds
[2024-12-18T01:14:01.602+0530] {processor.py:186} INFO - Started process (PID=13285) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-18T01:14:01.603+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-18T01:14:01.604+0530] {logging_mixin.py:190} INFO - [2024-12-18T01:14:01.604+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-18T01:14:01.607+0530] {logging_mixin.py:190} INFO - [2024-12-18T01:14:01.606+0530] {dagbag.py:387} ERROR - Failed to import: /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
Traceback (most recent call last):
  File "/Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/.venv/lib/python3.12/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py", line 1, in <module>
    from extract import *
ModuleNotFoundError: No module named 'extract'
[2024-12-18T01:14:01.608+0530] {processor.py:927} WARNING - No viable dags retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-18T01:14:01.724+0530] {logging_mixin.py:190} INFO - [2024-12-18T01:14:01.723+0530] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-12-18T01:14:01.749+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 0.151 seconds
[2024-12-18T01:16:39.215+0530] {processor.py:186} INFO - Started process (PID=13481) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-18T01:16:39.216+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-18T01:16:39.217+0530] {logging_mixin.py:190} INFO - [2024-12-18T01:16:39.217+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-18T01:16:39.220+0530] {logging_mixin.py:190} INFO - [2024-12-18T01:16:39.219+0530] {dagbag.py:387} ERROR - Failed to import: /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
Traceback (most recent call last):
  File "/Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/.venv/lib/python3.12/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py", line 1, in <module>
    from extract import *
ModuleNotFoundError: No module named 'extract'
[2024-12-18T01:16:39.220+0530] {processor.py:927} WARNING - No viable dags retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-18T01:16:39.335+0530] {logging_mixin.py:190} INFO - [2024-12-18T01:16:39.335+0530] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-12-18T01:16:39.358+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 0.146 seconds
[2024-12-18T01:19:13.254+0530] {processor.py:186} INFO - Started process (PID=13664) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-18T01:19:13.255+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-18T01:19:13.256+0530] {logging_mixin.py:190} INFO - [2024-12-18T01:19:13.256+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-18T01:19:13.259+0530] {logging_mixin.py:190} INFO - [2024-12-18T01:19:13.258+0530] {dagbag.py:387} ERROR - Failed to import: /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
Traceback (most recent call last):
  File "/Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/.venv/lib/python3.12/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py", line 1, in <module>
    from extract import *
ModuleNotFoundError: No module named 'extract'
[2024-12-18T01:19:13.259+0530] {processor.py:927} WARNING - No viable dags retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-18T01:19:13.373+0530] {logging_mixin.py:190} INFO - [2024-12-18T01:19:13.373+0530] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-12-18T01:19:13.396+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 0.146 seconds
[2024-12-18T01:21:47.637+0530] {processor.py:186} INFO - Started process (PID=13850) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-18T01:21:47.638+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-18T01:21:47.639+0530] {logging_mixin.py:190} INFO - [2024-12-18T01:21:47.639+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-18T01:21:47.642+0530] {logging_mixin.py:190} INFO - [2024-12-18T01:21:47.641+0530] {dagbag.py:387} ERROR - Failed to import: /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
Traceback (most recent call last):
  File "/Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/.venv/lib/python3.12/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py", line 1, in <module>
    from extract import *
ModuleNotFoundError: No module named 'extract'
[2024-12-18T01:21:47.642+0530] {processor.py:927} WARNING - No viable dags retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-18T01:21:47.761+0530] {logging_mixin.py:190} INFO - [2024-12-18T01:21:47.760+0530] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-12-18T01:21:47.784+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 0.150 seconds
[2024-12-18T01:24:28.497+0530] {processor.py:186} INFO - Started process (PID=14057) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-18T01:24:28.499+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-18T01:24:28.501+0530] {logging_mixin.py:190} INFO - [2024-12-18T01:24:28.500+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-18T01:24:28.503+0530] {logging_mixin.py:190} INFO - [2024-12-18T01:24:28.502+0530] {dagbag.py:387} ERROR - Failed to import: /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
Traceback (most recent call last):
  File "/Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/.venv/lib/python3.12/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py", line 1, in <module>
    from extract import *
ModuleNotFoundError: No module named 'extract'
[2024-12-18T01:24:28.504+0530] {processor.py:927} WARNING - No viable dags retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-18T01:24:28.618+0530] {logging_mixin.py:190} INFO - [2024-12-18T01:24:28.618+0530] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-12-18T01:24:28.642+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 0.149 seconds
[2024-12-18T01:27:05.943+0530] {processor.py:186} INFO - Started process (PID=14244) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-18T01:27:05.944+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-18T01:27:05.945+0530] {logging_mixin.py:190} INFO - [2024-12-18T01:27:05.945+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-18T01:27:05.948+0530] {logging_mixin.py:190} INFO - [2024-12-18T01:27:05.947+0530] {dagbag.py:387} ERROR - Failed to import: /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
Traceback (most recent call last):
  File "/Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/.venv/lib/python3.12/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py", line 1, in <module>
    from extract import *
ModuleNotFoundError: No module named 'extract'
[2024-12-18T01:27:05.948+0530] {processor.py:927} WARNING - No viable dags retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-18T01:27:06.065+0530] {logging_mixin.py:190} INFO - [2024-12-18T01:27:06.065+0530] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-12-18T01:27:06.089+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 0.149 seconds
[2024-12-18T01:29:44.457+0530] {processor.py:186} INFO - Started process (PID=14432) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-18T01:29:44.458+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-18T01:29:44.459+0530] {logging_mixin.py:190} INFO - [2024-12-18T01:29:44.459+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-18T01:29:44.462+0530] {logging_mixin.py:190} INFO - [2024-12-18T01:29:44.461+0530] {dagbag.py:387} ERROR - Failed to import: /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
Traceback (most recent call last):
  File "/Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/.venv/lib/python3.12/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py", line 1, in <module>
    from extract import *
ModuleNotFoundError: No module named 'extract'
[2024-12-18T01:29:44.462+0530] {processor.py:927} WARNING - No viable dags retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-18T01:29:44.580+0530] {logging_mixin.py:190} INFO - [2024-12-18T01:29:44.579+0530] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-12-18T01:29:44.602+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 0.149 seconds
[2024-12-18T01:32:21.392+0530] {processor.py:186} INFO - Started process (PID=14631) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-18T01:32:21.393+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-18T01:32:21.394+0530] {logging_mixin.py:190} INFO - [2024-12-18T01:32:21.394+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-18T01:32:21.396+0530] {logging_mixin.py:190} INFO - [2024-12-18T01:32:21.395+0530] {dagbag.py:387} ERROR - Failed to import: /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
Traceback (most recent call last):
  File "/Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/.venv/lib/python3.12/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py", line 1, in <module>
    from extract import *
ModuleNotFoundError: No module named 'extract'
[2024-12-18T01:32:21.397+0530] {processor.py:927} WARNING - No viable dags retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-18T01:32:21.512+0530] {logging_mixin.py:190} INFO - [2024-12-18T01:32:21.512+0530] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-12-18T01:32:21.535+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 0.146 seconds
[2024-12-18T01:34:58.759+0530] {processor.py:186} INFO - Started process (PID=14818) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-18T01:34:58.761+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-18T01:34:58.762+0530] {logging_mixin.py:190} INFO - [2024-12-18T01:34:58.761+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-18T01:34:58.764+0530] {logging_mixin.py:190} INFO - [2024-12-18T01:34:58.763+0530] {dagbag.py:387} ERROR - Failed to import: /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
Traceback (most recent call last):
  File "/Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/.venv/lib/python3.12/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py", line 1, in <module>
    from extract import *
ModuleNotFoundError: No module named 'extract'
[2024-12-18T01:34:58.765+0530] {processor.py:927} WARNING - No viable dags retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-18T01:34:58.886+0530] {logging_mixin.py:190} INFO - [2024-12-18T01:34:58.885+0530] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-12-18T01:34:58.909+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 0.153 seconds
[2024-12-18T01:37:37.125+0530] {processor.py:186} INFO - Started process (PID=15006) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-18T01:37:37.126+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-18T01:37:37.127+0530] {logging_mixin.py:190} INFO - [2024-12-18T01:37:37.127+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-18T01:37:37.129+0530] {logging_mixin.py:190} INFO - [2024-12-18T01:37:37.128+0530] {dagbag.py:387} ERROR - Failed to import: /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
Traceback (most recent call last):
  File "/Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/.venv/lib/python3.12/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py", line 1, in <module>
    from extract import *
ModuleNotFoundError: No module named 'extract'
[2024-12-18T01:37:37.130+0530] {processor.py:927} WARNING - No viable dags retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-18T01:37:37.244+0530] {logging_mixin.py:190} INFO - [2024-12-18T01:37:37.244+0530] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-12-18T01:37:37.267+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 0.146 seconds
[2024-12-18T01:40:16.603+0530] {processor.py:186} INFO - Started process (PID=15204) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-18T01:40:16.604+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-18T01:40:16.605+0530] {logging_mixin.py:190} INFO - [2024-12-18T01:40:16.605+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-18T01:40:16.608+0530] {logging_mixin.py:190} INFO - [2024-12-18T01:40:16.607+0530] {dagbag.py:387} ERROR - Failed to import: /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
Traceback (most recent call last):
  File "/Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/.venv/lib/python3.12/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py", line 1, in <module>
    from extract import *
ModuleNotFoundError: No module named 'extract'
[2024-12-18T01:40:16.608+0530] {processor.py:927} WARNING - No viable dags retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-18T01:40:16.727+0530] {logging_mixin.py:190} INFO - [2024-12-18T01:40:16.726+0530] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-12-18T01:40:16.751+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 0.151 seconds
[2024-12-18T01:42:56.315+0530] {processor.py:186} INFO - Started process (PID=15390) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-18T01:42:56.316+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-18T01:42:56.318+0530] {logging_mixin.py:190} INFO - [2024-12-18T01:42:56.317+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-18T01:42:56.320+0530] {logging_mixin.py:190} INFO - [2024-12-18T01:42:56.319+0530] {dagbag.py:387} ERROR - Failed to import: /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
Traceback (most recent call last):
  File "/Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/.venv/lib/python3.12/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py", line 1, in <module>
    from extract import *
ModuleNotFoundError: No module named 'extract'
[2024-12-18T01:42:56.320+0530] {processor.py:927} WARNING - No viable dags retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-18T01:42:56.434+0530] {logging_mixin.py:190} INFO - [2024-12-18T01:42:56.434+0530] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-12-18T01:42:56.457+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 0.146 seconds
[2024-12-18T01:45:37.287+0530] {processor.py:186} INFO - Started process (PID=15584) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-18T01:45:37.288+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-18T01:45:37.290+0530] {logging_mixin.py:190} INFO - [2024-12-18T01:45:37.289+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-18T01:45:37.292+0530] {logging_mixin.py:190} INFO - [2024-12-18T01:45:37.291+0530] {dagbag.py:387} ERROR - Failed to import: /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
Traceback (most recent call last):
  File "/Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/.venv/lib/python3.12/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py", line 1, in <module>
    from extract import *
ModuleNotFoundError: No module named 'extract'
[2024-12-18T01:45:37.292+0530] {processor.py:927} WARNING - No viable dags retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-18T01:45:37.407+0530] {logging_mixin.py:190} INFO - [2024-12-18T01:45:37.406+0530] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-12-18T01:45:37.430+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 0.147 seconds
[2024-12-18T01:48:09.273+0530] {processor.py:186} INFO - Started process (PID=15774) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-18T01:48:09.274+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-18T01:48:09.275+0530] {logging_mixin.py:190} INFO - [2024-12-18T01:48:09.275+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-18T01:48:09.278+0530] {logging_mixin.py:190} INFO - [2024-12-18T01:48:09.277+0530] {dagbag.py:387} ERROR - Failed to import: /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
Traceback (most recent call last):
  File "/Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/.venv/lib/python3.12/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py", line 1, in <module>
    from extract import *
ModuleNotFoundError: No module named 'extract'
[2024-12-18T01:48:09.278+0530] {processor.py:927} WARNING - No viable dags retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-18T01:48:09.391+0530] {logging_mixin.py:190} INFO - [2024-12-18T01:48:09.391+0530] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-12-18T01:48:09.415+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 0.145 seconds
[2024-12-18T01:50:45.936+0530] {processor.py:186} INFO - Started process (PID=15959) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-18T01:50:45.938+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-18T01:50:45.939+0530] {logging_mixin.py:190} INFO - [2024-12-18T01:50:45.939+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-18T01:50:45.941+0530] {logging_mixin.py:190} INFO - [2024-12-18T01:50:45.940+0530] {dagbag.py:387} ERROR - Failed to import: /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
Traceback (most recent call last):
  File "/Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/.venv/lib/python3.12/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py", line 1, in <module>
    from extract import *
ModuleNotFoundError: No module named 'extract'
[2024-12-18T01:50:45.942+0530] {processor.py:927} WARNING - No viable dags retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-18T01:50:46.057+0530] {logging_mixin.py:190} INFO - [2024-12-18T01:50:46.057+0530] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-12-18T01:50:46.080+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 0.147 seconds
[2024-12-18T01:53:26.176+0530] {processor.py:186} INFO - Started process (PID=16151) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-18T01:53:26.178+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-18T01:53:26.179+0530] {logging_mixin.py:190} INFO - [2024-12-18T01:53:26.179+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-18T01:53:26.181+0530] {logging_mixin.py:190} INFO - [2024-12-18T01:53:26.180+0530] {dagbag.py:387} ERROR - Failed to import: /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
Traceback (most recent call last):
  File "/Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/.venv/lib/python3.12/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py", line 1, in <module>
    from extract import *
ModuleNotFoundError: No module named 'extract'
[2024-12-18T01:53:26.182+0530] {processor.py:927} WARNING - No viable dags retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-18T01:53:26.301+0530] {logging_mixin.py:190} INFO - [2024-12-18T01:53:26.301+0530] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-12-18T01:53:26.328+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 0.155 seconds
[2024-12-18T01:56:04.598+0530] {processor.py:186} INFO - Started process (PID=16357) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-18T01:56:04.600+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-18T01:56:04.602+0530] {logging_mixin.py:190} INFO - [2024-12-18T01:56:04.602+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-18T01:56:04.604+0530] {logging_mixin.py:190} INFO - [2024-12-18T01:56:04.603+0530] {dagbag.py:387} ERROR - Failed to import: /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
Traceback (most recent call last):
  File "/Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/.venv/lib/python3.12/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py", line 1, in <module>
    from extract import *
ModuleNotFoundError: No module named 'extract'
[2024-12-18T01:56:04.605+0530] {processor.py:927} WARNING - No viable dags retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-18T01:56:04.724+0530] {logging_mixin.py:190} INFO - [2024-12-18T01:56:04.723+0530] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-12-18T01:56:04.748+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 0.154 seconds
[2024-12-18T01:58:38.644+0530] {processor.py:186} INFO - Started process (PID=16552) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-18T01:58:38.645+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-18T01:58:38.646+0530] {logging_mixin.py:190} INFO - [2024-12-18T01:58:38.646+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-18T01:58:38.649+0530] {logging_mixin.py:190} INFO - [2024-12-18T01:58:38.648+0530] {dagbag.py:387} ERROR - Failed to import: /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
Traceback (most recent call last):
  File "/Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/.venv/lib/python3.12/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py", line 1, in <module>
    from extract import *
ModuleNotFoundError: No module named 'extract'
[2024-12-18T01:58:38.649+0530] {processor.py:927} WARNING - No viable dags retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-18T01:58:38.766+0530] {logging_mixin.py:190} INFO - [2024-12-18T01:58:38.766+0530] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-12-18T01:58:38.790+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 0.149 seconds
[2024-12-18T02:01:18.444+0530] {processor.py:186} INFO - Started process (PID=16739) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-18T02:01:18.445+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-18T02:01:18.446+0530] {logging_mixin.py:190} INFO - [2024-12-18T02:01:18.446+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-18T02:01:18.448+0530] {logging_mixin.py:190} INFO - [2024-12-18T02:01:18.447+0530] {dagbag.py:387} ERROR - Failed to import: /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
Traceback (most recent call last):
  File "/Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/.venv/lib/python3.12/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py", line 1, in <module>
    from extract import *
ModuleNotFoundError: No module named 'extract'
[2024-12-18T02:01:18.449+0530] {processor.py:927} WARNING - No viable dags retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-18T02:01:18.566+0530] {logging_mixin.py:190} INFO - [2024-12-18T02:01:18.566+0530] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-12-18T02:01:18.589+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 0.149 seconds
[2024-12-18T02:03:56.695+0530] {processor.py:186} INFO - Started process (PID=16924) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-18T02:03:56.696+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-18T02:03:56.697+0530] {logging_mixin.py:190} INFO - [2024-12-18T02:03:56.697+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-18T02:03:56.699+0530] {logging_mixin.py:190} INFO - [2024-12-18T02:03:56.698+0530] {dagbag.py:387} ERROR - Failed to import: /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
Traceback (most recent call last):
  File "/Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/.venv/lib/python3.12/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py", line 1, in <module>
    from extract import *
ModuleNotFoundError: No module named 'extract'
[2024-12-18T02:03:56.700+0530] {processor.py:927} WARNING - No viable dags retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-18T02:03:56.820+0530] {logging_mixin.py:190} INFO - [2024-12-18T02:03:56.819+0530] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-12-18T02:03:56.843+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 0.152 seconds
[2024-12-18T02:06:36.796+0530] {processor.py:186} INFO - Started process (PID=17128) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-18T02:06:36.798+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-18T02:06:36.799+0530] {logging_mixin.py:190} INFO - [2024-12-18T02:06:36.798+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-18T02:06:36.801+0530] {logging_mixin.py:190} INFO - [2024-12-18T02:06:36.800+0530] {dagbag.py:387} ERROR - Failed to import: /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
Traceback (most recent call last):
  File "/Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/.venv/lib/python3.12/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py", line 1, in <module>
    from extract import *
ModuleNotFoundError: No module named 'extract'
[2024-12-18T02:06:36.802+0530] {processor.py:927} WARNING - No viable dags retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-18T02:06:36.921+0530] {logging_mixin.py:190} INFO - [2024-12-18T02:06:36.921+0530] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-12-18T02:06:36.944+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 0.151 seconds
[2024-12-18T02:09:15.627+0530] {processor.py:186} INFO - Started process (PID=17317) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-18T02:09:15.628+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-18T02:09:15.629+0530] {logging_mixin.py:190} INFO - [2024-12-18T02:09:15.629+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-18T02:09:15.632+0530] {logging_mixin.py:190} INFO - [2024-12-18T02:09:15.631+0530] {dagbag.py:387} ERROR - Failed to import: /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
Traceback (most recent call last):
  File "/Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/.venv/lib/python3.12/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py", line 1, in <module>
    from extract import *
ModuleNotFoundError: No module named 'extract'
[2024-12-18T02:09:15.632+0530] {processor.py:927} WARNING - No viable dags retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-18T02:09:15.752+0530] {logging_mixin.py:190} INFO - [2024-12-18T02:09:15.752+0530] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-12-18T02:09:15.776+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 0.153 seconds
[2024-12-18T02:11:56.798+0530] {processor.py:186} INFO - Started process (PID=17505) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-18T02:11:56.800+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-18T02:11:56.802+0530] {logging_mixin.py:190} INFO - [2024-12-18T02:11:56.801+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-18T02:11:56.804+0530] {logging_mixin.py:190} INFO - [2024-12-18T02:11:56.803+0530] {dagbag.py:387} ERROR - Failed to import: /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
Traceback (most recent call last):
  File "/Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/.venv/lib/python3.12/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py", line 1, in <module>
    from extract import *
ModuleNotFoundError: No module named 'extract'
[2024-12-18T02:11:56.805+0530] {processor.py:927} WARNING - No viable dags retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-18T02:11:56.926+0530] {logging_mixin.py:190} INFO - [2024-12-18T02:11:56.926+0530] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-12-18T02:11:56.952+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 0.158 seconds
[2024-12-18T02:14:34.284+0530] {processor.py:186} INFO - Started process (PID=17702) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-18T02:14:34.285+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-18T02:14:34.286+0530] {logging_mixin.py:190} INFO - [2024-12-18T02:14:34.286+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-18T02:14:34.289+0530] {logging_mixin.py:190} INFO - [2024-12-18T02:14:34.288+0530] {dagbag.py:387} ERROR - Failed to import: /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
Traceback (most recent call last):
  File "/Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/.venv/lib/python3.12/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py", line 1, in <module>
    from extract import *
ModuleNotFoundError: No module named 'extract'
[2024-12-18T02:14:34.289+0530] {processor.py:927} WARNING - No viable dags retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-18T02:14:34.405+0530] {logging_mixin.py:190} INFO - [2024-12-18T02:14:34.405+0530] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-12-18T02:14:34.428+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 0.147 seconds
[2024-12-18T02:17:12.739+0530] {processor.py:186} INFO - Started process (PID=17890) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-18T02:17:12.740+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-18T02:17:12.742+0530] {logging_mixin.py:190} INFO - [2024-12-18T02:17:12.741+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-18T02:17:12.744+0530] {logging_mixin.py:190} INFO - [2024-12-18T02:17:12.743+0530] {dagbag.py:387} ERROR - Failed to import: /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
Traceback (most recent call last):
  File "/Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/.venv/lib/python3.12/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py", line 1, in <module>
    from extract import *
ModuleNotFoundError: No module named 'extract'
[2024-12-18T02:17:12.745+0530] {processor.py:927} WARNING - No viable dags retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-18T02:17:12.861+0530] {logging_mixin.py:190} INFO - [2024-12-18T02:17:12.861+0530] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-12-18T02:17:12.885+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 0.149 seconds
[2024-12-18T02:19:48.431+0530] {processor.py:186} INFO - Started process (PID=18073) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-18T02:19:48.432+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-18T02:19:48.433+0530] {logging_mixin.py:190} INFO - [2024-12-18T02:19:48.433+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-18T02:19:48.436+0530] {logging_mixin.py:190} INFO - [2024-12-18T02:19:48.435+0530] {dagbag.py:387} ERROR - Failed to import: /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
Traceback (most recent call last):
  File "/Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/.venv/lib/python3.12/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py", line 1, in <module>
    from extract import *
ModuleNotFoundError: No module named 'extract'
[2024-12-18T02:19:48.436+0530] {processor.py:927} WARNING - No viable dags retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-18T02:19:48.556+0530] {logging_mixin.py:190} INFO - [2024-12-18T02:19:48.556+0530] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-12-18T02:19:48.581+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 0.153 seconds
[2024-12-18T02:22:28.610+0530] {processor.py:186} INFO - Started process (PID=18278) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-18T02:22:28.612+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-18T02:22:28.615+0530] {logging_mixin.py:190} INFO - [2024-12-18T02:22:28.614+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-18T02:22:28.618+0530] {logging_mixin.py:190} INFO - [2024-12-18T02:22:28.617+0530] {dagbag.py:387} ERROR - Failed to import: /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
Traceback (most recent call last):
  File "/Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/.venv/lib/python3.12/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py", line 1, in <module>
    from extract import *
ModuleNotFoundError: No module named 'extract'
[2024-12-18T02:22:28.619+0530] {processor.py:927} WARNING - No viable dags retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-18T02:22:28.734+0530] {logging_mixin.py:190} INFO - [2024-12-18T02:22:28.734+0530] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-12-18T02:22:28.758+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 0.152 seconds
[2024-12-18T02:25:07.463+0530] {processor.py:186} INFO - Started process (PID=18475) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-18T02:25:07.464+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-18T02:25:07.465+0530] {logging_mixin.py:190} INFO - [2024-12-18T02:25:07.465+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-18T02:25:07.468+0530] {logging_mixin.py:190} INFO - [2024-12-18T02:25:07.466+0530] {dagbag.py:387} ERROR - Failed to import: /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
Traceback (most recent call last):
  File "/Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/.venv/lib/python3.12/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py", line 1, in <module>
    from extract import *
ModuleNotFoundError: No module named 'extract'
[2024-12-18T02:25:07.468+0530] {processor.py:927} WARNING - No viable dags retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-18T02:25:07.584+0530] {logging_mixin.py:190} INFO - [2024-12-18T02:25:07.584+0530] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-12-18T02:25:07.607+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 0.148 seconds
[2024-12-18T02:27:44.057+0530] {processor.py:186} INFO - Started process (PID=18658) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-18T02:27:44.059+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-18T02:27:44.060+0530] {logging_mixin.py:190} INFO - [2024-12-18T02:27:44.059+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-18T02:27:44.062+0530] {logging_mixin.py:190} INFO - [2024-12-18T02:27:44.061+0530] {dagbag.py:387} ERROR - Failed to import: /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
Traceback (most recent call last):
  File "/Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/.venv/lib/python3.12/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py", line 1, in <module>
    from extract import *
ModuleNotFoundError: No module named 'extract'
[2024-12-18T02:27:44.063+0530] {processor.py:927} WARNING - No viable dags retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-18T02:27:44.179+0530] {logging_mixin.py:190} INFO - [2024-12-18T02:27:44.179+0530] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-12-18T02:27:44.202+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 0.148 seconds
[2024-12-18T02:30:24.543+0530] {processor.py:186} INFO - Started process (PID=18857) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-18T02:30:24.544+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-18T02:30:24.545+0530] {logging_mixin.py:190} INFO - [2024-12-18T02:30:24.545+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-18T02:30:24.548+0530] {logging_mixin.py:190} INFO - [2024-12-18T02:30:24.547+0530] {dagbag.py:387} ERROR - Failed to import: /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
Traceback (most recent call last):
  File "/Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/.venv/lib/python3.12/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py", line 1, in <module>
    from extract import *
ModuleNotFoundError: No module named 'extract'
[2024-12-18T02:30:24.549+0530] {processor.py:927} WARNING - No viable dags retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-18T02:30:24.664+0530] {logging_mixin.py:190} INFO - [2024-12-18T02:30:24.664+0530] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-12-18T02:30:24.687+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 0.148 seconds
[2024-12-18T02:33:01.503+0530] {processor.py:186} INFO - Started process (PID=19047) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-18T02:33:01.504+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-18T02:33:01.505+0530] {logging_mixin.py:190} INFO - [2024-12-18T02:33:01.505+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-18T02:33:01.508+0530] {logging_mixin.py:190} INFO - [2024-12-18T02:33:01.507+0530] {dagbag.py:387} ERROR - Failed to import: /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
Traceback (most recent call last):
  File "/Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/.venv/lib/python3.12/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py", line 1, in <module>
    from extract import *
ModuleNotFoundError: No module named 'extract'
[2024-12-18T02:33:01.508+0530] {processor.py:927} WARNING - No viable dags retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-18T02:33:01.627+0530] {logging_mixin.py:190} INFO - [2024-12-18T02:33:01.627+0530] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-12-18T02:33:01.650+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 0.150 seconds
[2024-12-18T02:35:38.023+0530] {processor.py:186} INFO - Started process (PID=19231) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-18T02:35:38.025+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-18T02:35:38.028+0530] {logging_mixin.py:190} INFO - [2024-12-18T02:35:38.027+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-18T02:35:38.030+0530] {logging_mixin.py:190} INFO - [2024-12-18T02:35:38.029+0530] {dagbag.py:387} ERROR - Failed to import: /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
Traceback (most recent call last):
  File "/Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/.venv/lib/python3.12/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py", line 1, in <module>
    from extract import *
ModuleNotFoundError: No module named 'extract'
[2024-12-18T02:35:38.031+0530] {processor.py:927} WARNING - No viable dags retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-18T02:35:38.147+0530] {logging_mixin.py:190} INFO - [2024-12-18T02:35:38.147+0530] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-12-18T02:35:38.170+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 0.150 seconds
[2024-12-18T02:38:15.263+0530] {processor.py:186} INFO - Started process (PID=19429) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-18T02:38:15.264+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-18T02:38:15.265+0530] {logging_mixin.py:190} INFO - [2024-12-18T02:38:15.265+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-18T02:38:15.268+0530] {logging_mixin.py:190} INFO - [2024-12-18T02:38:15.267+0530] {dagbag.py:387} ERROR - Failed to import: /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
Traceback (most recent call last):
  File "/Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/.venv/lib/python3.12/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py", line 1, in <module>
    from extract import *
ModuleNotFoundError: No module named 'extract'
[2024-12-18T02:38:15.268+0530] {processor.py:927} WARNING - No viable dags retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-18T02:38:15.386+0530] {logging_mixin.py:190} INFO - [2024-12-18T02:38:15.386+0530] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-12-18T02:38:15.409+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 0.150 seconds
[2024-12-18T02:40:51.255+0530] {processor.py:186} INFO - Started process (PID=19616) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-18T02:40:51.257+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-18T02:40:51.258+0530] {logging_mixin.py:190} INFO - [2024-12-18T02:40:51.258+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-18T02:40:51.260+0530] {logging_mixin.py:190} INFO - [2024-12-18T02:40:51.259+0530] {dagbag.py:387} ERROR - Failed to import: /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
Traceback (most recent call last):
  File "/Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/.venv/lib/python3.12/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py", line 1, in <module>
    from extract import *
ModuleNotFoundError: No module named 'extract'
[2024-12-18T02:40:51.261+0530] {processor.py:927} WARNING - No viable dags retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-18T02:40:51.376+0530] {logging_mixin.py:190} INFO - [2024-12-18T02:40:51.376+0530] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-12-18T02:40:51.400+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 0.148 seconds
[2024-12-18T02:43:31.474+0530] {processor.py:186} INFO - Started process (PID=19801) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-18T02:43:31.477+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-18T02:43:31.478+0530] {logging_mixin.py:190} INFO - [2024-12-18T02:43:31.478+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-18T02:43:31.483+0530] {logging_mixin.py:190} INFO - [2024-12-18T02:43:31.482+0530] {dagbag.py:387} ERROR - Failed to import: /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
Traceback (most recent call last):
  File "/Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/.venv/lib/python3.12/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py", line 1, in <module>
    from extract import *
ModuleNotFoundError: No module named 'extract'
[2024-12-18T02:43:31.484+0530] {processor.py:927} WARNING - No viable dags retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-18T02:43:31.610+0530] {logging_mixin.py:190} INFO - [2024-12-18T02:43:31.609+0530] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-12-18T02:43:31.633+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 0.162 seconds
[2024-12-18T02:46:12.048+0530] {processor.py:186} INFO - Started process (PID=20014) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-18T02:46:12.050+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-18T02:46:12.053+0530] {logging_mixin.py:190} INFO - [2024-12-18T02:46:12.053+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-18T02:46:12.059+0530] {logging_mixin.py:190} INFO - [2024-12-18T02:46:12.057+0530] {dagbag.py:387} ERROR - Failed to import: /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
Traceback (most recent call last):
  File "/Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/.venv/lib/python3.12/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py", line 1, in <module>
    from extract import *
ModuleNotFoundError: No module named 'extract'
[2024-12-18T02:46:12.060+0530] {processor.py:927} WARNING - No viable dags retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-18T02:46:12.196+0530] {logging_mixin.py:190} INFO - [2024-12-18T02:46:12.196+0530] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-12-18T02:46:12.226+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 0.183 seconds
[2024-12-18T02:49:04.927+0530] {processor.py:186} INFO - Started process (PID=20231) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-18T02:49:04.929+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-18T02:49:04.931+0530] {logging_mixin.py:190} INFO - [2024-12-18T02:49:04.930+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-18T02:49:04.935+0530] {logging_mixin.py:190} INFO - [2024-12-18T02:49:04.933+0530] {dagbag.py:387} ERROR - Failed to import: /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
Traceback (most recent call last):
  File "/Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/.venv/lib/python3.12/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py", line 1, in <module>
    from extract import *
ModuleNotFoundError: No module named 'extract'
[2024-12-18T02:49:04.935+0530] {processor.py:927} WARNING - No viable dags retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-18T02:49:05.065+0530] {logging_mixin.py:190} INFO - [2024-12-18T02:49:05.065+0530] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-12-18T02:49:05.089+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 0.165 seconds
[2024-12-18T02:51:46.008+0530] {processor.py:186} INFO - Started process (PID=20418) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-18T02:51:46.010+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-18T02:51:46.011+0530] {logging_mixin.py:190} INFO - [2024-12-18T02:51:46.011+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-18T02:51:46.013+0530] {logging_mixin.py:190} INFO - [2024-12-18T02:51:46.012+0530] {dagbag.py:387} ERROR - Failed to import: /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
Traceback (most recent call last):
  File "/Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/.venv/lib/python3.12/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py", line 1, in <module>
    from extract import *
ModuleNotFoundError: No module named 'extract'
[2024-12-18T02:51:46.014+0530] {processor.py:927} WARNING - No viable dags retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-18T02:51:46.148+0530] {logging_mixin.py:190} INFO - [2024-12-18T02:51:46.148+0530] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-12-18T02:51:46.172+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 0.167 seconds
[2024-12-18T02:54:23.717+0530] {processor.py:186} INFO - Started process (PID=20620) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-18T02:54:23.718+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-18T02:54:23.719+0530] {logging_mixin.py:190} INFO - [2024-12-18T02:54:23.719+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-18T02:54:23.722+0530] {logging_mixin.py:190} INFO - [2024-12-18T02:54:23.721+0530] {dagbag.py:387} ERROR - Failed to import: /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
Traceback (most recent call last):
  File "/Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/.venv/lib/python3.12/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py", line 1, in <module>
    from extract import *
ModuleNotFoundError: No module named 'extract'
[2024-12-18T02:54:23.722+0530] {processor.py:927} WARNING - No viable dags retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-18T02:54:23.849+0530] {logging_mixin.py:190} INFO - [2024-12-18T02:54:23.849+0530] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-12-18T02:54:23.872+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 0.159 seconds
[2024-12-18T02:57:05.486+0530] {processor.py:186} INFO - Started process (PID=20827) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-18T02:57:05.487+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-18T02:57:05.489+0530] {logging_mixin.py:190} INFO - [2024-12-18T02:57:05.488+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-18T02:57:05.491+0530] {logging_mixin.py:190} INFO - [2024-12-18T02:57:05.490+0530] {dagbag.py:387} ERROR - Failed to import: /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
Traceback (most recent call last):
  File "/Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/.venv/lib/python3.12/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py", line 1, in <module>
    from extract import *
ModuleNotFoundError: No module named 'extract'
[2024-12-18T02:57:05.492+0530] {processor.py:927} WARNING - No viable dags retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-18T02:57:05.619+0530] {logging_mixin.py:190} INFO - [2024-12-18T02:57:05.619+0530] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-12-18T02:57:05.645+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 0.162 seconds
[2024-12-18T02:59:43.956+0530] {processor.py:186} INFO - Started process (PID=21014) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-18T02:59:43.957+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-18T02:59:43.958+0530] {logging_mixin.py:190} INFO - [2024-12-18T02:59:43.958+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-18T02:59:43.960+0530] {logging_mixin.py:190} INFO - [2024-12-18T02:59:43.959+0530] {dagbag.py:387} ERROR - Failed to import: /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
Traceback (most recent call last):
  File "/Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/.venv/lib/python3.12/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py", line 1, in <module>
    from extract import *
ModuleNotFoundError: No module named 'extract'
[2024-12-18T02:59:43.961+0530] {processor.py:927} WARNING - No viable dags retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-18T02:59:44.091+0530] {logging_mixin.py:190} INFO - [2024-12-18T02:59:44.091+0530] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-12-18T02:59:44.116+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 0.164 seconds
[2024-12-18T03:02:26.164+0530] {processor.py:186} INFO - Started process (PID=21218) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-18T03:02:26.165+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-18T03:02:26.166+0530] {logging_mixin.py:190} INFO - [2024-12-18T03:02:26.166+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-18T03:02:26.169+0530] {logging_mixin.py:190} INFO - [2024-12-18T03:02:26.167+0530] {dagbag.py:387} ERROR - Failed to import: /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
Traceback (most recent call last):
  File "/Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/.venv/lib/python3.12/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py", line 1, in <module>
    from extract import *
ModuleNotFoundError: No module named 'extract'
[2024-12-18T03:02:26.169+0530] {processor.py:927} WARNING - No viable dags retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-18T03:02:26.284+0530] {logging_mixin.py:190} INFO - [2024-12-18T03:02:26.284+0530] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-12-18T03:02:26.307+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 0.146 seconds
[2024-12-18T03:05:04.680+0530] {processor.py:186} INFO - Started process (PID=21403) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-18T03:05:04.682+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-18T03:05:04.683+0530] {logging_mixin.py:190} INFO - [2024-12-18T03:05:04.683+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-18T03:05:04.685+0530] {logging_mixin.py:190} INFO - [2024-12-18T03:05:04.684+0530] {dagbag.py:387} ERROR - Failed to import: /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
Traceback (most recent call last):
  File "/Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/.venv/lib/python3.12/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py", line 1, in <module>
    from extract import *
ModuleNotFoundError: No module named 'extract'
[2024-12-18T03:05:04.686+0530] {processor.py:927} WARNING - No viable dags retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-18T03:05:04.801+0530] {logging_mixin.py:190} INFO - [2024-12-18T03:05:04.801+0530] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-12-18T03:05:04.825+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 0.148 seconds
[2024-12-18T03:07:45.877+0530] {processor.py:186} INFO - Started process (PID=21591) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-18T03:07:45.879+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-18T03:07:45.880+0530] {logging_mixin.py:190} INFO - [2024-12-18T03:07:45.880+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-18T03:07:45.883+0530] {logging_mixin.py:190} INFO - [2024-12-18T03:07:45.882+0530] {dagbag.py:387} ERROR - Failed to import: /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
Traceback (most recent call last):
  File "/Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/.venv/lib/python3.12/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py", line 1, in <module>
    from extract import *
ModuleNotFoundError: No module named 'extract'
[2024-12-18T03:07:45.883+0530] {processor.py:927} WARNING - No viable dags retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-18T03:07:46.002+0530] {logging_mixin.py:190} INFO - [2024-12-18T03:07:46.002+0530] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-12-18T03:07:46.025+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 0.153 seconds
[2024-12-18T03:10:21.605+0530] {processor.py:186} INFO - Started process (PID=21789) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-18T03:10:21.607+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-18T03:10:21.608+0530] {logging_mixin.py:190} INFO - [2024-12-18T03:10:21.607+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-18T03:10:21.610+0530] {logging_mixin.py:190} INFO - [2024-12-18T03:10:21.609+0530] {dagbag.py:387} ERROR - Failed to import: /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
Traceback (most recent call last):
  File "/Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/.venv/lib/python3.12/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py", line 1, in <module>
    from extract import *
ModuleNotFoundError: No module named 'extract'
[2024-12-18T03:10:21.611+0530] {processor.py:927} WARNING - No viable dags retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-18T03:10:21.727+0530] {logging_mixin.py:190} INFO - [2024-12-18T03:10:21.727+0530] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-12-18T03:10:21.750+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 0.148 seconds
[2024-12-18T03:12:57.686+0530] {processor.py:186} INFO - Started process (PID=21973) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-18T03:12:57.687+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-18T03:12:57.688+0530] {logging_mixin.py:190} INFO - [2024-12-18T03:12:57.688+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-18T03:12:57.691+0530] {logging_mixin.py:190} INFO - [2024-12-18T03:12:57.690+0530] {dagbag.py:387} ERROR - Failed to import: /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
Traceback (most recent call last):
  File "/Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/.venv/lib/python3.12/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py", line 1, in <module>
    from extract import *
ModuleNotFoundError: No module named 'extract'
[2024-12-18T03:12:57.691+0530] {processor.py:927} WARNING - No viable dags retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-18T03:12:57.804+0530] {logging_mixin.py:190} INFO - [2024-12-18T03:12:57.804+0530] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-12-18T03:12:57.827+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 0.145 seconds
[2024-12-18T03:15:36.068+0530] {processor.py:186} INFO - Started process (PID=22162) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-18T03:15:36.070+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-18T03:15:36.071+0530] {logging_mixin.py:190} INFO - [2024-12-18T03:15:36.070+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-18T03:15:36.073+0530] {logging_mixin.py:190} INFO - [2024-12-18T03:15:36.072+0530] {dagbag.py:387} ERROR - Failed to import: /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
Traceback (most recent call last):
  File "/Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/.venv/lib/python3.12/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py", line 1, in <module>
    from extract import *
ModuleNotFoundError: No module named 'extract'
[2024-12-18T03:15:36.074+0530] {processor.py:927} WARNING - No viable dags retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-18T03:15:36.193+0530] {logging_mixin.py:190} INFO - [2024-12-18T03:15:36.193+0530] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-12-18T03:15:36.218+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 0.153 seconds
[2024-12-18T03:18:13.485+0530] {processor.py:186} INFO - Started process (PID=22359) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-18T03:18:13.486+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-18T03:18:13.487+0530] {logging_mixin.py:190} INFO - [2024-12-18T03:18:13.487+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-18T03:18:13.490+0530] {logging_mixin.py:190} INFO - [2024-12-18T03:18:13.489+0530] {dagbag.py:387} ERROR - Failed to import: /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
Traceback (most recent call last):
  File "/Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/.venv/lib/python3.12/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py", line 1, in <module>
    from extract import *
ModuleNotFoundError: No module named 'extract'
[2024-12-18T03:18:13.490+0530] {processor.py:927} WARNING - No viable dags retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-18T03:18:13.607+0530] {logging_mixin.py:190} INFO - [2024-12-18T03:18:13.607+0530] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-12-18T03:18:13.630+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 0.149 seconds
[2024-12-18T03:20:49.702+0530] {processor.py:186} INFO - Started process (PID=22555) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-18T03:20:49.703+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-18T03:20:49.704+0530] {logging_mixin.py:190} INFO - [2024-12-18T03:20:49.704+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-18T03:20:49.707+0530] {logging_mixin.py:190} INFO - [2024-12-18T03:20:49.706+0530] {dagbag.py:387} ERROR - Failed to import: /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
Traceback (most recent call last):
  File "/Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/.venv/lib/python3.12/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py", line 1, in <module>
    from extract import *
ModuleNotFoundError: No module named 'extract'
[2024-12-18T03:20:49.707+0530] {processor.py:927} WARNING - No viable dags retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-18T03:20:49.823+0530] {logging_mixin.py:190} INFO - [2024-12-18T03:20:49.823+0530] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-12-18T03:20:49.847+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 0.148 seconds
[2024-12-18T03:23:26.663+0530] {processor.py:186} INFO - Started process (PID=22745) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-18T03:23:26.664+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-18T03:23:26.666+0530] {logging_mixin.py:190} INFO - [2024-12-18T03:23:26.665+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-18T03:23:26.668+0530] {logging_mixin.py:190} INFO - [2024-12-18T03:23:26.667+0530] {dagbag.py:387} ERROR - Failed to import: /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
Traceback (most recent call last):
  File "/Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/.venv/lib/python3.12/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py", line 1, in <module>
    from extract import *
ModuleNotFoundError: No module named 'extract'
[2024-12-18T03:23:26.668+0530] {processor.py:927} WARNING - No viable dags retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-18T03:23:26.783+0530] {logging_mixin.py:190} INFO - [2024-12-18T03:23:26.783+0530] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-12-18T03:23:26.805+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 0.146 seconds
[2024-12-18T03:26:05.747+0530] {processor.py:186} INFO - Started process (PID=22936) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-18T03:26:05.749+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-18T03:26:05.750+0530] {logging_mixin.py:190} INFO - [2024-12-18T03:26:05.750+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-18T03:26:05.752+0530] {logging_mixin.py:190} INFO - [2024-12-18T03:26:05.751+0530] {dagbag.py:387} ERROR - Failed to import: /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
Traceback (most recent call last):
  File "/Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/.venv/lib/python3.12/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py", line 1, in <module>
    from extract import *
ModuleNotFoundError: No module named 'extract'
[2024-12-18T03:26:05.753+0530] {processor.py:927} WARNING - No viable dags retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-18T03:26:05.880+0530] {logging_mixin.py:190} INFO - [2024-12-18T03:26:05.879+0530] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-12-18T03:26:05.905+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 0.161 seconds
[2024-12-18T03:28:43.159+0530] {processor.py:186} INFO - Started process (PID=23128) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-18T03:28:43.160+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-18T03:28:43.161+0530] {logging_mixin.py:190} INFO - [2024-12-18T03:28:43.161+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-18T03:28:43.163+0530] {logging_mixin.py:190} INFO - [2024-12-18T03:28:43.162+0530] {dagbag.py:387} ERROR - Failed to import: /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
Traceback (most recent call last):
  File "/Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/.venv/lib/python3.12/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py", line 1, in <module>
    from extract import *
ModuleNotFoundError: No module named 'extract'
[2024-12-18T03:28:43.164+0530] {processor.py:927} WARNING - No viable dags retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-18T03:28:43.296+0530] {logging_mixin.py:190} INFO - [2024-12-18T03:28:43.296+0530] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-12-18T03:28:43.321+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 0.166 seconds
[2024-12-18T03:31:21.931+0530] {processor.py:186} INFO - Started process (PID=23317) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-18T03:31:21.932+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-18T03:31:21.933+0530] {logging_mixin.py:190} INFO - [2024-12-18T03:31:21.933+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-18T03:31:21.935+0530] {logging_mixin.py:190} INFO - [2024-12-18T03:31:21.934+0530] {dagbag.py:387} ERROR - Failed to import: /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
Traceback (most recent call last):
  File "/Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/.venv/lib/python3.12/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py", line 1, in <module>
    from extract import *
ModuleNotFoundError: No module named 'extract'
[2024-12-18T03:31:21.936+0530] {processor.py:927} WARNING - No viable dags retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-18T03:31:22.053+0530] {logging_mixin.py:190} INFO - [2024-12-18T03:31:22.052+0530] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-12-18T03:31:22.078+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 0.151 seconds
[2024-12-18T03:34:01.448+0530] {processor.py:186} INFO - Started process (PID=23504) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-18T03:34:01.449+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-18T03:34:01.450+0530] {logging_mixin.py:190} INFO - [2024-12-18T03:34:01.450+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-18T03:34:01.453+0530] {logging_mixin.py:190} INFO - [2024-12-18T03:34:01.452+0530] {dagbag.py:387} ERROR - Failed to import: /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
Traceback (most recent call last):
  File "/Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/.venv/lib/python3.12/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py", line 1, in <module>
    from extract import *
ModuleNotFoundError: No module named 'extract'
[2024-12-18T03:34:01.453+0530] {processor.py:927} WARNING - No viable dags retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-18T03:34:01.567+0530] {logging_mixin.py:190} INFO - [2024-12-18T03:34:01.567+0530] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-12-18T03:34:01.591+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 0.146 seconds
[2024-12-18T03:36:44.831+0530] {processor.py:186} INFO - Started process (PID=23741) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-18T03:36:44.833+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-18T03:36:44.834+0530] {logging_mixin.py:190} INFO - [2024-12-18T03:36:44.834+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-18T03:36:44.836+0530] {logging_mixin.py:190} INFO - [2024-12-18T03:36:44.835+0530] {dagbag.py:387} ERROR - Failed to import: /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
Traceback (most recent call last):
  File "/Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/.venv/lib/python3.12/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py", line 1, in <module>
    from extract import *
ModuleNotFoundError: No module named 'extract'
[2024-12-18T03:36:44.837+0530] {processor.py:927} WARNING - No viable dags retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-18T03:36:44.950+0530] {logging_mixin.py:190} INFO - [2024-12-18T03:36:44.950+0530] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-12-18T03:36:44.973+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 0.145 seconds
[2024-12-18T03:39:20.677+0530] {processor.py:186} INFO - Started process (PID=23927) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-18T03:39:20.678+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-18T03:39:20.679+0530] {logging_mixin.py:190} INFO - [2024-12-18T03:39:20.679+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-18T03:39:20.682+0530] {logging_mixin.py:190} INFO - [2024-12-18T03:39:20.681+0530] {dagbag.py:387} ERROR - Failed to import: /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
Traceback (most recent call last):
  File "/Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/.venv/lib/python3.12/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py", line 1, in <module>
    from extract import *
ModuleNotFoundError: No module named 'extract'
[2024-12-18T03:39:20.682+0530] {processor.py:927} WARNING - No viable dags retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-18T03:39:20.798+0530] {logging_mixin.py:190} INFO - [2024-12-18T03:39:20.798+0530] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-12-18T03:39:20.822+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 0.148 seconds
[2024-12-18T03:42:00.727+0530] {processor.py:186} INFO - Started process (PID=24112) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-18T03:42:00.728+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-18T03:42:00.729+0530] {logging_mixin.py:190} INFO - [2024-12-18T03:42:00.729+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-18T03:42:00.731+0530] {logging_mixin.py:190} INFO - [2024-12-18T03:42:00.730+0530] {dagbag.py:387} ERROR - Failed to import: /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
Traceback (most recent call last):
  File "/Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/.venv/lib/python3.12/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py", line 1, in <module>
    from extract import *
ModuleNotFoundError: No module named 'extract'
[2024-12-18T03:42:00.732+0530] {processor.py:927} WARNING - No viable dags retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-18T03:42:00.847+0530] {logging_mixin.py:190} INFO - [2024-12-18T03:42:00.847+0530] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-12-18T03:42:00.871+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 0.147 seconds
[2024-12-18T03:44:39.035+0530] {processor.py:186} INFO - Started process (PID=24318) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-18T03:44:39.038+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-18T03:44:39.039+0530] {logging_mixin.py:190} INFO - [2024-12-18T03:44:39.039+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-18T03:44:39.044+0530] {logging_mixin.py:190} INFO - [2024-12-18T03:44:39.042+0530] {dagbag.py:387} ERROR - Failed to import: /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
Traceback (most recent call last):
  File "/Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/.venv/lib/python3.12/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py", line 1, in <module>
    from extract import *
ModuleNotFoundError: No module named 'extract'
[2024-12-18T03:44:39.044+0530] {processor.py:927} WARNING - No viable dags retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-18T03:44:39.175+0530] {logging_mixin.py:190} INFO - [2024-12-18T03:44:39.175+0530] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-12-18T03:44:39.203+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 0.171 seconds
[2024-12-18T03:47:16.104+0530] {processor.py:186} INFO - Started process (PID=24501) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-18T03:47:16.105+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-18T03:47:16.106+0530] {logging_mixin.py:190} INFO - [2024-12-18T03:47:16.106+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-18T03:47:16.109+0530] {logging_mixin.py:190} INFO - [2024-12-18T03:47:16.107+0530] {dagbag.py:387} ERROR - Failed to import: /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
Traceback (most recent call last):
  File "/Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/.venv/lib/python3.12/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py", line 1, in <module>
    from extract import *
ModuleNotFoundError: No module named 'extract'
[2024-12-18T03:47:16.109+0530] {processor.py:927} WARNING - No viable dags retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-18T03:47:16.238+0530] {logging_mixin.py:190} INFO - [2024-12-18T03:47:16.238+0530] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-12-18T03:47:16.261+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 0.161 seconds
[2024-12-18T03:49:56.946+0530] {processor.py:186} INFO - Started process (PID=24689) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-18T03:49:56.947+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-18T03:49:56.948+0530] {logging_mixin.py:190} INFO - [2024-12-18T03:49:56.948+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-18T03:49:56.951+0530] {logging_mixin.py:190} INFO - [2024-12-18T03:49:56.950+0530] {dagbag.py:387} ERROR - Failed to import: /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
Traceback (most recent call last):
  File "/Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/.venv/lib/python3.12/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py", line 1, in <module>
    from extract import *
ModuleNotFoundError: No module named 'extract'
[2024-12-18T03:49:56.951+0530] {processor.py:927} WARNING - No viable dags retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-18T03:49:57.068+0530] {logging_mixin.py:190} INFO - [2024-12-18T03:49:57.068+0530] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-12-18T03:49:57.091+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 0.149 seconds
[2024-12-18T03:52:35.098+0530] {processor.py:186} INFO - Started process (PID=24890) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-18T03:52:35.099+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-18T03:52:35.100+0530] {logging_mixin.py:190} INFO - [2024-12-18T03:52:35.100+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-18T03:52:35.103+0530] {logging_mixin.py:190} INFO - [2024-12-18T03:52:35.102+0530] {dagbag.py:387} ERROR - Failed to import: /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
Traceback (most recent call last):
  File "/Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/.venv/lib/python3.12/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py", line 1, in <module>
    from extract import *
ModuleNotFoundError: No module named 'extract'
[2024-12-18T03:52:35.103+0530] {processor.py:927} WARNING - No viable dags retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-18T03:52:35.247+0530] {logging_mixin.py:190} INFO - [2024-12-18T03:52:35.247+0530] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-12-18T03:52:35.274+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 0.179 seconds
[2024-12-18T03:55:11.427+0530] {processor.py:186} INFO - Started process (PID=25076) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-18T03:55:11.428+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-18T03:55:11.429+0530] {logging_mixin.py:190} INFO - [2024-12-18T03:55:11.429+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-18T03:55:11.431+0530] {logging_mixin.py:190} INFO - [2024-12-18T03:55:11.430+0530] {dagbag.py:387} ERROR - Failed to import: /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
Traceback (most recent call last):
  File "/Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/.venv/lib/python3.12/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py", line 1, in <module>
    from extract import *
ModuleNotFoundError: No module named 'extract'
[2024-12-18T03:55:11.432+0530] {processor.py:927} WARNING - No viable dags retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-18T03:55:11.551+0530] {logging_mixin.py:190} INFO - [2024-12-18T03:55:11.551+0530] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-12-18T03:55:11.574+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 0.151 seconds
[2024-12-18T03:57:51.612+0530] {processor.py:186} INFO - Started process (PID=25308) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-18T03:57:51.613+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-18T03:57:51.614+0530] {logging_mixin.py:190} INFO - [2024-12-18T03:57:51.614+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-18T03:57:51.616+0530] {logging_mixin.py:190} INFO - [2024-12-18T03:57:51.615+0530] {dagbag.py:387} ERROR - Failed to import: /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
Traceback (most recent call last):
  File "/Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/.venv/lib/python3.12/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py", line 1, in <module>
    from extract import *
ModuleNotFoundError: No module named 'extract'
[2024-12-18T03:57:51.617+0530] {processor.py:927} WARNING - No viable dags retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-18T03:57:51.732+0530] {logging_mixin.py:190} INFO - [2024-12-18T03:57:51.732+0530] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-12-18T03:57:51.755+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 0.147 seconds
[2024-12-18T04:00:31.368+0530] {processor.py:186} INFO - Started process (PID=25504) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-18T04:00:31.369+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-18T04:00:31.371+0530] {logging_mixin.py:190} INFO - [2024-12-18T04:00:31.370+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-18T04:00:31.373+0530] {logging_mixin.py:190} INFO - [2024-12-18T04:00:31.372+0530] {dagbag.py:387} ERROR - Failed to import: /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
Traceback (most recent call last):
  File "/Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/.venv/lib/python3.12/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py", line 1, in <module>
    from extract import *
ModuleNotFoundError: No module named 'extract'
[2024-12-18T04:00:31.374+0530] {processor.py:927} WARNING - No viable dags retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-18T04:00:31.490+0530] {logging_mixin.py:190} INFO - [2024-12-18T04:00:31.490+0530] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-12-18T04:00:31.514+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 0.150 seconds
[2024-12-18T04:03:11.288+0530] {processor.py:186} INFO - Started process (PID=25695) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-18T04:03:11.289+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-18T04:03:11.290+0530] {logging_mixin.py:190} INFO - [2024-12-18T04:03:11.290+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-18T04:03:11.294+0530] {logging_mixin.py:190} INFO - [2024-12-18T04:03:11.293+0530] {dagbag.py:387} ERROR - Failed to import: /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
Traceback (most recent call last):
  File "/Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/.venv/lib/python3.12/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py", line 1, in <module>
    from extract import *
ModuleNotFoundError: No module named 'extract'
[2024-12-18T04:03:11.294+0530] {processor.py:927} WARNING - No viable dags retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-18T04:03:11.411+0530] {logging_mixin.py:190} INFO - [2024-12-18T04:03:11.411+0530] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-12-18T04:03:11.434+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 0.150 seconds
[2024-12-18T04:05:52.861+0530] {processor.py:186} INFO - Started process (PID=25889) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-18T04:05:52.862+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-18T04:05:52.863+0530] {logging_mixin.py:190} INFO - [2024-12-18T04:05:52.863+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-18T04:05:52.865+0530] {logging_mixin.py:190} INFO - [2024-12-18T04:05:52.864+0530] {dagbag.py:387} ERROR - Failed to import: /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
Traceback (most recent call last):
  File "/Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/.venv/lib/python3.12/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py", line 1, in <module>
    from extract import *
ModuleNotFoundError: No module named 'extract'
[2024-12-18T04:05:52.866+0530] {processor.py:927} WARNING - No viable dags retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-18T04:05:52.979+0530] {logging_mixin.py:190} INFO - [2024-12-18T04:05:52.978+0530] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-12-18T04:05:53.003+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 0.145 seconds
[2024-12-18T04:08:34.590+0530] {processor.py:186} INFO - Started process (PID=26093) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-18T04:08:34.593+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-18T04:08:34.594+0530] {logging_mixin.py:190} INFO - [2024-12-18T04:08:34.594+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-18T04:08:34.598+0530] {logging_mixin.py:190} INFO - [2024-12-18T04:08:34.597+0530] {dagbag.py:387} ERROR - Failed to import: /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
Traceback (most recent call last):
  File "/Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/.venv/lib/python3.12/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py", line 1, in <module>
    from extract import *
ModuleNotFoundError: No module named 'extract'
[2024-12-18T04:08:34.599+0530] {processor.py:927} WARNING - No viable dags retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-18T04:08:34.735+0530] {logging_mixin.py:190} INFO - [2024-12-18T04:08:34.735+0530] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-12-18T04:08:34.758+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 0.172 seconds
[2024-12-18T04:11:15.071+0530] {processor.py:186} INFO - Started process (PID=26280) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-18T04:11:15.072+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-18T04:11:15.073+0530] {logging_mixin.py:190} INFO - [2024-12-18T04:11:15.073+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-18T04:11:15.075+0530] {logging_mixin.py:190} INFO - [2024-12-18T04:11:15.074+0530] {dagbag.py:387} ERROR - Failed to import: /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
Traceback (most recent call last):
  File "/Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/.venv/lib/python3.12/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py", line 1, in <module>
    from extract import *
ModuleNotFoundError: No module named 'extract'
[2024-12-18T04:11:15.076+0530] {processor.py:927} WARNING - No viable dags retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-18T04:11:15.201+0530] {logging_mixin.py:190} INFO - [2024-12-18T04:11:15.201+0530] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-12-18T04:11:15.225+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 0.158 seconds
[2024-12-18T04:13:52.166+0530] {processor.py:186} INFO - Started process (PID=26469) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-18T04:13:52.169+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-18T04:13:52.170+0530] {logging_mixin.py:190} INFO - [2024-12-18T04:13:52.170+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-18T04:13:52.174+0530] {logging_mixin.py:190} INFO - [2024-12-18T04:13:52.172+0530] {dagbag.py:387} ERROR - Failed to import: /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
Traceback (most recent call last):
  File "/Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/.venv/lib/python3.12/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py", line 1, in <module>
    from extract import *
ModuleNotFoundError: No module named 'extract'
[2024-12-18T04:13:52.174+0530] {processor.py:927} WARNING - No viable dags retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-18T04:13:52.292+0530] {logging_mixin.py:190} INFO - [2024-12-18T04:13:52.292+0530] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-12-18T04:13:52.316+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 0.153 seconds
[2024-12-18T04:16:30.605+0530] {processor.py:186} INFO - Started process (PID=26665) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-18T04:16:30.606+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-18T04:16:30.607+0530] {logging_mixin.py:190} INFO - [2024-12-18T04:16:30.607+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-18T04:16:30.609+0530] {logging_mixin.py:190} INFO - [2024-12-18T04:16:30.608+0530] {dagbag.py:387} ERROR - Failed to import: /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
Traceback (most recent call last):
  File "/Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/.venv/lib/python3.12/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py", line 1, in <module>
    from extract import *
ModuleNotFoundError: No module named 'extract'
[2024-12-18T04:16:30.610+0530] {processor.py:927} WARNING - No viable dags retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-18T04:16:30.727+0530] {logging_mixin.py:190} INFO - [2024-12-18T04:16:30.726+0530] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-12-18T04:16:30.750+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 0.149 seconds
[2024-12-18T04:19:12.332+0530] {processor.py:186} INFO - Started process (PID=26853) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-18T04:19:12.333+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-18T04:19:12.334+0530] {logging_mixin.py:190} INFO - [2024-12-18T04:19:12.334+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-18T04:19:12.337+0530] {logging_mixin.py:190} INFO - [2024-12-18T04:19:12.336+0530] {dagbag.py:387} ERROR - Failed to import: /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
Traceback (most recent call last):
  File "/Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/.venv/lib/python3.12/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py", line 1, in <module>
    from extract import *
ModuleNotFoundError: No module named 'extract'
[2024-12-18T04:19:12.337+0530] {processor.py:927} WARNING - No viable dags retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-18T04:19:12.462+0530] {logging_mixin.py:190} INFO - [2024-12-18T04:19:12.462+0530] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-12-18T04:19:12.489+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 0.162 seconds
[2024-12-18T04:21:53.122+0530] {processor.py:186} INFO - Started process (PID=27042) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-18T04:21:53.124+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-18T04:21:53.125+0530] {logging_mixin.py:190} INFO - [2024-12-18T04:21:53.124+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-18T04:21:53.127+0530] {logging_mixin.py:190} INFO - [2024-12-18T04:21:53.126+0530] {dagbag.py:387} ERROR - Failed to import: /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
Traceback (most recent call last):
  File "/Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/.venv/lib/python3.12/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py", line 1, in <module>
    from extract import *
ModuleNotFoundError: No module named 'extract'
[2024-12-18T04:21:53.128+0530] {processor.py:927} WARNING - No viable dags retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-18T04:21:53.241+0530] {logging_mixin.py:190} INFO - [2024-12-18T04:21:53.241+0530] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-12-18T04:21:53.266+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 0.147 seconds
[2024-12-18T04:24:30.720+0530] {processor.py:186} INFO - Started process (PID=27248) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-18T04:24:30.721+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-18T04:24:30.722+0530] {logging_mixin.py:190} INFO - [2024-12-18T04:24:30.722+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-18T04:24:30.725+0530] {logging_mixin.py:190} INFO - [2024-12-18T04:24:30.723+0530] {dagbag.py:387} ERROR - Failed to import: /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
Traceback (most recent call last):
  File "/Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/.venv/lib/python3.12/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py", line 1, in <module>
    from extract import *
ModuleNotFoundError: No module named 'extract'
[2024-12-18T04:24:30.725+0530] {processor.py:927} WARNING - No viable dags retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-18T04:24:30.841+0530] {logging_mixin.py:190} INFO - [2024-12-18T04:24:30.841+0530] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-12-18T04:24:30.865+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 0.149 seconds
[2024-12-18T04:27:10.490+0530] {processor.py:186} INFO - Started process (PID=27439) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-18T04:27:10.491+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-18T04:27:10.492+0530] {logging_mixin.py:190} INFO - [2024-12-18T04:27:10.492+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-18T04:27:10.495+0530] {logging_mixin.py:190} INFO - [2024-12-18T04:27:10.494+0530] {dagbag.py:387} ERROR - Failed to import: /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
Traceback (most recent call last):
  File "/Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/.venv/lib/python3.12/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py", line 1, in <module>
    from extract import *
ModuleNotFoundError: No module named 'extract'
[2024-12-18T04:27:10.495+0530] {processor.py:927} WARNING - No viable dags retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-18T04:27:10.617+0530] {logging_mixin.py:190} INFO - [2024-12-18T04:27:10.617+0530] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-12-18T04:27:10.641+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 0.155 seconds
[2024-12-18T04:29:49.876+0530] {processor.py:186} INFO - Started process (PID=27628) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-18T04:29:49.878+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-18T04:29:49.879+0530] {logging_mixin.py:190} INFO - [2024-12-18T04:29:49.878+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-18T04:29:49.881+0530] {logging_mixin.py:190} INFO - [2024-12-18T04:29:49.880+0530] {dagbag.py:387} ERROR - Failed to import: /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
Traceback (most recent call last):
  File "/Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/.venv/lib/python3.12/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py", line 1, in <module>
    from extract import *
ModuleNotFoundError: No module named 'extract'
[2024-12-18T04:29:49.882+0530] {processor.py:927} WARNING - No viable dags retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-18T04:29:49.998+0530] {logging_mixin.py:190} INFO - [2024-12-18T04:29:49.997+0530] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-12-18T04:29:50.021+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 0.148 seconds
[2024-12-18T04:32:28.256+0530] {processor.py:186} INFO - Started process (PID=27827) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-18T04:32:28.257+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-18T04:32:28.258+0530] {logging_mixin.py:190} INFO - [2024-12-18T04:32:28.258+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-18T04:32:28.261+0530] {logging_mixin.py:190} INFO - [2024-12-18T04:32:28.260+0530] {dagbag.py:387} ERROR - Failed to import: /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
Traceback (most recent call last):
  File "/Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/.venv/lib/python3.12/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py", line 1, in <module>
    from extract import *
ModuleNotFoundError: No module named 'extract'
[2024-12-18T04:32:28.261+0530] {processor.py:927} WARNING - No viable dags retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-18T04:32:28.381+0530] {logging_mixin.py:190} INFO - [2024-12-18T04:32:28.381+0530] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-12-18T04:32:28.406+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 0.154 seconds
[2024-12-18T04:35:08.261+0530] {processor.py:186} INFO - Started process (PID=28016) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-18T04:35:08.262+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-18T04:35:08.263+0530] {logging_mixin.py:190} INFO - [2024-12-18T04:35:08.263+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-18T04:35:08.266+0530] {logging_mixin.py:190} INFO - [2024-12-18T04:35:08.264+0530] {dagbag.py:387} ERROR - Failed to import: /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
Traceback (most recent call last):
  File "/Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/.venv/lib/python3.12/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py", line 1, in <module>
    from extract import *
ModuleNotFoundError: No module named 'extract'
[2024-12-18T04:35:08.266+0530] {processor.py:927} WARNING - No viable dags retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-18T04:35:08.399+0530] {logging_mixin.py:190} INFO - [2024-12-18T04:35:08.399+0530] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-12-18T04:35:08.423+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 0.166 seconds
[2024-12-18T04:37:50.201+0530] {processor.py:186} INFO - Started process (PID=28205) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-18T04:37:50.203+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-18T04:37:50.204+0530] {logging_mixin.py:190} INFO - [2024-12-18T04:37:50.204+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-18T04:37:50.207+0530] {logging_mixin.py:190} INFO - [2024-12-18T04:37:50.205+0530] {dagbag.py:387} ERROR - Failed to import: /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
Traceback (most recent call last):
  File "/Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/.venv/lib/python3.12/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py", line 1, in <module>
    from extract import *
ModuleNotFoundError: No module named 'extract'
[2024-12-18T04:37:50.207+0530] {processor.py:927} WARNING - No viable dags retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-18T04:37:50.324+0530] {logging_mixin.py:190} INFO - [2024-12-18T04:37:50.323+0530] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-12-18T04:37:50.348+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 0.150 seconds
[2024-12-18T04:40:27.683+0530] {processor.py:186} INFO - Started process (PID=28401) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-18T04:40:27.684+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-18T04:40:27.686+0530] {logging_mixin.py:190} INFO - [2024-12-18T04:40:27.685+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-18T04:40:27.688+0530] {logging_mixin.py:190} INFO - [2024-12-18T04:40:27.687+0530] {dagbag.py:387} ERROR - Failed to import: /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
Traceback (most recent call last):
  File "/Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/.venv/lib/python3.12/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py", line 1, in <module>
    from extract import *
ModuleNotFoundError: No module named 'extract'
[2024-12-18T04:40:27.689+0530] {processor.py:927} WARNING - No viable dags retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-18T04:40:27.803+0530] {logging_mixin.py:190} INFO - [2024-12-18T04:40:27.803+0530] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-12-18T04:40:27.826+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 0.147 seconds
[2024-12-18T04:43:06.124+0530] {processor.py:186} INFO - Started process (PID=28587) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-18T04:43:06.125+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-18T04:43:06.126+0530] {logging_mixin.py:190} INFO - [2024-12-18T04:43:06.126+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-18T04:43:06.129+0530] {logging_mixin.py:190} INFO - [2024-12-18T04:43:06.128+0530] {dagbag.py:387} ERROR - Failed to import: /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
Traceback (most recent call last):
  File "/Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/.venv/lib/python3.12/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py", line 1, in <module>
    from extract import *
ModuleNotFoundError: No module named 'extract'
[2024-12-18T04:43:06.129+0530] {processor.py:927} WARNING - No viable dags retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-18T04:43:06.242+0530] {logging_mixin.py:190} INFO - [2024-12-18T04:43:06.242+0530] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-12-18T04:43:06.267+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 0.146 seconds
[2024-12-18T04:45:43.757+0530] {processor.py:186} INFO - Started process (PID=28776) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-18T04:45:43.759+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-18T04:45:43.760+0530] {logging_mixin.py:190} INFO - [2024-12-18T04:45:43.760+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-18T04:45:43.762+0530] {logging_mixin.py:190} INFO - [2024-12-18T04:45:43.761+0530] {dagbag.py:387} ERROR - Failed to import: /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
Traceback (most recent call last):
  File "/Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/.venv/lib/python3.12/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py", line 1, in <module>
    from extract import *
ModuleNotFoundError: No module named 'extract'
[2024-12-18T04:45:43.763+0530] {processor.py:927} WARNING - No viable dags retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-18T04:45:43.882+0530] {logging_mixin.py:190} INFO - [2024-12-18T04:45:43.881+0530] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-12-18T04:45:43.907+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 0.153 seconds
[2024-12-18T04:48:24.938+0530] {processor.py:186} INFO - Started process (PID=28985) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-18T04:48:24.939+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-18T04:48:24.940+0530] {logging_mixin.py:190} INFO - [2024-12-18T04:48:24.940+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-18T04:48:24.943+0530] {logging_mixin.py:190} INFO - [2024-12-18T04:48:24.941+0530] {dagbag.py:387} ERROR - Failed to import: /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
Traceback (most recent call last):
  File "/Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/.venv/lib/python3.12/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py", line 1, in <module>
    from extract import *
ModuleNotFoundError: No module named 'extract'
[2024-12-18T04:48:24.943+0530] {processor.py:927} WARNING - No viable dags retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-18T04:48:25.057+0530] {logging_mixin.py:190} INFO - [2024-12-18T04:48:25.057+0530] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-12-18T04:48:25.080+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 0.146 seconds
[2024-12-18T04:51:03.202+0530] {processor.py:186} INFO - Started process (PID=29173) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-18T04:51:03.203+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-18T04:51:03.205+0530] {logging_mixin.py:190} INFO - [2024-12-18T04:51:03.204+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-18T04:51:03.207+0530] {logging_mixin.py:190} INFO - [2024-12-18T04:51:03.206+0530] {dagbag.py:387} ERROR - Failed to import: /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
Traceback (most recent call last):
  File "/Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/.venv/lib/python3.12/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py", line 1, in <module>
    from extract import *
ModuleNotFoundError: No module named 'extract'
[2024-12-18T04:51:03.207+0530] {processor.py:927} WARNING - No viable dags retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-18T04:51:03.322+0530] {logging_mixin.py:190} INFO - [2024-12-18T04:51:03.322+0530] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-12-18T04:51:03.346+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 0.147 seconds
[2024-12-18T04:53:43.065+0530] {processor.py:186} INFO - Started process (PID=29367) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-18T04:53:43.066+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-18T04:53:43.067+0530] {logging_mixin.py:190} INFO - [2024-12-18T04:53:43.067+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-18T04:53:43.070+0530] {logging_mixin.py:190} INFO - [2024-12-18T04:53:43.069+0530] {dagbag.py:387} ERROR - Failed to import: /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
Traceback (most recent call last):
  File "/Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/.venv/lib/python3.12/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py", line 1, in <module>
    from extract import *
ModuleNotFoundError: No module named 'extract'
[2024-12-18T04:53:43.070+0530] {processor.py:927} WARNING - No viable dags retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-18T04:53:43.187+0530] {logging_mixin.py:190} INFO - [2024-12-18T04:53:43.187+0530] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-12-18T04:53:43.210+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 0.149 seconds
[2024-12-18T04:56:23.817+0530] {processor.py:186} INFO - Started process (PID=29583) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-18T04:56:23.818+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-18T04:56:23.819+0530] {logging_mixin.py:190} INFO - [2024-12-18T04:56:23.819+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-18T04:56:23.822+0530] {logging_mixin.py:190} INFO - [2024-12-18T04:56:23.820+0530] {dagbag.py:387} ERROR - Failed to import: /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
Traceback (most recent call last):
  File "/Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/.venv/lib/python3.12/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py", line 1, in <module>
    from extract import *
ModuleNotFoundError: No module named 'extract'
[2024-12-18T04:56:23.822+0530] {processor.py:927} WARNING - No viable dags retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-18T04:56:23.939+0530] {logging_mixin.py:190} INFO - [2024-12-18T04:56:23.939+0530] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-12-18T04:56:23.962+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 0.148 seconds
[2024-12-18T04:58:59.943+0530] {processor.py:186} INFO - Started process (PID=29765) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-18T04:58:59.944+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-18T04:58:59.945+0530] {logging_mixin.py:190} INFO - [2024-12-18T04:58:59.945+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-18T04:58:59.948+0530] {logging_mixin.py:190} INFO - [2024-12-18T04:58:59.947+0530] {dagbag.py:387} ERROR - Failed to import: /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
Traceback (most recent call last):
  File "/Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/.venv/lib/python3.12/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py", line 1, in <module>
    from extract import *
ModuleNotFoundError: No module named 'extract'
[2024-12-18T04:58:59.949+0530] {processor.py:927} WARNING - No viable dags retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-18T04:59:00.067+0530] {logging_mixin.py:190} INFO - [2024-12-18T04:59:00.067+0530] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-12-18T04:59:00.090+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 0.151 seconds
[2024-12-18T05:01:42.804+0530] {processor.py:186} INFO - Started process (PID=29956) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-18T05:01:42.807+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-18T05:01:42.808+0530] {logging_mixin.py:190} INFO - [2024-12-18T05:01:42.808+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-18T05:01:42.811+0530] {logging_mixin.py:190} INFO - [2024-12-18T05:01:42.810+0530] {dagbag.py:387} ERROR - Failed to import: /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
Traceback (most recent call last):
  File "/Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/.venv/lib/python3.12/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py", line 1, in <module>
    from extract import *
ModuleNotFoundError: No module named 'extract'
[2024-12-18T05:01:42.811+0530] {processor.py:927} WARNING - No viable dags retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-18T05:01:42.926+0530] {logging_mixin.py:190} INFO - [2024-12-18T05:01:42.925+0530] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-12-18T05:01:42.948+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 0.147 seconds
[2024-12-18T05:04:21.338+0530] {processor.py:186} INFO - Started process (PID=30152) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-18T05:04:21.340+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-18T05:04:21.341+0530] {logging_mixin.py:190} INFO - [2024-12-18T05:04:21.341+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-18T05:04:21.344+0530] {logging_mixin.py:190} INFO - [2024-12-18T05:04:21.342+0530] {dagbag.py:387} ERROR - Failed to import: /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
Traceback (most recent call last):
  File "/Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/.venv/lib/python3.12/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py", line 1, in <module>
    from extract import *
ModuleNotFoundError: No module named 'extract'
[2024-12-18T05:04:21.344+0530] {processor.py:927} WARNING - No viable dags retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-18T05:04:21.463+0530] {logging_mixin.py:190} INFO - [2024-12-18T05:04:21.463+0530] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-12-18T05:04:21.486+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 0.152 seconds
[2024-12-18T05:06:54.326+0530] {processor.py:186} INFO - Started process (PID=30335) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-18T05:06:54.327+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-18T05:06:54.329+0530] {logging_mixin.py:190} INFO - [2024-12-18T05:06:54.328+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-18T05:06:54.331+0530] {logging_mixin.py:190} INFO - [2024-12-18T05:06:54.330+0530] {dagbag.py:387} ERROR - Failed to import: /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
Traceback (most recent call last):
  File "/Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/.venv/lib/python3.12/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py", line 1, in <module>
    from extract import *
ModuleNotFoundError: No module named 'extract'
[2024-12-18T05:06:54.332+0530] {processor.py:927} WARNING - No viable dags retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-18T05:06:54.449+0530] {logging_mixin.py:190} INFO - [2024-12-18T05:06:54.449+0530] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-12-18T05:06:54.472+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 0.149 seconds
[2024-12-18T05:09:29.833+0530] {processor.py:186} INFO - Started process (PID=30519) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-18T05:09:29.834+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-18T05:09:29.836+0530] {logging_mixin.py:190} INFO - [2024-12-18T05:09:29.835+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-18T05:09:29.838+0530] {logging_mixin.py:190} INFO - [2024-12-18T05:09:29.837+0530] {dagbag.py:387} ERROR - Failed to import: /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
Traceback (most recent call last):
  File "/Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/.venv/lib/python3.12/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py", line 1, in <module>
    from extract import *
ModuleNotFoundError: No module named 'extract'
[2024-12-18T05:09:29.838+0530] {processor.py:927} WARNING - No viable dags retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-18T05:09:29.959+0530] {logging_mixin.py:190} INFO - [2024-12-18T05:09:29.959+0530] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-12-18T05:09:30.013+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 0.184 seconds
[2024-12-18T05:12:02.564+0530] {processor.py:186} INFO - Started process (PID=30701) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-18T05:12:02.565+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-18T05:12:02.566+0530] {logging_mixin.py:190} INFO - [2024-12-18T05:12:02.566+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-18T05:12:02.569+0530] {logging_mixin.py:190} INFO - [2024-12-18T05:12:02.568+0530] {dagbag.py:387} ERROR - Failed to import: /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
Traceback (most recent call last):
  File "/Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/.venv/lib/python3.12/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py", line 1, in <module>
    from extract import *
ModuleNotFoundError: No module named 'extract'
[2024-12-18T05:12:02.569+0530] {processor.py:927} WARNING - No viable dags retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-18T05:12:02.689+0530] {logging_mixin.py:190} INFO - [2024-12-18T05:12:02.689+0530] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-12-18T05:12:02.712+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 0.152 seconds
[2024-12-18T05:14:37.281+0530] {processor.py:186} INFO - Started process (PID=30895) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-18T05:14:37.282+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-18T05:14:37.283+0530] {logging_mixin.py:190} INFO - [2024-12-18T05:14:37.283+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-18T05:14:37.286+0530] {logging_mixin.py:190} INFO - [2024-12-18T05:14:37.285+0530] {dagbag.py:387} ERROR - Failed to import: /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
Traceback (most recent call last):
  File "/Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/.venv/lib/python3.12/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py", line 1, in <module>
    from extract import *
ModuleNotFoundError: No module named 'extract'
[2024-12-18T05:14:37.286+0530] {processor.py:927} WARNING - No viable dags retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-18T05:14:37.404+0530] {logging_mixin.py:190} INFO - [2024-12-18T05:14:37.404+0530] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-12-18T05:14:37.428+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 0.150 seconds
[2024-12-18T05:17:14.742+0530] {processor.py:186} INFO - Started process (PID=31082) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-18T05:17:14.743+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-18T05:17:14.745+0530] {logging_mixin.py:190} INFO - [2024-12-18T05:17:14.744+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-18T05:17:14.747+0530] {logging_mixin.py:190} INFO - [2024-12-18T05:17:14.746+0530] {dagbag.py:387} ERROR - Failed to import: /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
Traceback (most recent call last):
  File "/Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/.venv/lib/python3.12/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py", line 1, in <module>
    from extract import *
ModuleNotFoundError: No module named 'extract'
[2024-12-18T05:17:14.747+0530] {processor.py:927} WARNING - No viable dags retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-18T05:17:14.861+0530] {logging_mixin.py:190} INFO - [2024-12-18T05:17:14.861+0530] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-12-18T05:17:14.885+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 0.146 seconds
[2024-12-18T05:19:53.203+0530] {processor.py:186} INFO - Started process (PID=31265) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-18T05:19:53.204+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-18T05:19:53.205+0530] {logging_mixin.py:190} INFO - [2024-12-18T05:19:53.205+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-18T05:19:53.209+0530] {logging_mixin.py:190} INFO - [2024-12-18T05:19:53.207+0530] {dagbag.py:387} ERROR - Failed to import: /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
Traceback (most recent call last):
  File "/Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/.venv/lib/python3.12/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py", line 1, in <module>
    from extract import *
ModuleNotFoundError: No module named 'extract'
[2024-12-18T05:19:53.209+0530] {processor.py:927} WARNING - No viable dags retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-18T05:19:53.329+0530] {logging_mixin.py:190} INFO - [2024-12-18T05:19:53.328+0530] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-12-18T05:19:53.354+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 0.155 seconds
[2024-12-18T05:22:31.643+0530] {processor.py:186} INFO - Started process (PID=31473) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-18T05:22:31.646+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-18T05:22:31.647+0530] {logging_mixin.py:190} INFO - [2024-12-18T05:22:31.647+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-18T05:22:31.650+0530] {logging_mixin.py:190} INFO - [2024-12-18T05:22:31.649+0530] {dagbag.py:387} ERROR - Failed to import: /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
Traceback (most recent call last):
  File "/Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/.venv/lib/python3.12/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py", line 1, in <module>
    from extract import *
ModuleNotFoundError: No module named 'extract'
[2024-12-18T05:22:31.650+0530] {processor.py:927} WARNING - No viable dags retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-18T05:22:31.769+0530] {logging_mixin.py:190} INFO - [2024-12-18T05:22:31.769+0530] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-12-18T05:22:31.796+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 0.157 seconds
[2024-12-18T05:25:04.266+0530] {processor.py:186} INFO - Started process (PID=31656) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-18T05:25:04.267+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-18T05:25:04.268+0530] {logging_mixin.py:190} INFO - [2024-12-18T05:25:04.268+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-18T05:25:04.271+0530] {logging_mixin.py:190} INFO - [2024-12-18T05:25:04.270+0530] {dagbag.py:387} ERROR - Failed to import: /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
Traceback (most recent call last):
  File "/Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/.venv/lib/python3.12/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py", line 1, in <module>
    from extract import *
ModuleNotFoundError: No module named 'extract'
[2024-12-18T05:25:04.272+0530] {processor.py:927} WARNING - No viable dags retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-18T05:25:04.390+0530] {logging_mixin.py:190} INFO - [2024-12-18T05:25:04.390+0530] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-12-18T05:25:04.418+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 0.155 seconds
[2024-12-18T05:27:42.446+0530] {processor.py:186} INFO - Started process (PID=31840) to work on /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-18T05:27:42.447+0530] {processor.py:914} INFO - Processing file /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py for tasks to queue
[2024-12-18T05:27:42.448+0530] {logging_mixin.py:190} INFO - [2024-12-18T05:27:42.448+0530] {dagbag.py:588} INFO - Filling up the DagBag from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-18T05:27:42.451+0530] {logging_mixin.py:190} INFO - [2024-12-18T05:27:42.449+0530] {dagbag.py:387} ERROR - Failed to import: /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
Traceback (most recent call last):
  File "/Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/.venv/lib/python3.12/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py", line 1, in <module>
    from extract import *
ModuleNotFoundError: No module named 'extract'
[2024-12-18T05:27:42.451+0530] {processor.py:927} WARNING - No viable dags retrieved from /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py
[2024-12-18T05:27:42.566+0530] {logging_mixin.py:190} INFO - [2024-12-18T05:27:42.566+0530] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-12-18T05:27:42.592+0530] {processor.py:208} INFO - Processing /Users/jayesh/Desktop/Personal Work/Movie_data_pipeline/src/Airflow/dags/pipeline_dag.py took 0.150 seconds
