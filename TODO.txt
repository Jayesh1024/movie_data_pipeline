load data to s3
run databricks notebook using spark
test the integrations of s3,databricks and rds using airflow


integrate spark workflow in airflow dag
Error handling in parse.py and extract.py
email when any task fails.
Orchestrate sql workflow in airflow dag
track and setup monitoring and alerting for data quality metrics.
deploy to cloud in a single ec2 instance (no distributed environment needed, keep it simple)